
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://example.com/3_PREDICTIVE_ANALYTICS/0_ASA_SRM/7.%20Tree%20Models/">
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-8.5.10">
    
    
      
        <title>Tree Models - Exam Notes</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.975780f9.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.2505c338.min.css">
        
          
          
          <meta name="theme-color" content="#000000">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../config/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#tree-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Exam Notes" class="md-header__button md-logo" aria-label="Exam Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Exam Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Tree Models
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Exam Notes" class="md-nav__button md-logo" aria-label="Exam Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Exam Notes
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_1">
          Introductory
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Introductory" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          Introductory
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1_INTRODUCTORY/0.%20Review%20of%20Mathematics/" class="md-nav__link">
        Review of Mathematics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Placeholder/" class="md-nav__link">
        ASA P
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Placeholder/" class="md-nav__link">
        ASA FM
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1_4" type="checkbox" id="__nav_1_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_1_4">
          ASA IFM
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ASA IFM" data-md-level="2">
        <label class="md-nav__title" for="__nav_1_4">
          <span class="md-nav__icon md-icon"></span>
          ASA IFM
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1_4_1" type="checkbox" id="__nav_1_4_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_1_4_1">
          Derivatives
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Derivatives" data-md-level="3">
        <label class="md-nav__title" for="__nav_1_4_1">
          <span class="md-nav__icon md-icon"></span>
          Derivatives
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1_INTRODUCTORY/2_ASA_IFM/1.%20Derivatives/1.%20Introduction/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1_INTRODUCTORY/2_ASA_IFM/1.%20Derivatives/3.%20Options/" class="md-nav__link">
        Options
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1_INTRODUCTORY/2_ASA_IFM/1.%20Derivatives/5.%20Binomial%20Model/" class="md-nav__link">
        Binomial Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1_INTRODUCTORY/2_ASA_IFM/1.%20Derivatives/6.%20Black%20Scholes%20Model/" class="md-nav__link">
        Black Scholes Model
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1_4_2" type="checkbox" id="__nav_1_4_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_1_4_2">
          Corporate Finance
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Corporate Finance" data-md-level="3">
        <label class="md-nav__title" for="__nav_1_4_2">
          <span class="md-nav__icon md-icon"></span>
          Corporate Finance
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../1_INTRODUCTORY/2_ASA_IFM/3.%20Corporate%20Finance/3.%20Risk%20Measures/" class="md-nav__link">
        Risk Measures
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          Actuarial Mathematics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Actuarial Mathematics" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Actuarial Mathematics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_1" type="checkbox" id="__nav_2_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_1">
          ASA FAMS
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ASA FAMS" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          ASA FAMS
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/0_ASA_FAMS/0.%20Short%20Term%20Insurance/" class="md-nav__link">
        Short Term Insurance
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/0_ASA_FAMS/1.%20Review%20of%20Probability%20Theory/" class="md-nav__link">
        Review of Probability Theory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/0_ASA_FAMS/2.%20Frequency%20Models/" class="md-nav__link">
        Frequency Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/0_ASA_FAMS/3.%20Severity%20Models/" class="md-nav__link">
        Severity Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/0_ASA_FAMS/4.%20Policy%20Modifications/" class="md-nav__link">
        Policy Modifications
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/0_ASA_FAMS/5.%20Aggregate%20Models/" class="md-nav__link">
        Aggregate Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/0_ASA_FAMS/6.%20Loss%20Reserving/" class="md-nav__link">
        Loss Reserving
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/0_ASA_FAMS/7.%20Ratemaking/" class="md-nav__link">
        Ratemaking
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/0_ASA_FAMS/8.%20Model%20Estimation/" class="md-nav__link">
        Model Estimation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/0_ASA_FAMS/9.%20Credibility%20Theory/" class="md-nav__link">
        Credibility Theory
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2" type="checkbox" id="__nav_2_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_2">
          ASA FAML
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ASA FAML" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          ASA FAML
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/1_ASA_FAML/0.%20Long%20Term%20Insurance/" class="md-nav__link">
        Long Term Insurance
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/1_ASA_FAML/1.%20Survival%20Models/" class="md-nav__link">
        Survival Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/1_ASA_FAML/2.%20Life%20Tables/" class="md-nav__link">
        Life Tables
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/1_ASA_FAML/3.%20Life%20Assurances/" class="md-nav__link">
        Life Assurances
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/1_ASA_FAML/4.%20Life%20Annuities/" class="md-nav__link">
        Life Annuities
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/1_ASA_FAML/5.%20Variable%20Benefits/" class="md-nav__link">
        Variable Benefits
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/1_ASA_FAML/6.%20Premiums/" class="md-nav__link">
        Premiums
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/1_ASA_FAML/7.%20Reserves/" class="md-nav__link">
        Reserves
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/1_ASA_FAML/8.%20Model%20Estimation/" class="md-nav__link">
        Model Estimation
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_3" type="checkbox" id="__nav_2_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_3">
          ASA ALTAM
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ASA ALTAM" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_3">
          <span class="md-nav__icon md-icon"></span>
          ASA ALTAM
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/2_ASA_ALTAM/1.%20Multi%20Life%20Models/" class="md-nav__link">
        Multi Life Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/2_ASA_ALTAM/2.%20Multi%20Decrement%20Models/" class="md-nav__link">
        Multi Decrement Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/2_ASA_ALTAM/3.%20Multi%20State%20Models/" class="md-nav__link">
        Multi State Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/2_ASA_ALTAM/4.%20Multi%20State%20Applications/" class="md-nav__link">
        Multi State Applications
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/2_ASA_ALTAM/5.%20Profit%20Testing/" class="md-nav__link">
        Profit Testing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/2_ASA_ALTAM/6.%20Universal%20Life/" class="md-nav__link">
        Universal Life
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/2_ASA_ALTAM/7.%20Embedded%20Options/" class="md-nav__link">
        Embedded Options
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../2_ACTUARIAL_MATHEMATICS/2_ASA_ALTAM/8.%20Pensions/" class="md-nav__link">
        Pensions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Predictive Analytics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Predictive Analytics" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Predictive Analytics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_1" type="checkbox" id="__nav_3_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3_1">
          ASA SRM
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ASA SRM" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_1">
          <span class="md-nav__icon md-icon"></span>
          ASA SRM
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../0.%20Review%20of%20Statistical%20Theory/" class="md-nav__link">
        Review of Statistical Theory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../1.%20Statistical%20Learning/" class="md-nav__link">
        Statistical Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../2.%20Simple%20Linear%20Regression/" class="md-nav__link">
        Simple Linear Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../3.%20Multiple%20Linear%20Regression/" class="md-nav__link">
        Multiple Linear Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../4.%20Linear%20Regression%20Assumptions/" class="md-nav__link">
        Linear Regression Assumptions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../5.%20Model%20Selection/" class="md-nav__link">
        Model Selection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../6.%20Generalized%20Linear%20Models/" class="md-nav__link">
        Generalized Linear Models
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Tree Models
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Tree Models
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    Overview
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#construction" class="md-nav__link">
    Construction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pruning" class="md-nav__link">
    Pruning
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#classification-trees" class="md-nav__link">
    Classification Trees
  </a>
  
    <nav class="md-nav" aria-label="Classification Trees">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#purity-measures" class="md-nav__link">
    Purity Measures
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#two-category-model" class="md-nav__link">
    Two Category Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#single-tree-analysis" class="md-nav__link">
    Single Tree Analysis
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ensemble-methods" class="md-nav__link">
    Ensemble Methods
  </a>
  
    <nav class="md-nav" aria-label="Ensemble Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bagging" class="md-nav__link">
    Bagging
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#random-forest" class="md-nav__link">
    Random Forest
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#boosting" class="md-nav__link">
    Boosting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#comparison" class="md-nav__link">
    Comparison
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluation" class="md-nav__link">
    Evaluation
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../8.%20Principal%20Components/" class="md-nav__link">
        Principal Components
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../9.%20Clustering/" class="md-nav__link">
        Clustering
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../10.%20Time%20Series/" class="md-nav__link">
        Time Series
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_2" type="checkbox" id="__nav_3_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3_2">
          ASA PA
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ASA PA" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          ASA PA
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1_ASA_PA/0_OVERVIEW/" class="md-nav__link">
        Overview
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    Overview
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#construction" class="md-nav__link">
    Construction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pruning" class="md-nav__link">
    Pruning
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#classification-trees" class="md-nav__link">
    Classification Trees
  </a>
  
    <nav class="md-nav" aria-label="Classification Trees">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#purity-measures" class="md-nav__link">
    Purity Measures
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#two-category-model" class="md-nav__link">
    Two Category Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#single-tree-analysis" class="md-nav__link">
    Single Tree Analysis
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ensemble-methods" class="md-nav__link">
    Ensemble Methods
  </a>
  
    <nav class="md-nav" aria-label="Ensemble Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bagging" class="md-nav__link">
    Bagging
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#random-forest" class="md-nav__link">
    Random Forest
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#boosting" class="md-nav__link">
    Boosting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#comparison" class="md-nav__link">
    Comparison
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluation" class="md-nav__link">
    Evaluation
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="tree-models"><strong>Tree Models</strong><a class="headerlink" href="#tree-models" title="Permanent link">&para;</a></h1>
<h2 id="overview"><strong>Overview</strong><a class="headerlink" href="#overview" title="Permanent link">&para;</a></h2>
<p>Tree Models are the first <strong>supervised AND non-parametric</strong> statistical learning method covered. As such, the model is mainly <strong>algorithmic</strong> in nature.</p>
<p>Broadly speaking, tree models <strong>sequentially partition the sample</strong> into several <strong>distinct regions</strong> based on the values of independent variables. For a prediction, the model determines which region the prediction falls in and sets the prediction equal to the <strong>average of the observations within that region</strong>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>This allows tree models to be more resilient to outliers as they can simply <strong>partition the outliers away</strong>, leaving the main bulk of the distribution unaffected.</p>
</div>
<p>Using the <strong>boundaries</strong> of the partitions, a <strong>tree-like structure</strong> can be formed to easily visualize the model. This is highly effective for datasets with many independent variables as the sample space itself is not easily visualized.</p>
<!-- Obtained from Research Gate -->
<p><img alt="TREE_PARTITION" class="center" src="../Assets/7.%20Tree%20Models.md/TREE_PARTITION.png" /></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Only “vertical” and “horizontal” lines (lines <strong>parallel to the axes</strong> in higher dimensions) can be used to partition the observation space.</p>
<p>When the lines intersect, each region is considered to be unique, even if the split does not change the prediction - regions are <strong>NOT combined</strong>. Graphically, this means that the predictor space must have always lines that touch both ends of the boundaries.</p>
<p><!-- Self Made -->
<img alt="REGION_SPLIT" src="../Assets/7.%20Tree%20Models.md/REGION_SPLIT.png" /></p>
<p>The above points are mainly applicable for questions which ask to identify <strong>valid predictor spaces</strong>, typically when there are only two predictors. If however, the question is asking for predictor values, then the image on the RHS is valid.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The predictor space is split using the <strong>Independent Variable</strong> as the boundaries, NOT the dependent variable.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Take note of the following "quirks":</p>
<ol>
<li>Not all features will be used to build the tree</li>
<li>The same features can be used more than once in the tree</li>
<li>Relatively <strong>more important</strong> features will be used for splits <strong>first</strong></li>
</ol>
<p>On point (3), the first predictor used in the split is the <strong>MOST important</strong> because it has the largest reduction in RSS.</p>
</div>
<p>Given the iterative nature of the tree, it <strong>automatically accounts for any possible interaction</strong> between variables. There is no need to create an interaction variable - in fact, it could lead to worse results as manually creating an interaction “locks” in the interaction, while a tree can <strong>find the best possible interaction</strong>.</p>
<p>The different points along the tree are known as <strong>Nodes</strong>:</p>
<ul>
<li><strong>Internal Nodes</strong> (Parent Nodes): Nodes where the tree <strong>splits further</strong></li>
<li><strong>Terminal Nodes</strong> (Child Nodes): Nodes where the tree <strong>no longer splits</strong></li>
</ul>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>They can also be interpreted using an actual tree analogy - Roots, Branches &amp; Leaves. In this case, the structure is more of an inverted tree.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>For the purposes of this exam, assume that <strong>TRUE is on the left</strong> while FALSE is on the right. In practice, either is fine as long as it is clearly defined.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Internal Nodes are also sometimes referred to as <strong>Split</strong>, given that the tree splits at that point. They are also commonly denoted using <span class="arithmatex">\(d\)</span>.</p>
<p>For a tree with <span class="arithmatex">\(n\)</span> internal nodes, it will always have <span class="arithmatex">\(n+1\)</span> terminal nodes:</p>
<div class="arithmatex">\[
    T = d + 1
\]</div>
</div>
<!-- Self Made -->
<p><img alt="TREE_TERMINOLOGY" class="center" src="../Assets/7.%20Tree%20Models.md/TREE_TERMINOLOGY.png" /></p>
<p>Mathematically, each region of the tree can also be expressed using a <strong>series of conditions</strong>. For large trees, this becomes tedious, which is why the diagram is generally preferred:</p>
<div class="arithmatex">\[
\begin{aligned}
    R_{1} &amp;= {y \mid x_{1} \lt a, x_{2} \lt b, x_{3} \lt c, \dots} \\
    R_{2} &amp;= {y \mid x_{1} \lt a, x_{2} \lt b, x_{3} \gt c, \dots} \\
    &amp;\dots
\end{aligned}
\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A tree with only a <strong>single internal node</strong> (only one split) is known as a <strong>Stump</strong>. It is a tree with only <strong>one predictor</strong>:</p>
<p><!-- Self Made -->
<img alt="TREE_STUMP" class="center" src="../Assets/7.%20Tree%20Models.md/TREE_STUMP.png" /></p>
</div>
<h2 id="construction"><strong>Construction</strong><a class="headerlink" href="#construction" title="Permanent link">&para;</a></h2>
<p>For the purposes of this exam, there is only one algorithm that is used to construct trees, <strong>Recursive Binary Splitting</strong>:</p>
<ol>
<li>Start with all observations in one region (the entire sample space)</li>
<li>Consider <strong>all possible splits</strong> for ALL predictors (Regardless if previously used)</li>
<li>For each possibility, compute the <strong>total RSS</strong> from the resulting two regions</li>
<li>Select the split with the <strong>lowest total RSS</strong></li>
<li>Repeat steps (2) - (4) for each resulting region</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The method gets its name from the following properties:</p>
<ul>
<li><strong>Binary</strong>: One region is always split into two; no more</li>
<li><strong>Recursive</strong>: Splits rely on previously made splits</li>
</ul>
<p>The possible split boundaries are not arbitrarily chosen. They are always the <strong>midpoint of between two observations</strong>:</p>
<ul>
<li><span class="arithmatex">\((X_{1}, X_{2}) = {(1,6), (2, 7), (3, 8), \dots}\)</span></li>
<li>Split <span class="arithmatex">\(X_{1}\)</span> at the following: <span class="arithmatex">\({1.5, 2.5, \dots}\)</span></li>
<li>Split <span class="arithmatex">\(X_{2}\)</span> at the following: <span class="arithmatex">\({6.5, 7.5, \dots}\)</span></li>
</ul>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If a categorical variable is being used for the split, the tree can split <strong>ANY combination</strong> of the categories:</p>
<ul>
<li>Group 1: Category 1 &amp; 2</li>
<li>Group 2: Category 3, 4 &amp; 5</li>
</ul>
<p>However, if the categorical variable has been converted into dummy variables, then the split can only be for one category at a time:</p>
<ul>
<li>Group 1: Category 1</li>
<li>Group 2: Category 2, 3, 4 &amp; 5</li>
</ul>
<p>Thus, it is generally <strong>NOT necessary to binarize</strong> variables beforehand when working with decision trees, which makes decision trees preferred when working with categorical variables.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Only useful variables will be used in the tree - it is possible that a variable is NOT used at all.</p>
<p>If two or more USEFUL variables are highly collinear, it is possible that only one of those variables will be used, given the great similarity between the two.</p>
</div>
<p>In general, the algorithm can be said to choose the <strong>split that minimizes the RSS AT THAT TIME</strong>; it does NOT consider future splits. Similar to subset selection, it is a <strong>greedy algorithm</strong> as it chooses the local best choice at each step, not the global best.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Notice that since the <strong>prediction is the average</strong> of the observations in the node, the RSS is essentially the <strong>mean square variance</strong> of the observations in the node. If given the observations or the relevant summed components, the RSS can be easily calculated.</p>
<p>The total RSS is simply the sum of the RSS for each node, <strong>no weighting</strong> required.</p>
<div class="arithmatex">\[
    \text{Total Tree RSS} = \sum \text{Node RSS}
\]</div>
</div>
<p>The performance of the tree is often compared against the Null Rate - performance of a tree with <strong>NO splits</strong>. The idea is similar to that of an F-test.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Competitor Splits</strong> are the next best alternative split(s) among the variables that were NOT chosen.</p>
<p><strong>Surrogate Splits</strong> are alternative splits that results in a similar division as the chosen split. It is best used when there are missing values in the chosen split, thus the surrogate split can be used to achieve the same effect.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The above methodology only works for regression trees; continuous data. For count data, the RSS should not be used. Instead, the split decision should be based on <strong>Log-Likelihood</strong> instead; Poisson Deviance. The resulting tree is thus known as a <strong>Poisson Tree</strong>.</p>
</div>
<h2 id="pruning"><strong>Pruning</strong><a class="headerlink" href="#pruning" title="Permanent link">&para;</a></h2>
<p>Recursive binary splitting tends to result in <strong>large complex trees</strong> with many internal nodes, which is prone to <strong>overfitting</strong>. In this case, certain <strong>internal nodes should be dropped</strong>, akin to <strong>Pruning the leaves</strong> of a tree.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Recall that the complexity of a linear regression model was proportional to the <strong>number of predictors</strong> it had. In a similar fashion, the <strong>number of internal nodes</strong> is proportional to its complexity.</p>
</div>
<p>The first type of pruning is known as <strong>Pre-pruning</strong>. The algorithm includes a <strong>stopping condition</strong> which <strong>prevents the tree from becoming too complex</strong> in the first place.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The conditions are typically chosen such that they <strong>avoid overfitting</strong> (smaller tree):</p>
<ul>
<li>Minimum number of observations in a node (internal vs terminal node; can be different)</li>
<li>Maximum number of terminal nodes</li>
</ul>
<p>Note that if set too harsh, it could result in a tree with <strong>no split</strong> (one node only). This could also occur if none of the features have any predictive power for the target (though this is unlikely).</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This type of pruning could <strong>already have been baked into the training algorithm</strong>, which is why it is not what people think of when pruning in is mentioned.</p>
</div>
<!-- Obtained from Research Gate -->
<p><img alt="PRE_PRUNING" class="center" src="../Assets/7.%20Tree%20Models.md/PRE_PRUNING.png" /></p>
<p>The second type of pruning is known as <strong>Post-pruning</strong>. A <strong>penalty</strong> is added to the RSS:</p>
<ul>
<li><span class="arithmatex">\(\alpha\)</span>: Complexity Parameter (Hyperparameter)</li>
<li>T: Number of terminal nodes (Leaves)</li>
</ul>
<div class="arithmatex">\[
    \min (\text{RSS} + \alpha \cdot T)
\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Similar to Ridge/Lasso, the RSS in the above is the POST-pruning RSS. This is important to distinguish as questions can provide the values in a format like this:</p>
<ol>
<li>Terminal Nodes - RSS for that node</li>
<li>Internal Nodes - RSS for that node, IF the leaves were pruned at that point</li>
</ol>
<p><!-- Obtained from Coaching Actuaries -->
<img alt="PRUNING_RSS" class="center" src="../Assets/7.%20Tree%20Models.md/PRUNING_RSS.png" /></p>
<p>As stated previously, the RSS of the tree at any point is the sum of all terminal RSS. However, if given the above, it is important to sum the correct ones (while also applying the correct amount of penalty).</p>
</div>
<p>For every possible <span class="arithmatex">\(T\)</span>, there exists a <strong>subtree that minimizes</strong> the above function. Among the above shortlisted subtrees, the one with the <strong>lowest penalized RSS</strong> is chosen as the final tree. This is why this method is known as <strong>Cost Complexity Pruning</strong>, as it adds a cost per terminal node (cost per complexity) to the minimization problem. As part of this process, splits that <strong>do not provide enough reduction in RSS</strong> (weak splits) will be pruned, which is why it is also known as <strong>Weakest Link Pruning</strong>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Following the second definition, the complexity parameter is thus also described as the minimum threshold of improvement for additional splits in the tree.</p>
</div>
<p>The key idea is that <span class="arithmatex">\(\alpha\)</span> and <span class="arithmatex">\(T\)</span> are <strong>inversely related</strong>. The higher the penalty, the smaller the tree must be; hence different <span class="arithmatex">\(\alpha\)</span> will result in a <strong>different final tree</strong>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The ideal scenario for pruning would be that <em>every possible subtree</em> is tested via Cross Validation. However, this is <strong>computationally expensive</strong>, which is why Cost Complexity pruning is used instead - which will always provide a <strong>sequence of subtrees</strong> with varying levels of <span class="arithmatex">\(\alpha\)</span>:</p>
<p><!-- Obtained from Coaching Actuaries -->
<img alt="ALPHA_INDEX" class="center" src="../Assets/7.%20Tree%20Models.md/ALPHA_INDEX.png" /></p>
<p>As seen above, since <span class="arithmatex">\(T\)</span> is an integer while <span class="arithmatex">\(\alpha\)</span>  is continuous, it leads to a <strong>step like function</strong> where a range of <span class="arithmatex">\(\alpha\)</span> results in the same subtree:</p>
<p><!-- Obtained from Coaching Actuaries -->
<img alt="ALPHA_RANGE" class="center" src="../Assets/7.%20Tree%20Models.md/ALPHA_RANGE.png" /></p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When <span class="arithmatex">\(\alpha=0\)</span>, the penalized RSS is the <strong>same as the regular RSS</strong> and hence will result in the original tree. Thus, the trees that minimizes the penalized RSS will <strong>always be a subtree</strong> of the original.</p>
<p>Intuitively, this is because the tree was fitted using a <strong>forward greedy algorithm</strong> to begin with. Thus, going backwards should result in the same tree.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Note that converting a subtree into a leaf is NOT simply taking the average of the two original leaves, be it for the predicted value or its RSS.</p>
</div>
<p>One common consideration is pruning a tree versus growing a new tree with the same complexity parameter from the get go. Generally speaking, it is better to allow the tree to <strong>grow fully first to allow valuable splits to occur</strong>, that might not have otherwise occurred. In the worse scenario, stunting the growth of the tree early might <strong>reduce a small amount of RSS early but result in an even larger RSS later on</strong>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The complexity parameter (<span class="arithmatex">\(\alpha\)</span>) and the other pre-pruning conditions are all the <strong>Hyperparameters</strong> of a decision tree. Given that there are multiple, they need to be <strong>tuned together</strong> to achieve the best results.</p>
</div>
<p>Recall that the main point of using a decision tree is that it can be illustrated with a <strong>simple visual</strong>. If the tree is too large, the visual becomes <strong>too complicated</strong>, undercutting the appeal of the tree. For this reason, pruning is important. However, if cross validation suggests that the full <strong>unpruned tree is best</strong>, then <strong>decision trees might not work well</strong> with the data.</p>
<p>An example of the above would be when working with a dataset where a <strong>continuous numeric variable</strong> is a strong predictor. The tree would need to make <strong>many splits each with marginal improvements</strong> to represent the continuous relationship, resulting in a large tree. In such cases, it would be much <strong>better to use linear regression</strong> which can inherently better capture the relationship.</p>
<h2 id="classification-trees"><strong>Classification Trees</strong><a class="headerlink" href="#classification-trees" title="Permanent link">&para;</a></h2>
<p>Decision Trees can also be used for <strong>categorical predictors</strong>, in which case it is known as a <strong>Classification Tree</strong>. The model assigns a category to the prediction based on the <strong>most common</strong> category within that region of the tree.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Most of the key concepts apply, but there are some notable exceptions due to having a categorical dependent variable.</p>
</div>
<p>Since residuals are unintuitive for categorical data, classification trees focus on <strong>Node Purity</strong> instead. Purity is a measure of the <strong>distribution/variance of the categories</strong>, which can measured using one of the following metrics:</p>
<ol>
<li>Classification Error Rate</li>
<li>Gini Index</li>
<li>Cross Entropy</li>
</ol>
<p>Thus, it can be said that the algorithm divides the predictor space such that <strong>node impurity is minimized</strong>; <strong>variability</strong> among categories is minimized. Phrased another way, the tree chooses the split that maximizes the <strong>information gain</strong>:</p>
<ul>
<li><strong>Classification Trees</strong>: Defined as reduction in Impurity Measure</li>
<li><strong>Regression Trees</strong>: Defined as reduction in RSS</li>
<li>Note that this is a <strong>double negative</strong>; the underlying metrics are already negative</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The lower the above metrics, the higher the purity of the node - the node is LARGELY made up of mainly one category. Mathematically, the <strong>proportion of categories</strong> within each region are <strong>close to 0 or 1</strong>.</p>
<p>This can be remembered by the fact that the Gini Index is often a measure of <strong>socio-economic inequality</strong> - it is higher when there is are a large number of poor and rich (loosely speaking).</p>
</div>
<h3 id="purity-measures"><strong>Purity Measures</strong><a class="headerlink" href="#purity-measures" title="Permanent link">&para;</a></h3>
<p>A key quantity that will be used is the <strong>proportion</strong> of each class in a particular region (<span class="arithmatex">\(p_{c}\)</span>).</p>
<div class="arithmatex">\[
    p_{c} = \frac{\text{Num Obs in Category &amp; Region}}
            {\text{Total Num Obs in Region}}
\]</div>
<p>Thus, the number of misclassifications in that region ONLY is the <strong>complement of the highest</strong> proportion:</p>
<div class="arithmatex">\[
    \text{Classification Error (Region)} = 1 - \max(p_{1}, p_{2}, \dots)
\]</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Recall that the model will predict a category based on the <strong>most frequent category</strong> (highest <span class="arithmatex">\(p_{c}\)</span>) in that region. Thus, any observations that are NOT actually part of that category are being <strong>misclassified</strong>.</p>
</div>
<p>Thus, the overall misclassification rate for the <strong>entire tree</strong> is the <strong>weighted average</strong> of the misclassifications for each region:</p>
<div class="arithmatex">\[
    \text{Classification Error (Total)} = \sum \text{Classification Error (Region)} \cdot \frac{n_{\text{Region}}}{n_{\text{Total}}}
\]</div>
<p>The above concept can be applied to the Gini Index and Cross Entropy as well, thus their total formulas will not be shown:</p>
<div class="arithmatex">\[
\begin{aligned}
    \text{Gini Index (Region)} &amp;= \sum_{c} p_{c} (1 - p_{c}) \\
    \text{Cross Entropy (Region)} &amp;= - \sum_{c} p_{c} \ln p_{c}
\end{aligned}
\]</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>All measures <strong>must be positive</strong>, but ONLY entropy can be larger than 1.</p>
<p>Gini Index and Entropy are <strong>similar</strong>, but the key difference is that it uses the <strong>SAME PROBABILITY</strong> in the summation.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Gini Index and Class Entropy are the preferred purity measures when <strong>growing</strong> out a tree. This is because they are <strong>sensitive to the distribution</strong> of categories, while classification error is NOT. This can be seen in their formulas, where they depend on the <strong>individual class proportions</strong> while the error rate just uses the highest one.</p>
<p>Consider the following two examples - the <strong>classification error rate is the same but the purity is higher</strong> in the second region:</p>
<p><!-- Self Made -->
<img alt="PURITY_DISTRIBUTION" class="center" src="../Assets/7.%20Tree%20Models.md/PURITY_DISTRIBUTION.png" /></p>
<p>Classification Error is a <strong>one-dimensional and less sensitive</strong> approach that tends to result in simpler trees, which is why is often used as the purity measure for <strong>pruning</strong>. This is coupled with the fact that the main goal of pruning is to improve predictive accuracy, which is directly what the classification error measures.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>In light of this, it is possible for a split to produce <strong>two nodes with the same predicted category</strong>. This is because the <strong>Gini or Entropy would have increased</strong> with the split, even though classification error remains the same.</p>
<p>When such a split occurs, it increases <strong>confidence</strong> in the data as the node is purer; we can be more sure that this split is correct:</p>
<ul>
<li>Scenario 1: 55% Pure Node results in classification A</li>
<li>Scenario 2: 90% Pure Node results in classification A (More certain)</li>
</ul>
</div>
<h3 id="two-category-model"><strong>Two Category Model</strong><a class="headerlink" href="#two-category-model" title="Permanent link">&para;</a></h3>
<p>The most common scenario to appear on the exam would be a model with <strong>ONLY TWO</strong> possible categories.</p>
<p>In such cases, there is a clearly defined order between the purity measures:</p>
<div class="arithmatex">\[
    \text{Entropy} \gt \text{Gini} \ge \text{Class Error}
\]</div>
<!-- Obtained from Coaching Actuaries -->
<p><img alt="RELATIVE_PURITY" class="center" src="../Assets/7.%20Tree%20Models.md/RELATIVE_PURITY.png" /></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>There is only one situation where Gini and Classification error are the same - when the two categories are <strong>equally distributed</strong>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The above relationship only holds for the measures WITHIN a single node. It CANNOT be used to compare across nodes.</p>
</div>
<h2 id="single-tree-analysis"><strong>Single Tree Analysis</strong><a class="headerlink" href="#single-tree-analysis" title="Permanent link">&para;</a></h2>
<ol>
<li><strong>Number of splits</strong> - Tree depth; should not be too big</li>
<li><strong>Size of terminal nodes</strong> - Proportion of data in the terminal node; should be sufficiently large </li>
<li><strong>Sequence of splits</strong> - Most important variables at the top; does it make sense?</li>
<li><strong>Location of splits</strong> - Where the splits occur; is it only for a particular region?</li>
<li><strong>Split conditions</strong> - Which variables are used? What range of the variables are used?</li>
</ol>
<p>Decision trees tend to "prefer" variables that have many possible splits (EG. Numeric variables or Categorical variables with many levels). This is because the higher possibilities increases the chance of finding a <strong>spurious split</strong> that happens to lead to information gain (due to inherent variance in the training data), resulting in overfitting. Thus, it is not recommended to include these variables in a decision tree.</p>
<p>There is a <strong>common misonception</strong> regarding variable importance for trees. In particular, that if a variable does not appear in a given tree, that it can be concluded that the other variables are not important - this is <strong>WRONG</strong>:</p>
<ul>
<li>A given tree is produced based on the training data and selected pruning parameters; a <strong>different set of conditions</strong> might produce <strong>vastly different trees</strong></li>
<li>Thus, it is <strong>not conclusive</strong> to say that omitted variables from a particular tree are not important</li>
</ul>
<h2 id="ensemble-methods"><strong>Ensemble Methods</strong><a class="headerlink" href="#ensemble-methods" title="Permanent link">&para;</a></h2>
<p>Ensemble Methods use a <strong>collection</strong> of statistical learning  techniques in conjunction with one another to produce better results than any of the individual methods.</p>
<p>Ensemble Methods are typically used with basic models known as <strong>Weak Learners</strong> to produce a stronger model.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The above implies that a tree on its own is <strong>NOT as good</strong> (in terms of predictive performance) as other statistical learning methods; likely due to overfitting.</p>
</div>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>Although Ensemble methods are only discussed in the context of trees, they can be used for other methods as well.</p>
</div>
<p>For regression trees, the output from all the underlying trees are <strong>averaged to form the final prediction</strong>. For classification trees, there are two possible methods:</p>
<ol>
<li>Determine the predicted class for each underlying tree then take the majority as the final prediction</li>
<li>Determine the average probability and use that as the prediction</li>
</ol>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Unless otherwise stated, assume that the first method is used.  </p>
</div>
<h3 id="bagging"><strong>Bagging</strong><a class="headerlink" href="#bagging" title="Permanent link">&para;</a></h3>
<p>Bagging refers to <strong>Bootstrap Aggregation</strong>. Bootstrapping is a process whereby “new” samples are created based on the existing sample. Thus, Bagging refers to constructing <strong>multiple trees using bootstrap samples and aggregating their results</strong> to form the final prediction.</p>
<p>Bootstrapping uses <strong>random sampling with replacement</strong> to create artificial samples of the <strong>same size</strong> from the original sample:</p>
<ul>
<li><span class="arithmatex">\(n^{n}\)</span> <strong>possible samples</strong> (Different permutations count)</li>
<li><span class="arithmatex">\(2n-1 \choose n-1\)</span> <strong>distinct samples</strong> (Different permutations NOT counted)</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since every sample has an equal chance of being selected, the probability that an observation is <strong>NOT selected for a draw</strong> is:</p>
<div class="arithmatex">\[
    \text{Probability NOT selected for draw} = 1 - \frac{1}{n}
\]</div>
<p>Thus, the probability that an observation is <strong>NOT selected at all across the entire sample</strong> (<span class="arithmatex">\(n\)</span> draws) is:</p>
<div class="arithmatex">\[
    \text{Probability NOT selected} = \left(1 - \frac{1}{n} \right)^{n}
\]</div>
<p>As the sample size approaches infinity, the above converges to the following, showing that <strong>about two-thirds of the observations are used</strong>:</p>
<div class="arithmatex">\[
\begin{aligned}
    \lim \left(1 - \frac{1}{n} \right)^{n} &amp; \approx \frac{1}{3}
    \\
    \text{Probability selected}
    &amp;= 1 - \frac{1}{3} \\
    &amp;= \frac{2}{3}
\end{aligned}
\]</div>
</div>
<p>Something unique about Bagging is that since <strong>only two-thirds</strong> of the original observations are used for each tree, it is possible to use the <strong>observations that were NOT drawn</strong> as the <strong>validation set</strong>. The resulting (average) error across all instances is known as the <strong>Out of Bag Error</strong> (OOB).</p>
<!-- Obtained from ISLR -->
<p><img alt="OOB_ERROR" class="center" src="../Assets/7.%20Tree%20Models.md/OOB_ERROR.png" /></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This would mean that there about <span class="arithmatex">\(\frac{1}{3}\)</span> OOB predictions for each observation.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The intuition is similar to cross-validation where the <strong>OOB observations were not used to train that particular bagged tree</strong>, thus serves as a good estimate for the test MSE.</p>
<p>In fact, for a sufficiently large number of trees, the OOB provides an reasonably good <strong>estimate of the LOOCV error</strong>:</p>
<ol>
<li>LOOCV error requires that all but one observation is used to train the model</li>
<li>For a large number of bootstrap samples, it increases the chances that ALL observations from the original dataset are used</li>
<li>Thus, the OOB will calculated based on ALL observations EXCEPT the observation being tested; akin to LOOCV</li>
</ol>
<p>The OOB error also has the added advantage of being <strong>computationally efficient</strong> compared to an actual LOOCV, as it only needs to be <strong>run once</strong> (the main run) to get the result.  </p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>OOB does NOT use every single tree in the Ensemble; it only uses those for which the specific observation was NOT used to train the tree.</p>
</div>
<p>The <strong>key advantage</strong> to bagging lies in <strong>Variance</strong>. An estimate coming from multiple samples is always <strong>more stable</strong> than estimate coming from just one sample. However, it is <strong>computationally expensive</strong>. Thus, the number of samples should be chosen such that the increase in accuracy is higher than the cost of computing.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Since bagging helps to reduce the variance, the underlying trees are often <strong>NOT pruned</strong> as the nature of bagging helps to counter the problem of high variance for unpruned trees.</p>
<p>This also means that ensemble methods tend to produce a wider range of predictions than a single decision tree, due to the greater number of terminal nodes.</p>
</div>
<p>However, the <strong>key disadvantage</strong> is that the method is that is <strong>not easily interpreted</strong>. A single tree is easily visualized while a “forest” is not. If interpretation is required, it is not recommended to use an ensemble method.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>These type of methods are known as a <strong>Black Box</strong>, because it is not clear to the user <em>how</em> the model arrives at the output.</p>
</div>
<p>One way to add more insight to the model is through the use of <strong>Variable Importance Plots</strong> - which ranks the most important variables to the model:</p>
<ul>
<li>Type of <strong>Sensitivity</strong> analysis, <strong>removing one variable at a time</strong> from the model</li>
<li>AVERAGE <strong>Change in MSE</strong> is recorded for each variable over all trees</li>
<li>The variable whose absence results in the <strong>greatest increase in MSE is the most important</strong> variable</li>
</ul>
<!-- Obtained from Research Gate -->
<p><img alt="VARIABLE_IMPORTANCE" class="center" src="../Assets/7.%20Tree%20Models.md/VARIABLE_IMPORTANCE.png" /></p>
<p>The above is plotted based on the absolute increase or the percentage increase for that variable. It also possible to present the data as a <strong>percentage of the most important variable</strong> instead:</p>
<!-- Obtained from Coaching Actuaries -->
<p><img alt="VARIABLE_IMPORTANCE_2" class="center" src="../Assets/7.%20Tree%20Models.md/VARIABLE_IMPORTANCE_2.png" /></p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>It is typical to call out the <strong>few most important/least important</strong> variables - and within those groups, their <strong>relative importance</strong>. For instance, A and B are the top two variables by a large margin, but B is only half as important as A.</p>
</div>
<p>However, despite the above, the plots do NOT ultimately describe <strong>HOW</strong> the variables impact the target. For this, a <strong>Partial Dependence Plot</strong> is considered instead - which describes the <strong>marginal</strong> impact that a variable has on the model output:</p>
<ol>
<li>Select an independent variable to be studied - consider all possible values for this variable</li>
<li>Modify the dataset such that all values for that variable is the first possible value</li>
<li>Re-run the model and record the predicted value</li>
<li>Repeat step (2) and (3) for all possible values and plot the results</li>
</ol>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The implicit assumption made is that the independent variable chosen is <strong>independent from OTHER independent variables</strong>, such that the PDP captures the true relationship between the two.</p>
<p>However, in reality, is likely that the chosen variable is <strong>correlated or has an interaction effect</strong> with some other variable - which makes the relationship captured in the PDP incomplete. Alternatively, there might be an <strong>impossible combination</strong> of values, making the results unintuitive.</p>
</div>
<h3 id="random-forest"><strong>Random Forest</strong><a class="headerlink" href="#random-forest" title="Permanent link">&para;</a></h3>
<p>One of the primary issues with Bagging is <strong>Correlation</strong>. Since the bootstrapped <strong>samples are all similar</strong> to the underlying sample, it is likely that each bootstrapped tree will contain the <strong>same set of predictors</strong> (especially if there especially strong predictors).</p>
<p>This can be overcome by <strong>limiting each bootstrapped tree</strong> to be constructed only based on a <strong>random subset of the predictors available</strong> at EACH SPLIT, reducing the monopoly of certain predictors and hence reducing the similarities. Hence, the method is called “Random Forest”.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>It can be understood that Bagging is a <strong>special case of Random Forests</strong> where all the predictors are allowed; bagging is a <strong>non-random forest</strong>.</p>
<p>Note that although Bagging generally refers to the learning method, it is also a shorthand for bootstrap aggregation. Thus, some texts may say that “Bagging is used in Random Forest”.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Phrased in a different way, the variable importance plot for Bagging tends to be <strong>dominated</strong> by the 1 or 2 most important variables. Random Forests naturally create a more balanced plot given that it is forced to use “less important” variables:</p>
<p><!--- Obtained from Coaching Actuaries --->
<img alt="VARIABLE_IMPORTANCE_COMPARISON" class="center" src="../Assets/7.%20Tree%20Models.md/VARIABLE_IMPORTANCE_COMPARISON.png" /></p>
<p>Note that even though both plots show the same strongest variable as 100%, the absolute amount of change in MSE is <strong>lower for the random forest</strong>, as the variable is <strong>used less in the random forest</strong> model.</p>
</div>
<p>If there are <span class="arithmatex">\(p\)</span> available predictors and <span class="arithmatex">\(m\)</span> are INCLUDED, then the probability that a <em>particular predictor</em> is not chosen at a given split is:</p>
<div class="arithmatex">\[
\begin{aligned}
    \text{P(Predictor not Chosen)}
    &amp;= \frac{{p-1 \choose m} \cdot {1 \choose 0}}{{p \choose m}} \\
    &amp;= \frac{p-m}{p}   
\end{aligned}
\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Naturally, a smaller <span class="arithmatex">\(m\)</span> will result in a greater de-correlation effect. However, a <strong>sufficiently large <span class="arithmatex">\(m\)</span></strong> still needs to be chosen to ensure the signal is correctly captured, thus the following values are common:</p>
<ul>
<li><strong>Regression Trees</strong>: <span class="arithmatex">\(m = \frac{p}{3}\)</span></li>
<li><strong>Classification Trees</strong>: <span class="arithmatex">\(m = \sqrt{p}\)</span></li>
</ul>
</div>
<h3 id="boosting"><strong>Boosting</strong><a class="headerlink" href="#boosting" title="Permanent link">&para;</a></h3>
<p>Boosting refers to the process of <strong>sequentially training</strong> a series of <strong>shallow trees</strong> on the <strong>error of the previous tree</strong>. The strength of weak trees are “boosted” by <strong>aggregating</strong> them together to form a stronger model.</p>
<p>The key idea is that the errors of the trees contain the effects of the <strong>predictors not included in the model</strong>. Thus, training each successive model on the <strong>errors of the prior ones</strong> forces the model to <strong>explained the unexplained</strong>.</p>
<!-- Obtained from Research Gate -->
<p><img alt="BOOSTING" class="center" src="../Assets/7.%20Tree%20Models.md/BOOSTING.png" /></p>
<!-- Obtained from Affine -->
<p><img alt="BOOSTING_VS_BAGGING" class="center" src="../Assets/7.%20Tree%20Models.md/BOOSTING_VS_BAGGING.png" /></p>
<p>There are three hyperparameters for Boosting:</p>
<ul>
<li><strong>Number of Trees</strong> (<span class="arithmatex">\(B\)</span>) - How many trees should be used; typically <strong>large</strong></li>
<li><strong>Depth of Trees</strong> (<span class="arithmatex">\(d\)</span>) - How many splits each tree should have; typically <strong>small</strong> (shallow trees)</li>
<li><strong>Learning Rate</strong> (<span class="arithmatex">\(\alpha\)</span>) - How much the trees contributes to the overall prediction; typically <strong>small</strong></li>
</ul>
<div class="arithmatex">\[
\begin{aligned}
    \text{Boosted Tree Prediction} &amp;= \alpha \cdot \text{Tree Prediction} \\
    \text{Overall Prediction} &amp;= \sum \text{Boosted Tree Prediction}
\end{aligned}
\]</div>
<p>The learning rate is also referred to as the <strong>Shrinkage Parameter</strong> as it controls the <strong>influence of each tree</strong> on the overall prediction. Since trees are typically trained to be shallow, it is preferable to have a large number of trees, taking <strong>only a fraction of each tree</strong> (slow learning rate) to form the final prediction. However, if only a small number of trees are used, then a large learning rate is required.</p>
<p>Boosting as a method will naturally result in <strong>low bias</strong>. However, if an <em>extreme</em> number of trees are used, the model could become <strong>overfitted</strong>:</p>
<!-- Self Made -->
<p><img alt="BOOSTING_ERROR" class="center" src="../Assets/7.%20Tree%20Models.md/BOOSTING_ERROR.png" /></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Some questions might cut off the graph at a certain point, omitting the potential increase in Test Error for Boosting. </p>
</div>
<h3 id="comparison"><strong>Comparison</strong><a class="headerlink" href="#comparison" title="Permanent link">&para;</a></h3>
<p><center></p>
<table>
<thead>
<tr>
<th align="center"><strong>Random Forest</strong></th>
<th align="center"><strong>Boosting</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Trees built independently</td>
<td align="center">Trees built sequentially</td>
</tr>
<tr>
<td align="center">From different bootstraps</td>
<td align="center">From previous residuals</td>
</tr>
<tr>
<td align="center">Random subset used each time</td>
<td align="center">Same underlying data each time</td>
</tr>
<tr>
<td align="center">Average of underlying; does not overfit</td>
<td align="center">Missed signals of underlying; can overfit</td>
</tr>
<tr>
<td align="center">Preferred for low variance</td>
<td align="center">Preferred for low bias</td>
</tr>
</tbody>
</table>
<p></center></p>
<h2 id="evaluation"><strong>Evaluation</strong><a class="headerlink" href="#evaluation" title="Permanent link">&para;</a></h2>
<p>Similar to all other statistical learning methods, the effectiveness of tree can also be determined visually by using a <strong>Predicted vs Actual Plot</strong>. The key difference is that for a single tree, it is possible to tell <strong>how many terminal nodes</strong> there are based on <strong>how many distinct predictions</strong> there are:</p>
<!-- Obtained from SOA Past Year Exams -->
<p><img alt="SINGLE_TREE_PRED_ACT_PLOT" class="center" src="../Assets/7.%20Tree%20Models.md/SINGLE_TREE_PRED_ACT_PLOT.png" /></p>
<p>It is NOT possible to do the same with Ensemble methods there would be a huge range of predictions from all the underlying trees; it is unlikely that two predictions from an ensemble method would be the same:</p>
<!-- Obtained from SOA Past Year Exams -->
<p><img alt="ENSEMBLE_TREE_PRED_ACT_PLOT" class="center" src="../Assets/7.%20Tree%20Models.md/ENSEMBLE_TREE_PRED_ACT_PLOT.png" /></p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../6.%20Generalized%20Linear%20Models/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Generalized Linear Models" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Generalized Linear Models
            </div>
          </div>
        </a>
      
      
        
        <a href="../8.%20Principal%20Components/" class="md-footer__link md-footer__link--next" aria-label="Next: Principal Components" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Principal Components
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.16e2a7d4.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.5a2dcb6a.min.js"></script>
      
        <script src="../../../config/mathjax.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
    
  </body>
</html>