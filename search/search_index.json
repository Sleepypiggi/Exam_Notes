{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Actuarial Exam Notes \u00b6 Hello! This website is meant to a be a repository of the notes that I have used for the various SOA exams, allowing me to reference my notes on the go. The notes were created by blending together information from multiple resources. The information is what I understand of the concepts and thus should not be taken as fact. All images used in this website are credited in the raw markdown files that were used to build the website. If there are any issues with the use of the images, please reach out to me via Github.","title":"**Actuarial Exam Notes**"},{"location":"#actuarial-exam-notes","text":"Hello! This website is meant to a be a repository of the notes that I have used for the various SOA exams, allowing me to reference my notes on the go. The notes were created by blending together information from multiple resources. The information is what I understand of the concepts and thus should not be taken as fact. All images used in this website are credited in the raw markdown files that were used to build the website. If there are any issues with the use of the images, please reach out to me via Github.","title":"Actuarial Exam Notes"},{"location":"0.%20Practical%20Exam%20Tips/","text":"Practical Exam Tips \u00b6 Clear calculator memory save to variables Key functions STAT function Table function Storing variables","title":"Practical Exam Tips"},{"location":"0.%20Practical%20Exam%20Tips/#practical-exam-tips","text":"Clear calculator memory save to variables Key functions STAT function Table function Storing variables","title":"Practical Exam Tips"},{"location":"Placeholder/","text":"Work In Progress \u00b6","title":"FSA-ALM"},{"location":"Placeholder/#work-in-progress","text":"","title":"Work In Progress"},{"location":"1.%20Introductory/0.%20Review%20of%20Mathematics/","text":"Review of Mathematics \u00b6 This section aims to cover the fundamental mathematical concepts needed for actuarial exams. It assumes that the reader has covered these topics before in school; it is NOT meant to introduce these topics to the reader for the first time. Algebra \u00b6 You've all taken Algebra. What are the main questions Algebra asks and answers? (1) Given an expression with variables, re-write it a certain way, so that what you end up with is the same thing as what you started with, just written differently. (2) Given an equation with variables, find all the solutions (the numbers you can substitute in place of the variable that make the equation true). Overall, Algebra is the study of equations. Equations \u00b6 Simultaneous Equations Quadractic Equations Logarithm and Exponents Functions \u00b6 Logarithm Rules \u00b6 A quick review of the rules of manipulating logarithms can be found below: Warning Note that rule 1 and 2 are often misunderstood , leading people to believe that the following is true when they are NOT: \\[ \\begin{aligned} \\ln (A + B) &= \\ln A \\cdot \\ln B \\\\ \\ln (A - B) &= \\frac{\\ln A}{\\ln B} \\end{aligned} \\] Linear Algebra \u00b6 For the most part, linear algebra is nothing more than a set of tools for dealing with multi-dimensional data, whether it data related to physical systems, or financial systems, or statistical models. Vectors are conceptually nothing more than that. Some of the transformations or functions that we might want to perform on multi-dimensional data have the special property of being linear. In that case, the maths works out particularly nicely, and the entire transformation can be represented by a matrix. Dealing with linear operators (matrices) rather than arbitrary operators (functions) gives a surprising degree of leverage with which to calculate. That might mean calculating the marginal probability of an event, or estimating the long term statistical behavior of a process for example. In fact, the mathematical properties of linear systems give us so much leverage with which to interrogate, and analyse, and solve, that even when faced with non-linear problems it is often useful to focus attention to the linear aspects, or even to model the non-linear system using a linear approximation. To a large extent, this is the reason why so many probabilistic modeling techniques are centered around the multivariate gaussian distribution: Because these models can be represented by a mean vector, and a covariance matrix. Vectors \u00b6 Matrices \u00b6 System of Linear Equations \u00b6 First I have to explain what a linear equation is. A linear equation is an equation with multiple variables where the exponent on each variable is 1 and are not passed to other functions. The following are linear equations: x + y - z = 0 a + b + c + d + e = 17 The following are NOT linear equations x^2 + y = 3 (x is raised to the 2 nd power) sin(x) + log(y) = 13 (x and y are passed to other functions) When first introduced, linear algebra is a way to take multiple linear equations and solving them. Take for example: x + y = 5 x - y = 3 What you learn is another way to write this equation: M * w = v Where M is something called a matrix and w is a vector holding x and y and v is a vector holding 5 and 3. Calculus \u00b6 Calculus is the the study of change : Differential Calculus : Rate of change Integral Calculus : Accumulation of changes Derivatives tell you the speed of change while integrals tell you the total change. Differential Calculus \u00b6 The Derivative of a function is the rate of change of the function AT that point . The process of obtaining it is known as Differentiation . Simple rules Chain, product, quotient rule Note Recall that a function \\(f\\) assigns as output \\(f(x)\\) every input \\(x\\) . The function is said to have a limit \\(L\\) at an input \\(p\\) if \\(f(x)\\) moves closer to \\(L\\) as \\(x\\) moves closer to \\(p\\) . \\[ \\begin{aligned} x \\to p,& \\space f(x) \\to L \\\\ \\lim_{x \\to p} f(x) &= L \\end{aligned} \\] Approximate otherwise impossible to calculate behaviour A \"Limit\" is just a fancy way of saying \"I can't know exactly what this answer is, but I can get really dang close and then just assume it's what we think it looks like it is\" The limit in calculus is when the formula gets close to something but will never touch it. As such, the best answer is that \"it\". Series \u00b6 Integrals \u00b6 Simple rules By parts inverse chain rule Subsituition By Parts \u00b6 Tabular Method One component that can be differentiated to 0 One component that is integratable Definite Integrals By parts Differential Equations \u00b6","title":"Review of Mathematics"},{"location":"1.%20Introductory/0.%20Review%20of%20Mathematics/#review-of-mathematics","text":"This section aims to cover the fundamental mathematical concepts needed for actuarial exams. It assumes that the reader has covered these topics before in school; it is NOT meant to introduce these topics to the reader for the first time.","title":"Review of Mathematics"},{"location":"1.%20Introductory/0.%20Review%20of%20Mathematics/#algebra","text":"You've all taken Algebra. What are the main questions Algebra asks and answers? (1) Given an expression with variables, re-write it a certain way, so that what you end up with is the same thing as what you started with, just written differently. (2) Given an equation with variables, find all the solutions (the numbers you can substitute in place of the variable that make the equation true). Overall, Algebra is the study of equations.","title":"Algebra"},{"location":"1.%20Introductory/0.%20Review%20of%20Mathematics/#equations","text":"Simultaneous Equations Quadractic Equations Logarithm and Exponents","title":"Equations"},{"location":"1.%20Introductory/0.%20Review%20of%20Mathematics/#functions","text":"","title":"Functions"},{"location":"1.%20Introductory/0.%20Review%20of%20Mathematics/#logarithm-rules","text":"A quick review of the rules of manipulating logarithms can be found below: Warning Note that rule 1 and 2 are often misunderstood , leading people to believe that the following is true when they are NOT: \\[ \\begin{aligned} \\ln (A + B) &= \\ln A \\cdot \\ln B \\\\ \\ln (A - B) &= \\frac{\\ln A}{\\ln B} \\end{aligned} \\]","title":"Logarithm Rules"},{"location":"1.%20Introductory/0.%20Review%20of%20Mathematics/#linear-algebra","text":"For the most part, linear algebra is nothing more than a set of tools for dealing with multi-dimensional data, whether it data related to physical systems, or financial systems, or statistical models. Vectors are conceptually nothing more than that. Some of the transformations or functions that we might want to perform on multi-dimensional data have the special property of being linear. In that case, the maths works out particularly nicely, and the entire transformation can be represented by a matrix. Dealing with linear operators (matrices) rather than arbitrary operators (functions) gives a surprising degree of leverage with which to calculate. That might mean calculating the marginal probability of an event, or estimating the long term statistical behavior of a process for example. In fact, the mathematical properties of linear systems give us so much leverage with which to interrogate, and analyse, and solve, that even when faced with non-linear problems it is often useful to focus attention to the linear aspects, or even to model the non-linear system using a linear approximation. To a large extent, this is the reason why so many probabilistic modeling techniques are centered around the multivariate gaussian distribution: Because these models can be represented by a mean vector, and a covariance matrix.","title":"Linear Algebra"},{"location":"1.%20Introductory/0.%20Review%20of%20Mathematics/#vectors","text":"","title":"Vectors"},{"location":"1.%20Introductory/0.%20Review%20of%20Mathematics/#matrices","text":"","title":"Matrices"},{"location":"1.%20Introductory/0.%20Review%20of%20Mathematics/#system-of-linear-equations","text":"First I have to explain what a linear equation is. A linear equation is an equation with multiple variables where the exponent on each variable is 1 and are not passed to other functions. The following are linear equations: x + y - z = 0 a + b + c + d + e = 17 The following are NOT linear equations x^2 + y = 3 (x is raised to the 2 nd power) sin(x) + log(y) = 13 (x and y are passed to other functions) When first introduced, linear algebra is a way to take multiple linear equations and solving them. Take for example: x + y = 5 x - y = 3 What you learn is another way to write this equation: M * w = v Where M is something called a matrix and w is a vector holding x and y and v is a vector holding 5 and 3.","title":"System of Linear Equations"},{"location":"1.%20Introductory/0.%20Review%20of%20Mathematics/#calculus","text":"Calculus is the the study of change : Differential Calculus : Rate of change Integral Calculus : Accumulation of changes Derivatives tell you the speed of change while integrals tell you the total change.","title":"Calculus"},{"location":"1.%20Introductory/0.%20Review%20of%20Mathematics/#differential-calculus","text":"The Derivative of a function is the rate of change of the function AT that point . The process of obtaining it is known as Differentiation . Simple rules Chain, product, quotient rule Note Recall that a function \\(f\\) assigns as output \\(f(x)\\) every input \\(x\\) . The function is said to have a limit \\(L\\) at an input \\(p\\) if \\(f(x)\\) moves closer to \\(L\\) as \\(x\\) moves closer to \\(p\\) . \\[ \\begin{aligned} x \\to p,& \\space f(x) \\to L \\\\ \\lim_{x \\to p} f(x) &= L \\end{aligned} \\] Approximate otherwise impossible to calculate behaviour A \"Limit\" is just a fancy way of saying \"I can't know exactly what this answer is, but I can get really dang close and then just assume it's what we think it looks like it is\" The limit in calculus is when the formula gets close to something but will never touch it. As such, the best answer is that \"it\".","title":"Differential Calculus"},{"location":"1.%20Introductory/0.%20Review%20of%20Mathematics/#series","text":"","title":"Series"},{"location":"1.%20Introductory/0.%20Review%20of%20Mathematics/#integrals","text":"Simple rules By parts inverse chain rule Subsituition","title":"Integrals"},{"location":"1.%20Introductory/0.%20Review%20of%20Mathematics/#by-parts","text":"Tabular Method One component that can be differentiated to 0 One component that is integratable Definite Integrals By parts","title":"By Parts"},{"location":"1.%20Introductory/0.%20Review%20of%20Mathematics/#differential-equations","text":"","title":"Differential Equations"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/1.%20Introduction/","text":"","title":"Introduction"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/2.%20Forwards%20%26%20Futures/","text":"","title":"2. Forwards & Futures"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/3.%20Options/","text":"Options \u00b6 This is not the original set of notes for IFM. It was recreated to fit the watered down option syllabus for exam FAM-S. Overview \u00b6 Options are contracts where the contract owner has the right to trade an asset at a fixed price on or before a specified future date. The buyer of contract is known as the Option Holder while the Seller is known as the Option Writer . The buyer pays the seller a Premium at time 0 for fulfilling the contract. The option holder can Exercise their Right to trade an asset while the option writer is Obligated to fulfil the trade. Naturally, option holders will only exercise their right when it is favourable to do so; resulting in a positive payoff . The right to Buy an Asset is known as a Call Option (Calls) while the right to Sell an Asset is known as a Put Option (Puts). Warning It is a common misconception to mix up the buyer and seller of the option with buying and selling the asset. For both Call and Put Options, there will always be a buyer and a seller. The fixed price is known as the Strike Price \\((K)\\) and the future date is known as the Expiration Date \\((T)\\) . There are three types of options, based on when they can be exercised: American Options : Any time on or before the expiration date Bermudan Options : Only during a specified window European Options : Only on the expiration date itself For the purposes of this exam, only European and American Options will be covered in depth. European Option Premiums are denoted in Lower Case \\((c_0, p_0)\\) while American Options are denoted using Upper Case \\((C_0, P_0)\\) . European Options \u00b6 Call Options \u00b6 Consider a Long Call Option : Exercise Do Not Exercise Intuition Cheaper to buy via Call Expensive to buy via Call Scenario Spot is larger than Strike \\((S_T \\gt K)\\) Spot is smaller than Strike \\((S_T \\lt K)\\) Payoff Buy at strike and sell at spot \\((S_T - K)\\) No trade occurs \\((0)\\) Profit Positive Profit \\((S_T - K - c_0)\\) Negative Profit \\((-c_0)\\) Note The payoff of a call option can be understood using the following example: Exercise the option to buy the stock at the lower strike price \\(K\\) and sell it at the higher market price \\(S_T\\) , earning the difference. This can also be expressed in a piecewise function : \\[ \\begin{aligned} \\text{Payoff}_\\text{Long} &= \\begin{cases} 0 & S_T \\leq K, \\\\ S_T - K & S_T > K \\end{cases} \\\\ &= \\max(S_T \u2013 K, 0) \\\\ \\\\ \\text{Profit}_\\text{Long} &= \\max(S_T \u2013 K, 0) \u2013 c_0 \\end{aligned} \\] Tip The key takeaway is that Options will only be exercised if there are positive payoffs, but positive payoffs do not guarantee that there are positive profits . Consider the possible payoffs and profits for a long call: Payoff Profit Minimum Scenario Not exercised, no trade occurs Premium already paid Minimum Value \\(\\text{Payoff} = 0\\) \\(\\text{Profit} = -c_0\\) Maximum Scenario Exercised, as high as the stock price Premium Already paid Maximum Value \\(\\text{Payoff} = \\infty\\) \\(\\text{Profit }= \\infty - c_0 = \\infty\\) Note There is no limit for the stock price, thus the maximum stock price is best represented using infinity. Thus, Long Call Options have Unlimited Upside \\((\\infty)\\) and Limited Downsides \\((0, -c_0)\\) : Using the zero-sum game property , Short Calls are simply the opposite of their long counterparts: \\[ \\begin{aligned} \\text{Payoff}_\\text{Short} &= \\begin{cases} 0 & S_T \\leq K, \\\\ -(S_T \u2013 K) & S_T > K \\end{cases} \\\\ &= -\\max(S_T \u2013 K, 0) \\\\ \\\\ \\text{Profit}_\\text{Short} &= c_0 - \\max(S_T \u2013 K, 0) \\end{aligned} \\] Unlike their long counterparts, they have Limited Upside \\((0, c_0)\\) and Unlimited Downside \\((-\\infty)\\) : Put Options \u00b6 A Long Put Option is the opposite of a long call as the perspective has been flipped to selling instead: Exercise Do Not Exercise Intuition Better to sell via Put Worse to sell via Put Scenario Spot is smaller than Strike \\((S_T \\lt K)\\) Spot is larger than Strike \\((S_T \\gt K)\\) Payoff Buy at spot and sell at strike \\((K - S_T)\\) No trade occurs \\((0)\\) Profit Positive Profit \\((K - S_T - p_0)\\) Negative Profit \\((-p_0)\\) Note The payoff of a put option can be understood using the following example: Buy the stock at the lower market price \\(S_T\\) and exercise the option to sell it at the higher strike price \\(K\\) , earning the difference. This can also be expressed in a piecewise function : \\[ \\begin{aligned} \\text{Payoff}_\\text{Long} &= \\begin{cases} K \u2013 S_T & S_T < K, \\\\ 0 & S_T \\geq K \\end{cases} \\\\ &= \\max(K - S_T, 0) \\\\ \\\\ \\text{Profit}_\\text{Long} &= \\max(K - S_T, 0) \u2013 p_0 \\end{aligned} \\] Consider the possible payoffs and profits for a long put: Payoff Profit Minimum Scenario Not exercised, no trade occurs Premium already paid Minimum Value \\(\\text{Payoff} = 0\\) \\(\\text{Profit} = -p_0\\) Maximum Scenario Exercised, as high as the strike price Premium Already paid Maximum Value \\(\\text{Payoff} = K\\) \\(\\text{Profit }= K - p_0\\) Note The lowest possible stock price is 0, which is why the maximum possible payoff is just the strike price \\(K-0 = K\\) . Thus, Long Call Options have higher upside \\((K, K-p_0)\\) and lower Downsides \\((0, -c_0)\\) : Using the zero-sum game property , Short Puts are simply the opposite of their long counterparts: \\[ \\begin{aligned} \\text{Payoff}_\\text{Short} &= \\begin{cases} -(K \u2013 S_T) & S_T < K, \\\\ 0 & S_T \\ge K \\end{cases} \\\\ &= -\\max(K - S_T, 0) \\\\ \\\\ \\text{Profit}_\\text{Short} &= p_0 - \\max(S_T \u2013 K, 0) \\end{aligned} \\] Unlike their long counterparts, they have Lower Upside \\((0, p_0)\\) and Higher Downside \\((-K, p_0 - K)\\) : Moneyness \u00b6 Option Moneyness is a notional indicator about the payoff of a LONG option should it be exercised immediately at the current spot price . It does not matter if the option can or cannot be exercised; it is meant to provide a rough gauge about what the spot price currently is without having to provide its exact value. In the Money (ITM) At the Money (ATM) Out the Money (OTM) Calls Positive Payoff \\((S_T > K)\\) Zero Payoff \\((S_T = K)\\) Negative Payoff \\((S_T < K)\\) Puts Positive Payoff \\((S_T < K)\\) Zero Payoff \\((S_T = K)\\) Negative Payoff \\((S_T = K)\\) Put Call Parity \u00b6 Put Call Parity (PCP) is an equation that relates Calls, Puts and an underlying Forward Contract: \\[ p_0 \\ \u2013 \\ c_0 = Ke^{-rt} \\ \u2013 \\ F_0^P \\] Warning The equation takes the perspective of the initial cashflows of the position, rather than the terminal cashflow, which is what we are more used to working with. Positive Sign : Cashflow in (Revenue from selling ) Negative Sign : Cashflow Out (Cost of buying ) No Arbitrage Proof \u00b6 Consider the following two portfolios which have the same payoffs: One Long Call & One Short Put One Long Forward only \\[ \\begin{aligned} \\text{Payoff} &= \\text{Payoff}_\\text{Long Call} + \\text{Payoff}_\\text{Short Put} \\\\ &= \\max(S_T \\ - \\ K, \\ 0) - \\max(K \\ - \\ S_T, \\ 0) \\\\ &= S_T \\ \u2013 \\ K \\end{aligned} \\] By the Law of One Price , both of the portfolios must be priced the same , resulting in the PCP equation. The price of the first portfolio is simply the difference in premiums of the two options: Pay premium for buying the Call Receive premium selling the Put \\[ \\text{Price}_\\text{Options} = - c_0 + p_0 \\] The price of the second portfolio is harder to determine as the forward price is usually paid at expiration rather than at the inception . Thus, we need to come up with a replicating portfolio to determine the time 0 price of a forward. A forward contract consists of two components: Buying the asset at a fixed price ; akin to selling a Zero Coupon Bond Selling the asset at the future spot price ; akin to buying a Prepaid Forward \\[ \\begin{aligned} \\text{Price}_\\text{Long Forward} &= Ke^{-rt} \u2013 F_0^P \\\\ &= Ke^{-rt} \u2013 S_0 \\cdot e^{rT} \\end{aligned} \\] Note As its name suggests, a prepaid forward is simply a forward contract whose price is paid for at time 0 rather than at expiration. Intuitively, this time 0 price is simply the PV of the price that would have been paid at maturity: \\[ F^P_0 = \\text{PV}(F_0) \\] However, since the forward price depends on the underlying, the prepaid forward price depends on the underlying as well: Prepaid Forward Price Forward Price No Dividends \\(S_0\\) \\(S_0 \\cdot e^{rT}\\) Discrete Dividends \\(S_0 - \\text{PV Dividends}\\) \\(S_0 - \\text{AV Dividends}\\) Continuous Dividends \\(S_0 \\cdot e^{-qt}\\) \\(S_0 \\cdot e^{(r-q)t}\\) For simplicity, the no dividend stock was used in the expression above. Thus, following the law of one price, the price of the two portfolios must be equal: \\[ p_0 \\ \u2013 \\ c_0 = Ke^{-rt} \\ \u2013 \\ F_0^P \\] Synthetic Positions \u00b6 One practical application of the PCP equation is to easily identify and create replicating portfolios for each of its individual components. This can be done by simply re-arranging the equation to match the time 0 cashflow of the position we want to replicate: Position Replicating Portfolio Long Call \\(-c_0 = -p_0 + Ke^{-rt} - F^P_0\\) Long Put \\(-p_0 = -c_0 - Ke^{-rt} + F^P_0\\) Long ZC Bond \\(-Ke^{-rt} = -p_0 + c_0 - F^P_0\\) Long Stock \\(-F^P_0 = - S_0 = -c_0 - Ke^{-rt} + p_0\\) Note For a short selling position, simply re-arrange the equations to obtain a positive expression on the LHS. Risk Free Rate \u00b6 Another application of the PCP equation is to solve for the risk free rates . This can be done when PCP is given for multiple dates \\[ \\begin{align*} p_{0, T} \\ \u2013 \\ c_{0, T} &= Ke^{-rT} \u2013 S_0 \\\\ p_{t, T} \\ \u2013 \\ c_{t, T} &= Ke^{-r(T-t)} \u2013 S_t \\\\ \\\\ \\therefore e^{rt} &= \\frac{Ke^{-rT}}{Ke^{-r(T-t)}} \\\\ &= \\frac{ p_{0, T} \u2013 c_{o, T} + S_0 }{ p_{t, T} \u2013 c_{t, T} + S_t} \\end{align*} \\]","title":"Options"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/3.%20Options/#options","text":"This is not the original set of notes for IFM. It was recreated to fit the watered down option syllabus for exam FAM-S.","title":"Options"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/3.%20Options/#overview","text":"Options are contracts where the contract owner has the right to trade an asset at a fixed price on or before a specified future date. The buyer of contract is known as the Option Holder while the Seller is known as the Option Writer . The buyer pays the seller a Premium at time 0 for fulfilling the contract. The option holder can Exercise their Right to trade an asset while the option writer is Obligated to fulfil the trade. Naturally, option holders will only exercise their right when it is favourable to do so; resulting in a positive payoff . The right to Buy an Asset is known as a Call Option (Calls) while the right to Sell an Asset is known as a Put Option (Puts). Warning It is a common misconception to mix up the buyer and seller of the option with buying and selling the asset. For both Call and Put Options, there will always be a buyer and a seller. The fixed price is known as the Strike Price \\((K)\\) and the future date is known as the Expiration Date \\((T)\\) . There are three types of options, based on when they can be exercised: American Options : Any time on or before the expiration date Bermudan Options : Only during a specified window European Options : Only on the expiration date itself For the purposes of this exam, only European and American Options will be covered in depth. European Option Premiums are denoted in Lower Case \\((c_0, p_0)\\) while American Options are denoted using Upper Case \\((C_0, P_0)\\) .","title":"Overview"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/3.%20Options/#european-options","text":"","title":"European Options"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/3.%20Options/#call-options","text":"Consider a Long Call Option : Exercise Do Not Exercise Intuition Cheaper to buy via Call Expensive to buy via Call Scenario Spot is larger than Strike \\((S_T \\gt K)\\) Spot is smaller than Strike \\((S_T \\lt K)\\) Payoff Buy at strike and sell at spot \\((S_T - K)\\) No trade occurs \\((0)\\) Profit Positive Profit \\((S_T - K - c_0)\\) Negative Profit \\((-c_0)\\) Note The payoff of a call option can be understood using the following example: Exercise the option to buy the stock at the lower strike price \\(K\\) and sell it at the higher market price \\(S_T\\) , earning the difference. This can also be expressed in a piecewise function : \\[ \\begin{aligned} \\text{Payoff}_\\text{Long} &= \\begin{cases} 0 & S_T \\leq K, \\\\ S_T - K & S_T > K \\end{cases} \\\\ &= \\max(S_T \u2013 K, 0) \\\\ \\\\ \\text{Profit}_\\text{Long} &= \\max(S_T \u2013 K, 0) \u2013 c_0 \\end{aligned} \\] Tip The key takeaway is that Options will only be exercised if there are positive payoffs, but positive payoffs do not guarantee that there are positive profits . Consider the possible payoffs and profits for a long call: Payoff Profit Minimum Scenario Not exercised, no trade occurs Premium already paid Minimum Value \\(\\text{Payoff} = 0\\) \\(\\text{Profit} = -c_0\\) Maximum Scenario Exercised, as high as the stock price Premium Already paid Maximum Value \\(\\text{Payoff} = \\infty\\) \\(\\text{Profit }= \\infty - c_0 = \\infty\\) Note There is no limit for the stock price, thus the maximum stock price is best represented using infinity. Thus, Long Call Options have Unlimited Upside \\((\\infty)\\) and Limited Downsides \\((0, -c_0)\\) : Using the zero-sum game property , Short Calls are simply the opposite of their long counterparts: \\[ \\begin{aligned} \\text{Payoff}_\\text{Short} &= \\begin{cases} 0 & S_T \\leq K, \\\\ -(S_T \u2013 K) & S_T > K \\end{cases} \\\\ &= -\\max(S_T \u2013 K, 0) \\\\ \\\\ \\text{Profit}_\\text{Short} &= c_0 - \\max(S_T \u2013 K, 0) \\end{aligned} \\] Unlike their long counterparts, they have Limited Upside \\((0, c_0)\\) and Unlimited Downside \\((-\\infty)\\) :","title":"Call Options"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/3.%20Options/#put-options","text":"A Long Put Option is the opposite of a long call as the perspective has been flipped to selling instead: Exercise Do Not Exercise Intuition Better to sell via Put Worse to sell via Put Scenario Spot is smaller than Strike \\((S_T \\lt K)\\) Spot is larger than Strike \\((S_T \\gt K)\\) Payoff Buy at spot and sell at strike \\((K - S_T)\\) No trade occurs \\((0)\\) Profit Positive Profit \\((K - S_T - p_0)\\) Negative Profit \\((-p_0)\\) Note The payoff of a put option can be understood using the following example: Buy the stock at the lower market price \\(S_T\\) and exercise the option to sell it at the higher strike price \\(K\\) , earning the difference. This can also be expressed in a piecewise function : \\[ \\begin{aligned} \\text{Payoff}_\\text{Long} &= \\begin{cases} K \u2013 S_T & S_T < K, \\\\ 0 & S_T \\geq K \\end{cases} \\\\ &= \\max(K - S_T, 0) \\\\ \\\\ \\text{Profit}_\\text{Long} &= \\max(K - S_T, 0) \u2013 p_0 \\end{aligned} \\] Consider the possible payoffs and profits for a long put: Payoff Profit Minimum Scenario Not exercised, no trade occurs Premium already paid Minimum Value \\(\\text{Payoff} = 0\\) \\(\\text{Profit} = -p_0\\) Maximum Scenario Exercised, as high as the strike price Premium Already paid Maximum Value \\(\\text{Payoff} = K\\) \\(\\text{Profit }= K - p_0\\) Note The lowest possible stock price is 0, which is why the maximum possible payoff is just the strike price \\(K-0 = K\\) . Thus, Long Call Options have higher upside \\((K, K-p_0)\\) and lower Downsides \\((0, -c_0)\\) : Using the zero-sum game property , Short Puts are simply the opposite of their long counterparts: \\[ \\begin{aligned} \\text{Payoff}_\\text{Short} &= \\begin{cases} -(K \u2013 S_T) & S_T < K, \\\\ 0 & S_T \\ge K \\end{cases} \\\\ &= -\\max(K - S_T, 0) \\\\ \\\\ \\text{Profit}_\\text{Short} &= p_0 - \\max(S_T \u2013 K, 0) \\end{aligned} \\] Unlike their long counterparts, they have Lower Upside \\((0, p_0)\\) and Higher Downside \\((-K, p_0 - K)\\) :","title":"Put Options"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/3.%20Options/#moneyness","text":"Option Moneyness is a notional indicator about the payoff of a LONG option should it be exercised immediately at the current spot price . It does not matter if the option can or cannot be exercised; it is meant to provide a rough gauge about what the spot price currently is without having to provide its exact value. In the Money (ITM) At the Money (ATM) Out the Money (OTM) Calls Positive Payoff \\((S_T > K)\\) Zero Payoff \\((S_T = K)\\) Negative Payoff \\((S_T < K)\\) Puts Positive Payoff \\((S_T < K)\\) Zero Payoff \\((S_T = K)\\) Negative Payoff \\((S_T = K)\\)","title":"Moneyness"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/3.%20Options/#put-call-parity","text":"Put Call Parity (PCP) is an equation that relates Calls, Puts and an underlying Forward Contract: \\[ p_0 \\ \u2013 \\ c_0 = Ke^{-rt} \\ \u2013 \\ F_0^P \\] Warning The equation takes the perspective of the initial cashflows of the position, rather than the terminal cashflow, which is what we are more used to working with. Positive Sign : Cashflow in (Revenue from selling ) Negative Sign : Cashflow Out (Cost of buying )","title":"Put Call Parity"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/3.%20Options/#no-arbitrage-proof","text":"Consider the following two portfolios which have the same payoffs: One Long Call & One Short Put One Long Forward only \\[ \\begin{aligned} \\text{Payoff} &= \\text{Payoff}_\\text{Long Call} + \\text{Payoff}_\\text{Short Put} \\\\ &= \\max(S_T \\ - \\ K, \\ 0) - \\max(K \\ - \\ S_T, \\ 0) \\\\ &= S_T \\ \u2013 \\ K \\end{aligned} \\] By the Law of One Price , both of the portfolios must be priced the same , resulting in the PCP equation. The price of the first portfolio is simply the difference in premiums of the two options: Pay premium for buying the Call Receive premium selling the Put \\[ \\text{Price}_\\text{Options} = - c_0 + p_0 \\] The price of the second portfolio is harder to determine as the forward price is usually paid at expiration rather than at the inception . Thus, we need to come up with a replicating portfolio to determine the time 0 price of a forward. A forward contract consists of two components: Buying the asset at a fixed price ; akin to selling a Zero Coupon Bond Selling the asset at the future spot price ; akin to buying a Prepaid Forward \\[ \\begin{aligned} \\text{Price}_\\text{Long Forward} &= Ke^{-rt} \u2013 F_0^P \\\\ &= Ke^{-rt} \u2013 S_0 \\cdot e^{rT} \\end{aligned} \\] Note As its name suggests, a prepaid forward is simply a forward contract whose price is paid for at time 0 rather than at expiration. Intuitively, this time 0 price is simply the PV of the price that would have been paid at maturity: \\[ F^P_0 = \\text{PV}(F_0) \\] However, since the forward price depends on the underlying, the prepaid forward price depends on the underlying as well: Prepaid Forward Price Forward Price No Dividends \\(S_0\\) \\(S_0 \\cdot e^{rT}\\) Discrete Dividends \\(S_0 - \\text{PV Dividends}\\) \\(S_0 - \\text{AV Dividends}\\) Continuous Dividends \\(S_0 \\cdot e^{-qt}\\) \\(S_0 \\cdot e^{(r-q)t}\\) For simplicity, the no dividend stock was used in the expression above. Thus, following the law of one price, the price of the two portfolios must be equal: \\[ p_0 \\ \u2013 \\ c_0 = Ke^{-rt} \\ \u2013 \\ F_0^P \\]","title":"No Arbitrage Proof"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/3.%20Options/#synthetic-positions","text":"One practical application of the PCP equation is to easily identify and create replicating portfolios for each of its individual components. This can be done by simply re-arranging the equation to match the time 0 cashflow of the position we want to replicate: Position Replicating Portfolio Long Call \\(-c_0 = -p_0 + Ke^{-rt} - F^P_0\\) Long Put \\(-p_0 = -c_0 - Ke^{-rt} + F^P_0\\) Long ZC Bond \\(-Ke^{-rt} = -p_0 + c_0 - F^P_0\\) Long Stock \\(-F^P_0 = - S_0 = -c_0 - Ke^{-rt} + p_0\\) Note For a short selling position, simply re-arrange the equations to obtain a positive expression on the LHS.","title":"Synthetic Positions"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/3.%20Options/#risk-free-rate","text":"Another application of the PCP equation is to solve for the risk free rates . This can be done when PCP is given for multiple dates \\[ \\begin{align*} p_{0, T} \\ \u2013 \\ c_{0, T} &= Ke^{-rT} \u2013 S_0 \\\\ p_{t, T} \\ \u2013 \\ c_{t, T} &= Ke^{-r(T-t)} \u2013 S_t \\\\ \\\\ \\therefore e^{rt} &= \\frac{Ke^{-rT}}{Ke^{-r(T-t)}} \\\\ &= \\frac{ p_{0, T} \u2013 c_{o, T} + S_0 }{ p_{t, T} \u2013 c_{t, T} + S_t} \\end{align*} \\]","title":"Risk Free Rate"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/4.%20Option%20Strategies/","text":"","title":"4. Option Strategies"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/5.%20Binomial%20Model/","text":"Binomial Model \u00b6 This is not the original set of notes for IFM. It was recreated to fit the watered down option syllabus for exam FAM-S. Overview \u00b6 The simplest option pricing model is the Binomial Model . All option pricing models attempt to predict the price of the underlying at expiration and thus work backwards to determine the price of the option. As its name suggests, the binomial model assumes that in each period \\((h)\\) , the stock price can only move in two ways - Up \\((u)\\) or Down \\((d)\\) : \\[ \\begin{aligned} S_u &= S_0 \\cdot u \\\\ S_d &= S_0 \\cdot d \\end{aligned} \\] Warning It is a common misconception to think that up or down is used relative to the starting stock price. It is meant to be used relative to one another , thus both can be higher or lower than the starting price but still smaller than the other. \\[ S_u > S_d \\equiv u > d \\] However, it is impossible to know which of the two ways that the stock will take. Thus, the probability of going up is denoted as \\(p\\) while the probability of going down is its complement \\(1-p\\) . Given that there only two outcomes, it can easily illustrated via a probability tree , which is why they are also referred to as Binomial Trees : They can have a single period (as shown below) or multiple periods. Note that the duration of a single period is \\(h\\) while the duration of all periods is denoted as \\(t\\) . Single Period Trees \u00b6 The simplest Binomial Model assumes that there is only one period from inception to expiration . Though unrealistic, they allow us to easily understand the core concepts behind binomial trees. SP Replicating Portfolio Method \u00b6 The first way to use a binomial tree is through the Replicating Portfolio Method . As its name suggests, it attempts to create a replicating portfolio that will payoff of the option regardless of which way the stock moves . By the law of one price, the cost of the option is the cost of the portfolio . The replicating portfolio is assumed to consist of Stocks \\((\\delta)\\) and Bonds \\((B)\\) . The payoff of the options are denoted using \\(V\\) . The goal is to set up a simultaneous equation for each of the two ways that the price can move and thus solve for the \\(\\Delta\\) and \\(B\\) : \\[ \\begin{aligned} \\Delta S_0 u + B e^{rh} &= V_u \\\\ \\Delta S_0 d + B e^{rh} &= V_d \\end{aligned} \\] Warning Bonds are usually stated in terms of their face values, but in here \\(B\\) is the price at time 0 . Do not be confused! Subtracting the down scenario from the up scenario, \\[ \\begin{aligned} \\Delta S_0 u + B e^{rh} - (\\Delta S_0 d + B e^{rh}) &= V_u - V_d \\\\ \\Delta S_0 (u - d) &= V_u - V_d \\\\ \\Delta &= \\frac{V_u - V_d}{S_0 (u - d)} \\end{aligned} \\] Substituting it back in, \\[ \\begin{aligned} \\Delta S_0 u + B e^{rh} &= V_u \\\\ \\frac{V_u - V_d}{S_0 (u - d)} \\cdot S_0 u + B e^{rh} &= V_u \\\\ \\frac{V_u - V_d}{(u - d)} \\cdot u + B e^{rh} &= V_u \\\\ B e^{rh} &= \\frac{uV_d - dV_u}{u-d} \\\\ B &= e^{-rt} \\cdot \\frac{uV_d - dV_u}{u-d} \\end{aligned} \\] Thus, the price of the option is equal to the price of the portfolio: \\[ \\text{Option Price} = \\Delta \\cdot S_0 + B \\] Calls Puts Buy Stock \\((\\Delta \\gt 0)\\) Sell Stock ( \\(\\Delta \\lt 0\\) ) Sell Bonds/Borrow Money \\((B \\lt 0)\\) Buy Bonds/Lend money \\((B \\gt 0)\\) The intuition can be understood as follows: Buy Stock : Stock will be sold at maturity , leading to a positive terminal cashflow Sell Bonds : Pay back the bondholder at maurity, leading to a negative terminal cashflow Tip The key is remembering the signs of \\(\\delta\\) and \\(B\\) must be opposite. SP Risk Neutral Method \u00b6 Given the probabilities assigned earlier, the price of the option should be equal to the PV of the expected payoff : \\[ \\text{Option Price} = e^{-rt} \\cdot [p \\cdot V_u + (1-p) \\cdot V_d] \\] Similarly, the initial stock price should be the PV of the expected final stock prices , which will allow us to solve for \\(p\\) and hence calculate the option price: \\[ \\begin{aligned} S_0 &= e^{-rt} \\cdot [p \\cdot S_0 u + (1 \u2013 p) \\cdot S_0 d] \\\\ e^{rt} &= p \\cdot u + (1-p) \\cdot d \\\\ e^{rt} &= pu + d - pd \\\\ e^{rt} \u2013 d &= p(u \u2013 d) \\\\ p &= \\frac{e^{rt} - d}{u - d} \\\\ p &= \\frac{e^{rh} - d}{u - d} \\end{aligned} \\] Note Naturally, for a single period model, \\(h = t\\) . Generally speaking, this method is preferred to the replicating portfolio one as it is much faster - simply plug in the appropriate values into the formula. Info Since \\(p\\) is a probability, it MUST be between 0 and 1 . Any violation of this would lead to an opportunity for arbitrage: \\[ \\begin{aligned} 0 &< p < 1 \\\\ 0 &< \\frac{e^{rh} - d}{u - d} \\\\ 0 &< e^{rh} \u2013 d < u - d \\\\ d &< e^{rh} < u \\end{aligned} \\] This further drives home the point that \\(u\\) and \\(d\\) do not have to be above or below 1 respectively - they just need to be larger or smaller relative to one another. Multi Period Trees \u00b6 A slightly more realistic binomial tree is one that has multiple periods , as the stock prices tend to change many times before expiration. For simplicity, we will only consider a two period binomial tree , but the concepts easily extend to more periods. We can think of multi-period trees as a single period tree that repeats itself . As such, all factors \\((u, d, p)\\) remain constant throughout the tree . The price of the option can thus be found by recursively solving one node at a time using any of the above methods from the RHS till the starting node is found. Warning The length of a period is now different from the length of the entire option, thus take special care when discounting. MP Replicating Portfolio Method \u00b6 Unfortunately, there is no shortcut for the replicating portfolio method - the components of the portfolio can only be calculated recursively. Note that the intuition of the method changes slightly. The portfolio formed at time 0 is a self-financing portfolio that will be sold each period to exactly purchase a new portfolio at that time , such that the final portfolio leading up to expiration will replicate the payoff. Tip If both the future nodes have a value of 0, then the earlier node must be 0 as well. This can be used to quickly recurse through layers of trees. MP Risk Neutral Method \u00b6 Fortunately, there is a shortcut for the risk neutral method. For a two period model, there are three ending possibilities - Up Up, Up Down & Down Down. Instead of having to recursively solve for intermediate option prices, we can use the Binomial Distribution to directly solve for the overall probability of the ending node and hence immediately solve for the option price. The binomial distribution has two parameters: Number of trials : Number of periods Probability of Success : Probability of moving Up \\[ \\begin{aligned} X &\\sim \\text{Binomial}(n, p) \\\\ \\\\ dd &\\rightarrow K = 0 \\\\ ud &\\rightarrow K = 1 \\\\ uu &\\rightarrow K = 2 \\\\ \\\\ P(X = K) &= \\binom{n}{K} \\cdot p^K \\cdot (1 \u2013 p)^{n-k} \\end{aligned} \\] Warning The first term counts the number of ways that the ending node can be reached. The top and bottom node can only be reached in 1 way each while the middle node can be reached in 2 ways . It is a common mistake to forget to include the 2 for the middle node. It is not uncommon for questions to create binomial trees that have WAY more than 3 periods . For such questions, there is usually ONLY 1-2 ending branches with a payoff . They can be easily found by starting from the lowest or highest possible price, depending on the option. Tip In the off chance that ALL branches have a positive payoff, then the table function should be used to quickly calculate the expectation. Given this shortcut, the risk neutral method is once again preferred for solving multi-period problems, unless explicitly stated otherwise.","title":"Binomial Model"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/5.%20Binomial%20Model/#binomial-model","text":"This is not the original set of notes for IFM. It was recreated to fit the watered down option syllabus for exam FAM-S.","title":"Binomial Model"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/5.%20Binomial%20Model/#overview","text":"The simplest option pricing model is the Binomial Model . All option pricing models attempt to predict the price of the underlying at expiration and thus work backwards to determine the price of the option. As its name suggests, the binomial model assumes that in each period \\((h)\\) , the stock price can only move in two ways - Up \\((u)\\) or Down \\((d)\\) : \\[ \\begin{aligned} S_u &= S_0 \\cdot u \\\\ S_d &= S_0 \\cdot d \\end{aligned} \\] Warning It is a common misconception to think that up or down is used relative to the starting stock price. It is meant to be used relative to one another , thus both can be higher or lower than the starting price but still smaller than the other. \\[ S_u > S_d \\equiv u > d \\] However, it is impossible to know which of the two ways that the stock will take. Thus, the probability of going up is denoted as \\(p\\) while the probability of going down is its complement \\(1-p\\) . Given that there only two outcomes, it can easily illustrated via a probability tree , which is why they are also referred to as Binomial Trees : They can have a single period (as shown below) or multiple periods. Note that the duration of a single period is \\(h\\) while the duration of all periods is denoted as \\(t\\) .","title":"Overview"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/5.%20Binomial%20Model/#single-period-trees","text":"The simplest Binomial Model assumes that there is only one period from inception to expiration . Though unrealistic, they allow us to easily understand the core concepts behind binomial trees.","title":"Single Period Trees"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/5.%20Binomial%20Model/#sp-replicating-portfolio-method","text":"The first way to use a binomial tree is through the Replicating Portfolio Method . As its name suggests, it attempts to create a replicating portfolio that will payoff of the option regardless of which way the stock moves . By the law of one price, the cost of the option is the cost of the portfolio . The replicating portfolio is assumed to consist of Stocks \\((\\delta)\\) and Bonds \\((B)\\) . The payoff of the options are denoted using \\(V\\) . The goal is to set up a simultaneous equation for each of the two ways that the price can move and thus solve for the \\(\\Delta\\) and \\(B\\) : \\[ \\begin{aligned} \\Delta S_0 u + B e^{rh} &= V_u \\\\ \\Delta S_0 d + B e^{rh} &= V_d \\end{aligned} \\] Warning Bonds are usually stated in terms of their face values, but in here \\(B\\) is the price at time 0 . Do not be confused! Subtracting the down scenario from the up scenario, \\[ \\begin{aligned} \\Delta S_0 u + B e^{rh} - (\\Delta S_0 d + B e^{rh}) &= V_u - V_d \\\\ \\Delta S_0 (u - d) &= V_u - V_d \\\\ \\Delta &= \\frac{V_u - V_d}{S_0 (u - d)} \\end{aligned} \\] Substituting it back in, \\[ \\begin{aligned} \\Delta S_0 u + B e^{rh} &= V_u \\\\ \\frac{V_u - V_d}{S_0 (u - d)} \\cdot S_0 u + B e^{rh} &= V_u \\\\ \\frac{V_u - V_d}{(u - d)} \\cdot u + B e^{rh} &= V_u \\\\ B e^{rh} &= \\frac{uV_d - dV_u}{u-d} \\\\ B &= e^{-rt} \\cdot \\frac{uV_d - dV_u}{u-d} \\end{aligned} \\] Thus, the price of the option is equal to the price of the portfolio: \\[ \\text{Option Price} = \\Delta \\cdot S_0 + B \\] Calls Puts Buy Stock \\((\\Delta \\gt 0)\\) Sell Stock ( \\(\\Delta \\lt 0\\) ) Sell Bonds/Borrow Money \\((B \\lt 0)\\) Buy Bonds/Lend money \\((B \\gt 0)\\) The intuition can be understood as follows: Buy Stock : Stock will be sold at maturity , leading to a positive terminal cashflow Sell Bonds : Pay back the bondholder at maurity, leading to a negative terminal cashflow Tip The key is remembering the signs of \\(\\delta\\) and \\(B\\) must be opposite.","title":"SP Replicating Portfolio Method"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/5.%20Binomial%20Model/#sp-risk-neutral-method","text":"Given the probabilities assigned earlier, the price of the option should be equal to the PV of the expected payoff : \\[ \\text{Option Price} = e^{-rt} \\cdot [p \\cdot V_u + (1-p) \\cdot V_d] \\] Similarly, the initial stock price should be the PV of the expected final stock prices , which will allow us to solve for \\(p\\) and hence calculate the option price: \\[ \\begin{aligned} S_0 &= e^{-rt} \\cdot [p \\cdot S_0 u + (1 \u2013 p) \\cdot S_0 d] \\\\ e^{rt} &= p \\cdot u + (1-p) \\cdot d \\\\ e^{rt} &= pu + d - pd \\\\ e^{rt} \u2013 d &= p(u \u2013 d) \\\\ p &= \\frac{e^{rt} - d}{u - d} \\\\ p &= \\frac{e^{rh} - d}{u - d} \\end{aligned} \\] Note Naturally, for a single period model, \\(h = t\\) . Generally speaking, this method is preferred to the replicating portfolio one as it is much faster - simply plug in the appropriate values into the formula. Info Since \\(p\\) is a probability, it MUST be between 0 and 1 . Any violation of this would lead to an opportunity for arbitrage: \\[ \\begin{aligned} 0 &< p < 1 \\\\ 0 &< \\frac{e^{rh} - d}{u - d} \\\\ 0 &< e^{rh} \u2013 d < u - d \\\\ d &< e^{rh} < u \\end{aligned} \\] This further drives home the point that \\(u\\) and \\(d\\) do not have to be above or below 1 respectively - they just need to be larger or smaller relative to one another.","title":"SP Risk Neutral Method"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/5.%20Binomial%20Model/#multi-period-trees","text":"A slightly more realistic binomial tree is one that has multiple periods , as the stock prices tend to change many times before expiration. For simplicity, we will only consider a two period binomial tree , but the concepts easily extend to more periods. We can think of multi-period trees as a single period tree that repeats itself . As such, all factors \\((u, d, p)\\) remain constant throughout the tree . The price of the option can thus be found by recursively solving one node at a time using any of the above methods from the RHS till the starting node is found. Warning The length of a period is now different from the length of the entire option, thus take special care when discounting.","title":"Multi Period Trees"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/5.%20Binomial%20Model/#mp-replicating-portfolio-method","text":"Unfortunately, there is no shortcut for the replicating portfolio method - the components of the portfolio can only be calculated recursively. Note that the intuition of the method changes slightly. The portfolio formed at time 0 is a self-financing portfolio that will be sold each period to exactly purchase a new portfolio at that time , such that the final portfolio leading up to expiration will replicate the payoff. Tip If both the future nodes have a value of 0, then the earlier node must be 0 as well. This can be used to quickly recurse through layers of trees.","title":"MP Replicating Portfolio Method"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/5.%20Binomial%20Model/#mp-risk-neutral-method","text":"Fortunately, there is a shortcut for the risk neutral method. For a two period model, there are three ending possibilities - Up Up, Up Down & Down Down. Instead of having to recursively solve for intermediate option prices, we can use the Binomial Distribution to directly solve for the overall probability of the ending node and hence immediately solve for the option price. The binomial distribution has two parameters: Number of trials : Number of periods Probability of Success : Probability of moving Up \\[ \\begin{aligned} X &\\sim \\text{Binomial}(n, p) \\\\ \\\\ dd &\\rightarrow K = 0 \\\\ ud &\\rightarrow K = 1 \\\\ uu &\\rightarrow K = 2 \\\\ \\\\ P(X = K) &= \\binom{n}{K} \\cdot p^K \\cdot (1 \u2013 p)^{n-k} \\end{aligned} \\] Warning The first term counts the number of ways that the ending node can be reached. The top and bottom node can only be reached in 1 way each while the middle node can be reached in 2 ways . It is a common mistake to forget to include the 2 for the middle node. It is not uncommon for questions to create binomial trees that have WAY more than 3 periods . For such questions, there is usually ONLY 1-2 ending branches with a payoff . They can be easily found by starting from the lowest or highest possible price, depending on the option. Tip In the off chance that ALL branches have a positive payoff, then the table function should be used to quickly calculate the expectation. Given this shortcut, the risk neutral method is once again preferred for solving multi-period problems, unless explicitly stated otherwise.","title":"MP Risk Neutral Method"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/6.%20Black%20Scholes%20Model/","text":"Black Scholes Model \u00b6 This is not the original set of notes for IFM. It was recreated to fit the watered down option syllabus for exam FAM-S. Lognormal Returns \u00b6 Consider the growth of a dividend paying stock over time: \\[ \\begin{aligned} S_0 \\cdot e^{(r-q)t} &= S_t \\\\ e^{(r-q)t} &= \\frac{S_t}{S_0} \\\\ (r-q)t &= \\ln \\frac{S_t}{S_0} \\end{aligned} \\] \\(r\\) is the annual return of the stock. We typically assume that it is Normally Distributed . \\[ \\begin{aligned} r &\\sim N(\\mu, \\sigma^2) \\\\ rt &\\sim N(\\mu t, \\sigma^2 t) \\end{aligned} \\] Note This is because the overall return can be decomposed into the returns of infinitely many non-overlapping smaller periods ; the combination of them is normally distributed based on the central limit theorem. Since \\(qt\\) is a constant, then \\((r-q)t\\) is normally distributed as well: \\[ \\begin{aligned} (r - q)t &\\sim N(\\mu t, \\sigma^2 t) \\\\ \\ln \\frac{S_t}{S_0} &\\sim N(\\mu t, \\sigma^2 t) \\\\ \\frac{S_t}{S_0} &\\sim \\text{Log Normal}(\\mu t, \\sigma^2 t) \\\\ \\therefore S_t &\\sim \\text{Log Normal}(\\mu t, \\sigma^2 t) \\end{aligned} \\] Let \\(\\alpha\\) denote the expected capital gain of the stock: \\[ \\begin{aligned} E(S_t) &= S_0 \\cdot e^{(\\alpha - q)t} \\\\ S_0 \\cdot e^{\\mu t + \\frac{1}{2} \\sigma^2 t} &= S_0 \\cdot e^{(\\alpha - q)t} \\\\ e^{\\mu t + \\frac{1}{2} \\sigma^2 t} &= e^{(\\alpha - q)t} \\\\ \\mu + \\frac{1}{2} \\sigma^2 &= \\alpha - q \\\\ \\mu &= \\alpha - q - \\frac{1}{2} \\sigma^2 \\end{aligned} \\] Info \\(\\alpha - q\\) is referred to as the Expected Rate of Appreciation while \\(r-q\\) is referred to as the Cost of Carry . Thus, we can obtain the distribution for the stock price: \\[ \\begin{aligned} \\ln \\frac{S_t}{S_0} &\\sim N(\\mu t, \\sigma^2 t) \\\\ \\ln S_t - \\ln S_0 &\\sim N(\\mu t, \\sigma^2 t) \\\\ \\ln S_t &\\sim N(\\ln S_0 + \\mu t, \\sigma^2 t) \\\\ \\ln S_t &\\sim N \\left(\\ln S_0 + (\\alpha - q - \\frac{1}{2} \\sigma^2)t, \\sigma^2 t \\right) \\\\ S_t &\\sim \\text{Log Normal} \\left(\\ln S_0 + (\\alpha - q - \\frac{1}{2} \\sigma^2)t, \\sigma^2 t \\right) \\end{aligned} \\] Similarly, we can obtain the key properties of the distribution: \\[ \\begin{aligned} S_t &= S_0 \\cdot e^{(\\alpha - q - \\frac{1}{2} \\sigma^2)t + \\sigma \\sqrt{t} \\cdot Z} \\\\ \\\\ E(S_t) &= e^{\\ln S_0 + (\\alpha - q - \\frac{1}{2} \\sigma^2)t +\\frac{1}{2} \\sigma^2 t} \\\\ &= S_0 \\cdot e^{(\\alpha - q)t} \\\\ \\\\ \\text{Var} (S_t) &= [E(S_t)]^2 \\cdot \\left(e^{\\frac{1}{2} \\sigma^2t} - 1 \\right) \\end{aligned} \\] Black Scholes Formula \u00b6 Using the distribution of the stock prices, a formula for the price of an option can be determined. Call Options \u00b6 Similar to the binomial model, the price of an option is equal to the PV of its expected payoff . \\[ \\begin{aligned} \\text{Call Payoff} &= \\begin{cases} 0,& S_t \\lt K \\\\ S_t - K,& S_t \\gt K \\end{cases} \\\\ \\\\ E(\\text{Payoff}) &= E(0 \\mid S_t \\lt K) \\cdot P(S_t \\lt K) + E(S_t - K \\mid S_t \\gt K) \\cdot P(S_t \\gt K) \\\\ &= E(S_t - K \\mid S_t \\gt K) \\cdot P(S_t \\gt K) \\\\ &= [E(S_t \\mid S_t \\gt K) - K] \\cdot P(S_t \\gt K) \\end{aligned} \\] We can determine the probability from the distribution of the stock price: \\[ \\begin{aligned} P(S_t \\gt K) &= 1 - P(S_t \\lt K) \\\\ &= 1 - P \\left(\\frac{S_t}{S_0} \\lt \\frac{K}{S_0} \\right) \\\\ &= 1 - P \\left(\\ln \\frac{S_t}{S_0} \\lt \\ln \\frac{K}{S_0} \\right) \\\\ &= 1 - P \\left(Z \\lt \\frac{\\ln \\frac{K}{S_0} - E(S_t)}{\\sqrt{\\text{Var}(S_t)}} \\right) \\\\ &= 1 - P \\left(Z \\lt \\frac{\\ln \\frac{K}{S_0} - (\\alpha - q - \\frac{1}{2} \\sigma^2)t}{\\sigma \\sqrt{t}} \\right) \\\\ &= 1 - P \\left(Z \\lt \\frac{- \\ln \\frac{S_0}{K} - (\\alpha - q - \\frac{1}{2} \\sigma^2)t}{\\sigma \\sqrt{t}} \\right) \\\\ &= 1 - P \\left(Z \\lt - \\frac{\\ln \\frac{S_0}{K} + (\\alpha - q - \\frac{1}{2} \\sigma^2)t}{\\sigma \\sqrt{t}} \\right) \\\\ &= 1 - \\Phi(-\\hat{d_2}) \\\\ &= \\Phi(\\hat{d_2}) \\end{aligned} \\] Note Thus, \\(\\Phi(\\hat{d_2})\\) can be intepreted as the unconditional probability that the option is exercised at maturity. The conditional expectation must be found via integration. Skipping the proof, \\[ \\begin{aligned} E(S_t \\mid S_t \\gt K) &= S_0 \\cdot e^{(\\alpha - q)t} \\cdot \\frac{\\Phi(\\hat{d_1})}{\\Phi(\\hat{d_2})} \\\\ \\\\ \\hat{d_1} &= \\hat{d_2} + \\sigma \\sqrt{t} \\\\ &= \\frac{\\ln \\frac{S_0}{K} + (\\alpha - q - \\frac{1}{2} \\sigma^2)t}{\\sigma \\sqrt{t}} + \\sigma \\sqrt{t} \\\\ &= \\frac{\\ln \\frac{S_0}{K} + (\\alpha - q - \\frac{1}{2} \\sigma^2)t}{\\sigma \\sqrt{t}} + \\frac{\\sigma^2 t}{\\sigma \\sqrt{t}} \\\\ &= \\frac{\\ln \\frac{S_0}{K} + (\\alpha - q - \\frac{1}{2} \\sigma^2)t + \\sigma^2 t}{\\sigma \\sqrt{t}} \\\\ &= \\frac{\\ln \\frac{S_0}{K} + (\\alpha - q + \\frac{1}{2} \\sigma^2)t}{\\sigma \\sqrt{t}} \\end{aligned} \\] Warning It is a common mistake to mix up the formulas for \\(\\hat{d_1}\\) and \\(\\hat{d_2}\\) because they are extremely similar. Remember that \\(\\hat{d_1}\\) adds the variance while \\(\\hat{d_2}\\) subtracts it. Thus, the price of the call option is the PV of the expected payoff : \\[ \\begin{aligned} c_0 &= e^{-\\gamma t} \\cdot [E(S_t \\mid S_t \\gt K) - K] \\cdot P(S_t \\gt K) \\\\ &= e^{-\\gamma t} \\left[S_0 \\cdot e^{(\\alpha - q)t} \\cdot \\frac{\\Phi(\\hat{d_1})}{\\Phi(\\hat{d_2})} - K \\right] \\cdot \\Phi(\\hat{d_2}) \\\\ &= e^{-\\gamma t} \\cdot \\left[S_0 e^{(\\alpha - q)t} \\cdot \\Phi(\\hat{d_1}) - K \\Phi(\\hat{d_2}) \\right] \\end{aligned} \\] Put Options \u00b6 The same exercise can be repeated for puts, resulting in the following price: \\[ p_0 = e^{-\\gamma t} \\cdot \\left[K \\Phi(-\\hat{d_2}) - S_0 e^{(\\alpha - q)t} \\cdot \\Phi(-\\hat{d_1}) \\right] \\] Risk Neutrality \u00b6 Notice that BS formula, the following two terms are used: Alpha , \\(\\alpha\\) : Actual growth rate of stock Gamma , \\(\\gamma\\) : Actual growth rate of option In practice, it is impossible to know the value of these two quantities beforehand. However, if the investor is risk neutral, then they do not consider risk in their investment decisions. This means that these terms can be replaced by the risk free rate : \\[ \\begin{aligned} \\alpha &= \\gamma = r \\\\ \\\\ c_0 &= e^{-rt} \\cdot \\left[S_0 e^{(r - q)t} \\cdot \\Phi(d_1) - K \\Phi(d_2) \\right] \\\\ &= S_0 e^{- qt} \\cdot \\Phi(d_1) - Ke^{-rt} \\Phi(d_2) \\\\ \\\\ p_0 &= e^{-rt} \\cdot \\left[K \\Phi(d_2) - S_0 e^{(r - q)t} \\cdot \\Phi(-d_1) \\right] \\\\ &= Ke^{-rt} \\cdot \\Phi(-d_2) - S_0 e^{-qt} \\cdot \\Phi(-d_1) \\\\ \\\\ d_2 &= \\frac{\\ln \\frac{S_0}{K} + (r - q - \\frac{1}{2} \\sigma^2)t}{\\sigma \\sqrt{t}} \\\\ d_1 &= d_2 + \\sigma \\sqrt{t} \\\\ &= \\frac{\\ln \\frac{S_0}{K} + (r - q + \\frac{1}{2} \\sigma^2)t}{\\sigma \\sqrt{t}} \\end{aligned} \\] Note that the formula can also be linked to the binomial model's self replicating portfolio : Coefficient of \\(S_0\\) : Number of shares to Long/Short Other Term : Amount of money to Borrow/Lend Special Cases \u00b6 There are two special cases that helps to simplify the calculation of \\(d\\) : At the Money Equal Rates \\(S_0 = K\\) \\(r = q\\) \\(\\ln \\frac{S_0}{K} = 0\\) \\((r-q \\pm \\frac{1}{2} \\sigma^2) = \\pm \\frac{1}{2} \\sigma^2\\)","title":"Black Scholes Model"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/6.%20Black%20Scholes%20Model/#black-scholes-model","text":"This is not the original set of notes for IFM. It was recreated to fit the watered down option syllabus for exam FAM-S.","title":"Black Scholes Model"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/6.%20Black%20Scholes%20Model/#lognormal-returns","text":"Consider the growth of a dividend paying stock over time: \\[ \\begin{aligned} S_0 \\cdot e^{(r-q)t} &= S_t \\\\ e^{(r-q)t} &= \\frac{S_t}{S_0} \\\\ (r-q)t &= \\ln \\frac{S_t}{S_0} \\end{aligned} \\] \\(r\\) is the annual return of the stock. We typically assume that it is Normally Distributed . \\[ \\begin{aligned} r &\\sim N(\\mu, \\sigma^2) \\\\ rt &\\sim N(\\mu t, \\sigma^2 t) \\end{aligned} \\] Note This is because the overall return can be decomposed into the returns of infinitely many non-overlapping smaller periods ; the combination of them is normally distributed based on the central limit theorem. Since \\(qt\\) is a constant, then \\((r-q)t\\) is normally distributed as well: \\[ \\begin{aligned} (r - q)t &\\sim N(\\mu t, \\sigma^2 t) \\\\ \\ln \\frac{S_t}{S_0} &\\sim N(\\mu t, \\sigma^2 t) \\\\ \\frac{S_t}{S_0} &\\sim \\text{Log Normal}(\\mu t, \\sigma^2 t) \\\\ \\therefore S_t &\\sim \\text{Log Normal}(\\mu t, \\sigma^2 t) \\end{aligned} \\] Let \\(\\alpha\\) denote the expected capital gain of the stock: \\[ \\begin{aligned} E(S_t) &= S_0 \\cdot e^{(\\alpha - q)t} \\\\ S_0 \\cdot e^{\\mu t + \\frac{1}{2} \\sigma^2 t} &= S_0 \\cdot e^{(\\alpha - q)t} \\\\ e^{\\mu t + \\frac{1}{2} \\sigma^2 t} &= e^{(\\alpha - q)t} \\\\ \\mu + \\frac{1}{2} \\sigma^2 &= \\alpha - q \\\\ \\mu &= \\alpha - q - \\frac{1}{2} \\sigma^2 \\end{aligned} \\] Info \\(\\alpha - q\\) is referred to as the Expected Rate of Appreciation while \\(r-q\\) is referred to as the Cost of Carry . Thus, we can obtain the distribution for the stock price: \\[ \\begin{aligned} \\ln \\frac{S_t}{S_0} &\\sim N(\\mu t, \\sigma^2 t) \\\\ \\ln S_t - \\ln S_0 &\\sim N(\\mu t, \\sigma^2 t) \\\\ \\ln S_t &\\sim N(\\ln S_0 + \\mu t, \\sigma^2 t) \\\\ \\ln S_t &\\sim N \\left(\\ln S_0 + (\\alpha - q - \\frac{1}{2} \\sigma^2)t, \\sigma^2 t \\right) \\\\ S_t &\\sim \\text{Log Normal} \\left(\\ln S_0 + (\\alpha - q - \\frac{1}{2} \\sigma^2)t, \\sigma^2 t \\right) \\end{aligned} \\] Similarly, we can obtain the key properties of the distribution: \\[ \\begin{aligned} S_t &= S_0 \\cdot e^{(\\alpha - q - \\frac{1}{2} \\sigma^2)t + \\sigma \\sqrt{t} \\cdot Z} \\\\ \\\\ E(S_t) &= e^{\\ln S_0 + (\\alpha - q - \\frac{1}{2} \\sigma^2)t +\\frac{1}{2} \\sigma^2 t} \\\\ &= S_0 \\cdot e^{(\\alpha - q)t} \\\\ \\\\ \\text{Var} (S_t) &= [E(S_t)]^2 \\cdot \\left(e^{\\frac{1}{2} \\sigma^2t} - 1 \\right) \\end{aligned} \\]","title":"Lognormal Returns"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/6.%20Black%20Scholes%20Model/#black-scholes-formula","text":"Using the distribution of the stock prices, a formula for the price of an option can be determined.","title":"Black Scholes Formula"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/6.%20Black%20Scholes%20Model/#call-options","text":"Similar to the binomial model, the price of an option is equal to the PV of its expected payoff . \\[ \\begin{aligned} \\text{Call Payoff} &= \\begin{cases} 0,& S_t \\lt K \\\\ S_t - K,& S_t \\gt K \\end{cases} \\\\ \\\\ E(\\text{Payoff}) &= E(0 \\mid S_t \\lt K) \\cdot P(S_t \\lt K) + E(S_t - K \\mid S_t \\gt K) \\cdot P(S_t \\gt K) \\\\ &= E(S_t - K \\mid S_t \\gt K) \\cdot P(S_t \\gt K) \\\\ &= [E(S_t \\mid S_t \\gt K) - K] \\cdot P(S_t \\gt K) \\end{aligned} \\] We can determine the probability from the distribution of the stock price: \\[ \\begin{aligned} P(S_t \\gt K) &= 1 - P(S_t \\lt K) \\\\ &= 1 - P \\left(\\frac{S_t}{S_0} \\lt \\frac{K}{S_0} \\right) \\\\ &= 1 - P \\left(\\ln \\frac{S_t}{S_0} \\lt \\ln \\frac{K}{S_0} \\right) \\\\ &= 1 - P \\left(Z \\lt \\frac{\\ln \\frac{K}{S_0} - E(S_t)}{\\sqrt{\\text{Var}(S_t)}} \\right) \\\\ &= 1 - P \\left(Z \\lt \\frac{\\ln \\frac{K}{S_0} - (\\alpha - q - \\frac{1}{2} \\sigma^2)t}{\\sigma \\sqrt{t}} \\right) \\\\ &= 1 - P \\left(Z \\lt \\frac{- \\ln \\frac{S_0}{K} - (\\alpha - q - \\frac{1}{2} \\sigma^2)t}{\\sigma \\sqrt{t}} \\right) \\\\ &= 1 - P \\left(Z \\lt - \\frac{\\ln \\frac{S_0}{K} + (\\alpha - q - \\frac{1}{2} \\sigma^2)t}{\\sigma \\sqrt{t}} \\right) \\\\ &= 1 - \\Phi(-\\hat{d_2}) \\\\ &= \\Phi(\\hat{d_2}) \\end{aligned} \\] Note Thus, \\(\\Phi(\\hat{d_2})\\) can be intepreted as the unconditional probability that the option is exercised at maturity. The conditional expectation must be found via integration. Skipping the proof, \\[ \\begin{aligned} E(S_t \\mid S_t \\gt K) &= S_0 \\cdot e^{(\\alpha - q)t} \\cdot \\frac{\\Phi(\\hat{d_1})}{\\Phi(\\hat{d_2})} \\\\ \\\\ \\hat{d_1} &= \\hat{d_2} + \\sigma \\sqrt{t} \\\\ &= \\frac{\\ln \\frac{S_0}{K} + (\\alpha - q - \\frac{1}{2} \\sigma^2)t}{\\sigma \\sqrt{t}} + \\sigma \\sqrt{t} \\\\ &= \\frac{\\ln \\frac{S_0}{K} + (\\alpha - q - \\frac{1}{2} \\sigma^2)t}{\\sigma \\sqrt{t}} + \\frac{\\sigma^2 t}{\\sigma \\sqrt{t}} \\\\ &= \\frac{\\ln \\frac{S_0}{K} + (\\alpha - q - \\frac{1}{2} \\sigma^2)t + \\sigma^2 t}{\\sigma \\sqrt{t}} \\\\ &= \\frac{\\ln \\frac{S_0}{K} + (\\alpha - q + \\frac{1}{2} \\sigma^2)t}{\\sigma \\sqrt{t}} \\end{aligned} \\] Warning It is a common mistake to mix up the formulas for \\(\\hat{d_1}\\) and \\(\\hat{d_2}\\) because they are extremely similar. Remember that \\(\\hat{d_1}\\) adds the variance while \\(\\hat{d_2}\\) subtracts it. Thus, the price of the call option is the PV of the expected payoff : \\[ \\begin{aligned} c_0 &= e^{-\\gamma t} \\cdot [E(S_t \\mid S_t \\gt K) - K] \\cdot P(S_t \\gt K) \\\\ &= e^{-\\gamma t} \\left[S_0 \\cdot e^{(\\alpha - q)t} \\cdot \\frac{\\Phi(\\hat{d_1})}{\\Phi(\\hat{d_2})} - K \\right] \\cdot \\Phi(\\hat{d_2}) \\\\ &= e^{-\\gamma t} \\cdot \\left[S_0 e^{(\\alpha - q)t} \\cdot \\Phi(\\hat{d_1}) - K \\Phi(\\hat{d_2}) \\right] \\end{aligned} \\]","title":"Call Options"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/6.%20Black%20Scholes%20Model/#put-options","text":"The same exercise can be repeated for puts, resulting in the following price: \\[ p_0 = e^{-\\gamma t} \\cdot \\left[K \\Phi(-\\hat{d_2}) - S_0 e^{(\\alpha - q)t} \\cdot \\Phi(-\\hat{d_1}) \\right] \\]","title":"Put Options"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/6.%20Black%20Scholes%20Model/#risk-neutrality","text":"Notice that BS formula, the following two terms are used: Alpha , \\(\\alpha\\) : Actual growth rate of stock Gamma , \\(\\gamma\\) : Actual growth rate of option In practice, it is impossible to know the value of these two quantities beforehand. However, if the investor is risk neutral, then they do not consider risk in their investment decisions. This means that these terms can be replaced by the risk free rate : \\[ \\begin{aligned} \\alpha &= \\gamma = r \\\\ \\\\ c_0 &= e^{-rt} \\cdot \\left[S_0 e^{(r - q)t} \\cdot \\Phi(d_1) - K \\Phi(d_2) \\right] \\\\ &= S_0 e^{- qt} \\cdot \\Phi(d_1) - Ke^{-rt} \\Phi(d_2) \\\\ \\\\ p_0 &= e^{-rt} \\cdot \\left[K \\Phi(d_2) - S_0 e^{(r - q)t} \\cdot \\Phi(-d_1) \\right] \\\\ &= Ke^{-rt} \\cdot \\Phi(-d_2) - S_0 e^{-qt} \\cdot \\Phi(-d_1) \\\\ \\\\ d_2 &= \\frac{\\ln \\frac{S_0}{K} + (r - q - \\frac{1}{2} \\sigma^2)t}{\\sigma \\sqrt{t}} \\\\ d_1 &= d_2 + \\sigma \\sqrt{t} \\\\ &= \\frac{\\ln \\frac{S_0}{K} + (r - q + \\frac{1}{2} \\sigma^2)t}{\\sigma \\sqrt{t}} \\end{aligned} \\] Note that the formula can also be linked to the binomial model's self replicating portfolio : Coefficient of \\(S_0\\) : Number of shares to Long/Short Other Term : Amount of money to Borrow/Lend","title":"Risk Neutrality"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/6.%20Black%20Scholes%20Model/#special-cases","text":"There are two special cases that helps to simplify the calculation of \\(d\\) : At the Money Equal Rates \\(S_0 = K\\) \\(r = q\\) \\(\\ln \\frac{S_0}{K} = 0\\) \\((r-q \\pm \\frac{1}{2} \\sigma^2) = \\pm \\frac{1}{2} \\sigma^2\\)","title":"Special Cases"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/7.%20Option%20Greeks/","text":"","title":"7. Option Greeks"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/8.%20Exotics/","text":"","title":"8. Exotics"},{"location":"1.%20Introductory/3.%20ASA-IFM/1.%20Derivatives/9.%20Embedded%20Options/","text":"","title":"9. Embedded Options"},{"location":"1.%20Introductory/3.%20ASA-IFM/2.%20Financial%20Markets/1.%20Modern%20Portfolio%20Theory/","text":"","title":"1. Modern Portfolio Theory"},{"location":"1.%20Introductory/3.%20ASA-IFM/2.%20Financial%20Markets/2.%20Asset%20Pricing%20Models/","text":"","title":"2. Asset Pricing Models"},{"location":"1.%20Introductory/3.%20ASA-IFM/2.%20Financial%20Markets/3.%20Efficient%20Market%20Hypothesis/","text":"","title":"3. Efficient Market Hypothesis"},{"location":"1.%20Introductory/3.%20ASA-IFM/3.%20Corporate%20Finance/1.%20Raising%20Capital/","text":"","title":"1. Raising Capital"},{"location":"1.%20Introductory/3.%20ASA-IFM/3.%20Corporate%20Finance/2.%20Cost%20of%20Capital/","text":"","title":"2. Cost of Capital"},{"location":"1.%20Introductory/3.%20ASA-IFM/3.%20Corporate%20Finance/3.%20Risk%20Measures/","text":"Risk Measures \u00b6 Value at Risk \u00b6 The Value at Risk (VaR) of a random variable is simply the percentile of its distribution. It might seem redundant, but it is simply a business application of the mathematical concept of percentiles. \\[ \\text{VaR}_{p}(X) = F^{-1}(p) \\] Warning It is a common mistake when in a rush to confuse Var and VaR together as they look extremely similar. Always read the question properly! Its intepretation changes depending on what the distribution is measuring: Loss Distribution : Large \\(p\\) , probability of obtaining a loss larger than VaR is \\((1-p)\\) Gain Distribution : Small \\(p\\) , probability of obtaining a return smaller than VaR is \\(p\\) Both define the thresholds for a \"bad\" situation For the remainder of this section, whenever we reference VaR, we refer to a loss distribution unless stated otherwise. Tail Value at Risk \u00b6 The Tail Value at Risk (TVaR) of a random variable is the expected value of the distribution, given that the distribution exceeds the VaR. This is why it is also known as the Conditional Tail Expectation (CTE). It is intepreted as the average gain/loss if the amount is smaller/larger than the VaR; if the threshold is crossed. \\[ \\begin{aligned} \\text{TVaR}(X) &= E[X \\mid X \\gt \\text{VaR}_{p}(X)] \\\\ &= \\frac{\\int^{\\infty}_{\\text{VaR}_{p}(X)} x \\cdot f(x)}{P(X \\gt \\text{VaR}_{p}(X))} \\\\ \\end{aligned} \\] Alternatively, it can be simplified by making use of the Excess Loss Variable from exam FAM-S: \\[ \\begin{aligned} \\text{TVaR}(X) &= E[X \\mid X \\gt \\text{VaR}_{p}(X)] \\\\ &= E[X - \\text{VaR}_{p}(X) \\mid X \\gt \\text{VaR}_{p}(X)] + \\text{VaR}_{p}(X) \\\\ &= \\frac{E(X) - E[X \\land \\text{VaR}_{p}(X)]}{P[X \\gt \\text{VaR}_{p}(X)]} + \\text{VaR}_{p}(X) \\end{aligned} \\] This formula is not only convenient because there is no need for integration, but because a formula for the limited loss variable is provided in the formula sheets. Note Recall that for a policy with a deductible \\(d\\) , the payment of the insurer \\(Y^P\\) is represented as: \\[ \\begin{aligned} Y^P &= Y^L \\mid Y^L \\gt 0 \\\\ &= X - d \\mid X \\gt d \\end{aligned} \\] The expectation can is calculated as: \\[ \\begin{aligned} E \\left(Y^P \\right) &= \\frac{E \\left(Y^L \\right)}{P(X \\gt d)} \\\\ &= \\frac{E(X) - E(X \\land d)}{P(X \\gt d)} \\end{aligned} \\] Coherence \u00b6 One way of determining whether a risk measure is \"good\" is to consider its Coherence . It has four key properties: The first is Translation Invariance . Linearly increasing the underlying risk linearly increases the risk measure by the same amount. \\[ \\rho(X+c) = \\rho(X) + c \\] The second is Positive Homogenity . Scaling the underlying risk scales the risk measure by the same amount. \\[ rho(cX) = c\\rho(X) \\] The third is Subadditivity . Its mathematical definition is that the combination should be less than the sum of its parts . In other words, it should recognise diversification benefits of multiple risks. \\[ \\rho(X + Y) \\le \\rho(X) + \\rho(Y) \\] Tip Recall the saying - \"The whole is greater than the sum of its parts\". Subadditivity is the opposite! The last is known as Monotonicity . Its mathematical definition is that it preserves the order of values . If one quantity is smaller than another, then its corresponding risk measure should be smaller as well. \\[ \\begin{aligned} X &\\le Y \\\\ \\rho(X) &\\le \\rho(Y) \\end{aligned} \\] The coherence of the most common risk measures can be found below: Coherence Mean Variance VaR TVaR Translation Invariance Yes No Yes Yes Positive Homogenity Yes No Yes Yes Subadditivity Yes No No Yes Monotonicity Yes No Yes Yes Verdict Coherent Not Coherent Not Coherent Coherent It is advised to only memorize the results for VaR and TVaR, as the results for mean and variance can be easily determined via first principles.","title":"Risk Measures"},{"location":"1.%20Introductory/3.%20ASA-IFM/3.%20Corporate%20Finance/3.%20Risk%20Measures/#risk-measures","text":"","title":"Risk Measures"},{"location":"1.%20Introductory/3.%20ASA-IFM/3.%20Corporate%20Finance/3.%20Risk%20Measures/#value-at-risk","text":"The Value at Risk (VaR) of a random variable is simply the percentile of its distribution. It might seem redundant, but it is simply a business application of the mathematical concept of percentiles. \\[ \\text{VaR}_{p}(X) = F^{-1}(p) \\] Warning It is a common mistake when in a rush to confuse Var and VaR together as they look extremely similar. Always read the question properly! Its intepretation changes depending on what the distribution is measuring: Loss Distribution : Large \\(p\\) , probability of obtaining a loss larger than VaR is \\((1-p)\\) Gain Distribution : Small \\(p\\) , probability of obtaining a return smaller than VaR is \\(p\\) Both define the thresholds for a \"bad\" situation For the remainder of this section, whenever we reference VaR, we refer to a loss distribution unless stated otherwise.","title":"Value at Risk"},{"location":"1.%20Introductory/3.%20ASA-IFM/3.%20Corporate%20Finance/3.%20Risk%20Measures/#tail-value-at-risk","text":"The Tail Value at Risk (TVaR) of a random variable is the expected value of the distribution, given that the distribution exceeds the VaR. This is why it is also known as the Conditional Tail Expectation (CTE). It is intepreted as the average gain/loss if the amount is smaller/larger than the VaR; if the threshold is crossed. \\[ \\begin{aligned} \\text{TVaR}(X) &= E[X \\mid X \\gt \\text{VaR}_{p}(X)] \\\\ &= \\frac{\\int^{\\infty}_{\\text{VaR}_{p}(X)} x \\cdot f(x)}{P(X \\gt \\text{VaR}_{p}(X))} \\\\ \\end{aligned} \\] Alternatively, it can be simplified by making use of the Excess Loss Variable from exam FAM-S: \\[ \\begin{aligned} \\text{TVaR}(X) &= E[X \\mid X \\gt \\text{VaR}_{p}(X)] \\\\ &= E[X - \\text{VaR}_{p}(X) \\mid X \\gt \\text{VaR}_{p}(X)] + \\text{VaR}_{p}(X) \\\\ &= \\frac{E(X) - E[X \\land \\text{VaR}_{p}(X)]}{P[X \\gt \\text{VaR}_{p}(X)]} + \\text{VaR}_{p}(X) \\end{aligned} \\] This formula is not only convenient because there is no need for integration, but because a formula for the limited loss variable is provided in the formula sheets. Note Recall that for a policy with a deductible \\(d\\) , the payment of the insurer \\(Y^P\\) is represented as: \\[ \\begin{aligned} Y^P &= Y^L \\mid Y^L \\gt 0 \\\\ &= X - d \\mid X \\gt d \\end{aligned} \\] The expectation can is calculated as: \\[ \\begin{aligned} E \\left(Y^P \\right) &= \\frac{E \\left(Y^L \\right)}{P(X \\gt d)} \\\\ &= \\frac{E(X) - E(X \\land d)}{P(X \\gt d)} \\end{aligned} \\]","title":"Tail Value at Risk"},{"location":"1.%20Introductory/3.%20ASA-IFM/3.%20Corporate%20Finance/3.%20Risk%20Measures/#coherence","text":"One way of determining whether a risk measure is \"good\" is to consider its Coherence . It has four key properties: The first is Translation Invariance . Linearly increasing the underlying risk linearly increases the risk measure by the same amount. \\[ \\rho(X+c) = \\rho(X) + c \\] The second is Positive Homogenity . Scaling the underlying risk scales the risk measure by the same amount. \\[ rho(cX) = c\\rho(X) \\] The third is Subadditivity . Its mathematical definition is that the combination should be less than the sum of its parts . In other words, it should recognise diversification benefits of multiple risks. \\[ \\rho(X + Y) \\le \\rho(X) + \\rho(Y) \\] Tip Recall the saying - \"The whole is greater than the sum of its parts\". Subadditivity is the opposite! The last is known as Monotonicity . Its mathematical definition is that it preserves the order of values . If one quantity is smaller than another, then its corresponding risk measure should be smaller as well. \\[ \\begin{aligned} X &\\le Y \\\\ \\rho(X) &\\le \\rho(Y) \\end{aligned} \\] The coherence of the most common risk measures can be found below: Coherence Mean Variance VaR TVaR Translation Invariance Yes No Yes Yes Positive Homogenity Yes No Yes Yes Subadditivity Yes No No Yes Monotonicity Yes No Yes Yes Verdict Coherent Not Coherent Not Coherent Coherent It is advised to only memorize the results for VaR and TVaR, as the results for mean and variance can be easily determined via first principles.","title":"Coherence"},{"location":"1.%20Introductory/3.%20ASA-IFM/3.%20Corporate%20Finance/4.%20Decision%20Making/","text":"","title":"4. Decision Making"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/0.%20Short%20Term%20Insurance/","text":"Short Term Insurance \u00b6 General Insurance \u00b6 Automobile Insurance \u00b6 Homeowners Insurance \u00b6 They typically have a Coinsurance Provision that penalizes the insured for under-insuring their house. Warning This is a completely different concept from a regular Coinsurance modification -- do not confuse the two. If the chosen sum insured \\((I)\\) is less than some proportion of the value of the house \\((cF)\\) , then the benefit paid out is scaled down by that proportion . Note If the sum insured is equal or more than the coinsurance provision, the benefit does not scale up . It is meant to be only a penalty. \\[ \\begin{aligned} Y &= \\begin{cases} \\min (I, \\frac{I}{cF} \\cdot L),& I \\lt cF \\\\ \\min(I, L),& I \\lt cF \\end{cases} \\end{aligned} \\] The coinsurance provision only affects the \"raw\" benefits. If the penalized loss still exceeds the insured amount , then only the insured amount will be paid. Workers Compensation \u00b6 Fire Insurance \u00b6 Marine Insurance \u00b6 Liability Insurance \u00b6 Health Insurance \u00b6 Major Medical \u00b6 Dental Insurance \u00b6 Unlike major medical, Dental Insurance is typically part of Group Insurance . The main purpose of insurance is to share large infrequent costs among a large number of policyholders. However, dental costs are usually small and frequent, thus there is little value in individual dental insurance. There are four types of dental coverage: I, Diagnostic & Preventive : Oral Examinations & Cleaning II, Basic Services : Extraction & Resotration III, Prostethics : Crowns & Inlays IV, Orthodontia : Braces & Retainers Affordable Healthcare Act \u00b6 Reinsurance \u00b6 Insurers can also purchase insurance for themselves, known as Reinsurance . Reinsurers can also purchase insurance for themselves, known as Retrocession . Info This process of an insurer buying insurance for themselves is known as Ceding their risk to another insurer. Thus, a reinsurer passing on the ceded risk to another is known as Retroceding . There are two broad ways that reinsurance can be categorised. The first way that they can be categorised are based on which risks are ceded under the reinsurance: Treaty - All risks under a line of business Facultative - Specific risks chosen by the reinsurer The second category is based on how much of these risks are covered -- Proportional or Excess of Loss , which will be covered in the following two sections. Proportional Reinsurance \u00b6 As its name suggests, proportional reinsurance provides coverage for losses in a fixed proportion . The most basic form is known as Quota Share , where the insurer and reinsurer share a proportion of total risk \\((\\alpha)\\) . For instance, the insurer may pay 30% of losses while the reinsurer covers the remaining 70%. Questions can also ask for the expected payment of either the insurer or reinsurer , which can be found mathematically if given the severity distribution: \\[ \\begin{aligned} Y_{\\text{Insurer}} = (1-\\alpha) \\cdot X \\\\ \\\\ Y_{\\text{Reinsurer}} = \\alpha \\cdot X \\end{aligned} \\] Note Some questions may not be very explicit about the proportions. Thus, if only one percentage is mentioned, then it is assumed to be the reinsurer's portion . For instance, \"70% Quota Share\" means that the reinsurer covers 70% of losses. The other form is known as Surplus Share . The insurer covers 100% of the losses up till a certain threshold , known as the Retention Limit \\((\\delta)\\) . Above this level , the insurer and reinsurer share the cost in some proportion like a quota share . \\[ \\begin{aligned} Y_{\\text{Insurer}} &= \\begin{cases} X,& X \\lt \\delta \\\\ (1-\\alpha) \\cdot (X - \\delta) + \\delta,& X \\lt \\delta \\\\ \\end{cases} \\\\ \\\\ Y_{\\text{Reinsurer}} &= \\begin{cases} 0,& X \\lt \\delta \\\\ \\alpha \\cdot (X - \\delta),& X \\lt \\delta \\\\ \\end{cases} \\end{aligned} \\] The sharing can also come in multiple layers , where the amount shared Retention Limit : $1,000,000 Layer 1 : 70% of $500,000 in excess of $1,000,000 Layer 2 : 80% of $500,000 in excess of $1,500,000 Layer 3 : 90% in excess of $2,000,000 Excess of Loss \u00b6 Excess of Loss (XOL) reinsurance provides coverage for the insurer past a certain threshold. In its simplest form, it can be thought of as a Surplus Share where the reinsurer covers 100% of the costs past the retention limit. \\[ \\begin{aligned} Y_{\\text{Insurer}} &= \\begin{cases} X,& X \\lt \\delta \\\\ \\delta,& X \\lt \\delta \\\\ \\end{cases} \\\\ \\\\ Y_{\\text{Reinsurer}} &= \\begin{cases} 0,& X \\lt \\delta \\\\ X - \\delta,& X \\lt \\delta \\\\ \\end{cases} \\end{aligned} \\] Note This is functionally the same as a deductible that is applied to primary policies. There are three different types of XOL reinsurance, based on how the losses are computed: Per Risk : Covers the losses on one policy Per Occurrence : Covers the losses from one occurrence that impacts multiple policies (EG. Earthquake) Aggregate Loss : Covers ALL the losses within a specified period Note When applied on aggregate losses, it is usually referred to as a Stop Loss Reinsurance , as it limits the losses of the insurer to just the retention amount.","title":"Short Term Insurance"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/0.%20Short%20Term%20Insurance/#short-term-insurance","text":"","title":"Short Term Insurance"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/0.%20Short%20Term%20Insurance/#general-insurance","text":"","title":"General Insurance"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/0.%20Short%20Term%20Insurance/#automobile-insurance","text":"","title":"Automobile Insurance"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/0.%20Short%20Term%20Insurance/#homeowners-insurance","text":"They typically have a Coinsurance Provision that penalizes the insured for under-insuring their house. Warning This is a completely different concept from a regular Coinsurance modification -- do not confuse the two. If the chosen sum insured \\((I)\\) is less than some proportion of the value of the house \\((cF)\\) , then the benefit paid out is scaled down by that proportion . Note If the sum insured is equal or more than the coinsurance provision, the benefit does not scale up . It is meant to be only a penalty. \\[ \\begin{aligned} Y &= \\begin{cases} \\min (I, \\frac{I}{cF} \\cdot L),& I \\lt cF \\\\ \\min(I, L),& I \\lt cF \\end{cases} \\end{aligned} \\] The coinsurance provision only affects the \"raw\" benefits. If the penalized loss still exceeds the insured amount , then only the insured amount will be paid.","title":"Homeowners Insurance"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/0.%20Short%20Term%20Insurance/#workers-compensation","text":"","title":"Workers Compensation"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/0.%20Short%20Term%20Insurance/#fire-insurance","text":"","title":"Fire Insurance"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/0.%20Short%20Term%20Insurance/#marine-insurance","text":"","title":"Marine Insurance"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/0.%20Short%20Term%20Insurance/#liability-insurance","text":"","title":"Liability Insurance"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/0.%20Short%20Term%20Insurance/#health-insurance","text":"","title":"Health Insurance"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/0.%20Short%20Term%20Insurance/#major-medical","text":"","title":"Major Medical"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/0.%20Short%20Term%20Insurance/#dental-insurance","text":"Unlike major medical, Dental Insurance is typically part of Group Insurance . The main purpose of insurance is to share large infrequent costs among a large number of policyholders. However, dental costs are usually small and frequent, thus there is little value in individual dental insurance. There are four types of dental coverage: I, Diagnostic & Preventive : Oral Examinations & Cleaning II, Basic Services : Extraction & Resotration III, Prostethics : Crowns & Inlays IV, Orthodontia : Braces & Retainers","title":"Dental Insurance"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/0.%20Short%20Term%20Insurance/#affordable-healthcare-act","text":"","title":"Affordable Healthcare Act"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/0.%20Short%20Term%20Insurance/#reinsurance","text":"Insurers can also purchase insurance for themselves, known as Reinsurance . Reinsurers can also purchase insurance for themselves, known as Retrocession . Info This process of an insurer buying insurance for themselves is known as Ceding their risk to another insurer. Thus, a reinsurer passing on the ceded risk to another is known as Retroceding . There are two broad ways that reinsurance can be categorised. The first way that they can be categorised are based on which risks are ceded under the reinsurance: Treaty - All risks under a line of business Facultative - Specific risks chosen by the reinsurer The second category is based on how much of these risks are covered -- Proportional or Excess of Loss , which will be covered in the following two sections.","title":"Reinsurance"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/0.%20Short%20Term%20Insurance/#proportional-reinsurance","text":"As its name suggests, proportional reinsurance provides coverage for losses in a fixed proportion . The most basic form is known as Quota Share , where the insurer and reinsurer share a proportion of total risk \\((\\alpha)\\) . For instance, the insurer may pay 30% of losses while the reinsurer covers the remaining 70%. Questions can also ask for the expected payment of either the insurer or reinsurer , which can be found mathematically if given the severity distribution: \\[ \\begin{aligned} Y_{\\text{Insurer}} = (1-\\alpha) \\cdot X \\\\ \\\\ Y_{\\text{Reinsurer}} = \\alpha \\cdot X \\end{aligned} \\] Note Some questions may not be very explicit about the proportions. Thus, if only one percentage is mentioned, then it is assumed to be the reinsurer's portion . For instance, \"70% Quota Share\" means that the reinsurer covers 70% of losses. The other form is known as Surplus Share . The insurer covers 100% of the losses up till a certain threshold , known as the Retention Limit \\((\\delta)\\) . Above this level , the insurer and reinsurer share the cost in some proportion like a quota share . \\[ \\begin{aligned} Y_{\\text{Insurer}} &= \\begin{cases} X,& X \\lt \\delta \\\\ (1-\\alpha) \\cdot (X - \\delta) + \\delta,& X \\lt \\delta \\\\ \\end{cases} \\\\ \\\\ Y_{\\text{Reinsurer}} &= \\begin{cases} 0,& X \\lt \\delta \\\\ \\alpha \\cdot (X - \\delta),& X \\lt \\delta \\\\ \\end{cases} \\end{aligned} \\] The sharing can also come in multiple layers , where the amount shared Retention Limit : $1,000,000 Layer 1 : 70% of $500,000 in excess of $1,000,000 Layer 2 : 80% of $500,000 in excess of $1,500,000 Layer 3 : 90% in excess of $2,000,000","title":"Proportional Reinsurance"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/0.%20Short%20Term%20Insurance/#excess-of-loss","text":"Excess of Loss (XOL) reinsurance provides coverage for the insurer past a certain threshold. In its simplest form, it can be thought of as a Surplus Share where the reinsurer covers 100% of the costs past the retention limit. \\[ \\begin{aligned} Y_{\\text{Insurer}} &= \\begin{cases} X,& X \\lt \\delta \\\\ \\delta,& X \\lt \\delta \\\\ \\end{cases} \\\\ \\\\ Y_{\\text{Reinsurer}} &= \\begin{cases} 0,& X \\lt \\delta \\\\ X - \\delta,& X \\lt \\delta \\\\ \\end{cases} \\end{aligned} \\] Note This is functionally the same as a deductible that is applied to primary policies. There are three different types of XOL reinsurance, based on how the losses are computed: Per Risk : Covers the losses on one policy Per Occurrence : Covers the losses from one occurrence that impacts multiple policies (EG. Earthquake) Aggregate Loss : Covers ALL the losses within a specified period Note When applied on aggregate losses, it is usually referred to as a Stop Loss Reinsurance , as it limits the losses of the insurer to just the retention amount.","title":"Excess of Loss"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/","text":"Review of Probability Theory \u00b6 Basic Probability \u00b6 Probability is the study of Experiments whose results cannot be predicted with certainty. The result of such an experiment is known as its Outcome . The Sample Space \\(\\left(\\Omega \\right)\\) is the set of ALL possible outcomes from an experiment. The Event Space \\((E)\\) is a subset of the sample space, representing only the outcomes that we are interested in studying. Conversely, its Complement \\((E^c)\\) is the set of all OTHER outcomes not inside \\(E\\) . The probability of the event occuring is the ratio of the number of elements in the event to the sample space. It is a measure of the chance that the outcome of the experiment is inside the event space. \\[ P(E) = \\frac{n(E)}{n\\left(\\Omega \\right)} \\] Consider the probability of rolling an odd number on a standard dice: Experiment - Rolling a dice Outcome - The number showed on the dice Sample Space - \\({1, 2, 3, 4, 5, 6}\\) Event Space - \\({1, 3, 5}\\) Complement - \\({2, 4, 6}\\) Probability of Event - \\(\\frac{3}{6}\\) Probability of Complement - \\(\\frac{3}{6}\\) Within the same experiment, there may be multiple events of interest. For any two events A and B, its Union \\((A \\cup B)\\) is the set with outcomes that are either in A or B while their Intersection \\((A \\cap B)\\) is the set with outcomes that are in BOTH A and B . If both A and B have no outcomes in common \\((A \\cap B = \\emptyset)\\) , then they are said to be Mutually Exclusive . Naturally, an event and its complement are always mutually exclusive. Warning The following seems intuitive , but is actually a common mistake: \\[ (A \\cap B)^c \\ne A^c \\cap B^c \\] This is properly explained through De-morgans Law : It can be easily remembered by applying the complement to all components of the expression, including the intersection/union symbol : \\[ \\begin{aligned} \\cap^c &= \\cup \\\\ \\cup^c &= \\cap \\end{aligned} \\] Probability Axioms \u00b6 Axiom 1 states that all probabilities must be non-negative : \\[ P(E) \\ge 0 \\] Axiom 2 states that probability of the Sample Space is exactly equal to 1: \\[ P(\\Omega) = 1 \\] Axiom 3 states the probability of a union of mutually exclusive events is equal to the sum of their probabilities, known as Countable Additivity . \\[ \\begin{aligned} A \\cap B &= \\emptyset \\\\ P(A \\cup B) &= P(A) + P(B) \\end{aligned} \\] Based on these axioms, several other important properties can also be deduced: Monoticity : \\(A \\subset B \\rightarrow P(A) \\le P(B)\\) Empty Set : \\(P(\\emptyset) = 0\\) Complement Rule : \\(P(E^c) = 1 - P(E)\\) Numeric Bound : \\(0 \\le P(E) \\le 1\\) Sum Rule : \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) Conditional Probability \u00b6 Conditional Probabilities are denoted by \\(P(A \\mid B)\\) , which is the probability of event A occuring given that event B has already occurred . The intuition is best understood by considering the following - Given that event B has already occured, what is the probability that event A also occurs? The event space is \\(A \\cap B\\) , as we are interested in the probability that both A and B occur. However, since event B has already occured, the sample space is no longer all possible outcomes but rather only the event space for B ! \\[ \\begin{aligned} P(A \\mid B) &= \\frac{n(A \\cap B)}{n(B)} \\\\ &= \\frac{\\frac{n(A \\cap B)}{n(\\Omega)}}{\\frac{n(B)}{n(\\Omega)}} \\\\ &= \\frac{P(A \\cap B)}{P(B)} \\end{aligned} \\] Following this expression, the probability of an intersection of two events is given by: \\[ P(A \\cap B) = P(A \\mid B) \\cdot P(B) = P(B \\mid A) \\cdot P(A) \\] Most experiments involving conditional probabilities are multi-staged experiments, which are best visualized using Probability Trees : Instead of calculating conditional probabilities from scratch, some questions provide the conditional probability \\(P(A \\mid B)\\) (or the components to do so!) and ask us to find the reverse - \\(P(B \\mid A)\\) . \\[ P(B \\mid A) = \\frac{P(B \\cap A)}{P(A)} \\] The formula is the same as before, but the issue is that the unconditional probability of event A is usually not given. This problem is accounted for in Bayes Theorem : \\[ \\begin{aligned} A &= (A \\cap B) + (A \\cap B^c) \\\\ P(A) &= P(A \\cap B) + P(A \\cap B^c) \\\\ P(A) &= P(A \\mid B) \\cdot P(B) + P(A \\mid B^c) \\cdot P(B^c) \\\\ \\\\ \\therefore P(B \\mid A) &= \\frac{P(A \\mid B) \\cdot P(B)}{P(A \\mid B) \\cdot P(B) + P(A \\mid B^c) \\cdot P(B^c)} \\end{aligned} \\] Note that if the Conditional Probability of A given B is the same as the unconditional probability of A, then events A and B are independent ; B has no effect on A. Thus, the probability of an intersection of two independent events is simply their product: \\[ \\begin{aligned} P(A \\mid B) &= \\frac{P(A \\cap B)}{P(B)} \\\\ P(A) &= \\frac{P(A \\cap B)}{P(B)} \\\\ P(A \\cap B) &= P(A) \\cdot P(B) \\end{aligned} \\] Random Variables \u00b6 Unlike rolling a dice, the outcome of most experiments are non-numeric , which makes them hard to work with. For instance, the outcomes of a coin toss are \"Heads\" and \"Tails\". A Random Variable is a many to one function that maps each outcome to a single real number. Each outcome must have only one corresponding number, but different outcomes can have the same value. Note Although the mapping is deterministic, the underlying experiment is still random which is why it is still a \"random\" variable. The range of possible values that the random variable can take is known as its Support . They are broadly categorized based on its support: Discrete Continuous Countable Support Uncountable Support 1, 2, 3, 4, ... 1, 1.1, 1.01, 1.001, ... Random variables are denoted using upper case letters (X, Y, Z) while their corresponding values are denoted using lower case letters (x, y, z) and their appropriate subscripts . The notation \\(X(s) = x_1\\) denotes that the random variable \\(X\\) maps the outcome \\(s\\) to the value of \\(x_1\\) . Thus, the corresponding probability is denoted by \\(P(X = x_1)\\) . Note If the subscript is omitted, then \\(P(X=x)\\) is a general expression that describes the entire distribution of \\(X\\) , not just a single probability. Probability Distributions \u00b6 Similar to how a random variable maps the outcomes to a real number, a Probability Distribution is a function that maps the outcomes to its probability of occurrence . For Discrete Random Variables , their distribution is described using a Probability Mass Function (PMF). The PMF provides the probability that the random variable is exactly equal to some value \\((X = x_1)\\) . It is typically denoted in lower case and sometimes includes a subscript of the random variable when working with multiple to distinguish them from one another. \\[ \\begin{aligned} P(X = a) &= p(a) \\\\ \\\\ P(X = a) &= p_X(a) \\\\ P(Y = a) &= p_Y(a) \\end{aligned} \\] Since it is a probability measure, the sum of the PMF over the support of the random variable must be equal to 1 (Probability Axiom). \\[ \\sum_{x \\in \\text{Support}} p(x) = 1 \\] PMFs can be represented in three main ways - Functions, Tables or Histograms. For Continuous Random Variables , their distribution is described using a Probability Density Function (PDF). The PDF is a non-negative function where the area under it provides the probability that the random variable takes on some range of values \\((a \\le X \\le b)\\) . Similarly, it is typically denoted in lower case and includes a subscript when working with multiple random variables: \\[ \\begin{aligned} P(a \\le X \\le b) = \\int^b_a f(x) \\\\ \\\\ P(a \\le X \\le b) = \\int^b_a f_X(x) \\\\ P(a \\le Y \\le b) = \\int^b_a f_Y(y) \\end{aligned} \\] Similarly, since the area is a probability measure, the total area under the graph must be equal to 1 : \\[ P(-\\infty \\le X \\le \\infty) = \\int^{\\infty}_{-\\infty} f(x) = 1 \\] Note \\(\\infty\\) is used as a catch all for the upper and lower bound of the random variable. If the actual bounds are known, then using them instead is more appropriate. Additionally, note that the probability of a specific value for a continuous RV is 0. This is because there is an infinite number of possible values , thus the probability of a specific value (EG. 1.45679383920) is infinitely small such that it is assumed to be 0. \\[ P(X = a) = \\int^{a}_{a} f(x) = 0 \\] The Cumulative Distribution Function (CDF) is the probability that the random variable is less than or equal to some value \\(X \\le t\\) . It is typically denoted in upper case to distinguish it from the PDF and includes subscripts as well when working with multiple random variables. \\[ \\begin{aligned} F(t) &= P(X < t) \\\\ \\\\ F_X(t) &= P(X < t) \\\\ F_Y(t) &= P(Y < t) \\end{aligned} \\] For discrete variables, the CDF is the sum of all probabilities before the specified value . Following this, the difference of consecutive CDFs allows us to obtain the PMFs at that value: \\[ \\begin{aligned} F(t) &= \\sum_{x \\le t} p(x) \\\\ p(x_i) &= F(x_i) - F(x_{i-1}) \\end{aligned} \\] For continuous variables, the CDF is the integral from the lower bound to the specified value . However, instead of integrating with respect to an actual value, it is better to integrate with respect to a dummy variable \\(t\\) to obtain a general expression for the CDF , allowing it to be easily calculated for any value. Although the CDF is very useful, it can only be used to calculate probabilities starting from the lower bound . When probabilities starting from other ranges are needed, the PDF can be obtained from the CDF by differentiating it and then re-integrating with different limits. \\[ \\begin{aligned} F(t) &= \\int^{t}_{-\\infty} f(x) dx \\\\ \\\\ F(t) &= \\int^{t}_{-\\infty} f(t) dt \\\\ f(t) &= F'(t) \\end{aligned} \\] Note The integral of the PDF can lead also lead to the Survival Function , as shown in the Survival Model Section . Piecewise Distributions \u00b6 The probability distribution may be defined using a Piecewise Function , which means that it is defined via multiple sub-functions where each applies to different intervals in the domain: \\[ \\begin{aligned} f(x) &= \\begin{cases} f_{1}(x),& 0 \\lt x \\lt t_1 \\\\ f_{2}(x),& t_1 \\lt x \\lt t_2 \\\\ \\vdots \\end{cases} \\end{aligned} \\] When calculating probabilities, we must choose the appropriate PDF based on the range of the probability: \\[ \\begin{aligned} P(X \\lt t_{0.5}) &= \\int^{t_{0.5}}_{0} f_{1}(x) \\\\ P(X \\gt t_{1.5}) &= \\int^{t_{2}}_{t_{1.5}} f_{2}(x) \\end{aligned} \\] If the required range spans across multiple intervals , then the probability should be split according to the intervals: \\[ \\begin{aligned} P(t_{0.5} \\lt X \\lt t_{1.5}) &= P(t_{0.5} \\lt X \\lt t_{1}) + P(t_{1} \\lt X \\lt t_{1.5}) \\\\ &= \\int^{t_{1}}_{t_{0.5}} f_{1}(x) + \\int^{t_{1.5}}_{t_{1}} f_{2}(x) \\end{aligned} \\] Thus, this means that the CDF for a piecewise distribution must fully integrate all \"earlier\" PDFs : \\[ \\begin{aligned} P(X \\lt t) &= P(0 \\lt X \\lt t_{1}) + P(t_{1} \\lt X \\lt t) \\\\ &= \\int^{t_{1}}_{0} f_{1}(x) + \\int^{t}_{t_{1}} f_{2}(x) \\end{aligned} \\] Moments \u00b6 The Moments of a distribution are quantities that describe characteristics of the distribution . First Moment \u00b6 Raw Moments are calculated with respect to the origin . The n-th raw moment is calculated as the following: \\[ \\begin{aligned} E(X^n) &= \\int x^n \\cdot f(x) dx \\\\ \\mu'_k &= \\sum x^n \\cdot p(x) \\end{aligned} \\] The first raw moment is known as the Mean , which is a measure of the Centrality of the distribution. It is commonly denoted as \\(\\mu\\) , without any super or subscripts. Note that the mean of a constant is the constant itself : \\[ E(c) = c \\] Second Moment \u00b6 Central Moments are calculated with respect to the mean . The n-th central moment is calculated as the following: \\[ \\begin{aligned} E[(X - \\mu)^n] &= \\int (x - \\mu)^n \\cdot f(x) dx \\\\ \\mu_k&= \\sum (x - \\mu)^n \\cdot p(x) \\end{aligned} \\] The second central moment is known as the Variance , which is a measure of the Spread of the distribution about the mean. Since calculating central moments directly is complicated, it can be simplified to an expression involving raw moments: \\[ \\begin{aligned} Var(X) &= E[(X - \\mu)^2] \\\\ &= E(X^2 - 2\\mu X + \\mu^2) \\\\ &= E(X^2) - 2\\mu^2 + \\mu^2 \\\\ &= E(X^2) - \\mu^2 \\\\ &= E(X^2) - [E(X)]^2 \\end{aligned} \\] Note that the variance of a constant is 0 as a constant cannot change: \\[ Var(c) = 0 \\] However, one problem with variance is that it uses squared units, which makes it hard to intepret. Thus, the squareroot of the variance is used instead, known as the Standard Deviation . \\[ \\sigma = \\sqrt{Var(X)} \\] Similarly, standard deviation cannot be used to compare data with different units . Thus, the Coefficient of Variation is used instead, which is a unitless measure of the spread of the distribution. \\[ CV(X) = \\frac{\\sigma}{\\mu} \\] Third Moment \u00b6 The third central moment is Skewness , which is a measure of the symmetry of distribution about the mean. Being left/right skewed means that the distribution has a \"longer tail\" on that side, which implies that values on the opposite side are more likely to occur . Tip Skewness is also sometimes referred to as being Positively or Negatively Skewed . An easy way to remember is that positive values occur to the right of the origin, hence is the same as being right skewed; vice-versa. \\[ \\begin{aligned} \\text{Skewness} &= \\frac{E[(X - \\mu)^3]}{\\sigma^3} \\\\ &= \\frac{E[(X - \\mu)^3]}{(\\sigma^2)^\\frac{3}{2}} \\\\ &= \\frac{E(X^3) - 3 E(X^2) \\cdot E(X) + 2 [E(X)]^3}{(E(X^2) - [E(X)]^2)^\\frac{3}{2}} \\end{aligned} \\] Fourth Moment \u00b6 The fourth central moment is Kurtosis , which is a measure of the flatness of the distribution, typically with respect to the normal distribution. It is indicative of the likelihood of producing extreme values (outliers). The normal distribution has a kurtosis of 3. If a distribution has a kurtosis greater than 3 , then it is flatter and hence more likely to produce outliers as compared to the normal distribution. \\[ \\begin{aligned} \\text{Kurtosis} &= \\frac{E[(X - \\mu)^4]}{\\sigma^4} \\\\ &= \\frac{E[(X - \\mu)^4]}{(\\sigma^2)^2} \\\\ &= \\frac{E(X^4) - 4 E(X^3) \\cdot E(X) + 6 E(X^2) \\cdot [E(X)]^2 - 3 [E(X)]^4}{(E(X^2) - [E(X)]^2)^2} \\end{aligned} \\] Tail Weights \u00b6 Another method of determining the likelihood of outliers to occur is through the Tail Weight of the distribution. Distributions with a heavier tails are more likely to produce outliers . Warning Although both Tail Weight and Kurtosis are measures of the likelihood of outliers, there is no link between them , despite what some authors might suggest. A distribution is said to have a heavier tail than another if it has STRICTLY more density at the \"ends\" of the distributions, past a certain threshold: Note For the purposes of this exam, we only consider the right tail of the distribution for large outliers. There are a few methods to compare the tail weight of the distribution, but for the purposes of this exam, only the Existence of Moments method will be considered. A distribution is light tailed if has finite RAW moments for all positive values of \\(k\\) : \\[ E(X^k) \\lt \\infty, \\ k \\gt 0 \\] Conversely, a distribution is heavy tailed if it DOES NOT have finite raw moments for all positive values of \\(k\\) . From another perspective, it only has them up till a certain threshold \\(k \\lt a\\) : \\[ E(X^k) \\lt \\infty, \\ k \\lt a \\] Consider the following two distributions: Exponential Pareto Raw moments exists for \\(k \\gt 0\\) Raw moments exists for \\(-1 \\lt k \\lt \\alpha\\) Unlimited number of finite raw moments Limited number of finite raw moments Light tailed Heavy tailed Consider the formula for raw moments: \\[ E(X^k) = \\int x^k \\cdot f(x) \\] As \\(k\\) increases, large values of \\(x\\) will be raised by large values, resulting in extremely large values. If the densities (probability of occurring) at this portion of the distribution are sufficiently large , then the resulting value would be too large \\(E \\left[X^k\\right] = \\infty\\) , resulting in a limited number of finite raw moments . This means that the fewer finite raw moments, the heavier the tail and hence higher the probability of outliers . Statistical Metrics \u00b6 Apart from the moment of the distribution, there are some other Statistical Metrics that provide useful information. The Mode is the value of the random variable that maximises the PMF or PDF. It is the most likely outcome of the experiment (loosely speaking for continuous variables). The Median is the value of the random variable that seperates the upper and lower half of the probability distribution. For discrete variables , the median \\(M\\) is the smallest value such that the following two expressions are satisfied: \\[ \\begin{aligned} P(X \\le M) &\\ge 0.5 \\\\ P(X \\ge M) &\\ge 0.5 \\end{aligned} \\] For continuous variables , the median \\(M\\) is found by solving the CDF : \\[ \\begin{aligned} F(M) &= 0.5 \\\\ M &= F^{-1}(0.5) \\end{aligned} \\] Note The key difference is that the continuous median is the value that exactly seperates the distribution while the discrete median approximately splits it, depending on the PMF. The Percentile is the value of the random variable below which a certain percentage of observations fall . For instance, the 85 th percentile is the value below which 85% of the observations fall. The median is the 50 th percentile ; in other words when \\(p = 0.5\\) . Thus, the previous formulas can be generalized for any \\(p\\) : \\[ \\begin{aligned} P(X \\le M^{\\text{Discrete}}) &\\ge p \\\\ P(X \\ge M^{\\text{Discrete}}) &\\ge p \\\\ \\\\ M^{\\text{Continuous}} &= F^{-1}(p) \\end{aligned} \\] Tip Remember that the CDF for a piecewise distribution is calculated differently. The 25 th , 50 th & 75 th percentile are known as the first, second & third Quartiles \\((q)\\) respectively. The difference between the 3 rd and 1 st quartile is known as the Inter Quartile Range . \\[ IQR = q_3 - q_1 \\] Shifting, Scaling & Transformation \u00b6 An existing random variable \\(X\\) can be adjusted in order to make a new random variable \\(Y\\) . If a constant \\(a\\) has been multiplied to the random variable, then it has been Scaled by \\(a\\) : \\[ \\begin{aligned} Y &= aX \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(aX \\le y) \\\\ &= P \\left(X \\le \\frac{y}{a} \\right) \\\\ &= F_X \\left(\\frac{y}{a} \\right) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= \\frac{1}{a} \\cdot f_X \\left(\\frac{y}{a} \\right) \\end{aligned} \\] If a constant \\(b\\) is added to the random variable instead, then it has been Shifted by \\(b\\) : \\[ \\begin{aligned} Y &= X + b \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(X + b \\le y) \\\\ &= P(X \\le y - b) \\\\ &= F_X (y - b) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= f_X (y - b) \\end{aligned} \\] For both shifting and scaling, the expectation and variance can be easily determined if that of the original is known as well: \\[ \\begin{aligned} E(cX) &= c \\cdot E(X) \\\\ E(X+c) &= E(X) + E(c) \\\\ &= E(X) + c \\\\ Var(cX) &= c^2 \\cdot Var(x) \\\\ Var(X+c) &= Var(X) + Var(c) \\\\ &= Var(X) \\end{aligned} \\] If the random variable has been raised by a power of \\(\\frac{1}{c}\\) where \\(c \\ne 1\\) , then it has been Power Transformed : \\[ \\begin{aligned} Y &= X^{\\frac{1}{c}} \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(X^{\\frac{1}{c}} \\le y) \\\\ &= P(X \\le y^c) \\\\ &= F_X (y^c) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= cy^{c-1}\\cdot f_X (y - b) \\end{aligned} \\] If the random variable has been exponentiated , then it has also been Exponential Transformed : \\[ \\begin{aligned} Y &= e^X \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(e^X \\le y) \\\\ &= P(X \\le \\ln y) \\\\ &= F_X (\\ln y) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= \\frac{1}{y} \\cdot f_X (\\ln y) \\end{aligned} \\] For both types of transformations, there is no simple method of determining the mean and variance. The various raw moments must be manually determined via integration . Joint Distributions \u00b6 Consider a scenario where there are two or more random variables \\((X_1, X_2, \\dots)\\) in the same probability space such that we are interested in some function of them \\(Y = g(x_1, x_2, \\dots)\\) . Note For the purposes of this exam, only Linear Combinations (sums) of random variables will be considered. For instance, the experiment could be studying the sum of rolling two dice . There would be two random variables \\((X_1, X_2)\\) , each denoting the value of their respective dice. Thus, the sum of the two dice would be \\(Y = X_1 + X_2\\) . Consider the probability of obtaining a sum of 3 - consider ALL combination of values of the two underlying variables will result in it: \\[ P(Y = 3) = P(X_1 = 1, X_2 = 2) + P(X_1 = 2, X_2 = 1) \\] This can be more generally expressed as the Joint Distribution of the two variables: \\[ \\begin{aligned} P(X = x, Y = y) &= P_{X,Y} (x,y) \\\\ P(a \\lt X \\lt b, c \\lt Y \\lt d) &= \\int^{d}_{c} \\int^{b}_{a} f_{X,Y} (x,y) \\ dx dy \\end{aligned} \\] The individual distributions within this shared probability space is known as the Marginal Distribution and is obtained by \"integrating out\" the other variable: \\[ \\begin{aligned} f_{X}(x) &= \\int^{d}_{c} f_{X,Y} (x,y) \\ dy \\\\ f_{Y}(y) &= \\int^{b}_{a} f_{X,Y} (x,y) \\ dx \\end{aligned} \\] Joint Domain \u00b6 One of the main concerns when dealing with joint distributions is the domain of the distribution. Each random variable is defined on their own domain, but the joint distribution is only defined in the domain in which two variables coincide . For a joint distribution of discrete variables , it is simply a matter of finding all possible combinations of the two variables. For a two variable distribution, this can be easily visualized via a Contingency Table : The table sometimes lists the frequency of observations while sometimes it lists their probability . In either case, by finding the cells of interest , the probability can be obtained. For a joint distribution of continuous variables , it is best visualized graphically . For a two variable distribution: \\[ 1 \\lt X_1 \\lt 5, \\ 2 \\lt X_2 \\lt 8 \\] Let \\(Y\\) be the sum of the two random variables. Consider the following probability: \\[ \\begin{aligned} Y &= X_1 + X_2 \\\\ P(Y \\lt 5) &= P(X_1 + X_2 \\lt 5) \\\\ &= P(X_2 \\lt 5 - X_1) \\end{aligned} \\] Thus, we need to find the domain of the above inequality WITHIN the joint domain . This can be done by plotting the graph of the upper/lower bound (simply change the inequality to an equal sign). The domain can be intepreted in one of two ways: \\(X_1\\) to line first: \\(1 \\lt X_1 \\lt 3, \\ 2 \\lt y \\lt 5-x\\) \\(X_2\\) to line first: \\(1 \\lt X_1 \\lt 5-y, \\ 2 \\lt y \\lt 4\\) Using these domains, the double integration can be performed to obtain the probability: \\[ \\begin{aligned} P(X_2 \\lt 5 - X_1) &= \\int^{1}_{3} \\int^{5-x}_{2} f_{X,Y}(x,y) \\ dy dx \\\\ &= \\int^{2}_{4} \\int^{5-y}_{1} f_{X,Y}(x,y) \\ dx dy \\end{aligned} \\] Tip Recall that when performing a double integration, the INNER integral is evaluated first . The inner and outer integral are interchangeable, thus set the inner integral such that it will ease the integration . In this case, the integral with an algebraic expression should always be evaluated first, ensuring that the final result is a probability and not an expression. If there is a Kink in the shape, then the area should be broken into two smaller areas without any kinks in their shape. The overall probability is the sum of the areas: \\[ P(X_2 \\lt 8 - X_1) = \\text{Orange Area} + \\text{Purple Area} \\] Joint Moments \u00b6 Given that \\(Y = X_1 + X_2\\) , its mean and variance is the following: \\[ \\begin{aligned} E(Y) &= E(X_1) + E(X_2) \\\\ \\\\ Var (Y) &= Var (X_1) + Var (X_2) + 2 \\cdot Cov (X,Y) \\end{aligned} \\] The last term is known as the Covariance , which is the first central moment about of the joint distribution. It measures the linear relationship between variables: Positive Covariance : Both variables move in the same direction Negative Covariance : Both variables move in opposite directions \\[ \\begin{aligned} Cov (X_1, X_2) &= E[(X_1 - E(X_1))(X_2 - E(X_2))] \\\\ &= E(X_1 \\cdot X_2) - E(X_1) \\cdot E(X_2) \\\\ \\\\ E(X_1 \\cdot X_2) &= \\int \\int (x_1 \\cdot x_2) \\cdot f_{X_1, X_2} (x_1, x_2) \\ dx_1 dx_2 \\end{aligned} \\] Note that it has some interesting properties: \\(Cov (X_1, c) = 0\\) \\(Cov (X_1, X_2) = Var (X_1)\\) \\(Cov (c \\cdot X_1, X_2) = c \\cdot Cov (X_1,X_2)\\) \\(Cov (X_1 + c, X_2) = Cov (X_1, X_2) + Cov (c, X_2)\\) However, covariance is both limitless and squared units , which makes hard to use as a metric for comparison. We can overcome these problems by scaling the covariance down by the individual standard deviations, obtaining the Correlation of the two variables: \\[ Corr (X_1, X_2) = \\frac{Cov (X_1, X_2)}{SD(X_1), SD(X_2)} \\] If \\(X_1\\) and \\(X_2\\) are Independent , then their distributions and moments become the following: \\[ \\begin{aligned} P(X_1 = x_1, X_2 = x_2) &= P(X_1 = x_1) \\cdot P(X_2 = x_2) \\\\ f_{X_1,X_2} (x_1,x_2) &= f_{X_1}(x_1) \\cdot f_{X_2}(x_2) \\\\ \\\\ E(X_1 \\cdot X_2) &= E(X_1) \\cdot E(X_2) \\\\ Var (X_1 + X_2) &= Var (X_1) + Var (X_2) \\end{aligned} \\] Warning If the domain of the random variables naturally contain other variables, then they are not independent, even if the above conditions are met. Note that having 0 covariance is a consequence of independence , but is NOT indicative of it. Variables may also have 0 covariance when they have a non-linear relationship . \\[ \\begin{aligned} Cov (X_1, X_2) &= E(X_1 \\cdot X_2) - E(X_1) \\cdot E(X_2) \\\\ &= E(X_1) \\cdot E(X_2) - E(X_1) \\cdot E(X_2) \\\\ &= 0 \\\\ \\\\ \\therefore Corr (X_1, X_2) &= 0 \\end{aligned} \\] Generating Functions \u00b6 A useful way to analyze the sum of independent random variables is to convert the PDF/PMF of the individual distributions into a Generating Function . The first kind is known as a Moment Generating Function (MGF): \\[ \\begin{aligned} M_{X}(t) &= E(e^{tX}) \\\\ &= \\sum e^{tx} \\cdot p_{X}(x) \\\\ &= \\int e^{tx} \\cdot f_{X}(x) \\end{aligned} \\] As its name suggests, the MGF can be used to calculate the raw moments of the distribution. To obtain the n-th raw moment , Differentiate the MGF k times Evaluate the expression at \\(t=0\\) \\[ E \\left(X^n \\right) = \\frac{d^n}{dt^n} \\cdot M_{X}(0) \\] However, the main benefit of MGFs is that they uniquely identify a distribution. If two random variables have the same MGF , then they have the same distribution . This becomes especially useful when dealing with sums of independent random variables . By determining the MGF of the combination, its exact distribution can be determined. \\[ \\begin{aligned} Y &= X_1 + X_2 + ... + X_n \\\\ \\\\ M_Y(t) &= E(e^{tY}) \\\\ &= E(e^{t(X_1 + X_2 + ... + X_n)}) \\\\ &= E(e^{tX_1} \\cdot e^{tX_2} \\cdot ... \\cdot e^{tX_n}) \\\\ &= \\prod E(e^{tx}) \\\\ &= \\prod M_{X}(t) \\end{aligned} \\] The other kind is known as a Probability Generating Function (PGF), which only applies for sums of discrete variables : \\[ \\begin{aligned} P_{X}(t) &= E(t^x) \\\\ &= \\sum t^X \\cdot p(x) \\end{aligned} \\] Note The PGF is denoted in upper case \\(P\\) while the PMF is denoted in lower case \\(p\\) . As its name suggests, the PGF can be used to calculate the individual probabilities of the distribution. To obtain the probability of the n-th value , Differentiate the PGF k times Divide the expression by n factorial Evaluate the expression at \\(t=0\\) \\[ P(X = k) = \\frac{d^n}{dt^n} \\cdot \\frac{P_{X}(0)}{n!} \\] Similarly, the PGF uniquely identifies the distribution and can be used in all the same ways as an MGF in this regard: \\[ \\begin{aligned} Y &= X_1 + X_2 + ... + X_n\\\\ \\\\ P_Y(t) &= E(t^Y) \\\\ &= E(t^{X_1 + X_2 + ... + X_n}) \\\\ &= E(t^{X_1} \\cdot t^{X_2} \\cdot ... \\cdot t^{X_n}) \\\\ &= \\prod E(t^{x}) \\\\ &= \\prod P_{X}(t) \\end{aligned} \\] Given how similar the two are, they can be easily converted to and from one another: \\[ \\begin{aligned} P_X(t) &= E(t^X) \\\\ &= E(e^{\\ln t^X}) \\\\ &= M_X(\\ln t^x) \\\\ \\\\ M_X(t) &= E(e^tX) \\\\ &= E[(e^t)^x] \\\\ &= P_X(e^t) \\end{aligned} \\] Conditional Distributions \u00b6 If a random variable \\(X\\) is conditioned on a range of values of itself , then it has the Conditional Distribution \\((X \\mid X>j)\\) : \\[ \\begin{aligned} P(X = x \\mid X > j) &= \\frac{P(X = x)}{P(X > j)} \\\\ f_{X \\mid X>j} (x) &= \\frac{f_X(x)}{P(X > j)} \\end{aligned} \\] Similarly, a random variable can be conditional on ANOTHER random variable , resulting in the Conditional Distribution \\((X \\mid Y)\\) . \\[ \\begin{aligned} P(X = x \\mid Y = y) &= \\frac{P(X = x, Y = y)}{P(Y = y)} \\\\ f_{X \\mid Y} (x,y) &= \\frac{f_{X,Y}(x,y)}{f_{Y}(y)} \\\\ \\text{Conditional} &= \\frac{\\text{Joint}}{\\text{Marginal}} \\end{aligned} \\] Any calculations involving the above distributions must be solved via first principles . However, most problems require us to find the Marginal Distribution given only the conditional distributions: \\(X\\) is the random variable denoting the test grades (A, B, C) \\(Y\\) is the random variable denoting the gender (M, F) The teacher would like to find the marginal distribution of test scores \\((X)\\) , but only has the conditional distribution of the scores of the students for each gender \\((X \\mid Y)\\) and the proportion of the Genders \\((Y)\\) . The Law of Total Probability can be used to determine the marginal probability of \\(X\\) : \\[ \\begin{aligned} P(X = a) &= E_Y[P(X = a \\mid Y)] \\\\ &= \\sum_{y} P(X = a \\mid y) \\cdot p(Y = y) \\end{aligned} \\] Note that this is equivalent to adding up the final probabilities from the relevant branches from a probability tree: Naturally, this also means that the marginal CDF can be obtained in similar fashion: \\[ \\begin{aligned} F_X(a) &= E_Y[F_{X \\mid Y}(a)] \\\\ &= \\sum_{y} F_{X \\mid Y}(a) \\cdot p(Y = y) \\end{aligned} \\] Following the same logic, the Law of Total Expectation can be used to determine the marginal expectation of \\(x\\) : \\[ \\begin{aligned} E(X) &= E_Y[E_X(X \\mid Y)] \\\\ &= \\sum_{y} E_X(X \\mid Y) \\cdot p(Y = y) \\end{aligned} \\] Warning Since \\(E(X)\\) is a constant, a common mistake is thinking that \\(E_X(X \\mid Y)\\) is a constant as well since they are both expectations. The issue is that it is conditional on \\(Y\\) , which is still random, which makes the conditional expectation still random . The Law of Total Variance can be used to determine the marginal variance of \\(x\\) . However, unlike the previous two, it is NOT simply the expectation of the conditional variance: \\[ \\begin{aligned} Var (X) &= E_Y [Var_X(X \\mid Y)] + Var_Y[E_X(X \\mid Y)] \\\\ \\\\ E_Y [Var_X(X \\mid Y)] &= \\sum_{y} Var_X(X \\mid Y) \\cdot p(Y = y) \\\\ \\\\ Var_Y[E_X(X \\mid Y)] &= E_Y[E_X(X \\mid Y)^2] - (E_Y[E_X(X \\mid Y)])^2 \\\\ &= \\sum_{y} E_X(X \\mid Y)^2 \\cdot p(Y = y) - \\sum_{y} E_X(X \\mid Y) \\cdot p(Y = y) \\end{aligned} \\] Alternatively, the Marginal Variance can be directly calculated using the typical formula of \\(E(X^2) - [E(X)]^2\\) , where the two marginal expectations are calculated using the law of total expectation. Alternatively once more, if the conditional distribution \\(Y\\) only has two outcomes, then the Bernoulli Shortcut (covered in a later section) can be used to quickly compute the value of \\(Var_Y[E_X(X \\mid Y)]\\) . Tip The discrete case was shown in this section due to its simplicity. All the same concepts apply to the continuous variables as well - simply replace the summation & PMFs with integrals and PDFs . Mixture Distributions \u00b6 A mixture distribution is a distribution whose values can be intepreted as being derived from an underlying set of other random variables . In an insurance context, a Homeowners Insurance claim could be from a fire, burglary or liability accident. To model it, we could use a mixture that is made up of the basic distributions used to individually model each type of accident. If the mixture contains a countable number of other distributions, then it is known as a Discrete Mixture . Otherwise, it is known as a Continuous Mixture . Warning It is a common mistake to think that a discrete mixture is only made up of discrete distributions, vice-versa. Any type of distribution can be included in a mixture; the classification is based on the number of distributions. The random variable \\(X\\) is a k-point mixture if its probability functions can be expressed as the weighted average of the probability functions of the \\(k\\) distributions \\(X_1, X_2, ... X_k\\) : \\[ \\begin{aligned} F_X(x) &= w_1 \\cdot F_{X_1}(x) + w_2 \\cdot F_{X_2}(x) + ... + w_k \\cdot F_{X_k}(x) \\\\ f_X(x) &= w_1 \\cdot f_{X_1}(x) + w_2 \\cdot f_{X_2}(x) + ... + w_k \\cdot f_{X_k}(x) \\end{aligned} \\] Note For this exam, questions will usually only use 2 or 3 point mixtures . \\(w\\) represents the mixing weights , such that \\(w_1 + w_2 + ... + w_k = 1\\) . It can be intepreted that \\(Y\\) follows the distribution of \\(X_1\\) \\(100w_1 \\%\\) of the time, follows the distribution of \\(X_2\\) \\(100w_2 \\%\\) of the time etc. Warning Another common mistake is confusing mixtures with Linear Combinations of random variables: \\[ X \\ne w_1 X_1 + w_2 X_2 + ... + w_k X_k \\] In a linear combination, \\(X\\) neither follows the distribution of any of the \\(X_k\\) . Furthermore, since \\(w_k\\) are not weights, they can be any real number and do not need to sum to 1 . The mixing weights can also be thought of as Discrete Probabilities that come from a random variable \\(Y\\) representing the \\(k\\) underlying distributions with the support \\(\\set{1, 2, ..., k}\\) where \\(P(Y = k) = w_k\\) . Thus, we can think of the overall mixture \\(X\\) as an unconditional distribution while each of the underlying distributions are conditional distributions \\(X \\mid Y\\) . This allows us to make use of the all the previous results from the conditional distributions: \\[ \\begin{aligned} P(X = a) &= E_Y[P(X = a \\mid Y)] \\\\ F_X(a) &= E_Y[F_{X \\mid Y}(a)] \\\\ E(X) &= E_Y[E_X(X \\mid Y)] \\\\ Var (X) &= E_Y [Var_X(X \\mid Y)] + Var_Y[E_X(X \\mid Y)] \\end{aligned} \\] In terms of the weights, for a simple two point mixture : \\[ \\begin{aligned} P(X = a) &= P(X = a \\mid Y=1) \\cdot w_1 + P(X = a \\mid Y=2) \\cdot w_2 \\\\ F_X(a) &= F_{X \\mid Y=1}(a) \\cdot w_1 + F_{X \\mid Y=2}(a) \\cdot w_2 \\\\ E(X) &= E_X(X \\mid Y=1) \\cdot w_1 + E_X(X \\mid Y=2) \\cdot w_2 \\end{aligned} \\]","title":"Review of Probability Theory"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#review-of-probability-theory","text":"","title":"Review of Probability Theory"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#basic-probability","text":"Probability is the study of Experiments whose results cannot be predicted with certainty. The result of such an experiment is known as its Outcome . The Sample Space \\(\\left(\\Omega \\right)\\) is the set of ALL possible outcomes from an experiment. The Event Space \\((E)\\) is a subset of the sample space, representing only the outcomes that we are interested in studying. Conversely, its Complement \\((E^c)\\) is the set of all OTHER outcomes not inside \\(E\\) . The probability of the event occuring is the ratio of the number of elements in the event to the sample space. It is a measure of the chance that the outcome of the experiment is inside the event space. \\[ P(E) = \\frac{n(E)}{n\\left(\\Omega \\right)} \\] Consider the probability of rolling an odd number on a standard dice: Experiment - Rolling a dice Outcome - The number showed on the dice Sample Space - \\({1, 2, 3, 4, 5, 6}\\) Event Space - \\({1, 3, 5}\\) Complement - \\({2, 4, 6}\\) Probability of Event - \\(\\frac{3}{6}\\) Probability of Complement - \\(\\frac{3}{6}\\) Within the same experiment, there may be multiple events of interest. For any two events A and B, its Union \\((A \\cup B)\\) is the set with outcomes that are either in A or B while their Intersection \\((A \\cap B)\\) is the set with outcomes that are in BOTH A and B . If both A and B have no outcomes in common \\((A \\cap B = \\emptyset)\\) , then they are said to be Mutually Exclusive . Naturally, an event and its complement are always mutually exclusive. Warning The following seems intuitive , but is actually a common mistake: \\[ (A \\cap B)^c \\ne A^c \\cap B^c \\] This is properly explained through De-morgans Law : It can be easily remembered by applying the complement to all components of the expression, including the intersection/union symbol : \\[ \\begin{aligned} \\cap^c &= \\cup \\\\ \\cup^c &= \\cap \\end{aligned} \\]","title":"Basic Probability"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#probability-axioms","text":"Axiom 1 states that all probabilities must be non-negative : \\[ P(E) \\ge 0 \\] Axiom 2 states that probability of the Sample Space is exactly equal to 1: \\[ P(\\Omega) = 1 \\] Axiom 3 states the probability of a union of mutually exclusive events is equal to the sum of their probabilities, known as Countable Additivity . \\[ \\begin{aligned} A \\cap B &= \\emptyset \\\\ P(A \\cup B) &= P(A) + P(B) \\end{aligned} \\] Based on these axioms, several other important properties can also be deduced: Monoticity : \\(A \\subset B \\rightarrow P(A) \\le P(B)\\) Empty Set : \\(P(\\emptyset) = 0\\) Complement Rule : \\(P(E^c) = 1 - P(E)\\) Numeric Bound : \\(0 \\le P(E) \\le 1\\) Sum Rule : \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)","title":"Probability Axioms"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#conditional-probability","text":"Conditional Probabilities are denoted by \\(P(A \\mid B)\\) , which is the probability of event A occuring given that event B has already occurred . The intuition is best understood by considering the following - Given that event B has already occured, what is the probability that event A also occurs? The event space is \\(A \\cap B\\) , as we are interested in the probability that both A and B occur. However, since event B has already occured, the sample space is no longer all possible outcomes but rather only the event space for B ! \\[ \\begin{aligned} P(A \\mid B) &= \\frac{n(A \\cap B)}{n(B)} \\\\ &= \\frac{\\frac{n(A \\cap B)}{n(\\Omega)}}{\\frac{n(B)}{n(\\Omega)}} \\\\ &= \\frac{P(A \\cap B)}{P(B)} \\end{aligned} \\] Following this expression, the probability of an intersection of two events is given by: \\[ P(A \\cap B) = P(A \\mid B) \\cdot P(B) = P(B \\mid A) \\cdot P(A) \\] Most experiments involving conditional probabilities are multi-staged experiments, which are best visualized using Probability Trees : Instead of calculating conditional probabilities from scratch, some questions provide the conditional probability \\(P(A \\mid B)\\) (or the components to do so!) and ask us to find the reverse - \\(P(B \\mid A)\\) . \\[ P(B \\mid A) = \\frac{P(B \\cap A)}{P(A)} \\] The formula is the same as before, but the issue is that the unconditional probability of event A is usually not given. This problem is accounted for in Bayes Theorem : \\[ \\begin{aligned} A &= (A \\cap B) + (A \\cap B^c) \\\\ P(A) &= P(A \\cap B) + P(A \\cap B^c) \\\\ P(A) &= P(A \\mid B) \\cdot P(B) + P(A \\mid B^c) \\cdot P(B^c) \\\\ \\\\ \\therefore P(B \\mid A) &= \\frac{P(A \\mid B) \\cdot P(B)}{P(A \\mid B) \\cdot P(B) + P(A \\mid B^c) \\cdot P(B^c)} \\end{aligned} \\] Note that if the Conditional Probability of A given B is the same as the unconditional probability of A, then events A and B are independent ; B has no effect on A. Thus, the probability of an intersection of two independent events is simply their product: \\[ \\begin{aligned} P(A \\mid B) &= \\frac{P(A \\cap B)}{P(B)} \\\\ P(A) &= \\frac{P(A \\cap B)}{P(B)} \\\\ P(A \\cap B) &= P(A) \\cdot P(B) \\end{aligned} \\]","title":"Conditional Probability"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#random-variables","text":"Unlike rolling a dice, the outcome of most experiments are non-numeric , which makes them hard to work with. For instance, the outcomes of a coin toss are \"Heads\" and \"Tails\". A Random Variable is a many to one function that maps each outcome to a single real number. Each outcome must have only one corresponding number, but different outcomes can have the same value. Note Although the mapping is deterministic, the underlying experiment is still random which is why it is still a \"random\" variable. The range of possible values that the random variable can take is known as its Support . They are broadly categorized based on its support: Discrete Continuous Countable Support Uncountable Support 1, 2, 3, 4, ... 1, 1.1, 1.01, 1.001, ... Random variables are denoted using upper case letters (X, Y, Z) while their corresponding values are denoted using lower case letters (x, y, z) and their appropriate subscripts . The notation \\(X(s) = x_1\\) denotes that the random variable \\(X\\) maps the outcome \\(s\\) to the value of \\(x_1\\) . Thus, the corresponding probability is denoted by \\(P(X = x_1)\\) . Note If the subscript is omitted, then \\(P(X=x)\\) is a general expression that describes the entire distribution of \\(X\\) , not just a single probability.","title":"Random Variables"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#probability-distributions","text":"Similar to how a random variable maps the outcomes to a real number, a Probability Distribution is a function that maps the outcomes to its probability of occurrence . For Discrete Random Variables , their distribution is described using a Probability Mass Function (PMF). The PMF provides the probability that the random variable is exactly equal to some value \\((X = x_1)\\) . It is typically denoted in lower case and sometimes includes a subscript of the random variable when working with multiple to distinguish them from one another. \\[ \\begin{aligned} P(X = a) &= p(a) \\\\ \\\\ P(X = a) &= p_X(a) \\\\ P(Y = a) &= p_Y(a) \\end{aligned} \\] Since it is a probability measure, the sum of the PMF over the support of the random variable must be equal to 1 (Probability Axiom). \\[ \\sum_{x \\in \\text{Support}} p(x) = 1 \\] PMFs can be represented in three main ways - Functions, Tables or Histograms. For Continuous Random Variables , their distribution is described using a Probability Density Function (PDF). The PDF is a non-negative function where the area under it provides the probability that the random variable takes on some range of values \\((a \\le X \\le b)\\) . Similarly, it is typically denoted in lower case and includes a subscript when working with multiple random variables: \\[ \\begin{aligned} P(a \\le X \\le b) = \\int^b_a f(x) \\\\ \\\\ P(a \\le X \\le b) = \\int^b_a f_X(x) \\\\ P(a \\le Y \\le b) = \\int^b_a f_Y(y) \\end{aligned} \\] Similarly, since the area is a probability measure, the total area under the graph must be equal to 1 : \\[ P(-\\infty \\le X \\le \\infty) = \\int^{\\infty}_{-\\infty} f(x) = 1 \\] Note \\(\\infty\\) is used as a catch all for the upper and lower bound of the random variable. If the actual bounds are known, then using them instead is more appropriate. Additionally, note that the probability of a specific value for a continuous RV is 0. This is because there is an infinite number of possible values , thus the probability of a specific value (EG. 1.45679383920) is infinitely small such that it is assumed to be 0. \\[ P(X = a) = \\int^{a}_{a} f(x) = 0 \\] The Cumulative Distribution Function (CDF) is the probability that the random variable is less than or equal to some value \\(X \\le t\\) . It is typically denoted in upper case to distinguish it from the PDF and includes subscripts as well when working with multiple random variables. \\[ \\begin{aligned} F(t) &= P(X < t) \\\\ \\\\ F_X(t) &= P(X < t) \\\\ F_Y(t) &= P(Y < t) \\end{aligned} \\] For discrete variables, the CDF is the sum of all probabilities before the specified value . Following this, the difference of consecutive CDFs allows us to obtain the PMFs at that value: \\[ \\begin{aligned} F(t) &= \\sum_{x \\le t} p(x) \\\\ p(x_i) &= F(x_i) - F(x_{i-1}) \\end{aligned} \\] For continuous variables, the CDF is the integral from the lower bound to the specified value . However, instead of integrating with respect to an actual value, it is better to integrate with respect to a dummy variable \\(t\\) to obtain a general expression for the CDF , allowing it to be easily calculated for any value. Although the CDF is very useful, it can only be used to calculate probabilities starting from the lower bound . When probabilities starting from other ranges are needed, the PDF can be obtained from the CDF by differentiating it and then re-integrating with different limits. \\[ \\begin{aligned} F(t) &= \\int^{t}_{-\\infty} f(x) dx \\\\ \\\\ F(t) &= \\int^{t}_{-\\infty} f(t) dt \\\\ f(t) &= F'(t) \\end{aligned} \\] Note The integral of the PDF can lead also lead to the Survival Function , as shown in the Survival Model Section .","title":"Probability Distributions"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#piecewise-distributions","text":"The probability distribution may be defined using a Piecewise Function , which means that it is defined via multiple sub-functions where each applies to different intervals in the domain: \\[ \\begin{aligned} f(x) &= \\begin{cases} f_{1}(x),& 0 \\lt x \\lt t_1 \\\\ f_{2}(x),& t_1 \\lt x \\lt t_2 \\\\ \\vdots \\end{cases} \\end{aligned} \\] When calculating probabilities, we must choose the appropriate PDF based on the range of the probability: \\[ \\begin{aligned} P(X \\lt t_{0.5}) &= \\int^{t_{0.5}}_{0} f_{1}(x) \\\\ P(X \\gt t_{1.5}) &= \\int^{t_{2}}_{t_{1.5}} f_{2}(x) \\end{aligned} \\] If the required range spans across multiple intervals , then the probability should be split according to the intervals: \\[ \\begin{aligned} P(t_{0.5} \\lt X \\lt t_{1.5}) &= P(t_{0.5} \\lt X \\lt t_{1}) + P(t_{1} \\lt X \\lt t_{1.5}) \\\\ &= \\int^{t_{1}}_{t_{0.5}} f_{1}(x) + \\int^{t_{1.5}}_{t_{1}} f_{2}(x) \\end{aligned} \\] Thus, this means that the CDF for a piecewise distribution must fully integrate all \"earlier\" PDFs : \\[ \\begin{aligned} P(X \\lt t) &= P(0 \\lt X \\lt t_{1}) + P(t_{1} \\lt X \\lt t) \\\\ &= \\int^{t_{1}}_{0} f_{1}(x) + \\int^{t}_{t_{1}} f_{2}(x) \\end{aligned} \\]","title":"Piecewise Distributions"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#moments","text":"The Moments of a distribution are quantities that describe characteristics of the distribution .","title":"Moments"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#first-moment","text":"Raw Moments are calculated with respect to the origin . The n-th raw moment is calculated as the following: \\[ \\begin{aligned} E(X^n) &= \\int x^n \\cdot f(x) dx \\\\ \\mu'_k &= \\sum x^n \\cdot p(x) \\end{aligned} \\] The first raw moment is known as the Mean , which is a measure of the Centrality of the distribution. It is commonly denoted as \\(\\mu\\) , without any super or subscripts. Note that the mean of a constant is the constant itself : \\[ E(c) = c \\]","title":"First Moment"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#second-moment","text":"Central Moments are calculated with respect to the mean . The n-th central moment is calculated as the following: \\[ \\begin{aligned} E[(X - \\mu)^n] &= \\int (x - \\mu)^n \\cdot f(x) dx \\\\ \\mu_k&= \\sum (x - \\mu)^n \\cdot p(x) \\end{aligned} \\] The second central moment is known as the Variance , which is a measure of the Spread of the distribution about the mean. Since calculating central moments directly is complicated, it can be simplified to an expression involving raw moments: \\[ \\begin{aligned} Var(X) &= E[(X - \\mu)^2] \\\\ &= E(X^2 - 2\\mu X + \\mu^2) \\\\ &= E(X^2) - 2\\mu^2 + \\mu^2 \\\\ &= E(X^2) - \\mu^2 \\\\ &= E(X^2) - [E(X)]^2 \\end{aligned} \\] Note that the variance of a constant is 0 as a constant cannot change: \\[ Var(c) = 0 \\] However, one problem with variance is that it uses squared units, which makes it hard to intepret. Thus, the squareroot of the variance is used instead, known as the Standard Deviation . \\[ \\sigma = \\sqrt{Var(X)} \\] Similarly, standard deviation cannot be used to compare data with different units . Thus, the Coefficient of Variation is used instead, which is a unitless measure of the spread of the distribution. \\[ CV(X) = \\frac{\\sigma}{\\mu} \\]","title":"Second Moment"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#third-moment","text":"The third central moment is Skewness , which is a measure of the symmetry of distribution about the mean. Being left/right skewed means that the distribution has a \"longer tail\" on that side, which implies that values on the opposite side are more likely to occur . Tip Skewness is also sometimes referred to as being Positively or Negatively Skewed . An easy way to remember is that positive values occur to the right of the origin, hence is the same as being right skewed; vice-versa. \\[ \\begin{aligned} \\text{Skewness} &= \\frac{E[(X - \\mu)^3]}{\\sigma^3} \\\\ &= \\frac{E[(X - \\mu)^3]}{(\\sigma^2)^\\frac{3}{2}} \\\\ &= \\frac{E(X^3) - 3 E(X^2) \\cdot E(X) + 2 [E(X)]^3}{(E(X^2) - [E(X)]^2)^\\frac{3}{2}} \\end{aligned} \\]","title":"Third Moment"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#fourth-moment","text":"The fourth central moment is Kurtosis , which is a measure of the flatness of the distribution, typically with respect to the normal distribution. It is indicative of the likelihood of producing extreme values (outliers). The normal distribution has a kurtosis of 3. If a distribution has a kurtosis greater than 3 , then it is flatter and hence more likely to produce outliers as compared to the normal distribution. \\[ \\begin{aligned} \\text{Kurtosis} &= \\frac{E[(X - \\mu)^4]}{\\sigma^4} \\\\ &= \\frac{E[(X - \\mu)^4]}{(\\sigma^2)^2} \\\\ &= \\frac{E(X^4) - 4 E(X^3) \\cdot E(X) + 6 E(X^2) \\cdot [E(X)]^2 - 3 [E(X)]^4}{(E(X^2) - [E(X)]^2)^2} \\end{aligned} \\]","title":"Fourth Moment"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#tail-weights","text":"Another method of determining the likelihood of outliers to occur is through the Tail Weight of the distribution. Distributions with a heavier tails are more likely to produce outliers . Warning Although both Tail Weight and Kurtosis are measures of the likelihood of outliers, there is no link between them , despite what some authors might suggest. A distribution is said to have a heavier tail than another if it has STRICTLY more density at the \"ends\" of the distributions, past a certain threshold: Note For the purposes of this exam, we only consider the right tail of the distribution for large outliers. There are a few methods to compare the tail weight of the distribution, but for the purposes of this exam, only the Existence of Moments method will be considered. A distribution is light tailed if has finite RAW moments for all positive values of \\(k\\) : \\[ E(X^k) \\lt \\infty, \\ k \\gt 0 \\] Conversely, a distribution is heavy tailed if it DOES NOT have finite raw moments for all positive values of \\(k\\) . From another perspective, it only has them up till a certain threshold \\(k \\lt a\\) : \\[ E(X^k) \\lt \\infty, \\ k \\lt a \\] Consider the following two distributions: Exponential Pareto Raw moments exists for \\(k \\gt 0\\) Raw moments exists for \\(-1 \\lt k \\lt \\alpha\\) Unlimited number of finite raw moments Limited number of finite raw moments Light tailed Heavy tailed Consider the formula for raw moments: \\[ E(X^k) = \\int x^k \\cdot f(x) \\] As \\(k\\) increases, large values of \\(x\\) will be raised by large values, resulting in extremely large values. If the densities (probability of occurring) at this portion of the distribution are sufficiently large , then the resulting value would be too large \\(E \\left[X^k\\right] = \\infty\\) , resulting in a limited number of finite raw moments . This means that the fewer finite raw moments, the heavier the tail and hence higher the probability of outliers .","title":"Tail Weights"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#statistical-metrics","text":"Apart from the moment of the distribution, there are some other Statistical Metrics that provide useful information. The Mode is the value of the random variable that maximises the PMF or PDF. It is the most likely outcome of the experiment (loosely speaking for continuous variables). The Median is the value of the random variable that seperates the upper and lower half of the probability distribution. For discrete variables , the median \\(M\\) is the smallest value such that the following two expressions are satisfied: \\[ \\begin{aligned} P(X \\le M) &\\ge 0.5 \\\\ P(X \\ge M) &\\ge 0.5 \\end{aligned} \\] For continuous variables , the median \\(M\\) is found by solving the CDF : \\[ \\begin{aligned} F(M) &= 0.5 \\\\ M &= F^{-1}(0.5) \\end{aligned} \\] Note The key difference is that the continuous median is the value that exactly seperates the distribution while the discrete median approximately splits it, depending on the PMF. The Percentile is the value of the random variable below which a certain percentage of observations fall . For instance, the 85 th percentile is the value below which 85% of the observations fall. The median is the 50 th percentile ; in other words when \\(p = 0.5\\) . Thus, the previous formulas can be generalized for any \\(p\\) : \\[ \\begin{aligned} P(X \\le M^{\\text{Discrete}}) &\\ge p \\\\ P(X \\ge M^{\\text{Discrete}}) &\\ge p \\\\ \\\\ M^{\\text{Continuous}} &= F^{-1}(p) \\end{aligned} \\] Tip Remember that the CDF for a piecewise distribution is calculated differently. The 25 th , 50 th & 75 th percentile are known as the first, second & third Quartiles \\((q)\\) respectively. The difference between the 3 rd and 1 st quartile is known as the Inter Quartile Range . \\[ IQR = q_3 - q_1 \\]","title":"Statistical Metrics"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#shifting-scaling-transformation","text":"An existing random variable \\(X\\) can be adjusted in order to make a new random variable \\(Y\\) . If a constant \\(a\\) has been multiplied to the random variable, then it has been Scaled by \\(a\\) : \\[ \\begin{aligned} Y &= aX \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(aX \\le y) \\\\ &= P \\left(X \\le \\frac{y}{a} \\right) \\\\ &= F_X \\left(\\frac{y}{a} \\right) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= \\frac{1}{a} \\cdot f_X \\left(\\frac{y}{a} \\right) \\end{aligned} \\] If a constant \\(b\\) is added to the random variable instead, then it has been Shifted by \\(b\\) : \\[ \\begin{aligned} Y &= X + b \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(X + b \\le y) \\\\ &= P(X \\le y - b) \\\\ &= F_X (y - b) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= f_X (y - b) \\end{aligned} \\] For both shifting and scaling, the expectation and variance can be easily determined if that of the original is known as well: \\[ \\begin{aligned} E(cX) &= c \\cdot E(X) \\\\ E(X+c) &= E(X) + E(c) \\\\ &= E(X) + c \\\\ Var(cX) &= c^2 \\cdot Var(x) \\\\ Var(X+c) &= Var(X) + Var(c) \\\\ &= Var(X) \\end{aligned} \\] If the random variable has been raised by a power of \\(\\frac{1}{c}\\) where \\(c \\ne 1\\) , then it has been Power Transformed : \\[ \\begin{aligned} Y &= X^{\\frac{1}{c}} \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(X^{\\frac{1}{c}} \\le y) \\\\ &= P(X \\le y^c) \\\\ &= F_X (y^c) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= cy^{c-1}\\cdot f_X (y - b) \\end{aligned} \\] If the random variable has been exponentiated , then it has also been Exponential Transformed : \\[ \\begin{aligned} Y &= e^X \\\\ \\\\ F_Y(y) &= P(Y \\le y) \\\\ &= P(e^X \\le y) \\\\ &= P(X \\le \\ln y) \\\\ &= F_X (\\ln y) \\\\ \\\\ f_Y(y) &= \\frac{d}{dy} F_Y(y) \\\\ &= \\frac{1}{y} \\cdot f_X (\\ln y) \\end{aligned} \\] For both types of transformations, there is no simple method of determining the mean and variance. The various raw moments must be manually determined via integration .","title":"Shifting, Scaling &amp; Transformation"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#joint-distributions","text":"Consider a scenario where there are two or more random variables \\((X_1, X_2, \\dots)\\) in the same probability space such that we are interested in some function of them \\(Y = g(x_1, x_2, \\dots)\\) . Note For the purposes of this exam, only Linear Combinations (sums) of random variables will be considered. For instance, the experiment could be studying the sum of rolling two dice . There would be two random variables \\((X_1, X_2)\\) , each denoting the value of their respective dice. Thus, the sum of the two dice would be \\(Y = X_1 + X_2\\) . Consider the probability of obtaining a sum of 3 - consider ALL combination of values of the two underlying variables will result in it: \\[ P(Y = 3) = P(X_1 = 1, X_2 = 2) + P(X_1 = 2, X_2 = 1) \\] This can be more generally expressed as the Joint Distribution of the two variables: \\[ \\begin{aligned} P(X = x, Y = y) &= P_{X,Y} (x,y) \\\\ P(a \\lt X \\lt b, c \\lt Y \\lt d) &= \\int^{d}_{c} \\int^{b}_{a} f_{X,Y} (x,y) \\ dx dy \\end{aligned} \\] The individual distributions within this shared probability space is known as the Marginal Distribution and is obtained by \"integrating out\" the other variable: \\[ \\begin{aligned} f_{X}(x) &= \\int^{d}_{c} f_{X,Y} (x,y) \\ dy \\\\ f_{Y}(y) &= \\int^{b}_{a} f_{X,Y} (x,y) \\ dx \\end{aligned} \\]","title":"Joint Distributions"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#joint-domain","text":"One of the main concerns when dealing with joint distributions is the domain of the distribution. Each random variable is defined on their own domain, but the joint distribution is only defined in the domain in which two variables coincide . For a joint distribution of discrete variables , it is simply a matter of finding all possible combinations of the two variables. For a two variable distribution, this can be easily visualized via a Contingency Table : The table sometimes lists the frequency of observations while sometimes it lists their probability . In either case, by finding the cells of interest , the probability can be obtained. For a joint distribution of continuous variables , it is best visualized graphically . For a two variable distribution: \\[ 1 \\lt X_1 \\lt 5, \\ 2 \\lt X_2 \\lt 8 \\] Let \\(Y\\) be the sum of the two random variables. Consider the following probability: \\[ \\begin{aligned} Y &= X_1 + X_2 \\\\ P(Y \\lt 5) &= P(X_1 + X_2 \\lt 5) \\\\ &= P(X_2 \\lt 5 - X_1) \\end{aligned} \\] Thus, we need to find the domain of the above inequality WITHIN the joint domain . This can be done by plotting the graph of the upper/lower bound (simply change the inequality to an equal sign). The domain can be intepreted in one of two ways: \\(X_1\\) to line first: \\(1 \\lt X_1 \\lt 3, \\ 2 \\lt y \\lt 5-x\\) \\(X_2\\) to line first: \\(1 \\lt X_1 \\lt 5-y, \\ 2 \\lt y \\lt 4\\) Using these domains, the double integration can be performed to obtain the probability: \\[ \\begin{aligned} P(X_2 \\lt 5 - X_1) &= \\int^{1}_{3} \\int^{5-x}_{2} f_{X,Y}(x,y) \\ dy dx \\\\ &= \\int^{2}_{4} \\int^{5-y}_{1} f_{X,Y}(x,y) \\ dx dy \\end{aligned} \\] Tip Recall that when performing a double integration, the INNER integral is evaluated first . The inner and outer integral are interchangeable, thus set the inner integral such that it will ease the integration . In this case, the integral with an algebraic expression should always be evaluated first, ensuring that the final result is a probability and not an expression. If there is a Kink in the shape, then the area should be broken into two smaller areas without any kinks in their shape. The overall probability is the sum of the areas: \\[ P(X_2 \\lt 8 - X_1) = \\text{Orange Area} + \\text{Purple Area} \\]","title":"Joint Domain"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#joint-moments","text":"Given that \\(Y = X_1 + X_2\\) , its mean and variance is the following: \\[ \\begin{aligned} E(Y) &= E(X_1) + E(X_2) \\\\ \\\\ Var (Y) &= Var (X_1) + Var (X_2) + 2 \\cdot Cov (X,Y) \\end{aligned} \\] The last term is known as the Covariance , which is the first central moment about of the joint distribution. It measures the linear relationship between variables: Positive Covariance : Both variables move in the same direction Negative Covariance : Both variables move in opposite directions \\[ \\begin{aligned} Cov (X_1, X_2) &= E[(X_1 - E(X_1))(X_2 - E(X_2))] \\\\ &= E(X_1 \\cdot X_2) - E(X_1) \\cdot E(X_2) \\\\ \\\\ E(X_1 \\cdot X_2) &= \\int \\int (x_1 \\cdot x_2) \\cdot f_{X_1, X_2} (x_1, x_2) \\ dx_1 dx_2 \\end{aligned} \\] Note that it has some interesting properties: \\(Cov (X_1, c) = 0\\) \\(Cov (X_1, X_2) = Var (X_1)\\) \\(Cov (c \\cdot X_1, X_2) = c \\cdot Cov (X_1,X_2)\\) \\(Cov (X_1 + c, X_2) = Cov (X_1, X_2) + Cov (c, X_2)\\) However, covariance is both limitless and squared units , which makes hard to use as a metric for comparison. We can overcome these problems by scaling the covariance down by the individual standard deviations, obtaining the Correlation of the two variables: \\[ Corr (X_1, X_2) = \\frac{Cov (X_1, X_2)}{SD(X_1), SD(X_2)} \\] If \\(X_1\\) and \\(X_2\\) are Independent , then their distributions and moments become the following: \\[ \\begin{aligned} P(X_1 = x_1, X_2 = x_2) &= P(X_1 = x_1) \\cdot P(X_2 = x_2) \\\\ f_{X_1,X_2} (x_1,x_2) &= f_{X_1}(x_1) \\cdot f_{X_2}(x_2) \\\\ \\\\ E(X_1 \\cdot X_2) &= E(X_1) \\cdot E(X_2) \\\\ Var (X_1 + X_2) &= Var (X_1) + Var (X_2) \\end{aligned} \\] Warning If the domain of the random variables naturally contain other variables, then they are not independent, even if the above conditions are met. Note that having 0 covariance is a consequence of independence , but is NOT indicative of it. Variables may also have 0 covariance when they have a non-linear relationship . \\[ \\begin{aligned} Cov (X_1, X_2) &= E(X_1 \\cdot X_2) - E(X_1) \\cdot E(X_2) \\\\ &= E(X_1) \\cdot E(X_2) - E(X_1) \\cdot E(X_2) \\\\ &= 0 \\\\ \\\\ \\therefore Corr (X_1, X_2) &= 0 \\end{aligned} \\]","title":"Joint Moments"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#generating-functions","text":"A useful way to analyze the sum of independent random variables is to convert the PDF/PMF of the individual distributions into a Generating Function . The first kind is known as a Moment Generating Function (MGF): \\[ \\begin{aligned} M_{X}(t) &= E(e^{tX}) \\\\ &= \\sum e^{tx} \\cdot p_{X}(x) \\\\ &= \\int e^{tx} \\cdot f_{X}(x) \\end{aligned} \\] As its name suggests, the MGF can be used to calculate the raw moments of the distribution. To obtain the n-th raw moment , Differentiate the MGF k times Evaluate the expression at \\(t=0\\) \\[ E \\left(X^n \\right) = \\frac{d^n}{dt^n} \\cdot M_{X}(0) \\] However, the main benefit of MGFs is that they uniquely identify a distribution. If two random variables have the same MGF , then they have the same distribution . This becomes especially useful when dealing with sums of independent random variables . By determining the MGF of the combination, its exact distribution can be determined. \\[ \\begin{aligned} Y &= X_1 + X_2 + ... + X_n \\\\ \\\\ M_Y(t) &= E(e^{tY}) \\\\ &= E(e^{t(X_1 + X_2 + ... + X_n)}) \\\\ &= E(e^{tX_1} \\cdot e^{tX_2} \\cdot ... \\cdot e^{tX_n}) \\\\ &= \\prod E(e^{tx}) \\\\ &= \\prod M_{X}(t) \\end{aligned} \\] The other kind is known as a Probability Generating Function (PGF), which only applies for sums of discrete variables : \\[ \\begin{aligned} P_{X}(t) &= E(t^x) \\\\ &= \\sum t^X \\cdot p(x) \\end{aligned} \\] Note The PGF is denoted in upper case \\(P\\) while the PMF is denoted in lower case \\(p\\) . As its name suggests, the PGF can be used to calculate the individual probabilities of the distribution. To obtain the probability of the n-th value , Differentiate the PGF k times Divide the expression by n factorial Evaluate the expression at \\(t=0\\) \\[ P(X = k) = \\frac{d^n}{dt^n} \\cdot \\frac{P_{X}(0)}{n!} \\] Similarly, the PGF uniquely identifies the distribution and can be used in all the same ways as an MGF in this regard: \\[ \\begin{aligned} Y &= X_1 + X_2 + ... + X_n\\\\ \\\\ P_Y(t) &= E(t^Y) \\\\ &= E(t^{X_1 + X_2 + ... + X_n}) \\\\ &= E(t^{X_1} \\cdot t^{X_2} \\cdot ... \\cdot t^{X_n}) \\\\ &= \\prod E(t^{x}) \\\\ &= \\prod P_{X}(t) \\end{aligned} \\] Given how similar the two are, they can be easily converted to and from one another: \\[ \\begin{aligned} P_X(t) &= E(t^X) \\\\ &= E(e^{\\ln t^X}) \\\\ &= M_X(\\ln t^x) \\\\ \\\\ M_X(t) &= E(e^tX) \\\\ &= E[(e^t)^x] \\\\ &= P_X(e^t) \\end{aligned} \\]","title":"Generating Functions"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#conditional-distributions","text":"If a random variable \\(X\\) is conditioned on a range of values of itself , then it has the Conditional Distribution \\((X \\mid X>j)\\) : \\[ \\begin{aligned} P(X = x \\mid X > j) &= \\frac{P(X = x)}{P(X > j)} \\\\ f_{X \\mid X>j} (x) &= \\frac{f_X(x)}{P(X > j)} \\end{aligned} \\] Similarly, a random variable can be conditional on ANOTHER random variable , resulting in the Conditional Distribution \\((X \\mid Y)\\) . \\[ \\begin{aligned} P(X = x \\mid Y = y) &= \\frac{P(X = x, Y = y)}{P(Y = y)} \\\\ f_{X \\mid Y} (x,y) &= \\frac{f_{X,Y}(x,y)}{f_{Y}(y)} \\\\ \\text{Conditional} &= \\frac{\\text{Joint}}{\\text{Marginal}} \\end{aligned} \\] Any calculations involving the above distributions must be solved via first principles . However, most problems require us to find the Marginal Distribution given only the conditional distributions: \\(X\\) is the random variable denoting the test grades (A, B, C) \\(Y\\) is the random variable denoting the gender (M, F) The teacher would like to find the marginal distribution of test scores \\((X)\\) , but only has the conditional distribution of the scores of the students for each gender \\((X \\mid Y)\\) and the proportion of the Genders \\((Y)\\) . The Law of Total Probability can be used to determine the marginal probability of \\(X\\) : \\[ \\begin{aligned} P(X = a) &= E_Y[P(X = a \\mid Y)] \\\\ &= \\sum_{y} P(X = a \\mid y) \\cdot p(Y = y) \\end{aligned} \\] Note that this is equivalent to adding up the final probabilities from the relevant branches from a probability tree: Naturally, this also means that the marginal CDF can be obtained in similar fashion: \\[ \\begin{aligned} F_X(a) &= E_Y[F_{X \\mid Y}(a)] \\\\ &= \\sum_{y} F_{X \\mid Y}(a) \\cdot p(Y = y) \\end{aligned} \\] Following the same logic, the Law of Total Expectation can be used to determine the marginal expectation of \\(x\\) : \\[ \\begin{aligned} E(X) &= E_Y[E_X(X \\mid Y)] \\\\ &= \\sum_{y} E_X(X \\mid Y) \\cdot p(Y = y) \\end{aligned} \\] Warning Since \\(E(X)\\) is a constant, a common mistake is thinking that \\(E_X(X \\mid Y)\\) is a constant as well since they are both expectations. The issue is that it is conditional on \\(Y\\) , which is still random, which makes the conditional expectation still random . The Law of Total Variance can be used to determine the marginal variance of \\(x\\) . However, unlike the previous two, it is NOT simply the expectation of the conditional variance: \\[ \\begin{aligned} Var (X) &= E_Y [Var_X(X \\mid Y)] + Var_Y[E_X(X \\mid Y)] \\\\ \\\\ E_Y [Var_X(X \\mid Y)] &= \\sum_{y} Var_X(X \\mid Y) \\cdot p(Y = y) \\\\ \\\\ Var_Y[E_X(X \\mid Y)] &= E_Y[E_X(X \\mid Y)^2] - (E_Y[E_X(X \\mid Y)])^2 \\\\ &= \\sum_{y} E_X(X \\mid Y)^2 \\cdot p(Y = y) - \\sum_{y} E_X(X \\mid Y) \\cdot p(Y = y) \\end{aligned} \\] Alternatively, the Marginal Variance can be directly calculated using the typical formula of \\(E(X^2) - [E(X)]^2\\) , where the two marginal expectations are calculated using the law of total expectation. Alternatively once more, if the conditional distribution \\(Y\\) only has two outcomes, then the Bernoulli Shortcut (covered in a later section) can be used to quickly compute the value of \\(Var_Y[E_X(X \\mid Y)]\\) . Tip The discrete case was shown in this section due to its simplicity. All the same concepts apply to the continuous variables as well - simply replace the summation & PMFs with integrals and PDFs .","title":"Conditional Distributions"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/1.%20Review%20of%20Probability%20Theory/#mixture-distributions","text":"A mixture distribution is a distribution whose values can be intepreted as being derived from an underlying set of other random variables . In an insurance context, a Homeowners Insurance claim could be from a fire, burglary or liability accident. To model it, we could use a mixture that is made up of the basic distributions used to individually model each type of accident. If the mixture contains a countable number of other distributions, then it is known as a Discrete Mixture . Otherwise, it is known as a Continuous Mixture . Warning It is a common mistake to think that a discrete mixture is only made up of discrete distributions, vice-versa. Any type of distribution can be included in a mixture; the classification is based on the number of distributions. The random variable \\(X\\) is a k-point mixture if its probability functions can be expressed as the weighted average of the probability functions of the \\(k\\) distributions \\(X_1, X_2, ... X_k\\) : \\[ \\begin{aligned} F_X(x) &= w_1 \\cdot F_{X_1}(x) + w_2 \\cdot F_{X_2}(x) + ... + w_k \\cdot F_{X_k}(x) \\\\ f_X(x) &= w_1 \\cdot f_{X_1}(x) + w_2 \\cdot f_{X_2}(x) + ... + w_k \\cdot f_{X_k}(x) \\end{aligned} \\] Note For this exam, questions will usually only use 2 or 3 point mixtures . \\(w\\) represents the mixing weights , such that \\(w_1 + w_2 + ... + w_k = 1\\) . It can be intepreted that \\(Y\\) follows the distribution of \\(X_1\\) \\(100w_1 \\%\\) of the time, follows the distribution of \\(X_2\\) \\(100w_2 \\%\\) of the time etc. Warning Another common mistake is confusing mixtures with Linear Combinations of random variables: \\[ X \\ne w_1 X_1 + w_2 X_2 + ... + w_k X_k \\] In a linear combination, \\(X\\) neither follows the distribution of any of the \\(X_k\\) . Furthermore, since \\(w_k\\) are not weights, they can be any real number and do not need to sum to 1 . The mixing weights can also be thought of as Discrete Probabilities that come from a random variable \\(Y\\) representing the \\(k\\) underlying distributions with the support \\(\\set{1, 2, ..., k}\\) where \\(P(Y = k) = w_k\\) . Thus, we can think of the overall mixture \\(X\\) as an unconditional distribution while each of the underlying distributions are conditional distributions \\(X \\mid Y\\) . This allows us to make use of the all the previous results from the conditional distributions: \\[ \\begin{aligned} P(X = a) &= E_Y[P(X = a \\mid Y)] \\\\ F_X(a) &= E_Y[F_{X \\mid Y}(a)] \\\\ E(X) &= E_Y[E_X(X \\mid Y)] \\\\ Var (X) &= E_Y [Var_X(X \\mid Y)] + Var_Y[E_X(X \\mid Y)] \\end{aligned} \\] In terms of the weights, for a simple two point mixture : \\[ \\begin{aligned} P(X = a) &= P(X = a \\mid Y=1) \\cdot w_1 + P(X = a \\mid Y=2) \\cdot w_2 \\\\ F_X(a) &= F_{X \\mid Y=1}(a) \\cdot w_1 + F_{X \\mid Y=2}(a) \\cdot w_2 \\\\ E(X) &= E_X(X \\mid Y=1) \\cdot w_1 + E_X(X \\mid Y=2) \\cdot w_2 \\end{aligned} \\]","title":"Mixture Distributions"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/2.%20Frequency%20Models/","text":"Frequency Models \u00b6 Frequency Distributions \u00b6 The number of claims is usually known as the Loss Frequency and is typically modelled with a discrete distribution. Let \\(N\\) be the random variable denoting the number of claims. This section will cover all the relevant frequency distributions, but note that there is no need to memorize anything as they are all provided in the formula sheet. Poisson Distribution \u00b6 The Poisson Distribution is used to count the number of random events in a specified unit of space or time . It only has one parameter \\(\\mu\\) , the mean number of occurrences in the specified unit of space or time. The key property is that its mean and variance are both equal to \\(\\mu\\) . \\[ \\begin{aligned} N &\\sim \\text{Poisson}(\\mu) \\\\ \\\\ p_n &= \\frac{e^{-\\mu} \\cdot \\mu^k}{k!} \\\\ \\\\ E(N) &= \\mu \\\\ Var (N) &= \\mu \\end{aligned} \\] The sum of \\(k\\) independent poisson random variables is still poisson : \\[ \\begin{aligned} N_k &\\sim \\text{Poisson}(\\mu_k) \\\\ N &= N_1 + N_2 + ... + N_k \\\\ \\therefore N &\\sim \\text{Poisson} (\\mu_1 + \\mu_2 + ... + \\mu_k) \\end{aligned} \\] Bernoulli Distribution \u00b6 The Bernoulli Distribution is used to determine the outcome of a Bernoulli Trial . They are experiments with only two possible outcomes . For a Standard Bernoulli Distribution , the two outcomes are Successes and Failures , denoted by 1 and 0 respectively: \\[ \\begin{aligned} X &\\sim \\text{Bernoulli} (q) \\\\ \\\\ p_x &= \\begin{cases} \\text{Success } (1),& \\text{Probability} = q, \\\\ \\text{Failure } (0),& \\text{Probability} = 1-q \\end{cases} \\\\ \\\\ E(X) &= q \\\\ Var (X) &= q(1-q) \\end{aligned} \\] However, the Bernoulli Shortcut generalizes this for any two mutually exclusive outcomes , denoted by \\(a\\) and \\(b\\) respectively: \\[ \\begin{aligned} Y &= (a-b)X + b \\\\ \\\\ Y &= \\begin{cases} \\text{Outcome 1 } (a),& \\text{Probability} = q, \\\\ \\text{Outcome 2 } (b),& \\text{Probability} = 1-q \\end{cases} \\\\ \\\\ E(Y) &= E[(a-b)X + b] \\\\ &= (a-b)q + b \\\\ \\\\ Var (Y) &= Var[(a-b)X + b] \\\\ &= (a-b)^2 \\cdot q(1-q) \\end{aligned} \\] Note that there is no need to memorize the mean and variance for the bernoulli shortcut - they can be easily derived from the standard distribution. Binomial Distribution \u00b6 The Binomial Distribution is used to count the number of successes out of a fixed number of independent Standard Bernoulli Trials . It has two parameters : The number of independent trials, \\(m\\) The probability of success for each trial, \\(q\\) \\[ \\begin{aligned} N &\\sim \\text{Binomial} (m, q) \\\\ \\\\ p_n &= {m \\choose k} q^k (1-q)^{1-k} \\\\ \\\\ E(N) &= nq \\\\ Var (N) &= nq(1-q) \\end{aligned} \\] The binomial distribution is actually the sum of \\(m\\) independent standard bernoulli variables with the same probability : \\[ \\begin{aligned} X_k &\\sim \\text{Bernoulli} (q) \\\\ N &= X_1 + X_2 + ... + X_m \\\\ \\therefore N &\\sim \\text{Binomial} (m, q) \\end{aligned} \\] Tip A bernoulli distribution is simply a binomial distribution with \\(m=1\\) . Thus, the sum of \\(k\\) independent binomial variables with the same probability will still binomial : \\[ \\begin{aligned} N_k &\\sim \\text{Binomial}(m, q) \\\\ N &= N_1 + N_2 + ... + N_k \\\\ \\therefore N &\\sim \\text{Binomial} (m_1 + m_2 + ... + m_k, q) \\end{aligned} \\] This is intuitive, because the sum of binomial variables is actually the sum of even more bernoulli variables , which as shown previously, will follow the binomial distribution. Geometric Distribution \u00b6 The Geometric Distribution can be understood in one of two ways, with respect to a sequence of independent bernoulli trials : Number of trials needed to get the first success , \\(N \\in \\set{1, 2, 3, ...}\\) Number of failures needed to get the first success , \\(F \\in \\set{0, 1, 2, ...}\\) If not explicitly stated, the two intepretations can be distinguished from their supports - there must be at least 1 trial but there can be 0 failures. It is useful to remember just one intepretation and understand how they are related: \\[ F = N-1 \\] The second intepretation is preferred , since that is what is provided in the formula sheet. Regardless, the Geometric distribution only has one parameter , the probability of success: \\[ \\begin{aligned} N &\\sim \\text{Geom} (q) \\\\ \\\\ p_k &= (1-q)^{k} \\cdot q \\\\ \\\\ E(N) &= \\frac{1-p}{p} \\\\ Var (N) &= \\frac{1-p}{p^2} \\end{aligned} \\] The key property is that it is Memoryless . Intuitively, the geometric distribution is \"waiting\" for the first success to occur. However, it does not matter how many failures have occurred , the probability of the first success occuring in another \\(k\\) failures is independent of the history of the process ; it remains constant over time. The distribution \"forgets\" (memoryless) the current state that the process is in. \\[ \\begin{aligned} P(N > m + n \\mid N \\ge m) &= P(N > n) \\\\ E(N > m + n \\mid N \\ge m) &= E(N) \\\\ Var(N > m + n \\mid N \\ge m) &= Var(N) \\end{aligned} \\] Negative Binomial Distribution \u00b6 The Negative Binomial Distribution is used to count the number of failures to get a fixed number of successes , with respect to a sequence of independent bernoulli trials. It has two parameters: The number of successes, \\(r\\) The probability of success, \\(q\\) \\[ \\begin{aligned} N &\\sim \\text{Negative Binomial} (r, q) \\\\ \\\\ p_n &= {{k+r-1} \\choose {k}} (1-p)^k p^{r} \\\\ \\\\ E(N) &= \\frac{r(1-p)}{p} \\\\ Var (N) &= \\frac{r(1-p)}{p^2} \\end{aligned} \\] Info The distribution is also referred to as Pascals Distribution , but is most commonly called the \"negative\" binomial because it uses a negative integer in the binomial coefficient : \\[ \\begin{aligned} {n \\choose k} &= \\frac{n!}{k! \\cdot (n-k)!} \\\\ \\\\ {{k+r-1} \\choose {k}} &= \\frac{(k+r-1)!}{k! \\cdot (r-1)!} \\\\ &= \\frac{(k+r-1) \\cdot (k+r-2) \\cdot \\dots \\cdot (r)}{k!} \\\\ &= (-1)^k \\cdot \\frac{(-r) \\cdot (-r-1) \\cdot \\dots \\cdot (-r-k+1)}{k!} \\\\ &= (-1)^k {-r \\choose k} \\end{aligned} \\] The negative binomial distribution is actually the sum of \\(r\\) i.i.d. geometric variables with the same probability : \\[ \\begin{aligned} X_k &\\sim \\text{Geom} (q) \\\\ N &= X_1 + X_2 + ... + X_r \\\\ \\therefore N &\\sim \\text{Negative Binomial} (r, q) \\end{aligned} \\] Tip A geometric distribution is simply a negative binomial distribution with \\(r=1\\) . Thus, following the same logic as before, the sum of \\(k\\) independent negative binomial variables with the same probability will still be negative binomial : \\[ \\begin{aligned} N_k &\\sim \\text{Negative Binomial}(r, q) \\\\ N &= N_1 + N_2 + ... + N_k \\\\ \\therefore N &\\sim \\text{Negative Binomial} (r_1 + r_2 + ... + r_k, q) \\end{aligned} \\] Poisson Gamma Mixture \u00b6 Consider a poisson distribution with mean \\(\\mu\\) where \\(\\mu\\) follows a gamma distribution. The unconditional distribution actually follows a Negative Binomial Distribution with the same parameters as the gamma distribution. \\[ \\begin{aligned} N \\mid \\mu &\\sim \\text{Poisson} (\\mu) \\\\ \\mu &\\sim \\text{Gamma} (\\alpha, \\theta) \\\\ \\therefore N &\\sim \\text{Negative Binomial}(r = \\alpha, \\beta = \\theta) \\end{aligned} \\] The above result is a Continuous Mixture - it is the result of a mixture of an infinite number of poisson distributions . Continuous Mixtures are beyond the scope of this exam, which is why the proof is not shown. However, this specific result is important to know. (a, b, 0) Class \u00b6 ONLY the distributions discussed above fall into the (a, b, 0) class of distributions, as their PMFs follow the same recursion : \\[ \\begin{aligned} \\frac{p_{n}}{p_{n-1}} &= a + \\frac{b}{n} \\\\ p_n &= \\left(a + \\frac{b}{n} \\right) p_{n-1} \\end{aligned} \\] \\(a\\) and \\(b\\) are constants that are unique to each distribution while the \"0\" comes from the fact that the recursion starts from \\(n-1=0\\) . Tip The parameters for each distribution can also be determined from the values of \\(a\\) and \\(b\\) . Notice that each distribution has a different signs for \\(a\\) . Thus, given the recursive equation, the underlying distribution can be determined based on the sign of \\(a\\) . Choosing Distributions \u00b6 Given a sample of data, we need to decide which distribution best represents it. The general idea is that each distribution has some unique characteristics - if the dataset matches those characteristics, then the distribution should be used. The first method involves the Mean and Variance of the distribution. Notice that for all three distributions, the mean and variance have a different relative size to one another . Thus, by comparing the Sample Mean and Unbiased Sample Variance of the dataset, it can be matched to the corresponding distribution. Warning It is almost impossible (whether in practice or theory) to find a sample mean and variance that is exactly equal to one another. Generally speaking, if they are sufficiently close together, then they can be assumed to be equal. However, being \"sufficiently close together\" is arbitrary. Thus, a better way would be to compare the ratio of the two values - if the ratio is around 1.00 , then it can be concluded that the two values are equal . The second method involves the sign of \\(a\\) in the recursion process. Since all three distributions have different signs for \\(a\\) , their probabilities will move differently as \\(n\\) increases. Thus, by observing how the probability of the sample changes, it can be matched to the corresponding distribution. However, we must first multiply the ratio of the probabilities by \\(n\\) in order to obtain a smooth linear line that clearly exhibits the relationships: \\[ \\begin{aligned} \\frac{p_{n}}{p_{n-1}} &= a + \\frac{b}{n} \\\\ \\frac{n \\cdot p_{n}}{p_{n-1}} &= an + b \\end{aligned} \\] (a, b, 1) Class \u00b6 When using the (a, b, 0) class of distributions to model experiments, one common problem is that the probability at \\(n=0\\) does not match experience . Thus, there is a need to modify the distribution such that the probability at 0 is at the desired level . This modification is known as the Zero-Modification . Note If the desired level is 0, then the modification is known as a Zero-Truncation , which is a special case of the zero-modification. This is because this truncates (removes) the possibility of 0 from the random variable. This is done by directly changing the probability at 0 to the desired level. However, this modification alone would cause the sum of probabilities to deviate from 1, thus the probabilities at all other levels of the support must be scaled such that they sum to 1 . Let \\(N^M\\) be the zero-modified distribution and \\(p^{M}_{n}\\) be its PMF. \\[ p^{M}_{n} = \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_n \\] If \\(N\\) is a poisson random variable, then \\(N^M\\) is known as a Zero-Modified Poisson Random Variable . However, it is important to note that \\(N^M\\) does NOT follow a poisson distribution - it has its own unique distribution. Thus, by applying the previous result, the moments of the new distribution can be calculated: \\[ \\begin{aligned} E \\left(N^M \\right) &= 0 \\cdot p^{M}_{0} + 1 \\cdot p^{M}_{1} + 2 \\cdot p^{M}_{2} + ... \\\\ &= 0 \\cdot \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_0 + 1 \\cdot \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_1 + 2 \\cdot \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_2 + ... \\\\ &= \\frac{1-p^{M}_{0}}{1-p_{0}} (0 \\cdot p_0 + 1 \\cdot p_1 + 2 \\cdot p_2 + ...) \\\\ &= \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot E(N) \\\\ \\\\ \\therefore E \\left[\\left(N^M \\right)^k \\right] &= \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot E \\left[N^k \\right] \\end{aligned} \\] Note that this result ONLY applies to raw moments . In order to calculate the central moments, express them in terms of raw moments and then calculate them from the above conversion. Even after modification, the PMFs can still follow the same recursion: \\[ \\begin{aligned} \\frac{p^{M}_{n}}{p^{M}_{n-1}} &= \\frac{\\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_{n}}{\\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_{n-1}} \\\\ &= \\frac{p_{n}}{p_{n-1}} \\\\ &= a + \\frac{b}{n} \\end{aligned} \\] However, since the distribution is zero-modified, the recursion can only start from \\(k-1=1\\) , which is why they are known as the (a, b, 1) class of distributions. The domain of the recursion is what allows us to distinguish the two classes of distributions. Warning There are only 4 distributions under the (a, b, 0) class: Poisson, Binomial, Geometric & Negative Binomial. It is seemingly intuitive to think that their 0-modified versions are the only members of the (a, b, 1) class. However, the Logarithmic Distribution is also a member of the (a, b, 1).","title":"Frequency Models"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/2.%20Frequency%20Models/#frequency-models","text":"","title":"Frequency Models"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/2.%20Frequency%20Models/#frequency-distributions","text":"The number of claims is usually known as the Loss Frequency and is typically modelled with a discrete distribution. Let \\(N\\) be the random variable denoting the number of claims. This section will cover all the relevant frequency distributions, but note that there is no need to memorize anything as they are all provided in the formula sheet.","title":"Frequency Distributions"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/2.%20Frequency%20Models/#poisson-distribution","text":"The Poisson Distribution is used to count the number of random events in a specified unit of space or time . It only has one parameter \\(\\mu\\) , the mean number of occurrences in the specified unit of space or time. The key property is that its mean and variance are both equal to \\(\\mu\\) . \\[ \\begin{aligned} N &\\sim \\text{Poisson}(\\mu) \\\\ \\\\ p_n &= \\frac{e^{-\\mu} \\cdot \\mu^k}{k!} \\\\ \\\\ E(N) &= \\mu \\\\ Var (N) &= \\mu \\end{aligned} \\] The sum of \\(k\\) independent poisson random variables is still poisson : \\[ \\begin{aligned} N_k &\\sim \\text{Poisson}(\\mu_k) \\\\ N &= N_1 + N_2 + ... + N_k \\\\ \\therefore N &\\sim \\text{Poisson} (\\mu_1 + \\mu_2 + ... + \\mu_k) \\end{aligned} \\]","title":"Poisson Distribution"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/2.%20Frequency%20Models/#bernoulli-distribution","text":"The Bernoulli Distribution is used to determine the outcome of a Bernoulli Trial . They are experiments with only two possible outcomes . For a Standard Bernoulli Distribution , the two outcomes are Successes and Failures , denoted by 1 and 0 respectively: \\[ \\begin{aligned} X &\\sim \\text{Bernoulli} (q) \\\\ \\\\ p_x &= \\begin{cases} \\text{Success } (1),& \\text{Probability} = q, \\\\ \\text{Failure } (0),& \\text{Probability} = 1-q \\end{cases} \\\\ \\\\ E(X) &= q \\\\ Var (X) &= q(1-q) \\end{aligned} \\] However, the Bernoulli Shortcut generalizes this for any two mutually exclusive outcomes , denoted by \\(a\\) and \\(b\\) respectively: \\[ \\begin{aligned} Y &= (a-b)X + b \\\\ \\\\ Y &= \\begin{cases} \\text{Outcome 1 } (a),& \\text{Probability} = q, \\\\ \\text{Outcome 2 } (b),& \\text{Probability} = 1-q \\end{cases} \\\\ \\\\ E(Y) &= E[(a-b)X + b] \\\\ &= (a-b)q + b \\\\ \\\\ Var (Y) &= Var[(a-b)X + b] \\\\ &= (a-b)^2 \\cdot q(1-q) \\end{aligned} \\] Note that there is no need to memorize the mean and variance for the bernoulli shortcut - they can be easily derived from the standard distribution.","title":"Bernoulli Distribution"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/2.%20Frequency%20Models/#binomial-distribution","text":"The Binomial Distribution is used to count the number of successes out of a fixed number of independent Standard Bernoulli Trials . It has two parameters : The number of independent trials, \\(m\\) The probability of success for each trial, \\(q\\) \\[ \\begin{aligned} N &\\sim \\text{Binomial} (m, q) \\\\ \\\\ p_n &= {m \\choose k} q^k (1-q)^{1-k} \\\\ \\\\ E(N) &= nq \\\\ Var (N) &= nq(1-q) \\end{aligned} \\] The binomial distribution is actually the sum of \\(m\\) independent standard bernoulli variables with the same probability : \\[ \\begin{aligned} X_k &\\sim \\text{Bernoulli} (q) \\\\ N &= X_1 + X_2 + ... + X_m \\\\ \\therefore N &\\sim \\text{Binomial} (m, q) \\end{aligned} \\] Tip A bernoulli distribution is simply a binomial distribution with \\(m=1\\) . Thus, the sum of \\(k\\) independent binomial variables with the same probability will still binomial : \\[ \\begin{aligned} N_k &\\sim \\text{Binomial}(m, q) \\\\ N &= N_1 + N_2 + ... + N_k \\\\ \\therefore N &\\sim \\text{Binomial} (m_1 + m_2 + ... + m_k, q) \\end{aligned} \\] This is intuitive, because the sum of binomial variables is actually the sum of even more bernoulli variables , which as shown previously, will follow the binomial distribution.","title":"Binomial Distribution"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/2.%20Frequency%20Models/#geometric-distribution","text":"The Geometric Distribution can be understood in one of two ways, with respect to a sequence of independent bernoulli trials : Number of trials needed to get the first success , \\(N \\in \\set{1, 2, 3, ...}\\) Number of failures needed to get the first success , \\(F \\in \\set{0, 1, 2, ...}\\) If not explicitly stated, the two intepretations can be distinguished from their supports - there must be at least 1 trial but there can be 0 failures. It is useful to remember just one intepretation and understand how they are related: \\[ F = N-1 \\] The second intepretation is preferred , since that is what is provided in the formula sheet. Regardless, the Geometric distribution only has one parameter , the probability of success: \\[ \\begin{aligned} N &\\sim \\text{Geom} (q) \\\\ \\\\ p_k &= (1-q)^{k} \\cdot q \\\\ \\\\ E(N) &= \\frac{1-p}{p} \\\\ Var (N) &= \\frac{1-p}{p^2} \\end{aligned} \\] The key property is that it is Memoryless . Intuitively, the geometric distribution is \"waiting\" for the first success to occur. However, it does not matter how many failures have occurred , the probability of the first success occuring in another \\(k\\) failures is independent of the history of the process ; it remains constant over time. The distribution \"forgets\" (memoryless) the current state that the process is in. \\[ \\begin{aligned} P(N > m + n \\mid N \\ge m) &= P(N > n) \\\\ E(N > m + n \\mid N \\ge m) &= E(N) \\\\ Var(N > m + n \\mid N \\ge m) &= Var(N) \\end{aligned} \\]","title":"Geometric Distribution"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/2.%20Frequency%20Models/#negative-binomial-distribution","text":"The Negative Binomial Distribution is used to count the number of failures to get a fixed number of successes , with respect to a sequence of independent bernoulli trials. It has two parameters: The number of successes, \\(r\\) The probability of success, \\(q\\) \\[ \\begin{aligned} N &\\sim \\text{Negative Binomial} (r, q) \\\\ \\\\ p_n &= {{k+r-1} \\choose {k}} (1-p)^k p^{r} \\\\ \\\\ E(N) &= \\frac{r(1-p)}{p} \\\\ Var (N) &= \\frac{r(1-p)}{p^2} \\end{aligned} \\] Info The distribution is also referred to as Pascals Distribution , but is most commonly called the \"negative\" binomial because it uses a negative integer in the binomial coefficient : \\[ \\begin{aligned} {n \\choose k} &= \\frac{n!}{k! \\cdot (n-k)!} \\\\ \\\\ {{k+r-1} \\choose {k}} &= \\frac{(k+r-1)!}{k! \\cdot (r-1)!} \\\\ &= \\frac{(k+r-1) \\cdot (k+r-2) \\cdot \\dots \\cdot (r)}{k!} \\\\ &= (-1)^k \\cdot \\frac{(-r) \\cdot (-r-1) \\cdot \\dots \\cdot (-r-k+1)}{k!} \\\\ &= (-1)^k {-r \\choose k} \\end{aligned} \\] The negative binomial distribution is actually the sum of \\(r\\) i.i.d. geometric variables with the same probability : \\[ \\begin{aligned} X_k &\\sim \\text{Geom} (q) \\\\ N &= X_1 + X_2 + ... + X_r \\\\ \\therefore N &\\sim \\text{Negative Binomial} (r, q) \\end{aligned} \\] Tip A geometric distribution is simply a negative binomial distribution with \\(r=1\\) . Thus, following the same logic as before, the sum of \\(k\\) independent negative binomial variables with the same probability will still be negative binomial : \\[ \\begin{aligned} N_k &\\sim \\text{Negative Binomial}(r, q) \\\\ N &= N_1 + N_2 + ... + N_k \\\\ \\therefore N &\\sim \\text{Negative Binomial} (r_1 + r_2 + ... + r_k, q) \\end{aligned} \\]","title":"Negative Binomial Distribution"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/2.%20Frequency%20Models/#poisson-gamma-mixture","text":"Consider a poisson distribution with mean \\(\\mu\\) where \\(\\mu\\) follows a gamma distribution. The unconditional distribution actually follows a Negative Binomial Distribution with the same parameters as the gamma distribution. \\[ \\begin{aligned} N \\mid \\mu &\\sim \\text{Poisson} (\\mu) \\\\ \\mu &\\sim \\text{Gamma} (\\alpha, \\theta) \\\\ \\therefore N &\\sim \\text{Negative Binomial}(r = \\alpha, \\beta = \\theta) \\end{aligned} \\] The above result is a Continuous Mixture - it is the result of a mixture of an infinite number of poisson distributions . Continuous Mixtures are beyond the scope of this exam, which is why the proof is not shown. However, this specific result is important to know.","title":"Poisson Gamma Mixture"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/2.%20Frequency%20Models/#a-b-0-class","text":"ONLY the distributions discussed above fall into the (a, b, 0) class of distributions, as their PMFs follow the same recursion : \\[ \\begin{aligned} \\frac{p_{n}}{p_{n-1}} &= a + \\frac{b}{n} \\\\ p_n &= \\left(a + \\frac{b}{n} \\right) p_{n-1} \\end{aligned} \\] \\(a\\) and \\(b\\) are constants that are unique to each distribution while the \"0\" comes from the fact that the recursion starts from \\(n-1=0\\) . Tip The parameters for each distribution can also be determined from the values of \\(a\\) and \\(b\\) . Notice that each distribution has a different signs for \\(a\\) . Thus, given the recursive equation, the underlying distribution can be determined based on the sign of \\(a\\) .","title":"(a, b, 0) Class"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/2.%20Frequency%20Models/#choosing-distributions","text":"Given a sample of data, we need to decide which distribution best represents it. The general idea is that each distribution has some unique characteristics - if the dataset matches those characteristics, then the distribution should be used. The first method involves the Mean and Variance of the distribution. Notice that for all three distributions, the mean and variance have a different relative size to one another . Thus, by comparing the Sample Mean and Unbiased Sample Variance of the dataset, it can be matched to the corresponding distribution. Warning It is almost impossible (whether in practice or theory) to find a sample mean and variance that is exactly equal to one another. Generally speaking, if they are sufficiently close together, then they can be assumed to be equal. However, being \"sufficiently close together\" is arbitrary. Thus, a better way would be to compare the ratio of the two values - if the ratio is around 1.00 , then it can be concluded that the two values are equal . The second method involves the sign of \\(a\\) in the recursion process. Since all three distributions have different signs for \\(a\\) , their probabilities will move differently as \\(n\\) increases. Thus, by observing how the probability of the sample changes, it can be matched to the corresponding distribution. However, we must first multiply the ratio of the probabilities by \\(n\\) in order to obtain a smooth linear line that clearly exhibits the relationships: \\[ \\begin{aligned} \\frac{p_{n}}{p_{n-1}} &= a + \\frac{b}{n} \\\\ \\frac{n \\cdot p_{n}}{p_{n-1}} &= an + b \\end{aligned} \\]","title":"Choosing Distributions"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/2.%20Frequency%20Models/#a-b-1-class","text":"When using the (a, b, 0) class of distributions to model experiments, one common problem is that the probability at \\(n=0\\) does not match experience . Thus, there is a need to modify the distribution such that the probability at 0 is at the desired level . This modification is known as the Zero-Modification . Note If the desired level is 0, then the modification is known as a Zero-Truncation , which is a special case of the zero-modification. This is because this truncates (removes) the possibility of 0 from the random variable. This is done by directly changing the probability at 0 to the desired level. However, this modification alone would cause the sum of probabilities to deviate from 1, thus the probabilities at all other levels of the support must be scaled such that they sum to 1 . Let \\(N^M\\) be the zero-modified distribution and \\(p^{M}_{n}\\) be its PMF. \\[ p^{M}_{n} = \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_n \\] If \\(N\\) is a poisson random variable, then \\(N^M\\) is known as a Zero-Modified Poisson Random Variable . However, it is important to note that \\(N^M\\) does NOT follow a poisson distribution - it has its own unique distribution. Thus, by applying the previous result, the moments of the new distribution can be calculated: \\[ \\begin{aligned} E \\left(N^M \\right) &= 0 \\cdot p^{M}_{0} + 1 \\cdot p^{M}_{1} + 2 \\cdot p^{M}_{2} + ... \\\\ &= 0 \\cdot \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_0 + 1 \\cdot \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_1 + 2 \\cdot \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_2 + ... \\\\ &= \\frac{1-p^{M}_{0}}{1-p_{0}} (0 \\cdot p_0 + 1 \\cdot p_1 + 2 \\cdot p_2 + ...) \\\\ &= \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot E(N) \\\\ \\\\ \\therefore E \\left[\\left(N^M \\right)^k \\right] &= \\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot E \\left[N^k \\right] \\end{aligned} \\] Note that this result ONLY applies to raw moments . In order to calculate the central moments, express them in terms of raw moments and then calculate them from the above conversion. Even after modification, the PMFs can still follow the same recursion: \\[ \\begin{aligned} \\frac{p^{M}_{n}}{p^{M}_{n-1}} &= \\frac{\\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_{n}}{\\frac{1-p^{M}_{0}}{1-p_{0}} \\cdot p_{n-1}} \\\\ &= \\frac{p_{n}}{p_{n-1}} \\\\ &= a + \\frac{b}{n} \\end{aligned} \\] However, since the distribution is zero-modified, the recursion can only start from \\(k-1=1\\) , which is why they are known as the (a, b, 1) class of distributions. The domain of the recursion is what allows us to distinguish the two classes of distributions. Warning There are only 4 distributions under the (a, b, 0) class: Poisson, Binomial, Geometric & Negative Binomial. It is seemingly intuitive to think that their 0-modified versions are the only members of the (a, b, 1) class. However, the Logarithmic Distribution is also a member of the (a, b, 1).","title":"(a, b, 1) Class"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/3.%20Severity%20Models/","text":"Severity Models \u00b6 Severity Distributions \u00b6 The size of the loss is usually known as Loss Severity and is typically modelled with a continuous distribution. Let \\(X\\) be the random variable denoting the size of the loss. This section will cover most of the relevant severity distributions, but similarly note that there is no need to memorize anything as they are all provided in the formula sheet. Normal Distribution \u00b6 The Normal Distribution is used to model processes where outcomes close to the mean are common and become increasingly less common as it strays further from it. The unique property of the normal distribution in that it has two parameters , which are already its Mean \\(\\mu\\) and Variance \\(\\sigma^2\\) : \\[ \\begin{aligned} X &\\sim N(\\mu, \\sigma^2) \\\\ \\\\ f(x) &= \\frac{1}{\\sigma \\sqrt{2\\pi}} \\cdot e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\\\ \\\\ E(X) &= \\mu \\\\ Var (X) &= \\sigma^2 \\\\ \\end{aligned} \\] Another interesting property of the normal distribution is that: Roughly 68.3% of the data is within 1 SD of the average ( \\(\\mu - 1\\sigma \\lt x \\lt \\mu + 1\\sigma\\) ) Roughly 95.5% of the data is within 2 SD of the average ( \\(\\mu - 2\\sigma \\lt x \\lt \\mu + 2\\sigma\\) ) Roughly 99.7% of the data is within 3 SD of the average ( \\(\\mu - 3\\sigma \\lt x \\lt \\mu + 3\\sigma\\) ) The sum of \\(k\\) independent normal random variables is still normal: \\[ \\begin{aligned} X_k &\\sim N(\\mu, \\sigma^2) \\\\ X &= X_1 + X_2 + \\dots + X_k \\\\ \\therefore X &\\sim N(\\mu_1 + \\mu_2 + ... + \\mu_k, \\sigma^2_1 + \\sigma^2_2 + ... + \\sigma_k) \\end{aligned} \\] Another key property is that they are symmetrical about their mean : \\[ \\begin{aligned} P(Z \\lt -a) &= P(Z \\gt a) \\\\ &= 1 - P(Z \\lt a) \\end{aligned} \\] Standard Normal Distribution \u00b6 If the parameters of the normal distribution are 0 and 1 respectively, then it is known as the Standard Normal Distribution , specially denoted by \\(Z\\) . \\[ Z \\sim N(0, 1) \\] Any normal distribution can be converted into a standard normal through Standardization : \\[ Z = \\frac{X - E(X)}{\\sqrt{Var (X)}} \\] Note An easy way to remember is that the above transformation sets the mean to 0; \\(E(X) - E(X) = 0\\) and sets the variance to 1; \\(\\frac{Var(X)}{Var(X)} = 1\\) . The CDF of a standard normal distribution is denoted by \\(\\Phi\\) : \\[ P(Z \\le z) = \\Phi(z) \\] Tip The standard normal distribution is symmetrical about the origin . This allows us to convert negative values to positive ones: \\[ \\begin{aligned} P(Z \\lt -z) &= P(Z \\gt z) \\\\ &= 1 - P(Z \\lt z) \\\\ &= 1 - \\Phi(z) \\end{aligned} \\] Since the exam provides values for the CDF of Z , most calculations involving a normal distribution will have first have to be standardized and then calculated using its CDF: \\[ \\begin{aligned} P(X \\le x) &= P \\left(\\frac{X - E(X)}{\\sqrt{Var (X)}} \\le \\frac{x - E(X)}{\\sqrt{Var (X)}} \\right) \\\\ &= P \\left(Z \\le \\frac{x - E(X)}{\\sqrt{Var (X)}} \\right) \\\\ &= \\Phi \\left(\\frac{x - E(X)}{\\sqrt{Var (X)}} \\right) \\end{aligned} \\] The reverse is also common whereby the probability is provided to solve for the value of Z and hence the value of the underlying variable. The issue is that most of the time, the probability provided will not be exactly equal to the values on the table. In such a case, the midpoint between two values of Z will be taken: Given: \\(P = 0.95\\) Table Value: \\(Z = 1.64, P = 0.9495\\) Table Value: \\(Z = 1.65, P = 0.9505\\) Midpoint: \\(Z = 1.645\\) Lognormal Distribution \u00b6 A exponential transformed normal distribution is known as the Lognormal Distribution as it has a logarithm applied to it. It shares the same two parameters as a regular normal distribution , but they are NOT the mean and variance of the lognormal distribution. \\[ \\begin{aligned} Y &= e^X \\\\ Y &\\sim \\text{Lognormal} (\\mu, \\sigma^2) \\\\ \\\\ f(y) &= \\frac{1}{x\\sigma \\sqrt{2\\pi}} \\cdot e^{-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}} \\\\ \\\\ E(Y) &= e^{\\mu + 0.5 \\sigma^2} \\\\ Var (Y) &= E(Y)^2 \\cdot (e^{\\sigma^2} - 1) \\end{aligned} \\] Warning Given how similar the two distributions are, it is easy to mix them up. It is a common mistake to think that the lognormal parameters are its mean and variance. Note that since it is transformed from a normal distribution, it must use its parameters , but they are NOT its mean and variance. Another common mistake is thinking that the PDFs are identical, given how similar they look. However, there are two key differences : The denominator has an extra \\(x\\) multiplied to it The \\(x\\) in the exponent has become \\(\\ln x\\) The CDF of the log distribution can also be calculated through the standard normal distribution : \\[ \\begin{aligned} P(Y \\le y) &= P(e^X \\le y) \\\\ &= P(X \\le \\ln y) \\\\ &= P \\left(\\frac{X - \\mu}{\\sigma} \\le \\frac{\\ln y - \\mu}{\\sigma} \\right) \\\\ &= P \\left(Z \\le \\frac{\\ln y - \\mu}{\\sigma} \\right) \\\\ &= \\Phi \\left(\\frac{\\ln y - \\mu}{\\sigma} \\right) \\end{aligned} \\] Note that since the sum of independent normal variables is still normal, then the product of indepdent lognormal variables is still lognormal : \\[ \\begin{aligned} X &= X_1 + X_2 + \\dots + X_k \\\\ X &\\sim N(\\mu_1 + \\mu_2 + ... + \\mu_k, \\sigma^2_1 + \\sigma^2_2 + ... + \\sigma_k) \\\\ \\\\ Y &= e^X \\\\ &= e^{X_1 + X_2 + \\dots + X_k} \\\\ &= e^{X_1} \\cdot e^{X_2} \\cdot (\\dots) \\cdot e^{X_k} \\\\ \\therefore Y &\\sim \\text{Lognormal}(\\mu_1 + \\mu_2 + ... + \\mu_k, \\sigma^2_1 + \\sigma^2_2 + ... + \\sigma_k) \\end{aligned} \\] Exponential Distribution \u00b6 The Exponential Distribution is used to model the time taken for the first event to occur in a Poisson Process . It has a single parameter \\(\\lambda\\) , which represents the rate at which events occur, which can be LOOSELY thought of as the \"probability\" of the event occuring per unit time: \\[ \\begin{aligned} X &\\sim \\text{Exponential} (\\lambda) \\\\ \\\\ f(x) &= 1 - \\lambda \\cdot e^{-\\lambda x} \\\\ F(x) &= 1 - e^{-\\lambda x} \\\\ \\\\ E(x) &= \\frac{1}{\\lambda} \\\\ Var (x) &= \\frac{1}{\\lambda^2} \\end{aligned} \\] Similar to the geometric distribution, the exponential distribution is Memoryless . Recall that the intuition is that the rate that the event occurs remains constant over time . \\[ \\begin{aligned} P(X > m + x \\mid X \\ge m) &= P(X > x) \\\\ E(X > m + x \\mid X \\ge m) &= E(X) \\\\ Var(X > m + x \\mid X \\ge m) &= Var(X) \\end{aligned} \\] Alternative Parameterization \u00b6 Given that the Exponential and Poisson Distributions are related, the exponential distribution can also be parameterized in terms the poisson parameter \\(\\mu\\) . In fact, the rate is simply the inverse of the mean : \\[ \\begin{aligned} \\lambda &= \\frac{1}{\\mu} \\\\ \\therefore X &\\sim \\text{Exponential} \\left(\\frac{1}{\\mu} \\right) \\\\ \\\\ f(x) &= 1 - \\lambda \\cdot e^{-\\frac{x}{\\mu}} \\\\ F(x) &= 1 - e^{-\\frac{x}{\\mu}} \\\\ \\\\ E(x) &= \\mu \\\\ Var (x) &= \\mu^2 \\end{aligned} \\] This version of the exponential distribution will be more useful for the exam as it is provided on the formula sheet. Note that the formula sheet uses \\(\\theta\\) instead of \\(\\mu\\) , which is what will be used for all other exponential related distributions. Gamma Distribution \u00b6 The Gamma Distribution is used to model the total time till the \\(\\alpha\\) -th event occurs in a Poisson Process. It has two parameters: \\(\\alpha\\) : The number of events \\(\\theta\\) : Mean \\[ \\begin{aligned} X &\\sim \\text{Gamma}(\\alpha, \\theta) \\\\ \\\\ f(x) &= \\frac{\\theta e^{-\\theta x} \\cdot (\\theta x)^{\\alpha - 1}}{\\Gamma(\\alpha)} \\\\ \\Gamma(\\alpha) &= (\\alpha - 1)! \\\\ \\\\ E(X) &= \\frac{\\alpha}{\\theta} \\\\ Var (X) &= \\frac{\\alpha}{\\theta^2} \\end{aligned} \\] The CDF is hard to evaluate, thus we can determine it intuitively using the poisson process and poisson distribution. If \\(x\\) is the time taken for the \\(\\alpha\\) -th event to occur, then it means that \\(\\alpha-1\\) events occurred in the timespan of \\(x\\) . Given that events occur at a rate of \\(\\theta\\) , it means that on average only \\(\\theta \\cdot x\\) occur in that period of time. Thus, the probability of waiting \\(x\\) for the \\(\\alpha\\) -th event to occur is equivalent to the probability that only \\(\\alpha-1\\) events occur in that time : \\[ \\begin{aligned} X &\\sim \\text{Gamma} (\\alpha, \\theta) \\\\ N &\\sim \\text{Poisson} (\\frac{x}{\\theta}) \\\\ \\\\ \\therefore P(X \\le x) &= P(N \\le \\alpha - 1) \\end{aligned} \\] The gamma distribution is actually the sum of \\(\\alpha\\) independent exponential variables with the same rate ( \\(\\theta\\) ): \\[ \\begin{aligned} Y_k &\\sim \\text{Exponential} (\\theta) \\\\ X &= Y_1 + Y_2 + \\dots + Y_k \\\\ X &\\sim \\text{Gamma} (\\alpha, \\theta) \\end{aligned} \\] Tip An exponential distribution is a special case of a gamma distribution with \\(\\alpha = 1\\) . Thus, the sum of independent gamma distributions is still gamma: \\[ \\begin{aligned} X_k &\\sim \\text{Gamma} (\\alpha_k, \\theta) \\\\ X &= X_1 + X_2 + \\dots + X_k \\\\ X &\\sim \\text{Gamma} (\\alpha_1 + \\alpha_2 + \\dots + \\alpha_k, \\theta) \\end{aligned} \\] Weibull Distribution \u00b6 Similar to the Exponential Distribution, the Weibull Distribution is used to the model the time taken for the first event to occur in a poisson process. However, the difference is that unlike the exponential distribution, the rate of the event occuring changes over time . Info It is commonly used to model machine survival , where the event of interest is the failure of the machine. In other words, it models the probability of the machine failing. It has two parameters: \\(\\theta' = \\frac{1}{\\mu^{\\frac{1}{k}}}\\) : The rate at which events occur \\(k\\) : The behaviour of the rate over time The rate changes depending on the value of \\(k\\) : Larger than 1 \\((k \\gt 1)\\) : Rate increases over time Smaller than 1 \\((k \\lt 1)\\) Rate decreases over time Note that if \\(k=1\\) , it means that the rate that events occur is constant over time, which is simply equivalent to an exponential distribution . Thus, the weibull distribution is simply a transformed exponential distribution: \\[ Y = X^{\\frac{1}{k}} \\] Tip The exponential distribution is a special case of the weibull distribution where \\(k=1\\) . \\[ \\begin{aligned} Y &\\sim \\text{Weibull} \\left(\\theta', k \\right) \\\\ Y &\\sim \\text{Weibull} \\left(\\frac{1}{\\mu^{\\frac{1}{k}}}, k \\right) \\\\ \\\\ f(y) &= \\frac{k \\left(\\frac{y}{\\theta'} \\right)^k \\cdot e^{- \\left(\\frac{y}{\\theta'} \\right)^k}}{y} \\\\ F(y) &= 1 - e^{\\left(\\frac{y}{\\theta'} \\right)^k} \\\\ \\\\ E(Y) &= \\\\ Var (Y) &= \\\\ \\end{aligned} \\] Beta Distribution \u00b6 The Beta Distribution is a probability distribution of probabilities . In other words, it represents the distribution of all possible probabilities when we are unsure of the actual probability. Although out of scope for this exam, in Bayesian Inference, the Beta distribution is the Conjugate Prior to the Binomial Distribution. In other words, the beta distribution models the probability parameter in the binomial distribution . It has only two parameters, which represent the number of prior successes and failures: \\(a\\) : There are \\(a-1\\) prior successes before the experiment \\(b\\) : There are \\(b-1\\) prior failures before the experiment \\[ \\begin{aligned} X &\\sim \\text{Beta} (a, b) \\\\ \\\\ f(x) &= f(x) = c \\cdot x^{a-1} \\cdot (1-x)^{b-1} \\\\ \\\\ E(X) &= \\frac{a}{a+b} \\\\ Var (X) &= \\frac{ab}{(a+b)^2 \\cdot (a+b+1)} \\end{aligned} \\] Although the above is more common, the beta distribution can also be expressed more generally with a third parameter \\(\\theta\\) : \\[ \\begin{aligned} X &\\sim \\text{Beta} (a, b, \\theta) \\\\ \\\\ f(x) &= \\frac{\\Gamma (a+b)}{\\Gamma (a) \\Gamma (b)} \\left(\\frac{x}{\\theta} \\right)^a \\left(1 - \\frac{x}{\\theta} \\right)^{b-1} \\cdot \\frac{1}{x} \\\\ \\\\ E(X) &= \\frac{a}{a+b} \\cdot \\theta \\\\ Var (X) &= \\frac{ab}{(a+b)^2 \\cdot (a+b+1)} \\cdot \\theta^2 \\end{aligned} \\] Tip The \"normal\" beta distribution is a special case of the Generalized Beta Distribution with \\(\\theta = 1\\) . Uniform Distribution \u00b6 The Uniform Distribution is used to model events where every outcome has an equal probability of occuring . It has two parameters, which respectively represent the lower and upper bound of the possible outcomes: \\[ \\begin{aligned} X &\\sim \\text{Uniform} (a, b) \\\\ \\\\ f(x) &= \\frac{1}{b-a} \\\\ F(x) &= \\frac{x - a}{b - a} \\\\ \\\\ E(X) &= \\frac{a+b}{2} \\\\ Var (X) &= \\frac{(a-b)^2}{12} \\end{aligned} \\] The key property of a uniform distribution is how the parameters change when it is shifted : \\[ \\begin{aligned} X &\\sim \\text{Uniform} (a, b) \\\\ X \\mid X \\gt d &\\sim \\text{Uniform} (d, b) \\\\ X-d &\\sim \\text{Uniform} (a-d, b-d) \\\\ \\therefore X-d \\mid X \\gt d &\\sim \\text{Uniform} (0, b-d) \\end{aligned} \\] Warning Although they look very similar, this should not be confused with the memoryless property of a distribution. Note that the \\((0,\\theta)\\) uniform distribution can also be thought of as a Generalized Beta Distribution with parameters \\(a = b = 1\\) . In other words, there are no prior successes or failures , thus each outcome has an equal probability . Pareto Distribution \u00b6 The Pareto Distribution is used to model phenomena where few items account for a lot of it and a lot of items account for little of it. For instance, the Pareto Principle is used to describe wealth inequality: A small 20% of people account for a large 80% of wealth while a large 80% of people account for 20% of wealth. In statistical terms, it means that large outcomes are rare (20% of people, 80% of wealth) while small outcomes are common (80% of people, 20% of wealth). It has two parameters: \\(\\alpha\\) : \\(\\theta\\) : Minimum value \\[ \\begin{aligned} X &\\sim \\text{Pareto} (\\alpha, \\theta) \\\\ \\\\ f(x) &= \\frac{\\alpha \\theta^{\\alpha}}{(x+\\theta)^{\\alpha+1}} \\\\ \\\\ E(X) &= \\frac{\\theta}{\\alpha - 1} \\\\ Var(X) &= E(X)^2 \\cdot \\frac{\\alpha}{\\alpha - 2} \\end{aligned} \\] The key property of a pareto distribution is how the parameters change when it is shifted : \\[ \\begin{aligned} X &\\sim \\text{Pareto} (\\alpha, \\theta) \\\\ X-d \\mid x \\gt d &\\sim \\text{Pareto} (\\alpha, \\theta + d) \\end{aligned} \\] Note This is similar to the shifting property of the uniform distribution. Thus the same warning applies -- although it looks similar, this should not be confused with the memoryless property of a distribution. Single Parameter Pareto \u00b6 If a pareto distribution is added by a constant, then the resulting distribution is known as a Single Parameter Pareto Distribution : \\[ \\begin{aligned} Y = X + \\theta \\\\ \\\\ X &\\sim \\text{Pareto}(\\alpha, \\theta) \\\\ Y &\\sim \\text{SP Pareto}(\\alpha, \\theta) \\end{aligned} \\] Info \\(\\theta\\) is not really a parameter as it must be determined in advance, which is why it it known as a Single Parameter Pareto Distribution. Parametric Distributions \u00b6 All the above distributions are those that are more commonly tested on the exam. However, many other distributions are also provided in the formula sheet but are much less likely to be tested due to their complexity. Parametric Distributions are distributions that are completely determined by a set of quantities known as Parameters. By assigning all possible numerical values to the parameters, the resulting set of distributions is known as a Parametric Distribution Family . An important parametric distribution family is the Linear Exponential Family (LEF). A distribution \\(f(x, \\theta)\\) is part of the LEF if it fulfils the following criteria: Its support does not depend on \\(\\theta\\) Its distribution function is in following form, where \\(p\\) , \\(q\\) and \\(r\\) are generic functions \\[ \\begin{aligned} f(x, \\theta) &= \\frac{p(x) \\cdot e^{r(\\theta) \\cdot x}}{q(\\theta)} \\\\ \\\\ E(X) &= \\frac{q'(\\theta)}{r'(\\theta) \\cdot q(\\theta)} \\\\ Var (X) &= \\frac{E'(X)}{r'(\\theta)} \\end{aligned} \\] Some examples of distributions belonging to the LEF: Discrete: Poisson, Binomial & Negative Binomial Gamma & Normal Consider adding example, only p(x) can contain x, every other function cannot Scale Distribution \u00b6 If a parametric distribution, when scaled by a constant is still within the same family of distributions , then is it known as a Scale Distribution . Scale Distributions typically have a special Scale Parameter , which fulfils the following conditions: Scale parameter is multiplied by the same constant All other parameters remain unchanged All other parameters are typically known as Shape Parameters . For the purposes of the exam, \\(\\theta\\) is used to specially denote Scale Parameters . However, note that there are multiple ways to parameterize a distribution and multiple ways to denote them, thus not all questions will follow the exam tables. Thus, it is important to practice how to identify them. For any of the above scale distributions, if scaled by a constant \\(c\\) , their parameters get changed in the following manner: \\[ \\begin{aligned} X &\\sim \\text{Distribution} (\\text{Shape Parameters}, \\theta) \\\\ cX &\\sim \\text{Distribution} (\\text{Shape Parameters}, c\\theta) \\\\ \\\\ X &\\sim \\text{Normal} (\\mu, \\sigma^2) \\\\ cX &\\sim \\text{Normal} (c\\mu, (c\\sigma)^2) \\\\ \\\\ X &\\sim \\text{Lognormal} (\\mu, \\sigma^2) \\\\ cX &\\sim \\text{Lognormal} (\\mu + \\ln c, \\sigma^2) \\end{aligned} \\]","title":"Severity Models"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/3.%20Severity%20Models/#severity-models","text":"","title":"Severity Models"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/3.%20Severity%20Models/#severity-distributions","text":"The size of the loss is usually known as Loss Severity and is typically modelled with a continuous distribution. Let \\(X\\) be the random variable denoting the size of the loss. This section will cover most of the relevant severity distributions, but similarly note that there is no need to memorize anything as they are all provided in the formula sheet.","title":"Severity Distributions"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/3.%20Severity%20Models/#normal-distribution","text":"The Normal Distribution is used to model processes where outcomes close to the mean are common and become increasingly less common as it strays further from it. The unique property of the normal distribution in that it has two parameters , which are already its Mean \\(\\mu\\) and Variance \\(\\sigma^2\\) : \\[ \\begin{aligned} X &\\sim N(\\mu, \\sigma^2) \\\\ \\\\ f(x) &= \\frac{1}{\\sigma \\sqrt{2\\pi}} \\cdot e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\\\ \\\\ E(X) &= \\mu \\\\ Var (X) &= \\sigma^2 \\\\ \\end{aligned} \\] Another interesting property of the normal distribution is that: Roughly 68.3% of the data is within 1 SD of the average ( \\(\\mu - 1\\sigma \\lt x \\lt \\mu + 1\\sigma\\) ) Roughly 95.5% of the data is within 2 SD of the average ( \\(\\mu - 2\\sigma \\lt x \\lt \\mu + 2\\sigma\\) ) Roughly 99.7% of the data is within 3 SD of the average ( \\(\\mu - 3\\sigma \\lt x \\lt \\mu + 3\\sigma\\) ) The sum of \\(k\\) independent normal random variables is still normal: \\[ \\begin{aligned} X_k &\\sim N(\\mu, \\sigma^2) \\\\ X &= X_1 + X_2 + \\dots + X_k \\\\ \\therefore X &\\sim N(\\mu_1 + \\mu_2 + ... + \\mu_k, \\sigma^2_1 + \\sigma^2_2 + ... + \\sigma_k) \\end{aligned} \\] Another key property is that they are symmetrical about their mean : \\[ \\begin{aligned} P(Z \\lt -a) &= P(Z \\gt a) \\\\ &= 1 - P(Z \\lt a) \\end{aligned} \\]","title":"Normal Distribution"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/3.%20Severity%20Models/#standard-normal-distribution","text":"If the parameters of the normal distribution are 0 and 1 respectively, then it is known as the Standard Normal Distribution , specially denoted by \\(Z\\) . \\[ Z \\sim N(0, 1) \\] Any normal distribution can be converted into a standard normal through Standardization : \\[ Z = \\frac{X - E(X)}{\\sqrt{Var (X)}} \\] Note An easy way to remember is that the above transformation sets the mean to 0; \\(E(X) - E(X) = 0\\) and sets the variance to 1; \\(\\frac{Var(X)}{Var(X)} = 1\\) . The CDF of a standard normal distribution is denoted by \\(\\Phi\\) : \\[ P(Z \\le z) = \\Phi(z) \\] Tip The standard normal distribution is symmetrical about the origin . This allows us to convert negative values to positive ones: \\[ \\begin{aligned} P(Z \\lt -z) &= P(Z \\gt z) \\\\ &= 1 - P(Z \\lt z) \\\\ &= 1 - \\Phi(z) \\end{aligned} \\] Since the exam provides values for the CDF of Z , most calculations involving a normal distribution will have first have to be standardized and then calculated using its CDF: \\[ \\begin{aligned} P(X \\le x) &= P \\left(\\frac{X - E(X)}{\\sqrt{Var (X)}} \\le \\frac{x - E(X)}{\\sqrt{Var (X)}} \\right) \\\\ &= P \\left(Z \\le \\frac{x - E(X)}{\\sqrt{Var (X)}} \\right) \\\\ &= \\Phi \\left(\\frac{x - E(X)}{\\sqrt{Var (X)}} \\right) \\end{aligned} \\] The reverse is also common whereby the probability is provided to solve for the value of Z and hence the value of the underlying variable. The issue is that most of the time, the probability provided will not be exactly equal to the values on the table. In such a case, the midpoint between two values of Z will be taken: Given: \\(P = 0.95\\) Table Value: \\(Z = 1.64, P = 0.9495\\) Table Value: \\(Z = 1.65, P = 0.9505\\) Midpoint: \\(Z = 1.645\\)","title":"Standard Normal Distribution"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/3.%20Severity%20Models/#lognormal-distribution","text":"A exponential transformed normal distribution is known as the Lognormal Distribution as it has a logarithm applied to it. It shares the same two parameters as a regular normal distribution , but they are NOT the mean and variance of the lognormal distribution. \\[ \\begin{aligned} Y &= e^X \\\\ Y &\\sim \\text{Lognormal} (\\mu, \\sigma^2) \\\\ \\\\ f(y) &= \\frac{1}{x\\sigma \\sqrt{2\\pi}} \\cdot e^{-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}} \\\\ \\\\ E(Y) &= e^{\\mu + 0.5 \\sigma^2} \\\\ Var (Y) &= E(Y)^2 \\cdot (e^{\\sigma^2} - 1) \\end{aligned} \\] Warning Given how similar the two distributions are, it is easy to mix them up. It is a common mistake to think that the lognormal parameters are its mean and variance. Note that since it is transformed from a normal distribution, it must use its parameters , but they are NOT its mean and variance. Another common mistake is thinking that the PDFs are identical, given how similar they look. However, there are two key differences : The denominator has an extra \\(x\\) multiplied to it The \\(x\\) in the exponent has become \\(\\ln x\\) The CDF of the log distribution can also be calculated through the standard normal distribution : \\[ \\begin{aligned} P(Y \\le y) &= P(e^X \\le y) \\\\ &= P(X \\le \\ln y) \\\\ &= P \\left(\\frac{X - \\mu}{\\sigma} \\le \\frac{\\ln y - \\mu}{\\sigma} \\right) \\\\ &= P \\left(Z \\le \\frac{\\ln y - \\mu}{\\sigma} \\right) \\\\ &= \\Phi \\left(\\frac{\\ln y - \\mu}{\\sigma} \\right) \\end{aligned} \\] Note that since the sum of independent normal variables is still normal, then the product of indepdent lognormal variables is still lognormal : \\[ \\begin{aligned} X &= X_1 + X_2 + \\dots + X_k \\\\ X &\\sim N(\\mu_1 + \\mu_2 + ... + \\mu_k, \\sigma^2_1 + \\sigma^2_2 + ... + \\sigma_k) \\\\ \\\\ Y &= e^X \\\\ &= e^{X_1 + X_2 + \\dots + X_k} \\\\ &= e^{X_1} \\cdot e^{X_2} \\cdot (\\dots) \\cdot e^{X_k} \\\\ \\therefore Y &\\sim \\text{Lognormal}(\\mu_1 + \\mu_2 + ... + \\mu_k, \\sigma^2_1 + \\sigma^2_2 + ... + \\sigma_k) \\end{aligned} \\]","title":"Lognormal Distribution"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/3.%20Severity%20Models/#exponential-distribution","text":"The Exponential Distribution is used to model the time taken for the first event to occur in a Poisson Process . It has a single parameter \\(\\lambda\\) , which represents the rate at which events occur, which can be LOOSELY thought of as the \"probability\" of the event occuring per unit time: \\[ \\begin{aligned} X &\\sim \\text{Exponential} (\\lambda) \\\\ \\\\ f(x) &= 1 - \\lambda \\cdot e^{-\\lambda x} \\\\ F(x) &= 1 - e^{-\\lambda x} \\\\ \\\\ E(x) &= \\frac{1}{\\lambda} \\\\ Var (x) &= \\frac{1}{\\lambda^2} \\end{aligned} \\] Similar to the geometric distribution, the exponential distribution is Memoryless . Recall that the intuition is that the rate that the event occurs remains constant over time . \\[ \\begin{aligned} P(X > m + x \\mid X \\ge m) &= P(X > x) \\\\ E(X > m + x \\mid X \\ge m) &= E(X) \\\\ Var(X > m + x \\mid X \\ge m) &= Var(X) \\end{aligned} \\]","title":"Exponential Distribution"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/3.%20Severity%20Models/#alternative-parameterization","text":"Given that the Exponential and Poisson Distributions are related, the exponential distribution can also be parameterized in terms the poisson parameter \\(\\mu\\) . In fact, the rate is simply the inverse of the mean : \\[ \\begin{aligned} \\lambda &= \\frac{1}{\\mu} \\\\ \\therefore X &\\sim \\text{Exponential} \\left(\\frac{1}{\\mu} \\right) \\\\ \\\\ f(x) &= 1 - \\lambda \\cdot e^{-\\frac{x}{\\mu}} \\\\ F(x) &= 1 - e^{-\\frac{x}{\\mu}} \\\\ \\\\ E(x) &= \\mu \\\\ Var (x) &= \\mu^2 \\end{aligned} \\] This version of the exponential distribution will be more useful for the exam as it is provided on the formula sheet. Note that the formula sheet uses \\(\\theta\\) instead of \\(\\mu\\) , which is what will be used for all other exponential related distributions.","title":"Alternative Parameterization"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/3.%20Severity%20Models/#gamma-distribution","text":"The Gamma Distribution is used to model the total time till the \\(\\alpha\\) -th event occurs in a Poisson Process. It has two parameters: \\(\\alpha\\) : The number of events \\(\\theta\\) : Mean \\[ \\begin{aligned} X &\\sim \\text{Gamma}(\\alpha, \\theta) \\\\ \\\\ f(x) &= \\frac{\\theta e^{-\\theta x} \\cdot (\\theta x)^{\\alpha - 1}}{\\Gamma(\\alpha)} \\\\ \\Gamma(\\alpha) &= (\\alpha - 1)! \\\\ \\\\ E(X) &= \\frac{\\alpha}{\\theta} \\\\ Var (X) &= \\frac{\\alpha}{\\theta^2} \\end{aligned} \\] The CDF is hard to evaluate, thus we can determine it intuitively using the poisson process and poisson distribution. If \\(x\\) is the time taken for the \\(\\alpha\\) -th event to occur, then it means that \\(\\alpha-1\\) events occurred in the timespan of \\(x\\) . Given that events occur at a rate of \\(\\theta\\) , it means that on average only \\(\\theta \\cdot x\\) occur in that period of time. Thus, the probability of waiting \\(x\\) for the \\(\\alpha\\) -th event to occur is equivalent to the probability that only \\(\\alpha-1\\) events occur in that time : \\[ \\begin{aligned} X &\\sim \\text{Gamma} (\\alpha, \\theta) \\\\ N &\\sim \\text{Poisson} (\\frac{x}{\\theta}) \\\\ \\\\ \\therefore P(X \\le x) &= P(N \\le \\alpha - 1) \\end{aligned} \\] The gamma distribution is actually the sum of \\(\\alpha\\) independent exponential variables with the same rate ( \\(\\theta\\) ): \\[ \\begin{aligned} Y_k &\\sim \\text{Exponential} (\\theta) \\\\ X &= Y_1 + Y_2 + \\dots + Y_k \\\\ X &\\sim \\text{Gamma} (\\alpha, \\theta) \\end{aligned} \\] Tip An exponential distribution is a special case of a gamma distribution with \\(\\alpha = 1\\) . Thus, the sum of independent gamma distributions is still gamma: \\[ \\begin{aligned} X_k &\\sim \\text{Gamma} (\\alpha_k, \\theta) \\\\ X &= X_1 + X_2 + \\dots + X_k \\\\ X &\\sim \\text{Gamma} (\\alpha_1 + \\alpha_2 + \\dots + \\alpha_k, \\theta) \\end{aligned} \\]","title":"Gamma Distribution"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/3.%20Severity%20Models/#weibull-distribution","text":"Similar to the Exponential Distribution, the Weibull Distribution is used to the model the time taken for the first event to occur in a poisson process. However, the difference is that unlike the exponential distribution, the rate of the event occuring changes over time . Info It is commonly used to model machine survival , where the event of interest is the failure of the machine. In other words, it models the probability of the machine failing. It has two parameters: \\(\\theta' = \\frac{1}{\\mu^{\\frac{1}{k}}}\\) : The rate at which events occur \\(k\\) : The behaviour of the rate over time The rate changes depending on the value of \\(k\\) : Larger than 1 \\((k \\gt 1)\\) : Rate increases over time Smaller than 1 \\((k \\lt 1)\\) Rate decreases over time Note that if \\(k=1\\) , it means that the rate that events occur is constant over time, which is simply equivalent to an exponential distribution . Thus, the weibull distribution is simply a transformed exponential distribution: \\[ Y = X^{\\frac{1}{k}} \\] Tip The exponential distribution is a special case of the weibull distribution where \\(k=1\\) . \\[ \\begin{aligned} Y &\\sim \\text{Weibull} \\left(\\theta', k \\right) \\\\ Y &\\sim \\text{Weibull} \\left(\\frac{1}{\\mu^{\\frac{1}{k}}}, k \\right) \\\\ \\\\ f(y) &= \\frac{k \\left(\\frac{y}{\\theta'} \\right)^k \\cdot e^{- \\left(\\frac{y}{\\theta'} \\right)^k}}{y} \\\\ F(y) &= 1 - e^{\\left(\\frac{y}{\\theta'} \\right)^k} \\\\ \\\\ E(Y) &= \\\\ Var (Y) &= \\\\ \\end{aligned} \\]","title":"Weibull Distribution"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/3.%20Severity%20Models/#beta-distribution","text":"The Beta Distribution is a probability distribution of probabilities . In other words, it represents the distribution of all possible probabilities when we are unsure of the actual probability. Although out of scope for this exam, in Bayesian Inference, the Beta distribution is the Conjugate Prior to the Binomial Distribution. In other words, the beta distribution models the probability parameter in the binomial distribution . It has only two parameters, which represent the number of prior successes and failures: \\(a\\) : There are \\(a-1\\) prior successes before the experiment \\(b\\) : There are \\(b-1\\) prior failures before the experiment \\[ \\begin{aligned} X &\\sim \\text{Beta} (a, b) \\\\ \\\\ f(x) &= f(x) = c \\cdot x^{a-1} \\cdot (1-x)^{b-1} \\\\ \\\\ E(X) &= \\frac{a}{a+b} \\\\ Var (X) &= \\frac{ab}{(a+b)^2 \\cdot (a+b+1)} \\end{aligned} \\] Although the above is more common, the beta distribution can also be expressed more generally with a third parameter \\(\\theta\\) : \\[ \\begin{aligned} X &\\sim \\text{Beta} (a, b, \\theta) \\\\ \\\\ f(x) &= \\frac{\\Gamma (a+b)}{\\Gamma (a) \\Gamma (b)} \\left(\\frac{x}{\\theta} \\right)^a \\left(1 - \\frac{x}{\\theta} \\right)^{b-1} \\cdot \\frac{1}{x} \\\\ \\\\ E(X) &= \\frac{a}{a+b} \\cdot \\theta \\\\ Var (X) &= \\frac{ab}{(a+b)^2 \\cdot (a+b+1)} \\cdot \\theta^2 \\end{aligned} \\] Tip The \"normal\" beta distribution is a special case of the Generalized Beta Distribution with \\(\\theta = 1\\) .","title":"Beta Distribution"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/3.%20Severity%20Models/#uniform-distribution","text":"The Uniform Distribution is used to model events where every outcome has an equal probability of occuring . It has two parameters, which respectively represent the lower and upper bound of the possible outcomes: \\[ \\begin{aligned} X &\\sim \\text{Uniform} (a, b) \\\\ \\\\ f(x) &= \\frac{1}{b-a} \\\\ F(x) &= \\frac{x - a}{b - a} \\\\ \\\\ E(X) &= \\frac{a+b}{2} \\\\ Var (X) &= \\frac{(a-b)^2}{12} \\end{aligned} \\] The key property of a uniform distribution is how the parameters change when it is shifted : \\[ \\begin{aligned} X &\\sim \\text{Uniform} (a, b) \\\\ X \\mid X \\gt d &\\sim \\text{Uniform} (d, b) \\\\ X-d &\\sim \\text{Uniform} (a-d, b-d) \\\\ \\therefore X-d \\mid X \\gt d &\\sim \\text{Uniform} (0, b-d) \\end{aligned} \\] Warning Although they look very similar, this should not be confused with the memoryless property of a distribution. Note that the \\((0,\\theta)\\) uniform distribution can also be thought of as a Generalized Beta Distribution with parameters \\(a = b = 1\\) . In other words, there are no prior successes or failures , thus each outcome has an equal probability .","title":"Uniform Distribution"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/3.%20Severity%20Models/#pareto-distribution","text":"The Pareto Distribution is used to model phenomena where few items account for a lot of it and a lot of items account for little of it. For instance, the Pareto Principle is used to describe wealth inequality: A small 20% of people account for a large 80% of wealth while a large 80% of people account for 20% of wealth. In statistical terms, it means that large outcomes are rare (20% of people, 80% of wealth) while small outcomes are common (80% of people, 20% of wealth). It has two parameters: \\(\\alpha\\) : \\(\\theta\\) : Minimum value \\[ \\begin{aligned} X &\\sim \\text{Pareto} (\\alpha, \\theta) \\\\ \\\\ f(x) &= \\frac{\\alpha \\theta^{\\alpha}}{(x+\\theta)^{\\alpha+1}} \\\\ \\\\ E(X) &= \\frac{\\theta}{\\alpha - 1} \\\\ Var(X) &= E(X)^2 \\cdot \\frac{\\alpha}{\\alpha - 2} \\end{aligned} \\] The key property of a pareto distribution is how the parameters change when it is shifted : \\[ \\begin{aligned} X &\\sim \\text{Pareto} (\\alpha, \\theta) \\\\ X-d \\mid x \\gt d &\\sim \\text{Pareto} (\\alpha, \\theta + d) \\end{aligned} \\] Note This is similar to the shifting property of the uniform distribution. Thus the same warning applies -- although it looks similar, this should not be confused with the memoryless property of a distribution.","title":"Pareto Distribution"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/3.%20Severity%20Models/#single-parameter-pareto","text":"If a pareto distribution is added by a constant, then the resulting distribution is known as a Single Parameter Pareto Distribution : \\[ \\begin{aligned} Y = X + \\theta \\\\ \\\\ X &\\sim \\text{Pareto}(\\alpha, \\theta) \\\\ Y &\\sim \\text{SP Pareto}(\\alpha, \\theta) \\end{aligned} \\] Info \\(\\theta\\) is not really a parameter as it must be determined in advance, which is why it it known as a Single Parameter Pareto Distribution.","title":"Single Parameter Pareto"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/3.%20Severity%20Models/#parametric-distributions","text":"All the above distributions are those that are more commonly tested on the exam. However, many other distributions are also provided in the formula sheet but are much less likely to be tested due to their complexity. Parametric Distributions are distributions that are completely determined by a set of quantities known as Parameters. By assigning all possible numerical values to the parameters, the resulting set of distributions is known as a Parametric Distribution Family . An important parametric distribution family is the Linear Exponential Family (LEF). A distribution \\(f(x, \\theta)\\) is part of the LEF if it fulfils the following criteria: Its support does not depend on \\(\\theta\\) Its distribution function is in following form, where \\(p\\) , \\(q\\) and \\(r\\) are generic functions \\[ \\begin{aligned} f(x, \\theta) &= \\frac{p(x) \\cdot e^{r(\\theta) \\cdot x}}{q(\\theta)} \\\\ \\\\ E(X) &= \\frac{q'(\\theta)}{r'(\\theta) \\cdot q(\\theta)} \\\\ Var (X) &= \\frac{E'(X)}{r'(\\theta)} \\end{aligned} \\] Some examples of distributions belonging to the LEF: Discrete: Poisson, Binomial & Negative Binomial Gamma & Normal Consider adding example, only p(x) can contain x, every other function cannot","title":"Parametric Distributions"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/3.%20Severity%20Models/#scale-distribution","text":"If a parametric distribution, when scaled by a constant is still within the same family of distributions , then is it known as a Scale Distribution . Scale Distributions typically have a special Scale Parameter , which fulfils the following conditions: Scale parameter is multiplied by the same constant All other parameters remain unchanged All other parameters are typically known as Shape Parameters . For the purposes of the exam, \\(\\theta\\) is used to specially denote Scale Parameters . However, note that there are multiple ways to parameterize a distribution and multiple ways to denote them, thus not all questions will follow the exam tables. Thus, it is important to practice how to identify them. For any of the above scale distributions, if scaled by a constant \\(c\\) , their parameters get changed in the following manner: \\[ \\begin{aligned} X &\\sim \\text{Distribution} (\\text{Shape Parameters}, \\theta) \\\\ cX &\\sim \\text{Distribution} (\\text{Shape Parameters}, c\\theta) \\\\ \\\\ X &\\sim \\text{Normal} (\\mu, \\sigma^2) \\\\ cX &\\sim \\text{Normal} (c\\mu, (c\\sigma)^2) \\\\ \\\\ X &\\sim \\text{Lognormal} (\\mu, \\sigma^2) \\\\ cX &\\sim \\text{Lognormal} (\\mu + \\ln c, \\sigma^2) \\end{aligned} \\]","title":"Scale Distribution"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/4.%20Policy%20Modifications/","text":"Policy Modifications \u00b6 Key Concepts \u00b6 Policy Modifications are the parts of an insurance policy that defines how much the insurer is required to pay for each loss incurred by the policyholder. Before continuing, there are two key notations ( \\(\\land, {}_{+}\\) )that will be used throughout this subsection: \\[ \\begin{aligned} \\min (a, b) &= \\begin{cases} a,& a \\lt b \\\\ b,& a \\ge b \\end{cases} \\\\ &= a \\land b \\\\ \\\\ \\max (a-b, 0) &= \\begin{cases} 0,& a \\lt b \\\\ a-b,& a \\ge b \\end{cases} \\\\ &= (a-b)_{+} \\\\ &= \\text{Non-negative part; floored at 0} \\end{aligned} \\] The two are related through the following expression: \\[ \\begin{aligned} (a-b)_{+} &= a - (a \\land b) \\\\ \\\\ \\text{If } a &\\gt b \\text{,} \\\\ (a-b)_{+} &= a-b \\\\ a - (a \\land b) &= a-b \\\\ \\therefore (a-b)_{+} &= a - (a \\land b) \\\\ \\\\ \\text{If } a &\\lt b \\text{,} \\\\ (a-b)_{+} &= 0 \\\\ a - (a \\land b) &= a-a = 0 \\\\ \\therefore (a-b)_{+} &= a - (a \\land b) \\\\ \\end{aligned} \\] There are three key random variables regarding claims and losses: \\(X\\) is the Ground Up Loss variable, representing the total loss incurred by the policyholder \\(Y^L\\) is the Payment per Loss variable, representing the amount paid by the insurer \\(Y^P\\) is the Payment per Payment variable, representing the amount paid by the insurer given that there is a positive payment To properly understand the difference between the three, consider an example of a policy with a deductible of 800 , but in two different situations: \\(X\\) or Loss \\(Y^L\\) \\(Y^P\\) 2000 1200 1200 600 0 NOT Observed There are two main takeaways: If \\(Y^L > 0\\) , then \\(Y^P = Y^L\\) If \\(Y^L = 0\\) , then \\(Y^P\\) is NOT Observed \\(\\therefore Y^P = Y^L \\mid Y^L > 0\\) Warning Do NOT mistakenly think that \\(Y^P\\) being unobserved is equivalent to \\(Y^P = 0\\) . They are completely DIFFERENT. \\(Y^L\\) can be thought of as a two point mixture distribution of \\(Y^P\\) and a not-so-random-variable that always takes 0 . Thus, it can also be thought of as a conditional distribution dependent on whether \\(Y^L \\gt 0\\) : \\[ \\begin{aligned} F_{Y^L} (t) &= F_{Y_L | Y^L > 0} (t) \\cdot P(Y^L > 0) + F_{Y_L | Y^L = 0} (t) \\cdot P(Y^L = 0) \\\\ &= F_{Y^P} (t) \\cdot P(Y^L > 0) + F_{0} (t) \\cdot P(Y^L = 0) \\\\ \\end{aligned} \\] Let \\(\\pi = P(Y^L > 0)\\) , which becomes the weight of the mixture: \\[ \\begin{aligned} F_{Y^L} (t) &= F_{Y_P} (t) \\cdot \\pi + F_{0}(t) \\cdot (1-\\pi) \\\\ 1 - F_{Y^L} (t) &= [1 - F_{Y_P} (t)] \\cdot \\pi + [1-F_{0}(t)] \\cdot (1-\\pi) \\\\ S_{Y^L} (t) &= S_{Y_P} (t) \\cdot \\pi \\\\ \\\\ E \\left(Y^L \\right) &= E \\left(Y^P \\right) \\cdot \\pi + E(0) \\cdot (1-\\pi) \\\\ E \\left(Y^L \\right) &= E \\left(Y^L \\right) \\cdot \\pi \\end{aligned} \\] Thus, the values for \\(Y^L\\) and \\(Y^P\\) can be easily converted to one another by using the weight of the mixture (probability of a positive payment). Additionally, since \\(Y^P\\) is essentially \\(Y^L\\) being conditional on itself, we can easily obtain its conditional distribution : \\[ \\begin{aligned} f_{Y_P} (y) &= f_{Y^L \\mid Y^L \\gt 0} (y) \\\\ &= \\frac{f_{Y^L} (y)}{P(Y^L \\gt 0)} \\end{aligned} \\] Tip If \\(Y^L\\) follows the exponential or geometric distribution which are memoryless, then the distribution of \\(Y^P\\) is the same as \\(Y^L\\) . \\[ \\begin{aligned} Y^L &\\sim \\text{Exponential/Geometric} \\\\ Y^P &= X \\mid X \\gt d \\\\ Y^P &\\sim \\text{Exponential/Geometric} \\end{aligned} \\] Similarly, if \\(Y^L\\) follows the uniform distribution, then the distribution of \\(Y^P\\) can be easily calculated: \\[ \\begin{aligned} X &\\sim \\text{Uniform} (a,b) \\\\ Y^P &= X - d \\mid X \\gt d \\\\ \\therefore Y^P &\\sim \\text{Uniform} (a, b-d) \\end{aligned} \\] Note that for all losses without deductibles , a loss will always result in a claim. Thus, \\(Y^P = Y^L\\) . Ordinary Deductibles \u00b6 The first type of policy modification is known as a Deductible , denoted by \\(d\\) . It is the amount that the policyholder is responsible for paying before the insurer pays a claim: If the loss amount does not exceed the deductible, then the insurer will pay out nothing If it does exceed the deductible , then the insurer will pay out the excess For each loss \\(X\\) , the policyholder is responsible for paying losses up to \\(d\\) . It is denoted by \\(X \\land d\\) , which is known as the Limited Loss Variable : \\[ \\begin{aligned} X \\land d &= \\begin{cases} X,& X \\le d\\\\ d,& X \\gt d \\end{cases} \\end{aligned} \\] The insurer will then cover the excess amount, represented by the Payment Per Loss variable: \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\le d\\\\ X - d,& X \\gt d \\end{cases} \\\\ &= (X-d)_{+} \\\\ &= X - (X \\land d) \\\\ \\\\ \\therefore E \\left(Y^L \\right) &= E(X) - E(X \\land d) \\end{aligned} \\] Tip Since \\(E(X)\\) is easily calculated, the main issue is calculating \\(E(X \\land d)\\) : \\(X\\) follows a known distribution -- \\(E(X \\land d)\\) formula provided \\(X\\) does NOT follow a known distribution -- \\(E(X \\land d)\\) calculated through first principles Most of the time, such questions assume that losses are discrete (EG. 40, 80, 120 & 160) with some deductible not exactly equal to any of the loss amounts (EG. 100). The tricky part is remembering that once the loss is larger the deductible, only the deductible will be paid : \\[ \\begin{aligned} X \\land 100 &= \\begin{cases} 40,& X = 40 \\\\ 80,& X = 80 \\\\ 100,& X \\ge 120 \\end{cases} \\\\ \\\\ \\therefore E(X \\land 100) &= 40 \\cdot P(X = 40) \\\\ &+ 80 \\cdot P(X = 80) \\\\ &+ 100 \\cdot [1 - P(X = 40) - P(X = 80)] \\end{aligned} \\] The key is understanding that the final case is simply the deductible itself, with its probability being the complement of all other cases below the deductible . The same logic can be applied in an empirical context , where \\(E(X \\land d)\\) is the average of the total LIMITED claim sizes : \\[ E(X \\land 300) = \\frac{x + y + (100 - 22) \\cdot 300}{100} \\] The key is once again understanding that if the loss per claim is above \\(d\\) , then it is limited to \\(d\\) . Since we are interested in total claim sizes, we need to multiply it by the number of limited claims as well. In practice, policyholders would not report losses below the deductible . Thus, the insurers payment is better represented by the Payment Per Payment variable, also known as the Excess Loss Variable : \\[ \\begin{aligned} Y^P &= Y^L \\mid Y^L \\gt 0 \\\\ &= X - d \\mid X \\gt d \\\\ &= \\begin{cases} \\text{Undefined},& X \\le d\\\\ X - d,& X \\gt d \\end{cases} \\\\ \\\\ f_{Y_P} (y) &= \\frac{f_{Y^L} (y)}{P(Y^L \\gt 0)} \\\\ &= \\frac{f_{Y^L} (y)}{P(X \\gt d)} \\\\ &= \\frac{f_{Y^L} (y)}{1 - F_{X}(d)} \\\\ \\\\ E \\left(Y^P \\right) &= \\int^{\\infty}_{d} y^P \\cdot f_{Y_P} (y) \\\\ &= \\int^{\\infty}_{d} (x-d) \\cdot \\frac{f_{Y^L} (y)}{1 - F_{X}(d)} \\\\ &= \\frac{1}{1 - F_{X}(d)} \\cdot \\int^{\\infty}_{d} (x-d) \\cdot f_{Y^L} (y) \\\\ &= \\frac{E \\left(Y^L \\right)}{1 - F_{X}(d)} \\\\ \\end{aligned} \\] The key metric insurers look out for is the Loss Elimintation Ratio (LER), which is a measure of how much the insurer saves by imposing the deductible; how much less they end up paying. Formally, it is defined as the ratio of a decrease in expected payment when a deductible is imposed to the expected payment without a deductible: \\[ \\begin{aligned} \\text{LER} &= \\frac{E(X) - E \\left(Y^L \\right)}{E(X)} \\\\ &= \\frac{E(X) - E [(X-d)_{+}]}{E(X)} \\\\ &= \\frac{E \\left[X-(X-d)_{+} \\right]}{E(X)} \\\\ &= \\frac{E(X \\land d)}{E(X)} \\end{aligned} \\] Variance of Payments \u00b6 Unfortunately, the formula for expectation ONLY holds for the first raw moment . Thus, the second moment can only be calculated via first principles . If severity is continuous , then the survival function method (FAM-L) can be used to calculate the second moment: \\[ \\begin{aligned} E \\left(Y^L \\right) &= E \\left[(X-d)_{+} \\right] \\\\ &= \\int^{d}_{0} 0 \\cdot f(x) + \\int^{\\infty}_{d} (x-d) \\cdot f(x) \\\\ &= \\int^{\\infty}_{d} (x-d) \\cdot f(x) \\\\ \\\\ E \\left[ \\left(Y^L \\right) \\right]^2 &= \\int^{\\infty}_{d} (x-d)^2 \\cdot f(x) \\\\ &= [S_X(x) \\cdot (x-d)^2]^{\\infty}_{d} - \\int^{\\infty}_{d} -S(x) \\cdot 2(x-d) \\\\ &= \\int^{\\infty}_{d} S(x) \\cdot 2(x-d) \\end{aligned} \\] If severity is discrete , then determine all possible values for \\(Y^L\\) and then manually calculate the second moment as the sum product. If the variance of the payment per payment variable is required, then the same conversion applies for the second moment as well: \\[ E \\left[ \\left(Y^P \\right) \\right]^2 = \\frac{E \\left[ \\left(Y^L \\right) \\right]^2}{P(X \\gt d)} \\] Franchise Deductibles \u00b6 A special kind of deductible is known as Franchise Deductible . Unlike a regular deductible that will only pay the excess, it will pay the full amount . \\[ \\begin{aligned} Y^L_{\\text{Franchise}} &= \\begin{cases} 0,& X \\le d\\\\ X,& X \\gt d \\end{cases} \\\\ \\\\ E \\left(Y^L_{\\text{Franchise}} \\right) &= \\int^{d}_{0} 0 \\cdot f(x) + \\int^{\\infty}_{d} x \\cdot f(x) \\\\ &= \\int^{\\infty}_{d} x \\cdot f(x) \\\\ &= \\int^{\\infty}_{d} (x-d+d) \\cdot f(x) \\\\ &= \\int^{\\infty}_{d} (x-d) \\cdot f(x) + d \\cdot \\int^{\\infty}_{d} f(x) \\\\ &= E[Y^L] + d \\cdot [1-F_{X}(d)] \\end{aligned} \\] The same relationship as before can be applied for the payment per payment variable: \\[ \\begin{aligned} Y^P &= \\begin{cases} \\text{Undefined},& X \\le d\\\\ X,& X \\gt d \\end{cases} \\\\ \\\\ E \\left(Y^P_{\\text{Franchise}} \\right) &= \\frac{E \\left(Y^L_{\\text{Franchise}} \\right)}{1 - F_{X}(d)} \\\\ &= \\frac{E[Y^L] + d \\cdot [1-F_{X}(d)]}{1 - F_{X}(d)} \\\\ &= \\frac{E[Y^L]}{1 - F_{X}(d)} + \\frac{d \\cdot [1-F_{X}(d)]}{1 - F_{X}(d)} \\\\ &= E \\left(Y^P \\right) + d \\end{aligned} \\] Disapearing Deductibles \u00b6 Another kind of special deductible is known as a Disappearing Deductible . For losses below a lower threshold \\((L)\\) , there is a constant deductible . After an upper threshold \\((U)\\) , there is no deductible . Between the two thresholds, the deductible decreases linearly . The payment made by the insurer is visualized below: There are four parts to the graph, depending on the size of the losses: Smaller than the deductible : No payment is made Smaller than the lower threshold : Payment in excess of flat deductible; increases at the same rate as losses Between lower and upper threshold : Payment in excess of linear deductible; increases faster than losses Above upper threshold : Payment equal to loss; increases at the same rate as losses The key is to find an graphical equation representing the portion of the graph we are interested in: \\[ y = mx + c \\] By plugging in the loss amount, we can determine the payment made. Coinsurance \u00b6 Another type of policy modification is known as Coinsurance , denoted by \\(\\alpha\\) . As its name suggests, the key idea is that both the insurer and policyholder insure the loss . The insurer covers \\(\\alpha\\) proportion of the loss while the poliyholder covers the remaining \\(1-\\alpha\\) proportion. \\[ \\begin{aligned} Y^L &= \\alpha \\cdot X \\\\ E \\left(Y^L \\right) &= \\alpha \\cdot E(X) \\\\ \\end{aligned} \\] Warning Co-insurance usually refers to the portion of the loss that the policyholder is responsible for. However, for this exam, we are taking the insurers perspective, thus it refers to the amount that the insurer is responsible for. Do not confuse the two! Coinsurance & Deductibles \u00b6 Coinsurance on its own is a simple concept - the difficulty comes about when it is mixed with a Deductible . If the coinsurance is applied after the deductible : Entire loss \\(X\\) is applied against the deductible; poliycholder pays first \\(d\\) Coinsurance is applied against the remaining loss ; insurer pays remaining \\(\\alpha(X-d)\\) Unless stated otherwise, it is usually assumed that the coinsurance is applied after the deductible \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\le d \\\\ \\alpha \\cdot (X - d),& X \\gt d \\end{cases} \\\\ &= \\begin{cases} \\alpha \\cdot 0,& X \\le d \\\\ \\alpha \\cdot (X - d),& X \\gt d \\end{cases} \\\\ &= \\alpha \\cdot \\begin{cases} 0,& X \\le d \\\\ (X - d),& X \\gt d \\end{cases} \\\\ &= \\alpha \\cdot (X-d)_{+} \\\\ &= \\alpha \\cdot [X - (X \\land d)] \\\\ \\\\ E \\left(Y^L \\right) &= \\alpha \\cdot E [X - (X \\land d)] \\\\ &= \\alpha \\cdot [E(X) - E(X \\land d)] \\\\ \\\\ E \\left(Y^P \\right) &= \\frac{E \\left(Y^L \\right)}{1 - F_{X}(d)} \\end{aligned} \\] Note The key is remembering that any value multiplied by 0 is 0, thus the coinsurance factor can be factored out. If the coinsurance is applied before the deductible , Coinsurance is applied against the full loss ; insurer covers \\(\\alpha X\\) Deductible is applied against the coinsured amount ; insurer pays \\(\\alpha X - d\\) The deductible effectively increases to \\(\\frac{d}{a}\\) \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& \\alpha X \\le d \\\\ \\alpha X - d,& \\alpha X \\gt d \\end{cases} \\\\ &= \\begin{cases} 0,& X \\le \\frac{d}{a} \\\\ \\alpha \\cdot \\left(X - \\frac{d}{a} \\right),& X \\gt \\frac{d}{a} \\end{cases} \\\\ &= \\alpha \\cdot \\left(X - \\frac{d}{a} \\right)_{+} \\\\ &= \\alpha \\cdot \\left[X - \\left(X \\land \\frac{d}{a} \\right) \\right] \\\\ \\\\ E \\left(Y^L \\right) &= \\alpha \\cdot E \\left[X - \\left(X \\land \\frac{d}{a} \\right) \\right] \\\\ &= \\alpha \\cdot \\left[E(X) - E \\left(X \\land \\frac{d}{a} \\right) \\right] \\\\ \\\\ E \\left(Y^P \\right) &= \\frac{E \\left(Y^L \\right)}{1 - F_{X} \\left(\\frac{d}{a} \\right)} \\end{aligned} \\] Warning It is a common mistake to forget to update the deductible to the new one when using the complement of the CDF for the conversion between \\(Y^L\\) and \\(Y^P\\) . The key is understanding that only amounts that the insured pays is counted towards the deductible. Bonus Incentive \u00b6 A special combination of a Deductible and Coinsurance is known as a Bonus Incentive . Insurers usually provide incentives to their agents if the total losses for the year are below some threshold , where the incentive is some percentage of the losses below this amount . Thus, the situation is similar to coinsurance applied after a deductible: \\[ \\begin{aligned} B &= \\begin{cases} \\alpha \\cdot (d - X),& X \\lt d \\\\ 0,& X \\ge d \\end{cases} \\\\ &= \\alpha \\cdot (d - X)_{+} \\\\ \\\\ \\therefore E(B) &= E(d-X)_{+} \\\\ &= E(d) - E(d \\land X) \\\\ &= E(d) - E(X \\land d) \\end{aligned} \\] Policy Limits \u00b6 The second type of policy modification is known as Policy Limit , denoted by \\(u\\) . It is the maximum amount that the insurer will payout. If \\(X\\) is below the limit, then the insurer will pay the loss If \\(X\\) is above the limit, then the insurer will only pay the limit \\[ \\begin{aligned} Y^L &= \\begin{cases} X,& X \\lt u \\\\ u,& X \\ge u \\end{cases} \\\\ &= X \\land u \\\\ \\\\ E \\left(Y^L \\right) &= \\int^{u}_{0} x \\cdot f(x) + \\int^{\\infty}_{u} u \\cdot f(x) \\\\ \\end{aligned} \\] Warning Under a deductible, \\(X \\land d\\) represents the limited loss of the policyholder . Under a policy limit, \\(x \\land u\\) represents the limited payment of the insurer . Note that the expectation can be simplified using the Survival Function Method (FAM-L): \\[ \\begin{aligned} E \\left(Y^L \\right) &= \\int^{u}_{0} x \\cdot f(x) + \\int^{\\infty}_{u} u \\cdot f(x) \\\\ &= \\left(\\left[x \\cdot (-S(x)) \\right]^u_0 - \\int^{u}_{0} (-S(x)) \\cdot 1 \\right) + u \\cdot S(u) \\\\ &= -u \\cdot S(u) - 0 + \\int^{u}_{0} S(x) + u \\cdot S(u) \\\\ &= \\int^{u}_{0} S(x) dx \\end{aligned} \\] Similar to the LER, insurers would like to find out how much more they have to pay if they were to increase the policy limit, measured through the Increased Limit Factor (ILF). It is calculated as the ratio of the expected payment with the new limit \\(b\\) to that of the current limit \\(u\\) : \\[ \\text{ILF} = \\frac{E(X \\land b)}{E(X \\land u)} \\] Limits & Deductibles \u00b6 Policy Limits on their own are a simple concept - the difficulty comes about when they are mixed with deductibles . Consider the possible values of the payment variable: \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\lt d \\\\ X-d,& d \\lt X \\lt d + u \\\\ u,& X \\gt d + u \\end{cases} \\end{aligned} \\] The key is understanding that only what the insurer pays is counted towards the limit - thus, the effective limit of the policy is \\(d + u\\) . This is known as the Maximum Covered Loss \\((m)\\) which is the smallest loss amount at or above which the insurer will pay the policy limit. In other words, if \\(X \\ge m\\) then \\(Y^L = u\\) . We can find an expression for payment by decomposing the piecewise function: \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\lt d \\\\ X-d,& d \\lt X \\lt d + u \\\\ m - d,& X \\gt d + u \\end{cases} \\\\ &= \\begin{cases} X - X,& X \\lt d \\\\ X-d,& d \\lt X \\lt d + u \\\\ m - d,& X \\gt d + u \\end{cases} \\\\ &= \\begin{cases} X,& X \\lt d \\\\ X,& d \\lt X \\lt d + u \\\\ m,& X \\gt d + u \\end{cases} - \\begin{cases} X,& X \\lt d \\\\ d,& d \\lt X \\lt d + u \\\\ d,& X \\gt d + u \\end{cases} \\\\ &= (X \\land m) - (X \\land d) \\\\ \\\\ \\therefore E(Y^L) &= E(X \\land m) - (X \\land d) \\end{aligned} \\] Note This is intuitive, as the amount paid is above the deductible \\(d\\) but below the limit \\(m\\) . Maximum Covered Loss \u00b6 The concept of maximum covered loss can be extended to all combinations of modifications involving a limit: Policy Limit ONLY : \\(m=u\\) With Ordinary Deductible : \\(m = u + d\\) With Coinsurance : \\(m = \\frac{u}{\\alpha}\\) With BOTH Ordinary Deductible and Coinsurance : \\(m = \\frac{u}{\\alpha} + d\\) Note For a franchise deductible, since the full loss is paid if the deductible has been met, \\(m=u\\) . Thus, policy limits are more generally expressed as: \\[ \\begin{aligned} Y^L_{\\text{Policy Limit}} &= \\begin{cases} X,& X \\lt m \\\\ u,& X \\ge m \\end{cases} \\\\ &= X \\land m \\\\ \\end{aligned} \\] Inflation \u00b6 Although not technically a policy modification, Inflation is another factor that affects how the payments are calculated. If the losses in the current year are \\(X\\) , then losses in the FOLLOWING year are expected to inflate by a factor of \\(r\\) , resulting in \\((1+r)X\\) . Since coinsurance is already a scaling of the loss random variable, adding an additional term for inflation does not impact much: \\[ \\begin{aligned} Y^L_{\\text{Coinsurance}} &= \\alpha \\cdot (1+r) \\cdot X \\\\ \\therefore E \\left(Y^L_{\\text{Coinsurance}} \\right) &= \\alpha \\cdot (1+r) \\cdot E(X) \\end{aligned} \\] However, policy limits involve a minimum function , which is more complicated to manipulate: \\[ \\begin{aligned} Y^L_{\\text{Policy Limit}} &= (1+r) \\cdot X \\land u \\\\ &= (1+r) \\left(X \\land \\frac{u}{1+r} \\right) \\\\ \\therefore E \\left(Y^L_{\\text{Policy Limit}} \\right) &= (1+r) \\cdot E \\left(X \\land \\frac{u}{1+r} \\right) \\\\ \\end{aligned} \\] Thus, the same logic can be applied for deductibles with inflation: \\[ \\begin{aligned} Y^L_{\\text{Deductible}} &= (1+r) \\cdot X - (1+r) \\cdot X \\land d \\\\ &= (1+r) \\cdot X - (1+r) \\cdot \\left(X \\land \\frac{d}{1+r} \\right) \\\\ &= (1+r) \\left[X - \\left(X \\land \\frac{d}{1+r} \\right) \\right] \\\\ \\therefore E(Y^L_{\\text{Deductible}}) &= (1+r) \\cdot E \\left[X - \\left(X \\land \\frac{d}{1+r} \\right) \\right] \\end{aligned} \\] Tip This is the exact same result as when coinsurance is applied before the deductible, as both involve scaling of \\(X\\) without the deductible. Thus, remember to use the updated deductible when converting between \\(Y^L\\) and \\(Y^P\\) . An alternative method is available if the loss distribution is a Scale Distribution : \\[ \\begin{aligned} X &\\sim \\text{Scale Distribution}(\\theta) \\\\ X(1+r) &\\sim \\text{Scale Distribution}(\\theta \\cdot (1+r)) \\end{aligned} \\] Thus, this allows us to adjust the distribution rather than the function . All Modifications \u00b6 Consider a policy with all three policy modifications (with the coinsurance applied after the deductible): Below the deductible , nothing will be paid Above the maximum covered loss , the policy limit will be paid Between these two values, the coinsured portion of the excess loss is paid \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& x \\lt d \\\\ \\alpha (X-d),& d \\lt X \\lt m \\\\ u,& X \\ge m \\end{cases} \\end{aligned} \\] This can then be simplified to yield the following results: \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\le d \\\\ \\alpha(X-d),& d \\lt X \\lt m \\\\ u,& X \\ge m \\end{cases} \\\\ &= \\begin{cases} \\alpha (X-X),& X \\le d \\\\ \\alpha \\cdot (X-d),& d \\lt X \\lt m \\\\ \\alpha (m-d),& X \\ge m \\end{cases} \\\\ &= \\alpha \\cdot \\begin{cases} X-X,& X \\le d \\\\ X-d,& d \\lt X \\lt m \\\\ m-d,& X \\ge m \\end{cases} \\\\ &= \\alpha \\cdot \\left [ \\begin{cases} X,& X \\le d \\\\ X,& d \\lt X \\lt m \\\\ m,& X \\ge m \\end{cases} - \\begin{cases} X,& X \\le d \\\\ d,& d \\lt X \\lt m \\\\ d,& X \\ge m \\end{cases} \\right] \\\\ &= \\alpha \\cdot \\left[(X \\land m) - (X \\land d) \\right] \\\\ \\\\ E \\left(Y^L \\right) &= \\alpha \\cdot E \\left[(X \\land m) - (X \\land d) \\right] \\\\ &= \\alpha \\cdot \\left (E \\left[(X \\land m) \\right] - E \\left[(X \\land d) \\right] \\right) \\\\ \\\\ E \\left(Y^P \\right) &= \\frac{E \\left(Y^L \\right)}{1 - F_{X}(d)} \\end{aligned} \\] If inflation is considered, then the above can be easily adjusted for the following year: \\[ \\begin{aligned} E \\left(Y^L \\right) &= \\alpha (1+r) \\cdot \\left (E \\left[\\left(X \\land \\frac{m}{1+r}\\right) \\right] - E \\left[\\left(X \\land \\frac{d}{1+r} \\right) \\right] \\right) \\\\ \\\\ E \\left(Y^P \\right) &= \\frac{E \\left(Y^L \\right)}{1 - F_{X} \\left(\\frac{d}{1+r} \\right)} \\end{aligned} \\] Hybrid Modification \u00b6 There may be policies that contain an unusual combination of features. The only way to solve them is by first principles . Consider the overall piecewise function and then split them into more recognisable ones, using the \"tricks\" shown above. Consider a two deductible example from SOA Sample Question #60 : \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\lt 1000 \\\\ 0.8 \\cdot (X - 1000),& 1000 \\lt X \\lt 6000 \\\\ 0.8 \\cdot (6000 - 1000),& 6000 \\lt X \\lt 14000 \\\\ 0.8 \\cdot (6000 - 1000) + 0.9 \\cdot (X - 14000),& X \\gt 14000 \\end{cases} \\end{aligned} \\] The tricky part about forming this function is the third level: Since the policyholder bears the full cost of the repairs, the insurer does not pay anything new ; they pay the maximum amount of the previous level . The policyholder has paid 1000 from the deductible and 1000 from the coinsurance , thus has to bear 8000 of losses themselves, resulting in an upper limit of 14000 . Using the methods shown earlier, it can be split into the following components: Tip For any question involving complicated tiers of payments, it is best to write out the piecewise function and manipulate it like above. The key manipulation is \\(0 = X - X\\) , allowing certain piecewise functions to be collapsed into \\(E(X)\\) or \\((X \\land d)\\) .","title":"Policy Modifications"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/4.%20Policy%20Modifications/#policy-modifications","text":"","title":"Policy Modifications"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/4.%20Policy%20Modifications/#key-concepts","text":"Policy Modifications are the parts of an insurance policy that defines how much the insurer is required to pay for each loss incurred by the policyholder. Before continuing, there are two key notations ( \\(\\land, {}_{+}\\) )that will be used throughout this subsection: \\[ \\begin{aligned} \\min (a, b) &= \\begin{cases} a,& a \\lt b \\\\ b,& a \\ge b \\end{cases} \\\\ &= a \\land b \\\\ \\\\ \\max (a-b, 0) &= \\begin{cases} 0,& a \\lt b \\\\ a-b,& a \\ge b \\end{cases} \\\\ &= (a-b)_{+} \\\\ &= \\text{Non-negative part; floored at 0} \\end{aligned} \\] The two are related through the following expression: \\[ \\begin{aligned} (a-b)_{+} &= a - (a \\land b) \\\\ \\\\ \\text{If } a &\\gt b \\text{,} \\\\ (a-b)_{+} &= a-b \\\\ a - (a \\land b) &= a-b \\\\ \\therefore (a-b)_{+} &= a - (a \\land b) \\\\ \\\\ \\text{If } a &\\lt b \\text{,} \\\\ (a-b)_{+} &= 0 \\\\ a - (a \\land b) &= a-a = 0 \\\\ \\therefore (a-b)_{+} &= a - (a \\land b) \\\\ \\end{aligned} \\] There are three key random variables regarding claims and losses: \\(X\\) is the Ground Up Loss variable, representing the total loss incurred by the policyholder \\(Y^L\\) is the Payment per Loss variable, representing the amount paid by the insurer \\(Y^P\\) is the Payment per Payment variable, representing the amount paid by the insurer given that there is a positive payment To properly understand the difference between the three, consider an example of a policy with a deductible of 800 , but in two different situations: \\(X\\) or Loss \\(Y^L\\) \\(Y^P\\) 2000 1200 1200 600 0 NOT Observed There are two main takeaways: If \\(Y^L > 0\\) , then \\(Y^P = Y^L\\) If \\(Y^L = 0\\) , then \\(Y^P\\) is NOT Observed \\(\\therefore Y^P = Y^L \\mid Y^L > 0\\) Warning Do NOT mistakenly think that \\(Y^P\\) being unobserved is equivalent to \\(Y^P = 0\\) . They are completely DIFFERENT. \\(Y^L\\) can be thought of as a two point mixture distribution of \\(Y^P\\) and a not-so-random-variable that always takes 0 . Thus, it can also be thought of as a conditional distribution dependent on whether \\(Y^L \\gt 0\\) : \\[ \\begin{aligned} F_{Y^L} (t) &= F_{Y_L | Y^L > 0} (t) \\cdot P(Y^L > 0) + F_{Y_L | Y^L = 0} (t) \\cdot P(Y^L = 0) \\\\ &= F_{Y^P} (t) \\cdot P(Y^L > 0) + F_{0} (t) \\cdot P(Y^L = 0) \\\\ \\end{aligned} \\] Let \\(\\pi = P(Y^L > 0)\\) , which becomes the weight of the mixture: \\[ \\begin{aligned} F_{Y^L} (t) &= F_{Y_P} (t) \\cdot \\pi + F_{0}(t) \\cdot (1-\\pi) \\\\ 1 - F_{Y^L} (t) &= [1 - F_{Y_P} (t)] \\cdot \\pi + [1-F_{0}(t)] \\cdot (1-\\pi) \\\\ S_{Y^L} (t) &= S_{Y_P} (t) \\cdot \\pi \\\\ \\\\ E \\left(Y^L \\right) &= E \\left(Y^P \\right) \\cdot \\pi + E(0) \\cdot (1-\\pi) \\\\ E \\left(Y^L \\right) &= E \\left(Y^L \\right) \\cdot \\pi \\end{aligned} \\] Thus, the values for \\(Y^L\\) and \\(Y^P\\) can be easily converted to one another by using the weight of the mixture (probability of a positive payment). Additionally, since \\(Y^P\\) is essentially \\(Y^L\\) being conditional on itself, we can easily obtain its conditional distribution : \\[ \\begin{aligned} f_{Y_P} (y) &= f_{Y^L \\mid Y^L \\gt 0} (y) \\\\ &= \\frac{f_{Y^L} (y)}{P(Y^L \\gt 0)} \\end{aligned} \\] Tip If \\(Y^L\\) follows the exponential or geometric distribution which are memoryless, then the distribution of \\(Y^P\\) is the same as \\(Y^L\\) . \\[ \\begin{aligned} Y^L &\\sim \\text{Exponential/Geometric} \\\\ Y^P &= X \\mid X \\gt d \\\\ Y^P &\\sim \\text{Exponential/Geometric} \\end{aligned} \\] Similarly, if \\(Y^L\\) follows the uniform distribution, then the distribution of \\(Y^P\\) can be easily calculated: \\[ \\begin{aligned} X &\\sim \\text{Uniform} (a,b) \\\\ Y^P &= X - d \\mid X \\gt d \\\\ \\therefore Y^P &\\sim \\text{Uniform} (a, b-d) \\end{aligned} \\] Note that for all losses without deductibles , a loss will always result in a claim. Thus, \\(Y^P = Y^L\\) .","title":"Key Concepts"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/4.%20Policy%20Modifications/#ordinary-deductibles","text":"The first type of policy modification is known as a Deductible , denoted by \\(d\\) . It is the amount that the policyholder is responsible for paying before the insurer pays a claim: If the loss amount does not exceed the deductible, then the insurer will pay out nothing If it does exceed the deductible , then the insurer will pay out the excess For each loss \\(X\\) , the policyholder is responsible for paying losses up to \\(d\\) . It is denoted by \\(X \\land d\\) , which is known as the Limited Loss Variable : \\[ \\begin{aligned} X \\land d &= \\begin{cases} X,& X \\le d\\\\ d,& X \\gt d \\end{cases} \\end{aligned} \\] The insurer will then cover the excess amount, represented by the Payment Per Loss variable: \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\le d\\\\ X - d,& X \\gt d \\end{cases} \\\\ &= (X-d)_{+} \\\\ &= X - (X \\land d) \\\\ \\\\ \\therefore E \\left(Y^L \\right) &= E(X) - E(X \\land d) \\end{aligned} \\] Tip Since \\(E(X)\\) is easily calculated, the main issue is calculating \\(E(X \\land d)\\) : \\(X\\) follows a known distribution -- \\(E(X \\land d)\\) formula provided \\(X\\) does NOT follow a known distribution -- \\(E(X \\land d)\\) calculated through first principles Most of the time, such questions assume that losses are discrete (EG. 40, 80, 120 & 160) with some deductible not exactly equal to any of the loss amounts (EG. 100). The tricky part is remembering that once the loss is larger the deductible, only the deductible will be paid : \\[ \\begin{aligned} X \\land 100 &= \\begin{cases} 40,& X = 40 \\\\ 80,& X = 80 \\\\ 100,& X \\ge 120 \\end{cases} \\\\ \\\\ \\therefore E(X \\land 100) &= 40 \\cdot P(X = 40) \\\\ &+ 80 \\cdot P(X = 80) \\\\ &+ 100 \\cdot [1 - P(X = 40) - P(X = 80)] \\end{aligned} \\] The key is understanding that the final case is simply the deductible itself, with its probability being the complement of all other cases below the deductible . The same logic can be applied in an empirical context , where \\(E(X \\land d)\\) is the average of the total LIMITED claim sizes : \\[ E(X \\land 300) = \\frac{x + y + (100 - 22) \\cdot 300}{100} \\] The key is once again understanding that if the loss per claim is above \\(d\\) , then it is limited to \\(d\\) . Since we are interested in total claim sizes, we need to multiply it by the number of limited claims as well. In practice, policyholders would not report losses below the deductible . Thus, the insurers payment is better represented by the Payment Per Payment variable, also known as the Excess Loss Variable : \\[ \\begin{aligned} Y^P &= Y^L \\mid Y^L \\gt 0 \\\\ &= X - d \\mid X \\gt d \\\\ &= \\begin{cases} \\text{Undefined},& X \\le d\\\\ X - d,& X \\gt d \\end{cases} \\\\ \\\\ f_{Y_P} (y) &= \\frac{f_{Y^L} (y)}{P(Y^L \\gt 0)} \\\\ &= \\frac{f_{Y^L} (y)}{P(X \\gt d)} \\\\ &= \\frac{f_{Y^L} (y)}{1 - F_{X}(d)} \\\\ \\\\ E \\left(Y^P \\right) &= \\int^{\\infty}_{d} y^P \\cdot f_{Y_P} (y) \\\\ &= \\int^{\\infty}_{d} (x-d) \\cdot \\frac{f_{Y^L} (y)}{1 - F_{X}(d)} \\\\ &= \\frac{1}{1 - F_{X}(d)} \\cdot \\int^{\\infty}_{d} (x-d) \\cdot f_{Y^L} (y) \\\\ &= \\frac{E \\left(Y^L \\right)}{1 - F_{X}(d)} \\\\ \\end{aligned} \\] The key metric insurers look out for is the Loss Elimintation Ratio (LER), which is a measure of how much the insurer saves by imposing the deductible; how much less they end up paying. Formally, it is defined as the ratio of a decrease in expected payment when a deductible is imposed to the expected payment without a deductible: \\[ \\begin{aligned} \\text{LER} &= \\frac{E(X) - E \\left(Y^L \\right)}{E(X)} \\\\ &= \\frac{E(X) - E [(X-d)_{+}]}{E(X)} \\\\ &= \\frac{E \\left[X-(X-d)_{+} \\right]}{E(X)} \\\\ &= \\frac{E(X \\land d)}{E(X)} \\end{aligned} \\]","title":"Ordinary Deductibles"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/4.%20Policy%20Modifications/#variance-of-payments","text":"Unfortunately, the formula for expectation ONLY holds for the first raw moment . Thus, the second moment can only be calculated via first principles . If severity is continuous , then the survival function method (FAM-L) can be used to calculate the second moment: \\[ \\begin{aligned} E \\left(Y^L \\right) &= E \\left[(X-d)_{+} \\right] \\\\ &= \\int^{d}_{0} 0 \\cdot f(x) + \\int^{\\infty}_{d} (x-d) \\cdot f(x) \\\\ &= \\int^{\\infty}_{d} (x-d) \\cdot f(x) \\\\ \\\\ E \\left[ \\left(Y^L \\right) \\right]^2 &= \\int^{\\infty}_{d} (x-d)^2 \\cdot f(x) \\\\ &= [S_X(x) \\cdot (x-d)^2]^{\\infty}_{d} - \\int^{\\infty}_{d} -S(x) \\cdot 2(x-d) \\\\ &= \\int^{\\infty}_{d} S(x) \\cdot 2(x-d) \\end{aligned} \\] If severity is discrete , then determine all possible values for \\(Y^L\\) and then manually calculate the second moment as the sum product. If the variance of the payment per payment variable is required, then the same conversion applies for the second moment as well: \\[ E \\left[ \\left(Y^P \\right) \\right]^2 = \\frac{E \\left[ \\left(Y^L \\right) \\right]^2}{P(X \\gt d)} \\]","title":"Variance of Payments"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/4.%20Policy%20Modifications/#franchise-deductibles","text":"A special kind of deductible is known as Franchise Deductible . Unlike a regular deductible that will only pay the excess, it will pay the full amount . \\[ \\begin{aligned} Y^L_{\\text{Franchise}} &= \\begin{cases} 0,& X \\le d\\\\ X,& X \\gt d \\end{cases} \\\\ \\\\ E \\left(Y^L_{\\text{Franchise}} \\right) &= \\int^{d}_{0} 0 \\cdot f(x) + \\int^{\\infty}_{d} x \\cdot f(x) \\\\ &= \\int^{\\infty}_{d} x \\cdot f(x) \\\\ &= \\int^{\\infty}_{d} (x-d+d) \\cdot f(x) \\\\ &= \\int^{\\infty}_{d} (x-d) \\cdot f(x) + d \\cdot \\int^{\\infty}_{d} f(x) \\\\ &= E[Y^L] + d \\cdot [1-F_{X}(d)] \\end{aligned} \\] The same relationship as before can be applied for the payment per payment variable: \\[ \\begin{aligned} Y^P &= \\begin{cases} \\text{Undefined},& X \\le d\\\\ X,& X \\gt d \\end{cases} \\\\ \\\\ E \\left(Y^P_{\\text{Franchise}} \\right) &= \\frac{E \\left(Y^L_{\\text{Franchise}} \\right)}{1 - F_{X}(d)} \\\\ &= \\frac{E[Y^L] + d \\cdot [1-F_{X}(d)]}{1 - F_{X}(d)} \\\\ &= \\frac{E[Y^L]}{1 - F_{X}(d)} + \\frac{d \\cdot [1-F_{X}(d)]}{1 - F_{X}(d)} \\\\ &= E \\left(Y^P \\right) + d \\end{aligned} \\]","title":"Franchise Deductibles"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/4.%20Policy%20Modifications/#disapearing-deductibles","text":"Another kind of special deductible is known as a Disappearing Deductible . For losses below a lower threshold \\((L)\\) , there is a constant deductible . After an upper threshold \\((U)\\) , there is no deductible . Between the two thresholds, the deductible decreases linearly . The payment made by the insurer is visualized below: There are four parts to the graph, depending on the size of the losses: Smaller than the deductible : No payment is made Smaller than the lower threshold : Payment in excess of flat deductible; increases at the same rate as losses Between lower and upper threshold : Payment in excess of linear deductible; increases faster than losses Above upper threshold : Payment equal to loss; increases at the same rate as losses The key is to find an graphical equation representing the portion of the graph we are interested in: \\[ y = mx + c \\] By plugging in the loss amount, we can determine the payment made.","title":"Disapearing Deductibles"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/4.%20Policy%20Modifications/#coinsurance","text":"Another type of policy modification is known as Coinsurance , denoted by \\(\\alpha\\) . As its name suggests, the key idea is that both the insurer and policyholder insure the loss . The insurer covers \\(\\alpha\\) proportion of the loss while the poliyholder covers the remaining \\(1-\\alpha\\) proportion. \\[ \\begin{aligned} Y^L &= \\alpha \\cdot X \\\\ E \\left(Y^L \\right) &= \\alpha \\cdot E(X) \\\\ \\end{aligned} \\] Warning Co-insurance usually refers to the portion of the loss that the policyholder is responsible for. However, for this exam, we are taking the insurers perspective, thus it refers to the amount that the insurer is responsible for. Do not confuse the two!","title":"Coinsurance"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/4.%20Policy%20Modifications/#coinsurance-deductibles","text":"Coinsurance on its own is a simple concept - the difficulty comes about when it is mixed with a Deductible . If the coinsurance is applied after the deductible : Entire loss \\(X\\) is applied against the deductible; poliycholder pays first \\(d\\) Coinsurance is applied against the remaining loss ; insurer pays remaining \\(\\alpha(X-d)\\) Unless stated otherwise, it is usually assumed that the coinsurance is applied after the deductible \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\le d \\\\ \\alpha \\cdot (X - d),& X \\gt d \\end{cases} \\\\ &= \\begin{cases} \\alpha \\cdot 0,& X \\le d \\\\ \\alpha \\cdot (X - d),& X \\gt d \\end{cases} \\\\ &= \\alpha \\cdot \\begin{cases} 0,& X \\le d \\\\ (X - d),& X \\gt d \\end{cases} \\\\ &= \\alpha \\cdot (X-d)_{+} \\\\ &= \\alpha \\cdot [X - (X \\land d)] \\\\ \\\\ E \\left(Y^L \\right) &= \\alpha \\cdot E [X - (X \\land d)] \\\\ &= \\alpha \\cdot [E(X) - E(X \\land d)] \\\\ \\\\ E \\left(Y^P \\right) &= \\frac{E \\left(Y^L \\right)}{1 - F_{X}(d)} \\end{aligned} \\] Note The key is remembering that any value multiplied by 0 is 0, thus the coinsurance factor can be factored out. If the coinsurance is applied before the deductible , Coinsurance is applied against the full loss ; insurer covers \\(\\alpha X\\) Deductible is applied against the coinsured amount ; insurer pays \\(\\alpha X - d\\) The deductible effectively increases to \\(\\frac{d}{a}\\) \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& \\alpha X \\le d \\\\ \\alpha X - d,& \\alpha X \\gt d \\end{cases} \\\\ &= \\begin{cases} 0,& X \\le \\frac{d}{a} \\\\ \\alpha \\cdot \\left(X - \\frac{d}{a} \\right),& X \\gt \\frac{d}{a} \\end{cases} \\\\ &= \\alpha \\cdot \\left(X - \\frac{d}{a} \\right)_{+} \\\\ &= \\alpha \\cdot \\left[X - \\left(X \\land \\frac{d}{a} \\right) \\right] \\\\ \\\\ E \\left(Y^L \\right) &= \\alpha \\cdot E \\left[X - \\left(X \\land \\frac{d}{a} \\right) \\right] \\\\ &= \\alpha \\cdot \\left[E(X) - E \\left(X \\land \\frac{d}{a} \\right) \\right] \\\\ \\\\ E \\left(Y^P \\right) &= \\frac{E \\left(Y^L \\right)}{1 - F_{X} \\left(\\frac{d}{a} \\right)} \\end{aligned} \\] Warning It is a common mistake to forget to update the deductible to the new one when using the complement of the CDF for the conversion between \\(Y^L\\) and \\(Y^P\\) . The key is understanding that only amounts that the insured pays is counted towards the deductible.","title":"Coinsurance &amp; Deductibles"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/4.%20Policy%20Modifications/#bonus-incentive","text":"A special combination of a Deductible and Coinsurance is known as a Bonus Incentive . Insurers usually provide incentives to their agents if the total losses for the year are below some threshold , where the incentive is some percentage of the losses below this amount . Thus, the situation is similar to coinsurance applied after a deductible: \\[ \\begin{aligned} B &= \\begin{cases} \\alpha \\cdot (d - X),& X \\lt d \\\\ 0,& X \\ge d \\end{cases} \\\\ &= \\alpha \\cdot (d - X)_{+} \\\\ \\\\ \\therefore E(B) &= E(d-X)_{+} \\\\ &= E(d) - E(d \\land X) \\\\ &= E(d) - E(X \\land d) \\end{aligned} \\]","title":"Bonus Incentive"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/4.%20Policy%20Modifications/#policy-limits","text":"The second type of policy modification is known as Policy Limit , denoted by \\(u\\) . It is the maximum amount that the insurer will payout. If \\(X\\) is below the limit, then the insurer will pay the loss If \\(X\\) is above the limit, then the insurer will only pay the limit \\[ \\begin{aligned} Y^L &= \\begin{cases} X,& X \\lt u \\\\ u,& X \\ge u \\end{cases} \\\\ &= X \\land u \\\\ \\\\ E \\left(Y^L \\right) &= \\int^{u}_{0} x \\cdot f(x) + \\int^{\\infty}_{u} u \\cdot f(x) \\\\ \\end{aligned} \\] Warning Under a deductible, \\(X \\land d\\) represents the limited loss of the policyholder . Under a policy limit, \\(x \\land u\\) represents the limited payment of the insurer . Note that the expectation can be simplified using the Survival Function Method (FAM-L): \\[ \\begin{aligned} E \\left(Y^L \\right) &= \\int^{u}_{0} x \\cdot f(x) + \\int^{\\infty}_{u} u \\cdot f(x) \\\\ &= \\left(\\left[x \\cdot (-S(x)) \\right]^u_0 - \\int^{u}_{0} (-S(x)) \\cdot 1 \\right) + u \\cdot S(u) \\\\ &= -u \\cdot S(u) - 0 + \\int^{u}_{0} S(x) + u \\cdot S(u) \\\\ &= \\int^{u}_{0} S(x) dx \\end{aligned} \\] Similar to the LER, insurers would like to find out how much more they have to pay if they were to increase the policy limit, measured through the Increased Limit Factor (ILF). It is calculated as the ratio of the expected payment with the new limit \\(b\\) to that of the current limit \\(u\\) : \\[ \\text{ILF} = \\frac{E(X \\land b)}{E(X \\land u)} \\]","title":"Policy Limits"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/4.%20Policy%20Modifications/#limits-deductibles","text":"Policy Limits on their own are a simple concept - the difficulty comes about when they are mixed with deductibles . Consider the possible values of the payment variable: \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\lt d \\\\ X-d,& d \\lt X \\lt d + u \\\\ u,& X \\gt d + u \\end{cases} \\end{aligned} \\] The key is understanding that only what the insurer pays is counted towards the limit - thus, the effective limit of the policy is \\(d + u\\) . This is known as the Maximum Covered Loss \\((m)\\) which is the smallest loss amount at or above which the insurer will pay the policy limit. In other words, if \\(X \\ge m\\) then \\(Y^L = u\\) . We can find an expression for payment by decomposing the piecewise function: \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\lt d \\\\ X-d,& d \\lt X \\lt d + u \\\\ m - d,& X \\gt d + u \\end{cases} \\\\ &= \\begin{cases} X - X,& X \\lt d \\\\ X-d,& d \\lt X \\lt d + u \\\\ m - d,& X \\gt d + u \\end{cases} \\\\ &= \\begin{cases} X,& X \\lt d \\\\ X,& d \\lt X \\lt d + u \\\\ m,& X \\gt d + u \\end{cases} - \\begin{cases} X,& X \\lt d \\\\ d,& d \\lt X \\lt d + u \\\\ d,& X \\gt d + u \\end{cases} \\\\ &= (X \\land m) - (X \\land d) \\\\ \\\\ \\therefore E(Y^L) &= E(X \\land m) - (X \\land d) \\end{aligned} \\] Note This is intuitive, as the amount paid is above the deductible \\(d\\) but below the limit \\(m\\) .","title":"Limits &amp; Deductibles"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/4.%20Policy%20Modifications/#maximum-covered-loss","text":"The concept of maximum covered loss can be extended to all combinations of modifications involving a limit: Policy Limit ONLY : \\(m=u\\) With Ordinary Deductible : \\(m = u + d\\) With Coinsurance : \\(m = \\frac{u}{\\alpha}\\) With BOTH Ordinary Deductible and Coinsurance : \\(m = \\frac{u}{\\alpha} + d\\) Note For a franchise deductible, since the full loss is paid if the deductible has been met, \\(m=u\\) . Thus, policy limits are more generally expressed as: \\[ \\begin{aligned} Y^L_{\\text{Policy Limit}} &= \\begin{cases} X,& X \\lt m \\\\ u,& X \\ge m \\end{cases} \\\\ &= X \\land m \\\\ \\end{aligned} \\]","title":"Maximum Covered Loss"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/4.%20Policy%20Modifications/#inflation","text":"Although not technically a policy modification, Inflation is another factor that affects how the payments are calculated. If the losses in the current year are \\(X\\) , then losses in the FOLLOWING year are expected to inflate by a factor of \\(r\\) , resulting in \\((1+r)X\\) . Since coinsurance is already a scaling of the loss random variable, adding an additional term for inflation does not impact much: \\[ \\begin{aligned} Y^L_{\\text{Coinsurance}} &= \\alpha \\cdot (1+r) \\cdot X \\\\ \\therefore E \\left(Y^L_{\\text{Coinsurance}} \\right) &= \\alpha \\cdot (1+r) \\cdot E(X) \\end{aligned} \\] However, policy limits involve a minimum function , which is more complicated to manipulate: \\[ \\begin{aligned} Y^L_{\\text{Policy Limit}} &= (1+r) \\cdot X \\land u \\\\ &= (1+r) \\left(X \\land \\frac{u}{1+r} \\right) \\\\ \\therefore E \\left(Y^L_{\\text{Policy Limit}} \\right) &= (1+r) \\cdot E \\left(X \\land \\frac{u}{1+r} \\right) \\\\ \\end{aligned} \\] Thus, the same logic can be applied for deductibles with inflation: \\[ \\begin{aligned} Y^L_{\\text{Deductible}} &= (1+r) \\cdot X - (1+r) \\cdot X \\land d \\\\ &= (1+r) \\cdot X - (1+r) \\cdot \\left(X \\land \\frac{d}{1+r} \\right) \\\\ &= (1+r) \\left[X - \\left(X \\land \\frac{d}{1+r} \\right) \\right] \\\\ \\therefore E(Y^L_{\\text{Deductible}}) &= (1+r) \\cdot E \\left[X - \\left(X \\land \\frac{d}{1+r} \\right) \\right] \\end{aligned} \\] Tip This is the exact same result as when coinsurance is applied before the deductible, as both involve scaling of \\(X\\) without the deductible. Thus, remember to use the updated deductible when converting between \\(Y^L\\) and \\(Y^P\\) . An alternative method is available if the loss distribution is a Scale Distribution : \\[ \\begin{aligned} X &\\sim \\text{Scale Distribution}(\\theta) \\\\ X(1+r) &\\sim \\text{Scale Distribution}(\\theta \\cdot (1+r)) \\end{aligned} \\] Thus, this allows us to adjust the distribution rather than the function .","title":"Inflation"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/4.%20Policy%20Modifications/#all-modifications","text":"Consider a policy with all three policy modifications (with the coinsurance applied after the deductible): Below the deductible , nothing will be paid Above the maximum covered loss , the policy limit will be paid Between these two values, the coinsured portion of the excess loss is paid \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& x \\lt d \\\\ \\alpha (X-d),& d \\lt X \\lt m \\\\ u,& X \\ge m \\end{cases} \\end{aligned} \\] This can then be simplified to yield the following results: \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\le d \\\\ \\alpha(X-d),& d \\lt X \\lt m \\\\ u,& X \\ge m \\end{cases} \\\\ &= \\begin{cases} \\alpha (X-X),& X \\le d \\\\ \\alpha \\cdot (X-d),& d \\lt X \\lt m \\\\ \\alpha (m-d),& X \\ge m \\end{cases} \\\\ &= \\alpha \\cdot \\begin{cases} X-X,& X \\le d \\\\ X-d,& d \\lt X \\lt m \\\\ m-d,& X \\ge m \\end{cases} \\\\ &= \\alpha \\cdot \\left [ \\begin{cases} X,& X \\le d \\\\ X,& d \\lt X \\lt m \\\\ m,& X \\ge m \\end{cases} - \\begin{cases} X,& X \\le d \\\\ d,& d \\lt X \\lt m \\\\ d,& X \\ge m \\end{cases} \\right] \\\\ &= \\alpha \\cdot \\left[(X \\land m) - (X \\land d) \\right] \\\\ \\\\ E \\left(Y^L \\right) &= \\alpha \\cdot E \\left[(X \\land m) - (X \\land d) \\right] \\\\ &= \\alpha \\cdot \\left (E \\left[(X \\land m) \\right] - E \\left[(X \\land d) \\right] \\right) \\\\ \\\\ E \\left(Y^P \\right) &= \\frac{E \\left(Y^L \\right)}{1 - F_{X}(d)} \\end{aligned} \\] If inflation is considered, then the above can be easily adjusted for the following year: \\[ \\begin{aligned} E \\left(Y^L \\right) &= \\alpha (1+r) \\cdot \\left (E \\left[\\left(X \\land \\frac{m}{1+r}\\right) \\right] - E \\left[\\left(X \\land \\frac{d}{1+r} \\right) \\right] \\right) \\\\ \\\\ E \\left(Y^P \\right) &= \\frac{E \\left(Y^L \\right)}{1 - F_{X} \\left(\\frac{d}{1+r} \\right)} \\end{aligned} \\]","title":"All Modifications"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/4.%20Policy%20Modifications/#hybrid-modification","text":"There may be policies that contain an unusual combination of features. The only way to solve them is by first principles . Consider the overall piecewise function and then split them into more recognisable ones, using the \"tricks\" shown above. Consider a two deductible example from SOA Sample Question #60 : \\[ \\begin{aligned} Y^L &= \\begin{cases} 0,& X \\lt 1000 \\\\ 0.8 \\cdot (X - 1000),& 1000 \\lt X \\lt 6000 \\\\ 0.8 \\cdot (6000 - 1000),& 6000 \\lt X \\lt 14000 \\\\ 0.8 \\cdot (6000 - 1000) + 0.9 \\cdot (X - 14000),& X \\gt 14000 \\end{cases} \\end{aligned} \\] The tricky part about forming this function is the third level: Since the policyholder bears the full cost of the repairs, the insurer does not pay anything new ; they pay the maximum amount of the previous level . The policyholder has paid 1000 from the deductible and 1000 from the coinsurance , thus has to bear 8000 of losses themselves, resulting in an upper limit of 14000 . Using the methods shown earlier, it can be split into the following components: Tip For any question involving complicated tiers of payments, it is best to write out the piecewise function and manipulate it like above. The key manipulation is \\(0 = X - X\\) , allowing certain piecewise functions to be collapsed into \\(E(X)\\) or \\((X \\land d)\\) .","title":"Hybrid Modification"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/5.%20Aggregate%20Models/","text":"Aggregate Models \u00b6 Collective Risk Model \u00b6 The combination of the Frequency Models \\((N)\\) and the Severity Models \\((X)\\) results in an Aggregate Model \\((S)\\) , which represents the total losses for all policies. The main type of aggregate model is known as the Collective Risk Model , which is a sum of an unknown number of random variables . \\[ \\begin{aligned} S &= X_1 + X_2 + ... + X_N \\end{aligned} \\] Info In probability theory, this is known as a Compound Distribution where \\(N\\) is known as the Primary Distribution and \\(X\\) is known as the Secondary Distribution . It makes the following assumptions: Losses are iid : \\(X_1, X_2, ..., X_N\\) are iid Frequency and Severity are independent : \\(X\\) and \\(N\\) are independent Moments \u00b6 Another perspective is that the collective risk model follows the conditional distribution \\(S \\mid N\\) . Thus, we can compute the probability, expectation and variance using previously found results. Note It can also be thought of as a mixture distribution where the mixing weights are the probability that the there that number of losses: \\[ w_k = P(N=K) \\] Thus, Compound Distributions and Mixture Distributions are essentially the same concept . Starting with the Law of Total Expectation , \\[ \\begin{aligned} E(S) &= E_{N} [E_{S}(S \\mid N)] \\\\ \\\\ E_{S}(S \\mid N) &= E(X_1 + X_2 + \\dots + X_N \\mid N) \\\\ &= E(X_1 \\mid N) + E(X_2 \\mid N) + \\dots + E(X_N \\mid N) \\\\ &= E(X_1) + E(X_2) + \\dots + E(X_N), \\ \\text{Independence} \\\\ &= E(X) + E(X)+ \\dots + E(X), \\ \\text{Identical} \\\\ &= N \\cdot \\underbrace{E(X)}_{\\text{Constant}} \\\\ \\\\ \\therefore E(S) &= E_{N} [E_{S}(S \\mid N)] \\\\ &= E_{N} [N \\cdot E(X)] \\\\ &= E(X)\\cdot E(N) \\end{aligned} \\] Note This result is highly intuitive as \\(\\text{Risk} = \\text{Frequency} \\cdot \\text{Severity}\\) . Moving on to the Law of Total Variance : \\[ \\begin{aligned} Var (S) &= E_N [Var (S \\mid N)] + Var_{N} [E(S \\mid N)] \\\\ \\\\ Var (S \\mid N) &= Var (X_1 + X_2 + ... + X_N \\mid N) \\\\ &= N \\cdot \\underbrace{Var(X)}_\\text{Constant}, \\ \\text{IID} \\\\ \\\\ Var (S) &= E_N [Var (S \\mid N)] + Var_{N} [E(S \\mid N)] \\\\ &= E [N \\cdot Var(X)] + Var_{N} [N \\cdot E(X)] \\\\ &= Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var(N) \\end{aligned} \\] Tip If the mean and variance of the frequency distribution are equal (EG. Poisson Distribution), then the Variance can be simplified: \\[ \\begin{aligned} Var (S) &= Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var(N) \\\\ &= E(N) \\cdot [Var(X) + [E(X)]^2] \\\\ &= E(N) \\cdot E(X^2) \\end{aligned} \\] Probability \u00b6 The probability of \\(S\\) is difficult to derive . Thus, for the purposes of this exam, we will assume a Discrete Severity for this portion. By the Law of Total Probability : Note If both the Frequency and Severity models are discrete, then the Aggrgegate Loss is discrete as well. \\[ \\begin{aligned} P(S = s) &= E_{N} [P(S = s \\mid N = n)] \\\\ &= \\sum P(S = s \\mid N = n) \\cdot P(N = n) \\end{aligned} \\] The term within the above summation is known as a Convolution , as it is the probability of a sum of random variables. Since there are \\(n\\) random variables, it is known as the n-fold Convolution . \\[ P(S = s \\mid N = n) = P(X_1 + X_2 + \\dots + X_n = s) \\] For simplicity, consider a two fold convolution: \\[ \\begin{aligned} P(S = s \\mid n = 2) &= \\sum P(X_1 = x, X_2 = s - x_1) \\\\ &= \\sum P(X_1 = x) \\cdot P(X_2 = s - x_1) \\\\ \\\\ \\therefore P(S = s) &= P(N = n) \\cdot \\sum P(X_1 = x) \\cdot P(X_2 = s - x_1) \\end{aligned} \\] A convolution is simply the probability of all possible combinations of the random variables that result in the specified value (Recall the two dice example). However, one implicit assumption for this to work is that the individual severities cannot be 0 . The reason is because it would lead to endless possibilities for each case. Consider \\(S=1\\) : \\(1 + 0 + 0 + \\dots + 0 = 1\\) \\(0 + 1 + 0 + \\dots + 0 = 1\\) \\(0 + 0 + 1 + \\dots + 0 = 1\\) \\(0 + 0 + 0 + \\dots + 1 = 1\\) However, then how is \\(S=0\\) determined, since there are no sums that result in 0 ? There is now only ONE way for aggregate losses to be 0 - if there are no claims at all . \\[ P(S = 0) = P(n = 0) \\] Generating Functions \u00b6 An alternative method to calculate the moments would be to use the MGF : \\[ M_{S}(t) = M_{N} [\\ln M_{X}(t)] \\] Similarly, if severity is discrete, then an alternative method to calculate its probability would be to use its PGF : \\[ P_{S}(t) = P_{N} [P_{X}(t)] \\] These methods tend to be more time consuming , thus they are preferred only when calculating higher moments or large values of \\(s\\) . Normal Approximation \u00b6 Another method of determining the probability would be to simply approximate it. This is especially useful when the individual losses are continuous, such that the previous methods would not work. By the Central Limit Theorem , as the number of random variables increase, their sum is approximately normal distributed : \\[ S \\sim (E[S], Var[S]) \\] Instead of using the normal distribution, the Lognormal Distribution can be used as well. However, we need to first solve for its parameters : \\[ \\begin{aligned} e^{\\mu + 0.5\\sigma^2} &= E(S) \\\\ \\left(e^{\\mu + 0.5\\sigma^2} \\right)^2 \\cdot (e^{\\sigma^2}-1) &= Var (S) \\\\ \\\\ \\therefore S &\\sim \\text{Log Normal} (\\mu, \\sigma^2) \\end{aligned} \\] Warning Although the notation is the same, do NOT confuse the lognormal parameters with the normal parameters. Continuity Correction \u00b6 We may still want to use the approximation even when losses are discrete as they may still be too complicated to compute. However, this may lead to some inaccuracies as we are using a continuous distribution to approximate a discrete distribution. Under a discrete distribution, the following are complements: \\[ P(X \\le n) + P(X \\gt n+1) = 1 \\] However, the same cannot be said for a continuous distribution, as there is some density between \\(n\\) and \\(n+1\\) that is not accounted for: \\[ P(S \\le n) + P(S \\gt n+1) \\ne 1 \\] Thus, Continuity Correction attempts to account for this by reducing the missing density. The general idea is to adjust the range such that the midpoint of the two values : \\[ \\begin{aligned} P(S \\le n) &= P \\left(S \\le \\frac{n+(n+1)}{2} \\right) \\\\ P(S \\ge n) &= P \\left(S \\ge \\frac{n+(n-1)}{2} \\right) \\end{aligned} \\] Another concept to remember when going from Continuous to Discrete is that a continuous \\(S \\lt n\\) is a discrete \\(S \\le (n-1)\\) since there are a countable number of values, vice versa. If required to approximate the probability at a specified value , then the above method will not work as a continuous distribution cannot find the probability at a single point. Thus, it will be converted to a range around the specified value instead, with the midpoint value used on both sides of the specified value: Tip If the discrete distribution does not support consecutive integers \\(\\set{0, 1, 2, \\dots}\\) , then simply take the midpoint of whatever consecutive values in its support \\(\\set{100, 200, 300}\\) . Non-Direct Frequency \u00b6 Questions may not directly the frequency distribution of interest. For instance, we may be interesting in modelling aggregate losses from earthquake insurance. We are ultimately interested in the total number of claims. However, this has to account for two aspects: Number of Claims per earthquake : \\(N \\sim \\text{Distribution}\\) Number of Earthquakes -- \\(N^* \\sim \\text{Distribution}\\) The first step is to follow the usual steps to find the aggregate losses for one earthquake , \\(S\\) . The aggregate loss for all possible earthquakes is thus represented as: \\[ S^* = S_1 + S_2 + \\dots + S_{N^*} \\] Repeating the same steps as before, assuming independence between earthquakes , \\[ \\begin{aligned} E(S^*) &= E_{N^*} [E(S^* \\mid N^*)] \\\\ &= E_{N^*} [N^* \\cdot E(S)] \\\\ &= E(N^*) \\cdot E(S) \\\\ \\\\ \\text{Var} (S^*) &= E_{N^*}[\\text{Var}(S \\mid N^*)] + \\text{Var}_{N^*} [E(S \\mid N^*)] \\\\ &= E_{N^*}[N^* \\cdot \\text{Var}(S)] + \\text{Var}_{N^*} [N^* \\cdot E(S)] \\\\ &= E(N^*) \\cdot \\text{Var}(S) + \\text{Var}(N^*) \\cdot [E(S)]^2 \\end{aligned} \\] Warning Always read the question carefully - there could be questions that provide the distribution of the number of earthquakes as a red herring but only want aggregate losses from a single earthquake. Alternatively, a question may provide the exact number of earthquakes , \\(N^*\\) . The aggregate loss is then a sum of a known number of variables : \\[ S^* = S_1 + S_2 \\] Thus, the moments are simply scaled by that number: \\[ \\begin{aligned} E(S^*) &= N^* \\cdot E(S) \\\\ \\text{Var}(S^*) &= N^* \\cdot \\text{Var}(S) \\end{aligned} \\] Non-Independence \u00b6 If frequency and severity are NOT independent , then the all the above methods cannot be used. This would mean that the distribution of \\(X\\) depends on the number of claims . Questions of this nature usually deal with Discrete Severity : We must first determine all possible values of \\(S\\) by considering all possible combinations of \\(X\\) and \\(N\\) : \\[ \\begin{aligned} S &= 0 \\\\ S &= 25 = 1 \\cdot 25 \\\\ S &= 150 = 1 \\cdot 150 \\\\ S &= 100 = 2 \\cdot 50 \\\\ S &= 250 = 1 \\cdot 50 + 1 \\cdot 200 \\\\ S &= 400 = 2 \\cdot 200 \\end{aligned} \\] Warning It is a common mistake to forget to include \\(S = 0\\) . As mentioned previously, it occurs only when \\(N=0\\) . A good way to check if there are any missing combinations or wrongly computed probabilities would be to ensure that all the probabilities sum to 1 . This process is a \"reverse\" of a convolution -- rather than finding the combinations that lead to a particular value, we are finding all possible values that could result from the combinations. The probability of each value of \\(S\\) is its convolution. Using this distribution, its mean and variance can be determined. Warning It is a common mistake to compute the wrong probability for cases when there are two or more claims. In the above example for \\(S = 250\\) , there are actually two possible combinations: First Loss \\(50\\) , Second Loss \\(200\\) First Loss \\(200\\) , Second Loss \\(50\\) Thus, the associated probability for this value is: \\[ \\begin{aligned} P(S = 250) &= P(N = 2) \\cdot [P(X_1 = 50, X_2 = 200 \\mid N = 2) + P(X_1 = 200, X_2 = 50 \\mid N = 2)] \\\\ &= P(N = 2) \\cdot 2 \\cdot P(X = 50, X = 200 \\mid N = 2) \\\\ &= P(N = 2) \\cdot 2 \\cdot P(X = 50 \\mid N = 2) \\cdot P(X = 200 \\mid N = 2) \\\\ &= \\frac{1}{5} \\cdot 2 \\cdot \\frac{2}{3} \\cdot \\frac{1}{3} \\\\ &= \\frac{4}{45} \\end{aligned} \\] Policy Modifications \u00b6 Aggregate Deductibles \u00b6 Similar to individual losses, we can also consider policy modifications to aggregate losses. For the purposes of this exam, we mainly consider deductibles. As before, our primary concern is the expected payment : \\[ \\begin{aligned} E(S-d)_{+} &= E(S) - E(S \\land d) \\end{aligned} \\] Info Recall that Stop Loss Reinsurance is modelled as a deductible applied to Aggregate Losses. The perspective has flipped in this scenario; the primary insurer experiences the limited loss \\(S \\land d\\) and the reinsurer pays \\(S-d\\) . Thus, the expected value of the payment is known as often known as the Pure Premium for Stop Loss Insurance , as it represents the reinsurer's pure loss. The problem is that the distribution of \\(S\\) is not known , thus \\(E(S \\land d)\\) is difficult to obtain. Thus, for these types of questions, the underlying severity is assumed to be discrete , allowing the distribution of \\(S\\) to be determined manually like in the non-independence case. It can then be adjusted to obtain the distribution \\(S \\land d\\) , following which all the usual calculations can proceeed. Aggregate Payments \u00b6 The discussion so far has solely been on Aggregate Losses . However, if each policy has a deductible, then there is a need to consider Aggregate Payments instead. Both types of aggregate payments (per loss and per payment) can be used to determine aggregate payments: If considering Payments per Losses \\((Y^L)\\) , then as its name suggests, the appropriate frequency to use is the Loss Frequency \\((N)\\) . \\[ \\begin{aligned} S &= Y^L_{1} + Y^L_{2} + \\dots + Y^L_{N} \\\\ \\\\ E(S) &= E(N) \\cdot E(Y^L) \\\\ Var (S) &= Var(Y^L) \\cdot E(N) + \\left[E \\left(Y^L \\right) \\right]^2 \\cdot Var (N) \\end{aligned} \\] If considering Payments per Payments \\((Y^P)\\) , then as its name suggests, the appropriate frequency to use is the Payment Frequency \\((N^{*})\\) . \\[ \\begin{aligned} S &= Y^P_{1} + Y^P_{2} + \\dots + Y^P_{N^{*}} \\\\ \\\\ E(S) &= E(N^{*}) \\cdot E(Y^P) \\\\ Var (S) &= Var(Y^P) \\cdot E(N^{*}) + \\left[E \\left(Y^P \\right) \\right]^2 \\cdot Var (N^{*}) \\end{aligned} \\] The key is to match the frequency distribution with the severity : Loss & Loss Frequency; Payments & Payment Frequency. Payment Frequency \u00b6 Imposing a deductible naturally results in fewer payments than losses as not all losses result in a payment. The Payment Frequency \\((N^{*})\\) is thus an adjusted version of the loss distribution that only takes into account the losses that result in a payment. Consider a Bernoulli Indicator variable that takes on the value 1 if a payment will be made and 0 if it will not: \\[ \\begin{aligned} I &= \\begin{cases} 1 \\ (\\text{Paid}),& X \\gt d \\\\ 0 \\ (\\text{Not Paid}),& X \\le d \\end{cases} \\\\ \\\\ v &= P(X \\gt d) \\\\ P_I(t) &= 1 + v(t-1) \\end{aligned} \\] \\(N^{*}\\) is the sum of \\(N\\) idendepent indicator variables : \\[ N^{*} = I_1 + I_2 + \\dots + I_N \\] It is essentially a Compound Distribution with \\(N\\) being the primary distribution and \\(I\\) being the secondary distribution. Thus, the PGF of \\(N^{*}\\) can be determined: \\[ \\begin{aligned} P_{N^*} &= P_{N}[P_{I}(t)] \\\\ &= P_{N}[1 + p(t-1)] \\end{aligned} \\] By expanding simplfying the expression, the distribution of \\(N^{*}\\) can be determined: \\[ \\begin{aligned} N &\\sim \\text{Poisson}(\\mu) \\\\ \\\\ P_{N}[1 + p(t-1)] &= e^{\\mu(1 + v(z-1) -1)} \\\\ &= e^{\\mu (v(z-1))} \\\\ &= e^{v\\mu (z-1)} \\\\ \\\\ \\therefore N^{*} &\\sim \\text{Poisson}(v \\cdot \\mu) \\end{aligned} \\] Repeating the process for the other two members of the (a, b, 0) class, \\[ \\begin{aligned} N &\\sim \\text{Binomial} (n, p) \\\\ \\therefore N^{*} &\\sim \\text{Binomial} (n, vp) \\\\ \\\\ N &\\sim \\text{Negative Binomial} (r, \\theta) \\\\ \\therefore N^{*} &\\sim \\text{Negative Binomial} (r, v \\theta) \\end{aligned} \\]","title":"Aggregate Models"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/5.%20Aggregate%20Models/#aggregate-models","text":"","title":"Aggregate Models"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/5.%20Aggregate%20Models/#collective-risk-model","text":"The combination of the Frequency Models \\((N)\\) and the Severity Models \\((X)\\) results in an Aggregate Model \\((S)\\) , which represents the total losses for all policies. The main type of aggregate model is known as the Collective Risk Model , which is a sum of an unknown number of random variables . \\[ \\begin{aligned} S &= X_1 + X_2 + ... + X_N \\end{aligned} \\] Info In probability theory, this is known as a Compound Distribution where \\(N\\) is known as the Primary Distribution and \\(X\\) is known as the Secondary Distribution . It makes the following assumptions: Losses are iid : \\(X_1, X_2, ..., X_N\\) are iid Frequency and Severity are independent : \\(X\\) and \\(N\\) are independent","title":"Collective Risk Model"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/5.%20Aggregate%20Models/#moments","text":"Another perspective is that the collective risk model follows the conditional distribution \\(S \\mid N\\) . Thus, we can compute the probability, expectation and variance using previously found results. Note It can also be thought of as a mixture distribution where the mixing weights are the probability that the there that number of losses: \\[ w_k = P(N=K) \\] Thus, Compound Distributions and Mixture Distributions are essentially the same concept . Starting with the Law of Total Expectation , \\[ \\begin{aligned} E(S) &= E_{N} [E_{S}(S \\mid N)] \\\\ \\\\ E_{S}(S \\mid N) &= E(X_1 + X_2 + \\dots + X_N \\mid N) \\\\ &= E(X_1 \\mid N) + E(X_2 \\mid N) + \\dots + E(X_N \\mid N) \\\\ &= E(X_1) + E(X_2) + \\dots + E(X_N), \\ \\text{Independence} \\\\ &= E(X) + E(X)+ \\dots + E(X), \\ \\text{Identical} \\\\ &= N \\cdot \\underbrace{E(X)}_{\\text{Constant}} \\\\ \\\\ \\therefore E(S) &= E_{N} [E_{S}(S \\mid N)] \\\\ &= E_{N} [N \\cdot E(X)] \\\\ &= E(X)\\cdot E(N) \\end{aligned} \\] Note This result is highly intuitive as \\(\\text{Risk} = \\text{Frequency} \\cdot \\text{Severity}\\) . Moving on to the Law of Total Variance : \\[ \\begin{aligned} Var (S) &= E_N [Var (S \\mid N)] + Var_{N} [E(S \\mid N)] \\\\ \\\\ Var (S \\mid N) &= Var (X_1 + X_2 + ... + X_N \\mid N) \\\\ &= N \\cdot \\underbrace{Var(X)}_\\text{Constant}, \\ \\text{IID} \\\\ \\\\ Var (S) &= E_N [Var (S \\mid N)] + Var_{N} [E(S \\mid N)] \\\\ &= E [N \\cdot Var(X)] + Var_{N} [N \\cdot E(X)] \\\\ &= Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var(N) \\end{aligned} \\] Tip If the mean and variance of the frequency distribution are equal (EG. Poisson Distribution), then the Variance can be simplified: \\[ \\begin{aligned} Var (S) &= Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var(N) \\\\ &= E(N) \\cdot [Var(X) + [E(X)]^2] \\\\ &= E(N) \\cdot E(X^2) \\end{aligned} \\]","title":"Moments"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/5.%20Aggregate%20Models/#probability","text":"The probability of \\(S\\) is difficult to derive . Thus, for the purposes of this exam, we will assume a Discrete Severity for this portion. By the Law of Total Probability : Note If both the Frequency and Severity models are discrete, then the Aggrgegate Loss is discrete as well. \\[ \\begin{aligned} P(S = s) &= E_{N} [P(S = s \\mid N = n)] \\\\ &= \\sum P(S = s \\mid N = n) \\cdot P(N = n) \\end{aligned} \\] The term within the above summation is known as a Convolution , as it is the probability of a sum of random variables. Since there are \\(n\\) random variables, it is known as the n-fold Convolution . \\[ P(S = s \\mid N = n) = P(X_1 + X_2 + \\dots + X_n = s) \\] For simplicity, consider a two fold convolution: \\[ \\begin{aligned} P(S = s \\mid n = 2) &= \\sum P(X_1 = x, X_2 = s - x_1) \\\\ &= \\sum P(X_1 = x) \\cdot P(X_2 = s - x_1) \\\\ \\\\ \\therefore P(S = s) &= P(N = n) \\cdot \\sum P(X_1 = x) \\cdot P(X_2 = s - x_1) \\end{aligned} \\] A convolution is simply the probability of all possible combinations of the random variables that result in the specified value (Recall the two dice example). However, one implicit assumption for this to work is that the individual severities cannot be 0 . The reason is because it would lead to endless possibilities for each case. Consider \\(S=1\\) : \\(1 + 0 + 0 + \\dots + 0 = 1\\) \\(0 + 1 + 0 + \\dots + 0 = 1\\) \\(0 + 0 + 1 + \\dots + 0 = 1\\) \\(0 + 0 + 0 + \\dots + 1 = 1\\) However, then how is \\(S=0\\) determined, since there are no sums that result in 0 ? There is now only ONE way for aggregate losses to be 0 - if there are no claims at all . \\[ P(S = 0) = P(n = 0) \\]","title":"Probability"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/5.%20Aggregate%20Models/#generating-functions","text":"An alternative method to calculate the moments would be to use the MGF : \\[ M_{S}(t) = M_{N} [\\ln M_{X}(t)] \\] Similarly, if severity is discrete, then an alternative method to calculate its probability would be to use its PGF : \\[ P_{S}(t) = P_{N} [P_{X}(t)] \\] These methods tend to be more time consuming , thus they are preferred only when calculating higher moments or large values of \\(s\\) .","title":"Generating Functions"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/5.%20Aggregate%20Models/#normal-approximation","text":"Another method of determining the probability would be to simply approximate it. This is especially useful when the individual losses are continuous, such that the previous methods would not work. By the Central Limit Theorem , as the number of random variables increase, their sum is approximately normal distributed : \\[ S \\sim (E[S], Var[S]) \\] Instead of using the normal distribution, the Lognormal Distribution can be used as well. However, we need to first solve for its parameters : \\[ \\begin{aligned} e^{\\mu + 0.5\\sigma^2} &= E(S) \\\\ \\left(e^{\\mu + 0.5\\sigma^2} \\right)^2 \\cdot (e^{\\sigma^2}-1) &= Var (S) \\\\ \\\\ \\therefore S &\\sim \\text{Log Normal} (\\mu, \\sigma^2) \\end{aligned} \\] Warning Although the notation is the same, do NOT confuse the lognormal parameters with the normal parameters.","title":"Normal Approximation"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/5.%20Aggregate%20Models/#continuity-correction","text":"We may still want to use the approximation even when losses are discrete as they may still be too complicated to compute. However, this may lead to some inaccuracies as we are using a continuous distribution to approximate a discrete distribution. Under a discrete distribution, the following are complements: \\[ P(X \\le n) + P(X \\gt n+1) = 1 \\] However, the same cannot be said for a continuous distribution, as there is some density between \\(n\\) and \\(n+1\\) that is not accounted for: \\[ P(S \\le n) + P(S \\gt n+1) \\ne 1 \\] Thus, Continuity Correction attempts to account for this by reducing the missing density. The general idea is to adjust the range such that the midpoint of the two values : \\[ \\begin{aligned} P(S \\le n) &= P \\left(S \\le \\frac{n+(n+1)}{2} \\right) \\\\ P(S \\ge n) &= P \\left(S \\ge \\frac{n+(n-1)}{2} \\right) \\end{aligned} \\] Another concept to remember when going from Continuous to Discrete is that a continuous \\(S \\lt n\\) is a discrete \\(S \\le (n-1)\\) since there are a countable number of values, vice versa. If required to approximate the probability at a specified value , then the above method will not work as a continuous distribution cannot find the probability at a single point. Thus, it will be converted to a range around the specified value instead, with the midpoint value used on both sides of the specified value: Tip If the discrete distribution does not support consecutive integers \\(\\set{0, 1, 2, \\dots}\\) , then simply take the midpoint of whatever consecutive values in its support \\(\\set{100, 200, 300}\\) .","title":"Continuity Correction"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/5.%20Aggregate%20Models/#non-direct-frequency","text":"Questions may not directly the frequency distribution of interest. For instance, we may be interesting in modelling aggregate losses from earthquake insurance. We are ultimately interested in the total number of claims. However, this has to account for two aspects: Number of Claims per earthquake : \\(N \\sim \\text{Distribution}\\) Number of Earthquakes -- \\(N^* \\sim \\text{Distribution}\\) The first step is to follow the usual steps to find the aggregate losses for one earthquake , \\(S\\) . The aggregate loss for all possible earthquakes is thus represented as: \\[ S^* = S_1 + S_2 + \\dots + S_{N^*} \\] Repeating the same steps as before, assuming independence between earthquakes , \\[ \\begin{aligned} E(S^*) &= E_{N^*} [E(S^* \\mid N^*)] \\\\ &= E_{N^*} [N^* \\cdot E(S)] \\\\ &= E(N^*) \\cdot E(S) \\\\ \\\\ \\text{Var} (S^*) &= E_{N^*}[\\text{Var}(S \\mid N^*)] + \\text{Var}_{N^*} [E(S \\mid N^*)] \\\\ &= E_{N^*}[N^* \\cdot \\text{Var}(S)] + \\text{Var}_{N^*} [N^* \\cdot E(S)] \\\\ &= E(N^*) \\cdot \\text{Var}(S) + \\text{Var}(N^*) \\cdot [E(S)]^2 \\end{aligned} \\] Warning Always read the question carefully - there could be questions that provide the distribution of the number of earthquakes as a red herring but only want aggregate losses from a single earthquake. Alternatively, a question may provide the exact number of earthquakes , \\(N^*\\) . The aggregate loss is then a sum of a known number of variables : \\[ S^* = S_1 + S_2 \\] Thus, the moments are simply scaled by that number: \\[ \\begin{aligned} E(S^*) &= N^* \\cdot E(S) \\\\ \\text{Var}(S^*) &= N^* \\cdot \\text{Var}(S) \\end{aligned} \\]","title":"Non-Direct Frequency"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/5.%20Aggregate%20Models/#non-independence","text":"If frequency and severity are NOT independent , then the all the above methods cannot be used. This would mean that the distribution of \\(X\\) depends on the number of claims . Questions of this nature usually deal with Discrete Severity : We must first determine all possible values of \\(S\\) by considering all possible combinations of \\(X\\) and \\(N\\) : \\[ \\begin{aligned} S &= 0 \\\\ S &= 25 = 1 \\cdot 25 \\\\ S &= 150 = 1 \\cdot 150 \\\\ S &= 100 = 2 \\cdot 50 \\\\ S &= 250 = 1 \\cdot 50 + 1 \\cdot 200 \\\\ S &= 400 = 2 \\cdot 200 \\end{aligned} \\] Warning It is a common mistake to forget to include \\(S = 0\\) . As mentioned previously, it occurs only when \\(N=0\\) . A good way to check if there are any missing combinations or wrongly computed probabilities would be to ensure that all the probabilities sum to 1 . This process is a \"reverse\" of a convolution -- rather than finding the combinations that lead to a particular value, we are finding all possible values that could result from the combinations. The probability of each value of \\(S\\) is its convolution. Using this distribution, its mean and variance can be determined. Warning It is a common mistake to compute the wrong probability for cases when there are two or more claims. In the above example for \\(S = 250\\) , there are actually two possible combinations: First Loss \\(50\\) , Second Loss \\(200\\) First Loss \\(200\\) , Second Loss \\(50\\) Thus, the associated probability for this value is: \\[ \\begin{aligned} P(S = 250) &= P(N = 2) \\cdot [P(X_1 = 50, X_2 = 200 \\mid N = 2) + P(X_1 = 200, X_2 = 50 \\mid N = 2)] \\\\ &= P(N = 2) \\cdot 2 \\cdot P(X = 50, X = 200 \\mid N = 2) \\\\ &= P(N = 2) \\cdot 2 \\cdot P(X = 50 \\mid N = 2) \\cdot P(X = 200 \\mid N = 2) \\\\ &= \\frac{1}{5} \\cdot 2 \\cdot \\frac{2}{3} \\cdot \\frac{1}{3} \\\\ &= \\frac{4}{45} \\end{aligned} \\]","title":"Non-Independence"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/5.%20Aggregate%20Models/#policy-modifications","text":"","title":"Policy Modifications"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/5.%20Aggregate%20Models/#aggregate-deductibles","text":"Similar to individual losses, we can also consider policy modifications to aggregate losses. For the purposes of this exam, we mainly consider deductibles. As before, our primary concern is the expected payment : \\[ \\begin{aligned} E(S-d)_{+} &= E(S) - E(S \\land d) \\end{aligned} \\] Info Recall that Stop Loss Reinsurance is modelled as a deductible applied to Aggregate Losses. The perspective has flipped in this scenario; the primary insurer experiences the limited loss \\(S \\land d\\) and the reinsurer pays \\(S-d\\) . Thus, the expected value of the payment is known as often known as the Pure Premium for Stop Loss Insurance , as it represents the reinsurer's pure loss. The problem is that the distribution of \\(S\\) is not known , thus \\(E(S \\land d)\\) is difficult to obtain. Thus, for these types of questions, the underlying severity is assumed to be discrete , allowing the distribution of \\(S\\) to be determined manually like in the non-independence case. It can then be adjusted to obtain the distribution \\(S \\land d\\) , following which all the usual calculations can proceeed.","title":"Aggregate Deductibles"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/5.%20Aggregate%20Models/#aggregate-payments","text":"The discussion so far has solely been on Aggregate Losses . However, if each policy has a deductible, then there is a need to consider Aggregate Payments instead. Both types of aggregate payments (per loss and per payment) can be used to determine aggregate payments: If considering Payments per Losses \\((Y^L)\\) , then as its name suggests, the appropriate frequency to use is the Loss Frequency \\((N)\\) . \\[ \\begin{aligned} S &= Y^L_{1} + Y^L_{2} + \\dots + Y^L_{N} \\\\ \\\\ E(S) &= E(N) \\cdot E(Y^L) \\\\ Var (S) &= Var(Y^L) \\cdot E(N) + \\left[E \\left(Y^L \\right) \\right]^2 \\cdot Var (N) \\end{aligned} \\] If considering Payments per Payments \\((Y^P)\\) , then as its name suggests, the appropriate frequency to use is the Payment Frequency \\((N^{*})\\) . \\[ \\begin{aligned} S &= Y^P_{1} + Y^P_{2} + \\dots + Y^P_{N^{*}} \\\\ \\\\ E(S) &= E(N^{*}) \\cdot E(Y^P) \\\\ Var (S) &= Var(Y^P) \\cdot E(N^{*}) + \\left[E \\left(Y^P \\right) \\right]^2 \\cdot Var (N^{*}) \\end{aligned} \\] The key is to match the frequency distribution with the severity : Loss & Loss Frequency; Payments & Payment Frequency.","title":"Aggregate Payments"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/5.%20Aggregate%20Models/#payment-frequency","text":"Imposing a deductible naturally results in fewer payments than losses as not all losses result in a payment. The Payment Frequency \\((N^{*})\\) is thus an adjusted version of the loss distribution that only takes into account the losses that result in a payment. Consider a Bernoulli Indicator variable that takes on the value 1 if a payment will be made and 0 if it will not: \\[ \\begin{aligned} I &= \\begin{cases} 1 \\ (\\text{Paid}),& X \\gt d \\\\ 0 \\ (\\text{Not Paid}),& X \\le d \\end{cases} \\\\ \\\\ v &= P(X \\gt d) \\\\ P_I(t) &= 1 + v(t-1) \\end{aligned} \\] \\(N^{*}\\) is the sum of \\(N\\) idendepent indicator variables : \\[ N^{*} = I_1 + I_2 + \\dots + I_N \\] It is essentially a Compound Distribution with \\(N\\) being the primary distribution and \\(I\\) being the secondary distribution. Thus, the PGF of \\(N^{*}\\) can be determined: \\[ \\begin{aligned} P_{N^*} &= P_{N}[P_{I}(t)] \\\\ &= P_{N}[1 + p(t-1)] \\end{aligned} \\] By expanding simplfying the expression, the distribution of \\(N^{*}\\) can be determined: \\[ \\begin{aligned} N &\\sim \\text{Poisson}(\\mu) \\\\ \\\\ P_{N}[1 + p(t-1)] &= e^{\\mu(1 + v(z-1) -1)} \\\\ &= e^{\\mu (v(z-1))} \\\\ &= e^{v\\mu (z-1)} \\\\ \\\\ \\therefore N^{*} &\\sim \\text{Poisson}(v \\cdot \\mu) \\end{aligned} \\] Repeating the process for the other two members of the (a, b, 0) class, \\[ \\begin{aligned} N &\\sim \\text{Binomial} (n, p) \\\\ \\therefore N^{*} &\\sim \\text{Binomial} (n, vp) \\\\ \\\\ N &\\sim \\text{Negative Binomial} (r, \\theta) \\\\ \\therefore N^{*} &\\sim \\text{Negative Binomial} (r, v \\theta) \\end{aligned} \\]","title":"Payment Frequency"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/6.%20Loss%20Reserving/","text":"Loss Reserving \u00b6 Overview \u00b6 General Insurance claims may be filed long after the loss event occurs, thus they must set up a Loss Reserve to ensure that they have enough capital to pay out these claims well into the future. Loss Terminology \u00b6 This subsection will properly introduce the proper terminology regarding losses There are three basic building blocks of a reported claim: Incurred Losses : Reported loss amount Paid Losses : Loss amounts that have been reported and paid out Case Reserve : Loss amounts that have been reported but not yet paid out \\[ \\text{Incurred Loss} = \\text{Paid Loss} + \\text{Case Reserve} \\] The problem is that there will always be losses stemming from accidents that occurred in the past that have yet to be reported to the insurer, appropriately named as Incurred But Not Reported (IBNR) losses. The main objective of loss reserving is to estimate this quantity by predicting how losses will grow over time thus ensuring that they have enough capital to cover it via an IBNR Reserve . This process of predicting how losses will grow is known as Loss Development . The final loss amount is known as the Ultimate Loss , from which no further development will occur. The difference between the final loss ( ultimate ) and what has been reported ( incurred ) is the IBNR reserve as of the current time: \\[ \\text{IBNR Reserve} = \\text{Ultimate Loss} - \\text{Incurred Loss} \\] Thus, the total loss reserve is thus the combination of the two reserves : \\[ \\begin{aligned} \\text{Loss Reserve} &= \\text{Case Reserve} + \\text{IBNR Reserve} \\\\ &= \\text{Ultimate Loss} - \\text{Incurred Loss} + \\text{Case Reserve} \\\\ &= \\text{Ultimate Loss} - \\text{Paid Loss} \\end{aligned} \\] Note Intuitively, the total reserve is simply the difference between the ultimate loss and what has been paid, as the total reserve is meant to cover all unpaid losses . Expected Loss Ratio Method \u00b6 The simplest method for estimating ultimate losses is known as the Expected Loss Ratio Method . Loss Ratios (LR) are the ratio of losses to premiums , which is a measure of the percentage of premiums that have been used to pay losses. \\[ \\text{LR} = \\frac{\\text{Loss}}{\\text{Premium}} \\] The Expected Loss Ratio (ELR) is the ultimate loss ratio ; the ratio of the ultimate loss to aggregate premiums collected over the life of the policy. Since premiums are known by the insurer, this can be used to obtain an estimate for the ultimate loss: \\[ \\begin{aligned} \\text{ELR} &= \\frac{\\text{Ultimate Loss}}{\\text{Total Premium}} \\\\ \\\\ \\therefore \\text{Ultimate Loss} &= \\text{Earned Premium} \\cdot \\text{ELR} \\end{aligned} \\] Chain Ladder Method \u00b6 Another method of estimating ultimate losses is known as the Chain Ladder (CL) Method. This methods works by finding patterns in historical loss development and assumes that future losses will continue to develop in a similar manner . In order to efficiently find patterns in historical losses, they are recorded in a convenient format known as a Development Table : Each Cell represents the paid losses in that year Each Row shows losses that occurred in the same Accident Year (AY) Each Column shows losses that occurred in the same Development Year (DY) Each Diagonal shows losses that occurred in the same Calendar Year (CY) Note The AY that the diagonal originates from is the CY for that calendar year. This can be seen by the colouring of the cells in the table below. The definition for AY and CY can be found in the ratemaking section.s DYs are also known as the age of the claim as they indicate how long after the AY when the claims are paid. For instance, DY3 represents that the claim was paid 3 years after the AY. The next step is to find the Cumulative paid Losses per AY. This is done by changing the values of each cell to the sum of all previous cells in the same row : \\[ \\begin{aligned} \\text{Cumulative Loss} &= \\text{Sum of Losses in the same AY} \\\\ \\text{Cumulative Cell}_{\\text{AY, DY}} &= \\sum_{DY} \\text{Cell}_{\\text{AY, DY}} \\end{aligned} \\] Note Some questions may directly provide the cumulative losses in the table rather than incremental losses, saving you the time of having to sum everything up. The number of columns indicates how long the insurer expects the losses to develop for. If the last column is DY6, then the cumulative loss at DY6 is the ultimate loss . Warning Some questions will require us to construct our own table. Thus, it is important to fully understand the intuition behind it. The key idea under the CL method is that losses all take the same duration to develop to their ultimate values. This means that the losses occurring in different years will take the same duration to mature at different dates . It is an extremely common mistake to think that all the losses develop to ultimate on the same date. Thus, the DYs can be thought of as the age of the losses . Losses from later AYs are \"younger\" and hence need more time to mature . Tip Instead of Development Year, some questions may use the term Calendar Year (CY). As will be mentioned in the ratemaking chapter, CY and AY are assumed to be synonmous for the purposes of losses. The difference between the CY and the AY is the development year. For instance, if the cumulative losses at CY7 are given for AY5 losses , then the losses are at DY2 . The goal is now to find the historical patterns in the losses and then use those patterns to fill up the blank half of the development table. The historical patterns are known as Loss Development Factors (LDF), which is the ratio of the current DY's cumulative loss to the previous DY's cumulative loss : \\[ \\text{LDF}_{\\text{AY, DY}} = \\frac{\\text{Cumulative Cell}_{\\text{AY, DY}}}{\\text{Cumulative Cell}_{\\text{AY, DY}-1}} \\] The next step would be to develop each years using the development factors , until it reaches the ultimate loss: \\[ \\text{Cumulative Cells}_{\\text{AY, DY}+1} = \\text{Cumulative Cells}_{\\text{AY, DY}} \\cdot \\text{LDF}_{\\text{AY, DY}} \\] However, the problem is that for all years other than the final year, there are usually multiple LDFs for each DY. Questions will usually specify to either the arithmetic average, recent average of weighted average. For simplicity, we assume that the arithmetic average is used. We will arrive at the following LDFs and thus the corresponding final development table : \\[ \\text{Ultimate Loss} = \\text{Paid Loss} \\cdot \\text{LDF}_{\\text{DY}} \\cdot \\text{LDF}_{\\text{DY}+1} \\cdot \\text{LDF}_{\\text{DY}+2} \\cdot \\dots \\] Tip Instead of slowly multiplying each years LDF, we can instead combine them such into an Age to Ultimate LDF which brings a loss from a specified age to its ultimate value : \\[ \\text{LDF}_{\\text{Age to Ult}} = \\prod^{\\text{Ultimate DY}}_{\\text{Current DY}} \\text{LDF}_{\\text{DY}} \\] Thus, given the age to ult for two consecutive ages , we can find the LDF at the age: \\[ \\begin{aligned} \\frac{\\text{LDF}_{2, \\text{Ult}}}{\\text{LDF}_{3, \\text{Ult}}} &= \\frac{f_{2} \\cdot f_{3} \\cdot f_{4} \\cdot \\dots}{f_{3} \\cdot f_{4} \\cdot \\dots} \\\\ &= f_{2} \\end{aligned} \\] Although all the examples used Paid Losses, the CL method can also use Reported Losses . It will still result in the same ultimate loss . Bornhautter Ferguson Method \u00b6 The last method is known as the Bornhautter Ferguson (BF) method, which is a combination of the above two methods. Consider the loss reserve under the CL method: \\[ \\begin{aligned} \\text{Loss Reserve} &= \\text{Ultimate Loss} - \\text{Paid Loss} \\\\ &= \\text{Ultimate Loss} - \\frac{\\text{Ultimate Loss}}{\\text{LDF}_\\text{Ultimate}} \\\\ &= \\text{Ultimate Loss} \\cdot \\left(1 - \\frac{1}{\\text{LDF}_\\text{Ultimate}} \\right) \\end{aligned} \\] Note Instead of providing the LDFs, questions may sometimes use the following terminology: \\[ \\begin{aligned} \\text{Percentage of Paid Losses} &= \\frac{1}{\\text{LDF}_\\text{Ultimate}} \\\\ \\text{Percentage of Unpaid Losses} &= 1 - \\frac{1}{\\text{LDF}_\\text{Ultimate}} \\end{aligned} \\] By using the Ultimate Loss from the ELR method , the two methods are combined, resulting in the BF reserves : \\[ \\text{Loss Reserve}_{\\text{BF}} = \\text{Ultimate Loss}_{\\text{ELR}} \\cdot \\left(1 - \\frac{1}{\\text{LDF}_{\\text{CL, Ultimate}}} \\right) \\] Unlike the previous two methods, the BF method directly estimates the loss reserves , not the Ultimate Losses. Warning It is an extremely common mistake to think that the BF Ultimate Loss is the ELR Ultimate Loss, because it was what is used in the equation. \\[ \\text{Ultimate Loss}_{\\text{BF}} \\ne \\text{Ultimate Loss}_{\\text{ELR}} \\] Since the BF method directly estimates the loss reserves, the ultimate losses must be determined from first principles : \\[ \\begin{aligned} \\text{Ultimate Loss}_{\\text{BF}} = \\text{Loss Reserve}_{\\text{BF}} + \\text{Paid Loss} \\\\ \\end{aligned} \\] Weighted Average Method \u00b6 Alternatively, the BF method method can be calculated as the weighted average of the two methods : \\[ \\begin{aligned} \\text{Loss Reserve}_{BF} &= w \\cdot \\text{Loss Reserve}_{CL} + (1-w) \\cdot \\text{Loss Reserve}_{ELR} \\\\ w &= \\frac{1}{\\text{LDF}_{\\text{CL, Ultimate}}} \\end{aligned} \\] Warning The above applies DOES NOT apply to the IBNR reserve! The intuition is that as the losses mature, their LDF factors decrease. This cases the weight given to the CL method to increase . This is intuitive, as more data is available for those claims, and thus more weight should be given to the method that uses claims experience . One key result is that the BF reserve is always calculated between the other two reserves : \\[ \\begin{aligned} \\text{ELR Loss Reserve} \\lt \\text{BF Loss Reserve} \\lt \\text{CL Loss Reserve} \\\\ \\text{ELR Loss Reserve} \\gt \\text{BF Loss Reserve} \\gt \\text{CL Loss Reserve} \\end{aligned} \\] There is no strict relationship as to which is larger or smaller; just that the BF method is in between. Reserve Calculation \u00b6 Using the ultimate loss from the first two methods, the reserve for each year can be calculated. It can be used to calculate either the IBNR or Total Reserve , depending on what is subtracted from the ultimate: \\[ \\begin{aligned} \\text{Loss Reserve} &= \\text{Ultimate Loss} - \\text{Paid Loss} \\\\ \\text{IBNR Reserve} &= \\text{Ultimate Loss} - \\text{Reported Loss} \\end{aligned} \\] For all three methods, the total reserve held by the insurer is the sum of the reserves for each accident year : \\[ \\begin{aligned} \\text{Total Loss Reserve} &= \\sum \\text{Loss Reserve}_\\text{AY} \\\\ \\text{Total IBNR Reserve} &= \\sum \\text{IBNR Reserve}_\\text{AY} \\end{aligned} \\] Note that the phrasing is very explicit -- \"total\" reserve implies that there is a reserve for more than one year. Note Other than accident year, losses can also be aggregated or categorized based on other factors such as line of business . Regardless, the concept remains the same -- the total reserve held is the sum of reserves for all blocks of business. Given the ultimate loss in one year, the ultimate loss in another year can be \"roughly\" estimated by considering the growth of the business each year: \\[ \\text{Ultimate Loss}_{t} = \\text{Ultimate Loss} \\cdot (\\text{Inflation})^{t} \\cdot (\\text{Business Growth})^{t} \\] Naturally, this method should only be used when there are no other options available .","title":"Loss Reserving"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/6.%20Loss%20Reserving/#loss-reserving","text":"","title":"Loss Reserving"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/6.%20Loss%20Reserving/#overview","text":"General Insurance claims may be filed long after the loss event occurs, thus they must set up a Loss Reserve to ensure that they have enough capital to pay out these claims well into the future.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/6.%20Loss%20Reserving/#loss-terminology","text":"This subsection will properly introduce the proper terminology regarding losses There are three basic building blocks of a reported claim: Incurred Losses : Reported loss amount Paid Losses : Loss amounts that have been reported and paid out Case Reserve : Loss amounts that have been reported but not yet paid out \\[ \\text{Incurred Loss} = \\text{Paid Loss} + \\text{Case Reserve} \\] The problem is that there will always be losses stemming from accidents that occurred in the past that have yet to be reported to the insurer, appropriately named as Incurred But Not Reported (IBNR) losses. The main objective of loss reserving is to estimate this quantity by predicting how losses will grow over time thus ensuring that they have enough capital to cover it via an IBNR Reserve . This process of predicting how losses will grow is known as Loss Development . The final loss amount is known as the Ultimate Loss , from which no further development will occur. The difference between the final loss ( ultimate ) and what has been reported ( incurred ) is the IBNR reserve as of the current time: \\[ \\text{IBNR Reserve} = \\text{Ultimate Loss} - \\text{Incurred Loss} \\] Thus, the total loss reserve is thus the combination of the two reserves : \\[ \\begin{aligned} \\text{Loss Reserve} &= \\text{Case Reserve} + \\text{IBNR Reserve} \\\\ &= \\text{Ultimate Loss} - \\text{Incurred Loss} + \\text{Case Reserve} \\\\ &= \\text{Ultimate Loss} - \\text{Paid Loss} \\end{aligned} \\] Note Intuitively, the total reserve is simply the difference between the ultimate loss and what has been paid, as the total reserve is meant to cover all unpaid losses .","title":"Loss Terminology"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/6.%20Loss%20Reserving/#expected-loss-ratio-method","text":"The simplest method for estimating ultimate losses is known as the Expected Loss Ratio Method . Loss Ratios (LR) are the ratio of losses to premiums , which is a measure of the percentage of premiums that have been used to pay losses. \\[ \\text{LR} = \\frac{\\text{Loss}}{\\text{Premium}} \\] The Expected Loss Ratio (ELR) is the ultimate loss ratio ; the ratio of the ultimate loss to aggregate premiums collected over the life of the policy. Since premiums are known by the insurer, this can be used to obtain an estimate for the ultimate loss: \\[ \\begin{aligned} \\text{ELR} &= \\frac{\\text{Ultimate Loss}}{\\text{Total Premium}} \\\\ \\\\ \\therefore \\text{Ultimate Loss} &= \\text{Earned Premium} \\cdot \\text{ELR} \\end{aligned} \\]","title":"Expected Loss Ratio Method"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/6.%20Loss%20Reserving/#chain-ladder-method","text":"Another method of estimating ultimate losses is known as the Chain Ladder (CL) Method. This methods works by finding patterns in historical loss development and assumes that future losses will continue to develop in a similar manner . In order to efficiently find patterns in historical losses, they are recorded in a convenient format known as a Development Table : Each Cell represents the paid losses in that year Each Row shows losses that occurred in the same Accident Year (AY) Each Column shows losses that occurred in the same Development Year (DY) Each Diagonal shows losses that occurred in the same Calendar Year (CY) Note The AY that the diagonal originates from is the CY for that calendar year. This can be seen by the colouring of the cells in the table below. The definition for AY and CY can be found in the ratemaking section.s DYs are also known as the age of the claim as they indicate how long after the AY when the claims are paid. For instance, DY3 represents that the claim was paid 3 years after the AY. The next step is to find the Cumulative paid Losses per AY. This is done by changing the values of each cell to the sum of all previous cells in the same row : \\[ \\begin{aligned} \\text{Cumulative Loss} &= \\text{Sum of Losses in the same AY} \\\\ \\text{Cumulative Cell}_{\\text{AY, DY}} &= \\sum_{DY} \\text{Cell}_{\\text{AY, DY}} \\end{aligned} \\] Note Some questions may directly provide the cumulative losses in the table rather than incremental losses, saving you the time of having to sum everything up. The number of columns indicates how long the insurer expects the losses to develop for. If the last column is DY6, then the cumulative loss at DY6 is the ultimate loss . Warning Some questions will require us to construct our own table. Thus, it is important to fully understand the intuition behind it. The key idea under the CL method is that losses all take the same duration to develop to their ultimate values. This means that the losses occurring in different years will take the same duration to mature at different dates . It is an extremely common mistake to think that all the losses develop to ultimate on the same date. Thus, the DYs can be thought of as the age of the losses . Losses from later AYs are \"younger\" and hence need more time to mature . Tip Instead of Development Year, some questions may use the term Calendar Year (CY). As will be mentioned in the ratemaking chapter, CY and AY are assumed to be synonmous for the purposes of losses. The difference between the CY and the AY is the development year. For instance, if the cumulative losses at CY7 are given for AY5 losses , then the losses are at DY2 . The goal is now to find the historical patterns in the losses and then use those patterns to fill up the blank half of the development table. The historical patterns are known as Loss Development Factors (LDF), which is the ratio of the current DY's cumulative loss to the previous DY's cumulative loss : \\[ \\text{LDF}_{\\text{AY, DY}} = \\frac{\\text{Cumulative Cell}_{\\text{AY, DY}}}{\\text{Cumulative Cell}_{\\text{AY, DY}-1}} \\] The next step would be to develop each years using the development factors , until it reaches the ultimate loss: \\[ \\text{Cumulative Cells}_{\\text{AY, DY}+1} = \\text{Cumulative Cells}_{\\text{AY, DY}} \\cdot \\text{LDF}_{\\text{AY, DY}} \\] However, the problem is that for all years other than the final year, there are usually multiple LDFs for each DY. Questions will usually specify to either the arithmetic average, recent average of weighted average. For simplicity, we assume that the arithmetic average is used. We will arrive at the following LDFs and thus the corresponding final development table : \\[ \\text{Ultimate Loss} = \\text{Paid Loss} \\cdot \\text{LDF}_{\\text{DY}} \\cdot \\text{LDF}_{\\text{DY}+1} \\cdot \\text{LDF}_{\\text{DY}+2} \\cdot \\dots \\] Tip Instead of slowly multiplying each years LDF, we can instead combine them such into an Age to Ultimate LDF which brings a loss from a specified age to its ultimate value : \\[ \\text{LDF}_{\\text{Age to Ult}} = \\prod^{\\text{Ultimate DY}}_{\\text{Current DY}} \\text{LDF}_{\\text{DY}} \\] Thus, given the age to ult for two consecutive ages , we can find the LDF at the age: \\[ \\begin{aligned} \\frac{\\text{LDF}_{2, \\text{Ult}}}{\\text{LDF}_{3, \\text{Ult}}} &= \\frac{f_{2} \\cdot f_{3} \\cdot f_{4} \\cdot \\dots}{f_{3} \\cdot f_{4} \\cdot \\dots} \\\\ &= f_{2} \\end{aligned} \\] Although all the examples used Paid Losses, the CL method can also use Reported Losses . It will still result in the same ultimate loss .","title":"Chain Ladder Method"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/6.%20Loss%20Reserving/#bornhautter-ferguson-method","text":"The last method is known as the Bornhautter Ferguson (BF) method, which is a combination of the above two methods. Consider the loss reserve under the CL method: \\[ \\begin{aligned} \\text{Loss Reserve} &= \\text{Ultimate Loss} - \\text{Paid Loss} \\\\ &= \\text{Ultimate Loss} - \\frac{\\text{Ultimate Loss}}{\\text{LDF}_\\text{Ultimate}} \\\\ &= \\text{Ultimate Loss} \\cdot \\left(1 - \\frac{1}{\\text{LDF}_\\text{Ultimate}} \\right) \\end{aligned} \\] Note Instead of providing the LDFs, questions may sometimes use the following terminology: \\[ \\begin{aligned} \\text{Percentage of Paid Losses} &= \\frac{1}{\\text{LDF}_\\text{Ultimate}} \\\\ \\text{Percentage of Unpaid Losses} &= 1 - \\frac{1}{\\text{LDF}_\\text{Ultimate}} \\end{aligned} \\] By using the Ultimate Loss from the ELR method , the two methods are combined, resulting in the BF reserves : \\[ \\text{Loss Reserve}_{\\text{BF}} = \\text{Ultimate Loss}_{\\text{ELR}} \\cdot \\left(1 - \\frac{1}{\\text{LDF}_{\\text{CL, Ultimate}}} \\right) \\] Unlike the previous two methods, the BF method directly estimates the loss reserves , not the Ultimate Losses. Warning It is an extremely common mistake to think that the BF Ultimate Loss is the ELR Ultimate Loss, because it was what is used in the equation. \\[ \\text{Ultimate Loss}_{\\text{BF}} \\ne \\text{Ultimate Loss}_{\\text{ELR}} \\] Since the BF method directly estimates the loss reserves, the ultimate losses must be determined from first principles : \\[ \\begin{aligned} \\text{Ultimate Loss}_{\\text{BF}} = \\text{Loss Reserve}_{\\text{BF}} + \\text{Paid Loss} \\\\ \\end{aligned} \\]","title":"Bornhautter Ferguson Method"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/6.%20Loss%20Reserving/#weighted-average-method","text":"Alternatively, the BF method method can be calculated as the weighted average of the two methods : \\[ \\begin{aligned} \\text{Loss Reserve}_{BF} &= w \\cdot \\text{Loss Reserve}_{CL} + (1-w) \\cdot \\text{Loss Reserve}_{ELR} \\\\ w &= \\frac{1}{\\text{LDF}_{\\text{CL, Ultimate}}} \\end{aligned} \\] Warning The above applies DOES NOT apply to the IBNR reserve! The intuition is that as the losses mature, their LDF factors decrease. This cases the weight given to the CL method to increase . This is intuitive, as more data is available for those claims, and thus more weight should be given to the method that uses claims experience . One key result is that the BF reserve is always calculated between the other two reserves : \\[ \\begin{aligned} \\text{ELR Loss Reserve} \\lt \\text{BF Loss Reserve} \\lt \\text{CL Loss Reserve} \\\\ \\text{ELR Loss Reserve} \\gt \\text{BF Loss Reserve} \\gt \\text{CL Loss Reserve} \\end{aligned} \\] There is no strict relationship as to which is larger or smaller; just that the BF method is in between.","title":"Weighted Average Method"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/6.%20Loss%20Reserving/#reserve-calculation","text":"Using the ultimate loss from the first two methods, the reserve for each year can be calculated. It can be used to calculate either the IBNR or Total Reserve , depending on what is subtracted from the ultimate: \\[ \\begin{aligned} \\text{Loss Reserve} &= \\text{Ultimate Loss} - \\text{Paid Loss} \\\\ \\text{IBNR Reserve} &= \\text{Ultimate Loss} - \\text{Reported Loss} \\end{aligned} \\] For all three methods, the total reserve held by the insurer is the sum of the reserves for each accident year : \\[ \\begin{aligned} \\text{Total Loss Reserve} &= \\sum \\text{Loss Reserve}_\\text{AY} \\\\ \\text{Total IBNR Reserve} &= \\sum \\text{IBNR Reserve}_\\text{AY} \\end{aligned} \\] Note that the phrasing is very explicit -- \"total\" reserve implies that there is a reserve for more than one year. Note Other than accident year, losses can also be aggregated or categorized based on other factors such as line of business . Regardless, the concept remains the same -- the total reserve held is the sum of reserves for all blocks of business. Given the ultimate loss in one year, the ultimate loss in another year can be \"roughly\" estimated by considering the growth of the business each year: \\[ \\text{Ultimate Loss}_{t} = \\text{Ultimate Loss} \\cdot (\\text{Inflation})^{t} \\cdot (\\text{Business Growth})^{t} \\] Naturally, this method should only be used when there are no other options available .","title":"Reserve Calculation"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/","text":"Ratemaking \u00b6 Fundamental Equation \u00b6 Ratemaking is the process of determining how much premium to charge a policyholder. It is important to set rates such that there is enough premium collected to cover losses and expenses while still generating a profit for the insurer. This is illustrated through the Fundamental Insurance Equation : \\[ \\text{Premium} = \\text{Loss} + \\text{Expense} + \\text{Profit} \\] The remaining subsections will go in-depth into each component of this equation. Expenses \u00b6 There are two main types of expenses - Loss Adjustment Expenses (LAE) and Other Expenses . LAE are expenses that are incurred during the claim settlement process and are typically included as part of the incurred losses . They can be further split into two categories: Allocated LAE (ALAE): Expenses associated with a specific claim (EG. Lawyer's Fee) Unallocated LAE (ULAE): Expenses that are NOT associated with a specific claim (EG. Salaries) Warning It is a common mistake to include LAE as part of one of the expenses. As its name suggests, other expenses is a catch all for non-LAE expenses. They are generally split into two categories: Variable Expenses ( \\(\\text{VE}\\) ): Expenses that vary with premium (EG. Commissions) Fixed Expenses ( \\(\\text{FE}\\) ): Expenses that do NOT vary with premium (EG. Salaries) \\[ \\text{Expenses} = \\text{VE} \\cdot P + \\text{FE} \\] Profit \u00b6 Profits can be included in ratemaking either implicitly or explicitly. By using more conservative estimates than usual, actual experience should be better than expected, which implicitly results in profit for the insurer. Alternatively, they can explicitly include a margin for the target profit (T) in the ratemaking process, as seen in the fundamental insurance equation. \\[ \\pi = \\pi_{T} \\cdot P \\] Based on this, we can find the target loss ratio such that the premiums are sufficient to cover both variable expenses and meet the profitability target, known as the Permissible Loss Ratio (PLR): \\[ \\begin{aligned} P &= L + \\text{FE} + \\text{VE} \\cdot P + \\pi_{T} \\cdot P \\\\ P - \\text{VE} \\cdot P - \\pi_{T} \\cdot P &= L + \\text{FE} \\\\ P \\cdot (\\text{PLR}) &= L + \\text{FE} \\\\ \\text{PLR} &= \\frac{L + \\text{FE}}{P} \\\\ \\\\ \\therefore \\text{PLR} &= \\text{PLR} \\end{aligned} \\] Losses \u00b6 Losses are the main component of determining premiums. However, before historical losses can be used, they must be Aggregated , Developed & Trended . Loss Aggregation \u00b6 Losses can be aggregated according to the following categories: Calendar Year (CY): Losses that only occur from 1 Jan to 31 Dec of that year Accident Year (AY): ALL losses from an accident that occurred in a specified 12 month period Policy Year (PY): ALL losses from a policy that was issued in a specified 12 month period Unless otherwise stated, the specified 12 month period follows the calendar year . In otherwise, Policy Year 2015 refers to a policy that was issued from 1 Jan 2015 to 31 Dec 2015. Since AY and PY consider ALL losses, they usually have a cut off date known as the Valuation Date , such that they only consider the losses occuring at or before that date . The valuation date is usually referred to as the losses \"As at\" or \"As of\". They refer to the cumulative losses at that time. \\[ \\begin{aligned} \\text{CY Paid} &= \\text{Sum of all paid losses in CY} \\\\ \\text{CY Incurred} &= \\text{CY Paid} + \\text{Change in Case Reserves} \\\\ \\\\ \\text{AY/PY Paid} &= \\text{Sum of all paid losses at Valuation Date} \\\\ \\text{AY/PY Incurred} &= \\text{AY/PY Paid} + \\text{Case Reserves at Valuation Date} \\end{aligned} \\] Since the default is to assume that PY and AY follow CY, unless stated otherwise as well, the valuation date is the end of the year . Most questions already provide the aggregated losses . However, if given raw data , we are expected to know how to aggregate them ourselves using the above method. Loss Development \u00b6 Using the above aggregations, the losses can be developed to their ultimate values using the chain ladder method described in the previous reserving section. Loss Trending \u00b6 Lastly, losses must be trended to account for factors that drive losses such as inflation, increased medical costs or technological advances. Warning Loss Trending and Development are often confused with one another as both involve adjusting the loss amount. Some people also believe that only one of them should be applied to a loss because applying both would double count certain factors , known as the Overlap Fallacy . However, note that they are extremely different concepts: Loss Development is about bringing an immature loss to its ultimate amount , as losses take time to be discovered and reported to the insurer. Loss Trending is about bringing the ultimate loss today to what it would be at a future time , as losses are also subject to external factors such as inflation . Similar to development, trending involves using historical data to find trends and then project them into the future: \\[ \\text{Trended Ultimate Loss} = \\text{Ultimate Loss} \\cdot (1 + \\text{Trend Factor})^\\text{Trend Period} \\] However, finding trend factors is out of scope for this exam. Thus, the main focus will be on determining the trend period . Formally speaking, it is the difference between the midpoint of the Experience Period and the midpoint of the Forecast Period : Experience Period : Duration in which losses can occur for policies with old rates Forecast Period : Duration in which losses can occur for policies with new rates The midpoint is used as a general \"summary\" of each period \\[ \\text{Trend Period} = \\text{Forecast Period Midpoint} - \\text{Experience Period Midpoint} \\] Tip For all date related computations, assume that the base unit is 1 year . Most questions provides dates that are usually on the same day of the month (EG. 1 st ). The calculation mainly involves a difference in months : \\[ \\text{1 Jan} - \\text{1 Oct} = \\frac{3}{12} = \\frac{1}{4} \\] However, questions can also provide dates that are on different days of the month (EG. 15 th vs 1 st ). The difference in days must also be accounted for: \\[ \\text{1 Jan} - \\text{15 Oct} = \\frac{2.5}{12} = \\frac{5}{24} \\] The most common mistake when calculating dates is getting a quarter and third mixed up: 3 Months difference is a quarter \\(\\frac{1}{4}\\) 4 Months difference is a third \\(\\frac{1}{3}\\) The 3 and 4 get swapped in the denominator! The difficulty comes from determining the length and hence midpoint of each period. If the periods are determined via AY , then the length of the period is simply the calendar year . Thus, its midpoint is always the middle of the year . \\[ \\text{AY Period} = \\text{Middle of Calendar Year} \\] If the periods are determined via PY , then the length of the period is the CY PLUS the length of the policy . Assuming that policies are written uniformly throughout the year , it means that there will be policies written on the first and last day of the PY : Written on First Day : Expires on the last day of PY Written on Last Day : Expires on last day of PY plus policy term Both of the above policies are written using the same set of rates. Thus, the maximum possible timespan that policies written with the same set of rates can exists are: \\[ \\text{PY Period} = \\text{Rates Duration} + \\text{Policy Term} \\] Warning The above example assumed that rates are in effect for one year exactly at the start and end of the PY. In other words, it assumes that the new rates are put into place at the start of the next PY. In practice, rates can start and end at any time . Other Trending Methods \u00b6 Rather than using just one trending factor, insurers might sometimes use multiple trending factors to obtain a trended ultimate and then take a weighted average of all of them instead. Similarly, they could also use regression to project the trended ultimate. Based on the regression slope \\((m)\\) , the trended ultimate can be calculated: \\[ \\begin{aligned} L^T &= L^U + mt \\\\ \\\\ \\ln L^T &= \\ln L^U + mt \\\\ \\ln \\frac{L^T}{L^U} &= mt \\\\ \\frac{L^T}{L^U} &= e^{mt} \\\\ L^T &= L^U \\cdot e^{mt} \\end{aligned} \\] Premiums \u00b6 Similar to losses, historical premiums must be first aggregated and then brough to the current level via trending . Premium Aggregation \u00b6 Since premiums cannot be linked to specific accidents, they can only be aggregated by CY or PY. Premiums can also be categorized in three ways, based on their accounting definitions: Written Premium (WP): Total premium collected Earned Premium (EP): Portion of premium used to provide insurance Unearned Premium (UP): Portion of premium that has yet to be used to provide insurance \\[ \\text{WP} = \\text{EP} + \\text{UP} \\] The above are all calculated at a specific point in time known as the Valuation Date . For CY aggregation, the valuation date is always the end of the year while for PY aggregation it can be any date. \\[ \\text{EP} = \\text{WP} \\cdot \\frac{|\\text{Valuation Date} - \\text{Policy Start Date}|}{\\text{Policy Term}} \\] Tip For a few policies, this process can be easily repeated. However, when policies are written uniformly throughout the year , there will be way too many to be feasibly use this method. Since policies are written uniformly, each month writes an equal number of policies. We can decompose the problem into monthly computations . \\[ \\text{WP}_\\text{Per Month} = \\text{WP} \\cdot \\frac{1}{12} \\] Depending on the policy term, the premiums for some months may already be fully earned . For instance, if the valuation date is the end of the year and policies lasts 3 months, then all policies written in January to September would have been fully earned by the end of the year . \\[ \\text{EP}_{\\text{Fully Earned Month}} = \\text{WP}_\\text{Per Month} \\] For the remaining months, their premiums are only partially earned . Since policies are written evenly throughout the month, we can use the middle of the month as a reference point. The earned portion can then be calculated using the original method: \\[ \\text{EP}_\\text{Partially Earned Month} = \\text{WP}_\\text{Per Month} \\cdot \\frac{|\\text{Valuation Date} - \\text{Middle Month}|}{\\text{Policy Term}} \\] Thus, the total EP is the combination of ALL the fully and partically earned months: \\[ \\text{EP} = \\sum \\text{EP}_{\\text{Fully Earned Month}} + \\sum \\text{EP}_\\text{Partially Earned Month} \\] The above is used to calculate the earned premium over a particular year . The same logic can thus be extended to the earned premium over a particular month . As with loss aggregation, most questions already provide the aggregated premiums . However, if given raw data, we are expected to know how to aggregate them ourselves using the above method. Premium Trending \u00b6 Premiums then need to be brought to their current level, similar to how ultimate losses were trended. Extension of Exposures \u00b6 The first method is known as the Extension of Exposures method, which adjusts premiums for each individual policyholder by accounting for rate changes in every single exposure. For some background, premiums are calculated as the product of a rate and an some base: \\[ \\text{Premium} = \\text{Exposure Base} \\cdot \\text{Rate per Exposure} \\] However, exposures need not be continuous (EG. Gender - Male, Female). Possible exposures are known as Risk Classifications . For simplicity, this section will consider only discrete exposures . These premiums are then calculated relatively - in other words, it assumes some base risk classification will be charged a certain premium and then adjusts it accordingly if the risk classification is different . Note The concept is similar to dummy variable regression where \\(x=0\\) is assumed as the base case and then changes accordingly. Formally speaking, the base risk classification is known as the Base Cell , which is usually the most common risk class . The premium for this risk class is known as the Base Rate . In order to adjust the premiums, each risk class has its own rate , collectively known as the Rate Differentials for the rating variable. Naturally, the rates for the base cell are 1 to reflect that it is the base. \\[ \\text{Premium} = \\text{Base Rate} \\cdot \\text{Rate Relativities} \\] Thus, old premiums are brought to the current level by simply recalculating them at the current rates instead: \\[ \\begin{aligned} \\text{Old Premium} &= 300 \\cdot 1.2 \\cdot 0.8 = 288 \\\\ \\text{On Level Premium} &= 330 \\cdot 1.30 \\cdot 0.70 = 300.30 \\end{aligned} \\] Tip Rather than providing the exact rates, rates are often expressed in the form of rate changes: \\[ \\text{New Rates} = \\text{Old Rates} \\cdot \\text{Rate Change} \\] Thus, rather than needing to have the exact rates, we can simply multiply the old premiums by the rate changes to bring them to the current level. Parallelogram Method \u00b6 Although the above method is more \"correct\", the problem is that it requires highly granular data for every policyholder and the individual rates. However, insurers may only have access to aggregated data for both premiums and rates, in which case the Parallelogram Method must be used. A series of rate changes can be reorganized into a series of parallelograms : Each square represents a calendar year while diagonal lines represent policies . Each diagonal has a horizontal length equal to the policy term in years. Tip In practice, we only need 3 squares , with the middle square being the CY whose premium we would like to trend upwards. In particular, diagonals start from the first day of a new rate . Thus, the area between any two diagonals forms a Parellogram , which represent all policies written under a particular set of rates. Note Recall from Loss Trending that the timespan is from the first day of rates to the last day of rates plus the policy term; the parallelograms are a visualization of the timespan . Since the length of each square is exactly one calendar year, it is equal to 1. Thus, the area of the square is 1 . Due to all the diagonals, the square can be \"broken up\" into different shapes, whose area must all still sum to 1 : Each shape within the square corresponds to policies written under different set of rates . Assuming that premiums are aggregated by CY , the premiums collected in the year is the sum of the premiums from all these policies. Since the area of the square is 1, the area of these different shapes that represent different rates can be intepreted as weights . Thus, the aggregate premium collected for that year is based on the weighted average rate of all rates within the square: \\[ \\text{Average Rate} = \\sum \\text{Area} \\cdot \\text{Yearly Rate} \\] Note The rate for each year is the cumultive rate changes up till that time. Using the given rate changes in the table above, \\[ \\begin{aligned} \\text{CY 2014 Rates} &= P \\\\ \\text{CY 2015 Rates} &= P \\cdot (1.05) \\\\ \\text{CY 2016 Rates} &= P \\cdot (1.05)(1.1) \\\\ \\text{CY 2017 Rates} &= P \\cdot (1.05)(1.1)(0.98) \\\\ \\end{aligned} \\] \\(P\\) can also be substituted for \\(1\\) , as it is usually cancelled out when calculating the on-level factor. Recall from the extension method that the Historical and Current premiums are essentially the same exposure base multiplied by different rates. Using that logic, we can come up with an adjustment factor that transforms historical premiums to current premiums, known as the On Level Factor : \\[ \\begin{aligned} \\text{Historical Premium} &= \\text{Exposure Base} \\cdot \\text{Average Rate} \\\\ \\\\ \\text{Current Premium} &= \\text{Exposure Base} \\cdot \\text{Current Rate} \\\\ &= \\text{Exposure Base} \\cdot \\text{Current Rate} \\cdot \\frac{\\text{Average Rate}}{\\text{Average Rate}} \\\\ &= \\text{Exposure Base} \\cdot \\text{Average Rate} \\cdot \\frac{\\text{Current Rate}}{\\text{Average Rate}} \\\\ &= \\text{Historical Premium} \\cdot \\text{On Level Factor} \\\\ \\\\ \\therefore \\text{On Level Factor} &= \\frac{\\text{Current Rate}}{\\text{Average Rate}} \\end{aligned} \\] Warning It is a common misconception to think that the On Level Factor must be greater than 1. There is no hard rule that states that premiums must increase over time. Paralellogram Geometry \u00b6 The difficulty comes from being able to correctly calculate the lengths of various components , which requires us to understand the geometry behind a parallelogram. Every parallelogram has the following basic layout: Warning If the policy term is lesser than one year, then depending on the start date, the policy may not even cross even to the next year. For instance, a 6 month policy bought on 1 Jun will end on 1 Jan, not crossing the CY boundary. Thus, always perform a sense check when dealing with a policy of under a year. Given the start date of the rates, the time to the start of the next year \\((x)\\) can be calculated. Assuming a policy term of 1 year , the length of the small triangle is simply the complement \\((1-x)\\) . In order to calculate the height of the small triangle, geometric rules must be used. Since the two triangles share the same angle at the tip and a right angle, the two triangles thus have the same angle at every vertice and thus are considered to be similar triangles . For two similar triangles, the ratio of their lengths and heights must be equal: \\[ \\begin{aligned} \\frac{L_1}{H_1} &= \\frac{L_2}{H_2} \\\\ \\\\ \\frac{1}{1} &= \\frac{x}{H_2} \\\\ \\therefore H_2 &= x \\end{aligned} \\] Using the above method, the dimensions of any triangles can be found and hence used to calculate the area. Generally speaking, it is better to calculate the area of the two triangles and then take the area of the trapezium as their complement . Warning Although 1 year policy terms are the most common, do NOT mistakenly believe that the triangles are always isoceles. Repeating the same process for a policy term of 6 months will result in a non-isoceles triangle : Thus, it is best to apply first principles to determine the dimensions of the triangles. Indicative Premium \u00b6 The last part of the ratemaking process is to ensure that there the premiums are sufficient to cover future losses, expenses and target profits. The sufficient premium is known as the Indicative Premium \\((P^I)\\) , which can be solved for using the fundamental insurance equation: \\[ \\begin{aligned} P^I &= L + \\text{FE} + \\text{VE} \\cdot P^I + \\pi_{T} \\cdot P \\\\ P^I - \\text{VE} \\cdot P^I - \\pi_{T} \\cdot P^I &= L + \\text{FE} \\\\ P^I \\cdot (\\text{PLR}) &= L + \\text{FE} \\\\ P^I &= \\frac{L + \\text{FE}}{\\text{PLR}} \\\\ P^I &= \\frac{L + \\text{FE}}{\\text{PLR}} \\\\ \\end{aligned} \\] The difficulty comes from filling in the various values. Depending on the type of data provided, there are two methods to do so. Loss Ratio Method \u00b6 Instead of directly calculating the indicative premium, we can calculate a factor to adjust the current premium to the indicated level, known as a Rate Change . \\[ \\text{Indicative Premium} = \\text{Current Premium} \\cdot \\text{Rate Change} \\] This method is known as the Loss Ratio Method because it converts the loss on the RHS into a loss ratio by dividing the entire equation by the current premium: \\[ \\begin{aligned} P^I &= \\frac{L + \\text{FE}}{\\text{PLR}} \\\\ \\frac{P^I}{P^c} &= \\frac{\\frac{L}{P^c} + \\frac{\\text{FE}}{P^c}}{\\text{PLR}} \\\\ \\frac{P^I}{P^c} - 1 &= \\frac{LR + \\frac{\\text{FE}}{P^c}}{\\text{PLR}} - 1 \\\\ \\frac{P^I - P^c}{P^c} &= \\frac{LR + \\frac{\\text{FE}}{P^c}}{\\text{PLR}} - 1 \\\\ \\\\ \\therefore \\text{Rate Change} &= \\frac{LR + \\frac{\\text{FE}}{P^c}}{\\text{PLR}} - 1 \\end{aligned} \\] Pure Premium Method \u00b6 Alternatively, the indicative premium can be calculated directly using the Pure Premium Method . Similar to the loss ratio method, it involves dividing the equation by the Number of Exposures \\((n^e)\\) , which turns the loss on the RHS into a pure premium (net premium): \\[ \\begin{aligned} P^I &= \\frac{L + \\text{FE}}{\\text{PLR}1} \\\\ \\frac{P^I}{n^e} &= \\frac{\\frac{L}{n^e} + \\frac{\\text{FE}}{n^e}}{\\text{PLR}} \\\\ \\frac{P^I}{n^e} &= \\frac{\\bar{L} + \\bar{FE}}{\\text{PLR}} \\\\ \\\\ \\therefore \\text{Indicative Rate} &= \\frac{\\bar{L} + \\bar{FE}}{\\text{PLR}} \\end{aligned} \\] The bar notation is used in this context as they are technically the average losses and fixed expenses per exposure . Warning Although both methods are equivalent, they are NOT expected to produce the similar results. Do NOT use them to sense check one another. Practical Tips \u00b6 Remember that this is the final step for ratemaking. This means that all other steps must be completed before this. Thus, if the question provides non-trended or non-aggregated data, then it must be done before continuing. If multiple years of data are provided, then the indicative premium should be calculated using the weighted average of the Loss Ratios \\((\\text{LR})\\) or Pure Premiums \\((\\bar{L})\\) . The question will usually specify the weights to use. Warning It is a common mistake to confuse this step with the aggregation step. Aggregation adds up the raw losses and premiums, while thie step takes the weighted average of LRs and Pure Premiums.","title":"Ratemaking"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#ratemaking","text":"","title":"Ratemaking"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#fundamental-equation","text":"Ratemaking is the process of determining how much premium to charge a policyholder. It is important to set rates such that there is enough premium collected to cover losses and expenses while still generating a profit for the insurer. This is illustrated through the Fundamental Insurance Equation : \\[ \\text{Premium} = \\text{Loss} + \\text{Expense} + \\text{Profit} \\] The remaining subsections will go in-depth into each component of this equation.","title":"Fundamental Equation"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#expenses","text":"There are two main types of expenses - Loss Adjustment Expenses (LAE) and Other Expenses . LAE are expenses that are incurred during the claim settlement process and are typically included as part of the incurred losses . They can be further split into two categories: Allocated LAE (ALAE): Expenses associated with a specific claim (EG. Lawyer's Fee) Unallocated LAE (ULAE): Expenses that are NOT associated with a specific claim (EG. Salaries) Warning It is a common mistake to include LAE as part of one of the expenses. As its name suggests, other expenses is a catch all for non-LAE expenses. They are generally split into two categories: Variable Expenses ( \\(\\text{VE}\\) ): Expenses that vary with premium (EG. Commissions) Fixed Expenses ( \\(\\text{FE}\\) ): Expenses that do NOT vary with premium (EG. Salaries) \\[ \\text{Expenses} = \\text{VE} \\cdot P + \\text{FE} \\]","title":"Expenses"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#profit","text":"Profits can be included in ratemaking either implicitly or explicitly. By using more conservative estimates than usual, actual experience should be better than expected, which implicitly results in profit for the insurer. Alternatively, they can explicitly include a margin for the target profit (T) in the ratemaking process, as seen in the fundamental insurance equation. \\[ \\pi = \\pi_{T} \\cdot P \\] Based on this, we can find the target loss ratio such that the premiums are sufficient to cover both variable expenses and meet the profitability target, known as the Permissible Loss Ratio (PLR): \\[ \\begin{aligned} P &= L + \\text{FE} + \\text{VE} \\cdot P + \\pi_{T} \\cdot P \\\\ P - \\text{VE} \\cdot P - \\pi_{T} \\cdot P &= L + \\text{FE} \\\\ P \\cdot (\\text{PLR}) &= L + \\text{FE} \\\\ \\text{PLR} &= \\frac{L + \\text{FE}}{P} \\\\ \\\\ \\therefore \\text{PLR} &= \\text{PLR} \\end{aligned} \\]","title":"Profit"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#losses","text":"Losses are the main component of determining premiums. However, before historical losses can be used, they must be Aggregated , Developed & Trended .","title":"Losses"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#loss-aggregation","text":"Losses can be aggregated according to the following categories: Calendar Year (CY): Losses that only occur from 1 Jan to 31 Dec of that year Accident Year (AY): ALL losses from an accident that occurred in a specified 12 month period Policy Year (PY): ALL losses from a policy that was issued in a specified 12 month period Unless otherwise stated, the specified 12 month period follows the calendar year . In otherwise, Policy Year 2015 refers to a policy that was issued from 1 Jan 2015 to 31 Dec 2015. Since AY and PY consider ALL losses, they usually have a cut off date known as the Valuation Date , such that they only consider the losses occuring at or before that date . The valuation date is usually referred to as the losses \"As at\" or \"As of\". They refer to the cumulative losses at that time. \\[ \\begin{aligned} \\text{CY Paid} &= \\text{Sum of all paid losses in CY} \\\\ \\text{CY Incurred} &= \\text{CY Paid} + \\text{Change in Case Reserves} \\\\ \\\\ \\text{AY/PY Paid} &= \\text{Sum of all paid losses at Valuation Date} \\\\ \\text{AY/PY Incurred} &= \\text{AY/PY Paid} + \\text{Case Reserves at Valuation Date} \\end{aligned} \\] Since the default is to assume that PY and AY follow CY, unless stated otherwise as well, the valuation date is the end of the year . Most questions already provide the aggregated losses . However, if given raw data , we are expected to know how to aggregate them ourselves using the above method.","title":"Loss Aggregation"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#loss-development","text":"Using the above aggregations, the losses can be developed to their ultimate values using the chain ladder method described in the previous reserving section.","title":"Loss Development"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#loss-trending","text":"Lastly, losses must be trended to account for factors that drive losses such as inflation, increased medical costs or technological advances. Warning Loss Trending and Development are often confused with one another as both involve adjusting the loss amount. Some people also believe that only one of them should be applied to a loss because applying both would double count certain factors , known as the Overlap Fallacy . However, note that they are extremely different concepts: Loss Development is about bringing an immature loss to its ultimate amount , as losses take time to be discovered and reported to the insurer. Loss Trending is about bringing the ultimate loss today to what it would be at a future time , as losses are also subject to external factors such as inflation . Similar to development, trending involves using historical data to find trends and then project them into the future: \\[ \\text{Trended Ultimate Loss} = \\text{Ultimate Loss} \\cdot (1 + \\text{Trend Factor})^\\text{Trend Period} \\] However, finding trend factors is out of scope for this exam. Thus, the main focus will be on determining the trend period . Formally speaking, it is the difference between the midpoint of the Experience Period and the midpoint of the Forecast Period : Experience Period : Duration in which losses can occur for policies with old rates Forecast Period : Duration in which losses can occur for policies with new rates The midpoint is used as a general \"summary\" of each period \\[ \\text{Trend Period} = \\text{Forecast Period Midpoint} - \\text{Experience Period Midpoint} \\] Tip For all date related computations, assume that the base unit is 1 year . Most questions provides dates that are usually on the same day of the month (EG. 1 st ). The calculation mainly involves a difference in months : \\[ \\text{1 Jan} - \\text{1 Oct} = \\frac{3}{12} = \\frac{1}{4} \\] However, questions can also provide dates that are on different days of the month (EG. 15 th vs 1 st ). The difference in days must also be accounted for: \\[ \\text{1 Jan} - \\text{15 Oct} = \\frac{2.5}{12} = \\frac{5}{24} \\] The most common mistake when calculating dates is getting a quarter and third mixed up: 3 Months difference is a quarter \\(\\frac{1}{4}\\) 4 Months difference is a third \\(\\frac{1}{3}\\) The 3 and 4 get swapped in the denominator! The difficulty comes from determining the length and hence midpoint of each period. If the periods are determined via AY , then the length of the period is simply the calendar year . Thus, its midpoint is always the middle of the year . \\[ \\text{AY Period} = \\text{Middle of Calendar Year} \\] If the periods are determined via PY , then the length of the period is the CY PLUS the length of the policy . Assuming that policies are written uniformly throughout the year , it means that there will be policies written on the first and last day of the PY : Written on First Day : Expires on the last day of PY Written on Last Day : Expires on last day of PY plus policy term Both of the above policies are written using the same set of rates. Thus, the maximum possible timespan that policies written with the same set of rates can exists are: \\[ \\text{PY Period} = \\text{Rates Duration} + \\text{Policy Term} \\] Warning The above example assumed that rates are in effect for one year exactly at the start and end of the PY. In other words, it assumes that the new rates are put into place at the start of the next PY. In practice, rates can start and end at any time .","title":"Loss Trending"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#other-trending-methods","text":"Rather than using just one trending factor, insurers might sometimes use multiple trending factors to obtain a trended ultimate and then take a weighted average of all of them instead. Similarly, they could also use regression to project the trended ultimate. Based on the regression slope \\((m)\\) , the trended ultimate can be calculated: \\[ \\begin{aligned} L^T &= L^U + mt \\\\ \\\\ \\ln L^T &= \\ln L^U + mt \\\\ \\ln \\frac{L^T}{L^U} &= mt \\\\ \\frac{L^T}{L^U} &= e^{mt} \\\\ L^T &= L^U \\cdot e^{mt} \\end{aligned} \\]","title":"Other Trending Methods"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#premiums","text":"Similar to losses, historical premiums must be first aggregated and then brough to the current level via trending .","title":"Premiums"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#premium-aggregation","text":"Since premiums cannot be linked to specific accidents, they can only be aggregated by CY or PY. Premiums can also be categorized in three ways, based on their accounting definitions: Written Premium (WP): Total premium collected Earned Premium (EP): Portion of premium used to provide insurance Unearned Premium (UP): Portion of premium that has yet to be used to provide insurance \\[ \\text{WP} = \\text{EP} + \\text{UP} \\] The above are all calculated at a specific point in time known as the Valuation Date . For CY aggregation, the valuation date is always the end of the year while for PY aggregation it can be any date. \\[ \\text{EP} = \\text{WP} \\cdot \\frac{|\\text{Valuation Date} - \\text{Policy Start Date}|}{\\text{Policy Term}} \\] Tip For a few policies, this process can be easily repeated. However, when policies are written uniformly throughout the year , there will be way too many to be feasibly use this method. Since policies are written uniformly, each month writes an equal number of policies. We can decompose the problem into monthly computations . \\[ \\text{WP}_\\text{Per Month} = \\text{WP} \\cdot \\frac{1}{12} \\] Depending on the policy term, the premiums for some months may already be fully earned . For instance, if the valuation date is the end of the year and policies lasts 3 months, then all policies written in January to September would have been fully earned by the end of the year . \\[ \\text{EP}_{\\text{Fully Earned Month}} = \\text{WP}_\\text{Per Month} \\] For the remaining months, their premiums are only partially earned . Since policies are written evenly throughout the month, we can use the middle of the month as a reference point. The earned portion can then be calculated using the original method: \\[ \\text{EP}_\\text{Partially Earned Month} = \\text{WP}_\\text{Per Month} \\cdot \\frac{|\\text{Valuation Date} - \\text{Middle Month}|}{\\text{Policy Term}} \\] Thus, the total EP is the combination of ALL the fully and partically earned months: \\[ \\text{EP} = \\sum \\text{EP}_{\\text{Fully Earned Month}} + \\sum \\text{EP}_\\text{Partially Earned Month} \\] The above is used to calculate the earned premium over a particular year . The same logic can thus be extended to the earned premium over a particular month . As with loss aggregation, most questions already provide the aggregated premiums . However, if given raw data, we are expected to know how to aggregate them ourselves using the above method.","title":"Premium Aggregation"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#premium-trending","text":"Premiums then need to be brought to their current level, similar to how ultimate losses were trended.","title":"Premium Trending"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#extension-of-exposures","text":"The first method is known as the Extension of Exposures method, which adjusts premiums for each individual policyholder by accounting for rate changes in every single exposure. For some background, premiums are calculated as the product of a rate and an some base: \\[ \\text{Premium} = \\text{Exposure Base} \\cdot \\text{Rate per Exposure} \\] However, exposures need not be continuous (EG. Gender - Male, Female). Possible exposures are known as Risk Classifications . For simplicity, this section will consider only discrete exposures . These premiums are then calculated relatively - in other words, it assumes some base risk classification will be charged a certain premium and then adjusts it accordingly if the risk classification is different . Note The concept is similar to dummy variable regression where \\(x=0\\) is assumed as the base case and then changes accordingly. Formally speaking, the base risk classification is known as the Base Cell , which is usually the most common risk class . The premium for this risk class is known as the Base Rate . In order to adjust the premiums, each risk class has its own rate , collectively known as the Rate Differentials for the rating variable. Naturally, the rates for the base cell are 1 to reflect that it is the base. \\[ \\text{Premium} = \\text{Base Rate} \\cdot \\text{Rate Relativities} \\] Thus, old premiums are brought to the current level by simply recalculating them at the current rates instead: \\[ \\begin{aligned} \\text{Old Premium} &= 300 \\cdot 1.2 \\cdot 0.8 = 288 \\\\ \\text{On Level Premium} &= 330 \\cdot 1.30 \\cdot 0.70 = 300.30 \\end{aligned} \\] Tip Rather than providing the exact rates, rates are often expressed in the form of rate changes: \\[ \\text{New Rates} = \\text{Old Rates} \\cdot \\text{Rate Change} \\] Thus, rather than needing to have the exact rates, we can simply multiply the old premiums by the rate changes to bring them to the current level.","title":"Extension of Exposures"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#parallelogram-method","text":"Although the above method is more \"correct\", the problem is that it requires highly granular data for every policyholder and the individual rates. However, insurers may only have access to aggregated data for both premiums and rates, in which case the Parallelogram Method must be used. A series of rate changes can be reorganized into a series of parallelograms : Each square represents a calendar year while diagonal lines represent policies . Each diagonal has a horizontal length equal to the policy term in years. Tip In practice, we only need 3 squares , with the middle square being the CY whose premium we would like to trend upwards. In particular, diagonals start from the first day of a new rate . Thus, the area between any two diagonals forms a Parellogram , which represent all policies written under a particular set of rates. Note Recall from Loss Trending that the timespan is from the first day of rates to the last day of rates plus the policy term; the parallelograms are a visualization of the timespan . Since the length of each square is exactly one calendar year, it is equal to 1. Thus, the area of the square is 1 . Due to all the diagonals, the square can be \"broken up\" into different shapes, whose area must all still sum to 1 : Each shape within the square corresponds to policies written under different set of rates . Assuming that premiums are aggregated by CY , the premiums collected in the year is the sum of the premiums from all these policies. Since the area of the square is 1, the area of these different shapes that represent different rates can be intepreted as weights . Thus, the aggregate premium collected for that year is based on the weighted average rate of all rates within the square: \\[ \\text{Average Rate} = \\sum \\text{Area} \\cdot \\text{Yearly Rate} \\] Note The rate for each year is the cumultive rate changes up till that time. Using the given rate changes in the table above, \\[ \\begin{aligned} \\text{CY 2014 Rates} &= P \\\\ \\text{CY 2015 Rates} &= P \\cdot (1.05) \\\\ \\text{CY 2016 Rates} &= P \\cdot (1.05)(1.1) \\\\ \\text{CY 2017 Rates} &= P \\cdot (1.05)(1.1)(0.98) \\\\ \\end{aligned} \\] \\(P\\) can also be substituted for \\(1\\) , as it is usually cancelled out when calculating the on-level factor. Recall from the extension method that the Historical and Current premiums are essentially the same exposure base multiplied by different rates. Using that logic, we can come up with an adjustment factor that transforms historical premiums to current premiums, known as the On Level Factor : \\[ \\begin{aligned} \\text{Historical Premium} &= \\text{Exposure Base} \\cdot \\text{Average Rate} \\\\ \\\\ \\text{Current Premium} &= \\text{Exposure Base} \\cdot \\text{Current Rate} \\\\ &= \\text{Exposure Base} \\cdot \\text{Current Rate} \\cdot \\frac{\\text{Average Rate}}{\\text{Average Rate}} \\\\ &= \\text{Exposure Base} \\cdot \\text{Average Rate} \\cdot \\frac{\\text{Current Rate}}{\\text{Average Rate}} \\\\ &= \\text{Historical Premium} \\cdot \\text{On Level Factor} \\\\ \\\\ \\therefore \\text{On Level Factor} &= \\frac{\\text{Current Rate}}{\\text{Average Rate}} \\end{aligned} \\] Warning It is a common misconception to think that the On Level Factor must be greater than 1. There is no hard rule that states that premiums must increase over time.","title":"Parallelogram Method"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#paralellogram-geometry","text":"The difficulty comes from being able to correctly calculate the lengths of various components , which requires us to understand the geometry behind a parallelogram. Every parallelogram has the following basic layout: Warning If the policy term is lesser than one year, then depending on the start date, the policy may not even cross even to the next year. For instance, a 6 month policy bought on 1 Jun will end on 1 Jan, not crossing the CY boundary. Thus, always perform a sense check when dealing with a policy of under a year. Given the start date of the rates, the time to the start of the next year \\((x)\\) can be calculated. Assuming a policy term of 1 year , the length of the small triangle is simply the complement \\((1-x)\\) . In order to calculate the height of the small triangle, geometric rules must be used. Since the two triangles share the same angle at the tip and a right angle, the two triangles thus have the same angle at every vertice and thus are considered to be similar triangles . For two similar triangles, the ratio of their lengths and heights must be equal: \\[ \\begin{aligned} \\frac{L_1}{H_1} &= \\frac{L_2}{H_2} \\\\ \\\\ \\frac{1}{1} &= \\frac{x}{H_2} \\\\ \\therefore H_2 &= x \\end{aligned} \\] Using the above method, the dimensions of any triangles can be found and hence used to calculate the area. Generally speaking, it is better to calculate the area of the two triangles and then take the area of the trapezium as their complement . Warning Although 1 year policy terms are the most common, do NOT mistakenly believe that the triangles are always isoceles. Repeating the same process for a policy term of 6 months will result in a non-isoceles triangle : Thus, it is best to apply first principles to determine the dimensions of the triangles.","title":"Paralellogram Geometry"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#indicative-premium","text":"The last part of the ratemaking process is to ensure that there the premiums are sufficient to cover future losses, expenses and target profits. The sufficient premium is known as the Indicative Premium \\((P^I)\\) , which can be solved for using the fundamental insurance equation: \\[ \\begin{aligned} P^I &= L + \\text{FE} + \\text{VE} \\cdot P^I + \\pi_{T} \\cdot P \\\\ P^I - \\text{VE} \\cdot P^I - \\pi_{T} \\cdot P^I &= L + \\text{FE} \\\\ P^I \\cdot (\\text{PLR}) &= L + \\text{FE} \\\\ P^I &= \\frac{L + \\text{FE}}{\\text{PLR}} \\\\ P^I &= \\frac{L + \\text{FE}}{\\text{PLR}} \\\\ \\end{aligned} \\] The difficulty comes from filling in the various values. Depending on the type of data provided, there are two methods to do so.","title":"Indicative Premium"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#loss-ratio-method","text":"Instead of directly calculating the indicative premium, we can calculate a factor to adjust the current premium to the indicated level, known as a Rate Change . \\[ \\text{Indicative Premium} = \\text{Current Premium} \\cdot \\text{Rate Change} \\] This method is known as the Loss Ratio Method because it converts the loss on the RHS into a loss ratio by dividing the entire equation by the current premium: \\[ \\begin{aligned} P^I &= \\frac{L + \\text{FE}}{\\text{PLR}} \\\\ \\frac{P^I}{P^c} &= \\frac{\\frac{L}{P^c} + \\frac{\\text{FE}}{P^c}}{\\text{PLR}} \\\\ \\frac{P^I}{P^c} - 1 &= \\frac{LR + \\frac{\\text{FE}}{P^c}}{\\text{PLR}} - 1 \\\\ \\frac{P^I - P^c}{P^c} &= \\frac{LR + \\frac{\\text{FE}}{P^c}}{\\text{PLR}} - 1 \\\\ \\\\ \\therefore \\text{Rate Change} &= \\frac{LR + \\frac{\\text{FE}}{P^c}}{\\text{PLR}} - 1 \\end{aligned} \\]","title":"Loss Ratio Method"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#pure-premium-method","text":"Alternatively, the indicative premium can be calculated directly using the Pure Premium Method . Similar to the loss ratio method, it involves dividing the equation by the Number of Exposures \\((n^e)\\) , which turns the loss on the RHS into a pure premium (net premium): \\[ \\begin{aligned} P^I &= \\frac{L + \\text{FE}}{\\text{PLR}1} \\\\ \\frac{P^I}{n^e} &= \\frac{\\frac{L}{n^e} + \\frac{\\text{FE}}{n^e}}{\\text{PLR}} \\\\ \\frac{P^I}{n^e} &= \\frac{\\bar{L} + \\bar{FE}}{\\text{PLR}} \\\\ \\\\ \\therefore \\text{Indicative Rate} &= \\frac{\\bar{L} + \\bar{FE}}{\\text{PLR}} \\end{aligned} \\] The bar notation is used in this context as they are technically the average losses and fixed expenses per exposure . Warning Although both methods are equivalent, they are NOT expected to produce the similar results. Do NOT use them to sense check one another.","title":"Pure Premium Method"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/7.%20Ratemaking/#practical-tips","text":"Remember that this is the final step for ratemaking. This means that all other steps must be completed before this. Thus, if the question provides non-trended or non-aggregated data, then it must be done before continuing. If multiple years of data are provided, then the indicative premium should be calculated using the weighted average of the Loss Ratios \\((\\text{LR})\\) or Pure Premiums \\((\\bar{L})\\) . The question will usually specify the weights to use. Warning It is a common mistake to confuse this step with the aggregation step. Aggregation adds up the raw losses and premiums, while thie step takes the weighted average of LRs and Pure Premiums.","title":"Practical Tips"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/8.%20Model%20Estimation/","text":"Model Estimation \u00b6 This section assumes some basic knowledge on Maximum Likelihood Estimation (MLE), which can be found under another set of notes covering a Review of Statistical Theory . Motivation \u00b6 Rather than making an assumption about the distribution of frequency or losses or their parameters, we can study the raw data directly to gain a better understanding of it. However, there are several key data-related concepts that must be covered. Complete Data \u00b6 If given a full dataset where the value of each observation is known, then it is a complete dataset. Distribution parameters can be estimated normally through MLE. For complete data only, the MOM shortcut can be used for the following distributions: (a, b, 0) distributions Normal family distributions Exponetial family distributions Notice that all of the (a, b, 0) distributions are listed. Thus, their zero-modified counterparts can also be found using the shortcut: \\[ \\begin{aligned} E \\left(X^M \\right) &= \\frac{1 - p^{m}_{0}}{1 - p_{0}} \\cdot E(X) \\\\ &= \\frac{1 - p^{m}_{0}}{1 - p_{0}} \\cdot \\bar{x} \\end{aligned} \\] Incomplete Data \u00b6 In practice, insurers often work with incomplete data , where they do not have the exact values of observations or only have a partial dataset . Truncated \u00b6 The first type of incomplete data is known as Truncated Data . It means that the dataset does not include observations past a specified threshold. Left Truncation Right Truncation Truncated from below Truncated from above Lower Threshold Upper Threshold No values below \\(d\\) No values above \\(u\\) The most common case would be Left Truncated data, which occurs when the policy has a Deductible . This is because when the loss amount does not need meet the deductible, policyholders would not bother to report it. Thus, the insurer only receives loss amounts that exceed the deductible . Warning The notation for the threshold is \\(d\\) to denote deductible. HOWEVER, the notation \\(u\\) here DOES NOT represent policy limits! In terms of MLE, this means that the ENTIRE distribution is conditioned on having values above \\(d\\) or below \\(u\\) . Thus, the likelihood functions should be constructed using conditional PMFs/PDFs of EVERY observation : \\[ \\begin{aligned} P_{\\theta} (X = x \\mid X \\gt d) &= \\frac{P_{\\theta}(X = x)}{P(X \\gt d)} \\\\ \\\\ \\therefore L^{LT}(\\theta) &= \\prod P_{\\theta} (X = x \\mid X \\gt d) \\\\ &= \\prod \\frac{P_{\\theta}(X = x)}{P(X \\gt d)} \\\\ &= \\frac{1}{[P(X \\gt d)]^n} \\cdot \\prod P_{\\theta}(X = x) \\\\ \\end{aligned} \\] Losses VS Payments \u00b6 Recall that for a deductible, there is a clear distinction between Ground Up Losses and Claim Payments . \\[ \\text{Payment} = \\text{Ground Up Loss} - \\text{Deductible} \\] The data provided could be either payments or losses , and the distribution fitted could either be for the payments or losses. If the data and the distribution are different, then the above adjustment needs to be made for every observation. Warning The terminology used in these types of questions tend to be slightly ambiguous. If the question only uses \"claims\", then it should be intepreted as losses. In other words, unless \"payments\" are EXPLICITLY mentioned , it should be intepreted as losses. Memoryless Distribution \u00b6 If the distribution being fitted is memoryless, then the method of moments can be used even for truncated data. Loss data is truncated at the deductible. If converted to payment data using the above method, then the converted data is NOT truncated. Thus, the MOM can be used to easily fit a distribution to the payment data . The issue is that the distribution is fitted to the losses, not the payments . However, if the loss distribution is memoryless, then the payments will follow the same distribution and thus the MOM can be used. \\[ \\begin{aligned} X &\\sim \\text{Exponential}(\\theta) \\\\ X - d &\\sim \\text{Exponential}(\\theta) \\end{aligned} \\] Censored \u00b6 The second type of incomplete data is known as Censored Data . It means that exact values of certain observations are not known. Left Censored Right Censored Censored from below Censored from above Values at most \\(d\\) ; record as \\(d\\) Values at least \\(u\\) ; record as \\(u\\) The most common case would be Right Censored data, which occurs when the policy has a Policy Limit . This is because when the policyholder submits a claim, they will only submit up to the amount that the limit covers , even when the actual loss exceeds that amount. Thus, insurers will only receive loss amounts at the limit . Warning Similar to the warning from before, the notation for the threshold is \\(u\\) to denote policy limit. HOWEVER, the notation \\(d\\) here DOES NOT represent deductibles! \\(u\\) and \\(d\\) are still used in order to be consistent and ease understanding. In terms of MLE, this means that the likelihoods of censored observations are incorrect . Instead of the PMF/PDF, they should be represented by the probability that they are larger/smaller than the threshold . Thus, for censored observations at the threshold, the updated likelihoods must be used: \\[ \\begin{aligned} P(X = x_{\\text{Censored}}) &= P(X \\gt u) \\\\ \\\\ \\therefore L^{RC}(\\theta) &= \\prod P_{\\theta} (X = x_{\\text{Not Censored}}) \\cdot \\prod P(X = x_{\\text{Censored}}) \\end{aligned} \\] Warning Unlike truncation which affects every observation, censoring only affects certain observations . Similarly, each observation is affected differently , unlike truncation where they are affected the same way. Grouped \u00b6 The last type of incomplete data is known as Grouped Data . Similar to censoring, the exact values of ALL observations are not known. Instead, they are presented as the number of observations in a distinct interval . Tip Ranges are concisely expressed using the bracket notation - () denotes that the bound is exlcusive while [] denotes that it is inclusive. For instance, (15, 20] denoted that the amount is strictly larger than 15 but less than or equal to 20; \\(15 \\lt 5 \\le 20\\) . In terms of MLE, the likelihood of an observation within this range is simply the difference of the CDFs at the end and beginning of the interval: \\[ P_{\\theta}(a \\le X \\le b) = P_{\\theta}(X \\le b) - P_{\\theta}(X \\le a) \\] For a discrete distribution, this can be simplified to the sum of PMFs at each value: \\[ P_{\\theta}(a \\le X \\le b) = P_{\\theta}(X = a) + \\dots + P_{\\theta}(X = b) \\] Tip Some questions may provide a seemingly \"wrong\" range For instance, in the following SOA Sample Question, the group starts from 0, but the lower bound of the distribution is \\(\\theta\\) : The trick is to simply change the upper/lower bound of the group to match the distribution .","title":"Model Estimation"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/8.%20Model%20Estimation/#model-estimation","text":"This section assumes some basic knowledge on Maximum Likelihood Estimation (MLE), which can be found under another set of notes covering a Review of Statistical Theory .","title":"Model Estimation"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/8.%20Model%20Estimation/#motivation","text":"Rather than making an assumption about the distribution of frequency or losses or their parameters, we can study the raw data directly to gain a better understanding of it. However, there are several key data-related concepts that must be covered.","title":"Motivation"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/8.%20Model%20Estimation/#complete-data","text":"If given a full dataset where the value of each observation is known, then it is a complete dataset. Distribution parameters can be estimated normally through MLE. For complete data only, the MOM shortcut can be used for the following distributions: (a, b, 0) distributions Normal family distributions Exponetial family distributions Notice that all of the (a, b, 0) distributions are listed. Thus, their zero-modified counterparts can also be found using the shortcut: \\[ \\begin{aligned} E \\left(X^M \\right) &= \\frac{1 - p^{m}_{0}}{1 - p_{0}} \\cdot E(X) \\\\ &= \\frac{1 - p^{m}_{0}}{1 - p_{0}} \\cdot \\bar{x} \\end{aligned} \\]","title":"Complete Data"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/8.%20Model%20Estimation/#incomplete-data","text":"In practice, insurers often work with incomplete data , where they do not have the exact values of observations or only have a partial dataset .","title":"Incomplete Data"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/8.%20Model%20Estimation/#truncated","text":"The first type of incomplete data is known as Truncated Data . It means that the dataset does not include observations past a specified threshold. Left Truncation Right Truncation Truncated from below Truncated from above Lower Threshold Upper Threshold No values below \\(d\\) No values above \\(u\\) The most common case would be Left Truncated data, which occurs when the policy has a Deductible . This is because when the loss amount does not need meet the deductible, policyholders would not bother to report it. Thus, the insurer only receives loss amounts that exceed the deductible . Warning The notation for the threshold is \\(d\\) to denote deductible. HOWEVER, the notation \\(u\\) here DOES NOT represent policy limits! In terms of MLE, this means that the ENTIRE distribution is conditioned on having values above \\(d\\) or below \\(u\\) . Thus, the likelihood functions should be constructed using conditional PMFs/PDFs of EVERY observation : \\[ \\begin{aligned} P_{\\theta} (X = x \\mid X \\gt d) &= \\frac{P_{\\theta}(X = x)}{P(X \\gt d)} \\\\ \\\\ \\therefore L^{LT}(\\theta) &= \\prod P_{\\theta} (X = x \\mid X \\gt d) \\\\ &= \\prod \\frac{P_{\\theta}(X = x)}{P(X \\gt d)} \\\\ &= \\frac{1}{[P(X \\gt d)]^n} \\cdot \\prod P_{\\theta}(X = x) \\\\ \\end{aligned} \\]","title":"Truncated"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/8.%20Model%20Estimation/#losses-vs-payments","text":"Recall that for a deductible, there is a clear distinction between Ground Up Losses and Claim Payments . \\[ \\text{Payment} = \\text{Ground Up Loss} - \\text{Deductible} \\] The data provided could be either payments or losses , and the distribution fitted could either be for the payments or losses. If the data and the distribution are different, then the above adjustment needs to be made for every observation. Warning The terminology used in these types of questions tend to be slightly ambiguous. If the question only uses \"claims\", then it should be intepreted as losses. In other words, unless \"payments\" are EXPLICITLY mentioned , it should be intepreted as losses.","title":"Losses VS Payments"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/8.%20Model%20Estimation/#memoryless-distribution","text":"If the distribution being fitted is memoryless, then the method of moments can be used even for truncated data. Loss data is truncated at the deductible. If converted to payment data using the above method, then the converted data is NOT truncated. Thus, the MOM can be used to easily fit a distribution to the payment data . The issue is that the distribution is fitted to the losses, not the payments . However, if the loss distribution is memoryless, then the payments will follow the same distribution and thus the MOM can be used. \\[ \\begin{aligned} X &\\sim \\text{Exponential}(\\theta) \\\\ X - d &\\sim \\text{Exponential}(\\theta) \\end{aligned} \\]","title":"Memoryless Distribution"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/8.%20Model%20Estimation/#censored","text":"The second type of incomplete data is known as Censored Data . It means that exact values of certain observations are not known. Left Censored Right Censored Censored from below Censored from above Values at most \\(d\\) ; record as \\(d\\) Values at least \\(u\\) ; record as \\(u\\) The most common case would be Right Censored data, which occurs when the policy has a Policy Limit . This is because when the policyholder submits a claim, they will only submit up to the amount that the limit covers , even when the actual loss exceeds that amount. Thus, insurers will only receive loss amounts at the limit . Warning Similar to the warning from before, the notation for the threshold is \\(u\\) to denote policy limit. HOWEVER, the notation \\(d\\) here DOES NOT represent deductibles! \\(u\\) and \\(d\\) are still used in order to be consistent and ease understanding. In terms of MLE, this means that the likelihoods of censored observations are incorrect . Instead of the PMF/PDF, they should be represented by the probability that they are larger/smaller than the threshold . Thus, for censored observations at the threshold, the updated likelihoods must be used: \\[ \\begin{aligned} P(X = x_{\\text{Censored}}) &= P(X \\gt u) \\\\ \\\\ \\therefore L^{RC}(\\theta) &= \\prod P_{\\theta} (X = x_{\\text{Not Censored}}) \\cdot \\prod P(X = x_{\\text{Censored}}) \\end{aligned} \\] Warning Unlike truncation which affects every observation, censoring only affects certain observations . Similarly, each observation is affected differently , unlike truncation where they are affected the same way.","title":"Censored"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/8.%20Model%20Estimation/#grouped","text":"The last type of incomplete data is known as Grouped Data . Similar to censoring, the exact values of ALL observations are not known. Instead, they are presented as the number of observations in a distinct interval . Tip Ranges are concisely expressed using the bracket notation - () denotes that the bound is exlcusive while [] denotes that it is inclusive. For instance, (15, 20] denoted that the amount is strictly larger than 15 but less than or equal to 20; \\(15 \\lt 5 \\le 20\\) . In terms of MLE, the likelihood of an observation within this range is simply the difference of the CDFs at the end and beginning of the interval: \\[ P_{\\theta}(a \\le X \\le b) = P_{\\theta}(X \\le b) - P_{\\theta}(X \\le a) \\] For a discrete distribution, this can be simplified to the sum of PMFs at each value: \\[ P_{\\theta}(a \\le X \\le b) = P_{\\theta}(X = a) + \\dots + P_{\\theta}(X = b) \\] Tip Some questions may provide a seemingly \"wrong\" range For instance, in the following SOA Sample Question, the group starts from 0, but the lower bound of the distribution is \\(\\theta\\) : The trick is to simply change the upper/lower bound of the group to match the distribution .","title":"Grouped"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/9.%20Credibility%20Theory/","text":"Credibility Theory \u00b6 Overview \u00b6 Credibility Theory is a formal method of determining how much weight we should place on different sources of data when estimating future outcomes. Consider two sources of data: Internal Data : More specific but smaller sample size External Data : Too general but larger sample size Credibility \\((Z)\\) is the weight placed on our own internally sourced data . The overall estimate is the weighted average of the two sources: \\[ \\text{Overall Estimate} = Z \\cdot \\text{Internal Estimate} + (1-Z) \\cdot \\text{External Estimate} \\] There are two unique properties of the credibility factor: As the number of internal observations increases, the credibility factor increases As the number of internal observations increases, the rate of increase decreases \\[ \\begin{aligned} \\frac{dZ}{dn} \\gt 0 \\\\ \\frac{d^2Z}{dn^2} \\lt 0 \\end{aligned} \\] Full Credibility \u00b6 The internal observations have Full Credibility when \\(Z=1\\) . This occurs when there are a large number of observations. The goal is to quantify what constitutes a \"large\" sample size. Generally speaking, a source is credible if its sample mean is equal to its theoretical mean . The smallest number of observations where this occurs is what constitutes a sufficiently large sample size. In practice, it is unreasonable to expect that the sample mean would be exactly equal to the theoretical mean. Thus, we instead expect that they will be close to one another, within some small margin of error , equivalent to some percentage of the theoretical mean : \\[ |\\bar{x} - E(X)| \\le k \\cdot E(X) \\] Additionally, since \\(\\bar{x}\\) is a random variable, it is unreasonable to expect that it will always be within this margin of error. Thus, we instead expect that they will be close together most of the time, with some minimum probability : \\[ P(|\\bar{x} - E(X)| \\le k \\cdot E(X)) \\ge p \\] Thus, full credibility is dictated by two parameters: \\(k\\) : Permitted fluctuation; usually small & close to 0 \\(p\\) : Minimum probability; usually large & close to 1 Given that there are a large number of observations, the distribution of the sample mean is approximately normal via the Central Limit Theorem : Using the normal distribution, we can set up an equation to solve for the minimum number of observations \\(n\\) : \\[ \\begin{aligned} \\bar{x} \\sim N(\\mu_{\\bar{x}}, \\sigma^2_{\\bar{x}}) \\\\ \\\\ P(|\\bar{x} - E(X)| \\le kE(X)) &= p \\\\ P(-kE(X) \\le \\bar{x} - E(X) \\le kE(X)) &= p \\\\ P \\left(\\frac{-kE(X)}{\\sqrt {Var(\\bar{x})}} \\le \\frac{\\bar{x} - E(\\bar{x})}{Var (\\bar{x})} \\le \\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} \\right) &= p \\\\ P \\left(Z \\le \\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} \\right) &= 1 - \\frac{1-p}{2} \\\\ \\Phi \\left(\\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} \\right) &= \\frac{1+p}{2} \\end{aligned} \\] Note Since we are solving for the minimum number of observations, we consider the minimum case to have full credibility - where the probability is EXACTLY \\(p\\) . This means that each of the areas in the two tails are \\(\\frac{1-p}{2}\\) . We then solve the equation to obtain the following expression: \\[ \\begin{aligned} \\Phi \\left(\\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} \\right) &= \\frac{1+p}{2} \\\\ \\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} &= z_{(1+p)/2} \\\\ \\frac{kE(X)}{\\frac{\\sqrt {Var(X)}}{\\sqrt{n}}} &= z_{(1+p)/2} \\\\ \\sqrt{n} &= z_{(1+p)/2} \\cdot \\frac{1}{k} \\cdot \\frac{\\sqrt {Var(X)}}{E(X)} \\\\ \\sqrt{n} &= \\frac{z_{(1+p)/2}}{k} \\cdot CV_{X} \\\\ n &= \\left(\\frac{z_{(1+p)/2}}{k} \\cdot CV_{X} \\right)^2 \\end{aligned} \\] Tip Although it is not hard to derive this expression on the spot, it is a rather long and tedious proof. Thus, it is recommended to memorize this formula to save time. Number of Exposures \u00b6 In an actuarial context, credibility is often used when analyzing aggregate claims data ( \\(S\\) ). Each observation is called an Exposure , thus we are solving for the minimum Number of Exposures . Note Exposures are calculated based on a per unit, per year basis. For instance, two exposures could refer to either of the following scenarios: Two Insureds for one year One Insureds for two years If using the collective risk model, the mean and variance of the aggregate model can be decomposed into that of the underlying Frequency and Severity models : \\[ \\begin{aligned} n_{e} &= \\left(\\frac{z_{(1+p)/2}}{k} \\cdot \\frac{\\sqrt{Var(S)}}{E(S)} \\right)^2 \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\frac{Var (S)}{E(S)^2} \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\frac{Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var (N)}{E(N)^2 \\cdot E(X)^2} \\\\ \\end{aligned} \\] Number of Claims \u00b6 Alternatively, the number of exposures can also be expressed in terms of number of claims . If each exposure is a policyholder and the average number of claims per policyholder is given, then the number of claims is simply: \\[ \\begin{aligned} n_c &= n_e \\cdot E(N) \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\frac{Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var (N)}{E(N)^2 \\cdot E(X)^2} \\cdot E(N) \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\frac{Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var (N)}{E(N) \\cdot E(X)^2} \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left(\\frac{Var(X)}{E(X)^2} + \\frac{Var (N)}{E(N)} \\right) \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left( \\left(\\frac{SD(X)}{E(X)} \\right)^2 + \\frac{Var (N)}{E(N)} \\right) \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left( CV_{X}^2 + \\frac{Var (N)}{E(N)} \\right) \\end{aligned} \\] Notice that the number of claims nicely cleanly seperates out the Severity and Frequency components. Thus, this allows credibility to be determined based on ONLY frequency or severity data by setting the other component to 0 : \\[ \\begin{aligned} n^{\\text{Frequency}}_c &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left(0 + \\frac{Var (N)}{E(N)} \\right) \\\\ n^{\\text{Severity}}_c &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left(CV_{X}^2 + 0 \\right) \\end{aligned} \\] Warning The wording for this section can be extremely confusing. Most questions will use the following format: \"Calculate the number of claims/exposures needed for aggregate loss/frequency/severity to be within \\(k\\) % of the true value with \\(p\\) probability.\" If only frequency or severity are specified, then the above formulas are used. Note that Frequency and severity can be also be referred to as Number of Claims or Claims Size . However, if frequency is expressed as the number of claims, the following tricky question can be produced: \"Calculate the number of claims needed for the total number of claims to be within \\(k\\) % of the true value with \\(p\\) probability.\" We can then express them in terms of the number of exposures: \\[ \\begin{aligned} n_c &= n_e \\cdot E(N) \\\\ n_e &= \\frac{n_c}{E(N)} \\end{aligned} \\] The intuition is that each exposure is expected to make \\(E(N)\\) claims. Tip Given that the number of claims method can be used for Aggregate, Frequency & Severity data, it is much better to remember to always start with the number of claims and then convert to the number of exposures if needed. Shortcuts \u00b6 Note that if frequency follows a poisson distribution , a simplification can be made: \\[ \\begin{aligned} N &\\sim \\text{Poisson}(N) \\\\ \\\\ E(N) &= \\text{Var(N)} \\\\ \\frac{E(N)}{Var(N)} &= 1 \\\\ \\\\ \\therefore n_c &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left(CV_{X}^2 + 1 \\right) \\end{aligned} \\] Similarly, if severity follows an exponential distribution , a simplification can be made: \\[ \\begin{aligned} X &\\sim \\text{Exponential} \\\\ \\\\ E(X) &= \\theta \\\\ \\text{Var}(X) &= \\theta^2 \\\\ \\frac{\\text{Var}(X)}{E(X)^2} &= 1 \\\\ \\\\ \\therefore n_c &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left(1 + \\frac{Var (N)}{E(N)} \\right) \\end{aligned} \\] Partial Credibility \u00b6 If there are insufficient observations , then the data is said to have Partial Credibility \\((Z \\lt 1)\\) . The appropriate factor to use can be determined via the square root rule , where \\(n'\\) denotes the actual number of exposures or claims in the data: \\[ Z = \\sqrt{\\frac{n'_{e}}{n_e}} = \\sqrt{\\frac{n'_{c}}{n_c}} \\]","title":"Credibility Theory"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/9.%20Credibility%20Theory/#credibility-theory","text":"","title":"Credibility Theory"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/9.%20Credibility%20Theory/#overview","text":"Credibility Theory is a formal method of determining how much weight we should place on different sources of data when estimating future outcomes. Consider two sources of data: Internal Data : More specific but smaller sample size External Data : Too general but larger sample size Credibility \\((Z)\\) is the weight placed on our own internally sourced data . The overall estimate is the weighted average of the two sources: \\[ \\text{Overall Estimate} = Z \\cdot \\text{Internal Estimate} + (1-Z) \\cdot \\text{External Estimate} \\] There are two unique properties of the credibility factor: As the number of internal observations increases, the credibility factor increases As the number of internal observations increases, the rate of increase decreases \\[ \\begin{aligned} \\frac{dZ}{dn} \\gt 0 \\\\ \\frac{d^2Z}{dn^2} \\lt 0 \\end{aligned} \\]","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/9.%20Credibility%20Theory/#full-credibility","text":"The internal observations have Full Credibility when \\(Z=1\\) . This occurs when there are a large number of observations. The goal is to quantify what constitutes a \"large\" sample size. Generally speaking, a source is credible if its sample mean is equal to its theoretical mean . The smallest number of observations where this occurs is what constitutes a sufficiently large sample size. In practice, it is unreasonable to expect that the sample mean would be exactly equal to the theoretical mean. Thus, we instead expect that they will be close to one another, within some small margin of error , equivalent to some percentage of the theoretical mean : \\[ |\\bar{x} - E(X)| \\le k \\cdot E(X) \\] Additionally, since \\(\\bar{x}\\) is a random variable, it is unreasonable to expect that it will always be within this margin of error. Thus, we instead expect that they will be close together most of the time, with some minimum probability : \\[ P(|\\bar{x} - E(X)| \\le k \\cdot E(X)) \\ge p \\] Thus, full credibility is dictated by two parameters: \\(k\\) : Permitted fluctuation; usually small & close to 0 \\(p\\) : Minimum probability; usually large & close to 1 Given that there are a large number of observations, the distribution of the sample mean is approximately normal via the Central Limit Theorem : Using the normal distribution, we can set up an equation to solve for the minimum number of observations \\(n\\) : \\[ \\begin{aligned} \\bar{x} \\sim N(\\mu_{\\bar{x}}, \\sigma^2_{\\bar{x}}) \\\\ \\\\ P(|\\bar{x} - E(X)| \\le kE(X)) &= p \\\\ P(-kE(X) \\le \\bar{x} - E(X) \\le kE(X)) &= p \\\\ P \\left(\\frac{-kE(X)}{\\sqrt {Var(\\bar{x})}} \\le \\frac{\\bar{x} - E(\\bar{x})}{Var (\\bar{x})} \\le \\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} \\right) &= p \\\\ P \\left(Z \\le \\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} \\right) &= 1 - \\frac{1-p}{2} \\\\ \\Phi \\left(\\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} \\right) &= \\frac{1+p}{2} \\end{aligned} \\] Note Since we are solving for the minimum number of observations, we consider the minimum case to have full credibility - where the probability is EXACTLY \\(p\\) . This means that each of the areas in the two tails are \\(\\frac{1-p}{2}\\) . We then solve the equation to obtain the following expression: \\[ \\begin{aligned} \\Phi \\left(\\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} \\right) &= \\frac{1+p}{2} \\\\ \\frac{kE(X)}{\\sqrt {Var(\\bar{x})}} &= z_{(1+p)/2} \\\\ \\frac{kE(X)}{\\frac{\\sqrt {Var(X)}}{\\sqrt{n}}} &= z_{(1+p)/2} \\\\ \\sqrt{n} &= z_{(1+p)/2} \\cdot \\frac{1}{k} \\cdot \\frac{\\sqrt {Var(X)}}{E(X)} \\\\ \\sqrt{n} &= \\frac{z_{(1+p)/2}}{k} \\cdot CV_{X} \\\\ n &= \\left(\\frac{z_{(1+p)/2}}{k} \\cdot CV_{X} \\right)^2 \\end{aligned} \\] Tip Although it is not hard to derive this expression on the spot, it is a rather long and tedious proof. Thus, it is recommended to memorize this formula to save time.","title":"Full Credibility"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/9.%20Credibility%20Theory/#number-of-exposures","text":"In an actuarial context, credibility is often used when analyzing aggregate claims data ( \\(S\\) ). Each observation is called an Exposure , thus we are solving for the minimum Number of Exposures . Note Exposures are calculated based on a per unit, per year basis. For instance, two exposures could refer to either of the following scenarios: Two Insureds for one year One Insureds for two years If using the collective risk model, the mean and variance of the aggregate model can be decomposed into that of the underlying Frequency and Severity models : \\[ \\begin{aligned} n_{e} &= \\left(\\frac{z_{(1+p)/2}}{k} \\cdot \\frac{\\sqrt{Var(S)}}{E(S)} \\right)^2 \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\frac{Var (S)}{E(S)^2} \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\frac{Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var (N)}{E(N)^2 \\cdot E(X)^2} \\\\ \\end{aligned} \\]","title":"Number of Exposures"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/9.%20Credibility%20Theory/#number-of-claims","text":"Alternatively, the number of exposures can also be expressed in terms of number of claims . If each exposure is a policyholder and the average number of claims per policyholder is given, then the number of claims is simply: \\[ \\begin{aligned} n_c &= n_e \\cdot E(N) \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\frac{Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var (N)}{E(N)^2 \\cdot E(X)^2} \\cdot E(N) \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\frac{Var(X) \\cdot E(N) + [E(X)]^2 \\cdot Var (N)}{E(N) \\cdot E(X)^2} \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left(\\frac{Var(X)}{E(X)^2} + \\frac{Var (N)}{E(N)} \\right) \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left( \\left(\\frac{SD(X)}{E(X)} \\right)^2 + \\frac{Var (N)}{E(N)} \\right) \\\\ &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left( CV_{X}^2 + \\frac{Var (N)}{E(N)} \\right) \\end{aligned} \\] Notice that the number of claims nicely cleanly seperates out the Severity and Frequency components. Thus, this allows credibility to be determined based on ONLY frequency or severity data by setting the other component to 0 : \\[ \\begin{aligned} n^{\\text{Frequency}}_c &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left(0 + \\frac{Var (N)}{E(N)} \\right) \\\\ n^{\\text{Severity}}_c &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left(CV_{X}^2 + 0 \\right) \\end{aligned} \\] Warning The wording for this section can be extremely confusing. Most questions will use the following format: \"Calculate the number of claims/exposures needed for aggregate loss/frequency/severity to be within \\(k\\) % of the true value with \\(p\\) probability.\" If only frequency or severity are specified, then the above formulas are used. Note that Frequency and severity can be also be referred to as Number of Claims or Claims Size . However, if frequency is expressed as the number of claims, the following tricky question can be produced: \"Calculate the number of claims needed for the total number of claims to be within \\(k\\) % of the true value with \\(p\\) probability.\" We can then express them in terms of the number of exposures: \\[ \\begin{aligned} n_c &= n_e \\cdot E(N) \\\\ n_e &= \\frac{n_c}{E(N)} \\end{aligned} \\] The intuition is that each exposure is expected to make \\(E(N)\\) claims. Tip Given that the number of claims method can be used for Aggregate, Frequency & Severity data, it is much better to remember to always start with the number of claims and then convert to the number of exposures if needed.","title":"Number of Claims"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/9.%20Credibility%20Theory/#shortcuts","text":"Note that if frequency follows a poisson distribution , a simplification can be made: \\[ \\begin{aligned} N &\\sim \\text{Poisson}(N) \\\\ \\\\ E(N) &= \\text{Var(N)} \\\\ \\frac{E(N)}{Var(N)} &= 1 \\\\ \\\\ \\therefore n_c &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left(CV_{X}^2 + 1 \\right) \\end{aligned} \\] Similarly, if severity follows an exponential distribution , a simplification can be made: \\[ \\begin{aligned} X &\\sim \\text{Exponential} \\\\ \\\\ E(X) &= \\theta \\\\ \\text{Var}(X) &= \\theta^2 \\\\ \\frac{\\text{Var}(X)}{E(X)^2} &= 1 \\\\ \\\\ \\therefore n_c &= \\left(\\frac{z_{(1+p)/2}}{k} \\right)^2 \\cdot \\left(1 + \\frac{Var (N)}{E(N)} \\right) \\end{aligned} \\]","title":"Shortcuts"},{"location":"2.%20Actuarial%20Mathematics/1.%20ASA-FAMS/9.%20Credibility%20Theory/#partial-credibility","text":"If there are insufficient observations , then the data is said to have Partial Credibility \\((Z \\lt 1)\\) . The appropriate factor to use can be determined via the square root rule , where \\(n'\\) denotes the actual number of exposures or claims in the data: \\[ Z = \\sqrt{\\frac{n'_{e}}{n_e}} = \\sqrt{\\frac{n'_{c}}{n_c}} \\]","title":"Partial Credibility"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/","text":"Long Term Insurance \u00b6 Sourced directly from tb because SOA crafts question based on it History \u00b6 Life Insurance is a contract (Policy) in which the entity providing the insurance (Insurer) agrees to pay out a pre-determined lump sum (Sum Insured) to the contract owner (Policyholder) in the event that a nominated individual (Insured) dies while the contract is valid (In Force). Info In the UK, \"assurance\" is used to refer to contracts involving lives while \"insurance\" is used to refer to contract involving property. The earliest form of life insurance were annual policies that provided coverage one year at a time. The insurer charges premiums such that the premium inflow received each year should approximately match the expected benefit outgo each year. This process is known as Assessmentism and is essentially a \"pay as you go\" form of pricing. As the insured ages , the probability of death and hence the expected outgo each year increases, increasing the premiums . Info Assessmentism is still used today to price Group Life Insurance . It is a year to year life insurance policy that an employer purchases for their employees. Unlike individual lives whose probability of making a claim inevitably increases each year, the influx of new members into the group helps to offset and stabilize the cost of insurance, keeping it affordable. Level Premium \u00b6 One major problem with this is that the annual increases in premium discouraged policyholders from renewing their contracts. This is counter-intuitive, as they would be priced out of the insurance contract when it is most needed . This led to the development of Level Premium contracts, where premiums are fixed over the term of the contract , which could now be for multiple years at a time. This led to a win-win situation: People remain insured at older ages and insurers receive more premiums over a longer period. However, these contracts resulted in a higher risks for the insurer , resulting in the need to develop techniques to model mortality over multiple years and incorporate the time value of money into their calculations, which led to the traditional actuarial techniques we know today. Insurable Interest \u00b6 Another problem was that people were purchasing life insurance on people that they had no connection to (often public figures), as a form of gambling on when they would die. In some cases, this also incentivised people to hasten the death of the insured person, which would cause the mortality of the person to be higher than what the insurer expects. Thus, life insurance the concept of Insurable Interest to solve these problems. It states that policyholders should face a financial loss in the event of the death of the insured. In other words, life insurance should NOT leave the insured better off than if the insured had not died. Note This is similar to the concept of Indemnification of short term insurance. For the purposes of this exam, it is usually assumed that the policyholder and the insured are the same person , thus insurable interest is always fulfilled. Traditional Life Insurance \u00b6 There are three traditional forms of life insurance that has existed since its inception: Term, Whole Life & Endowment Insurance. Term Insurance \u00b6 Term Insurance covers the insured for a specified term , which usually ranges from 10 to 30 years. The policy will make a payout only if the insured dies within this period . Regular premium payments must be made throughout the policy term , otherwise the policy will cease (lapse). Premiums for term insurance are very small relative to the sum insured . This is because the insurer only expects to make payouts for a small proportion of the people who bought the policy; the remaining large proportion will expire without having a payout, thus their premiums can be used to subsidize the benefits for the small proportion , allowing premiums to remain low for all. Term Insurance is mainly used at an individual level for family protection . Less commonly, companies can also purchase term insurance for individual employees . However, they must be able to prove that the business has insurable interest in this employee; thus they are usually key persons that are vital to running the company. Note There is a special rider known as the Family Income Benefit (FIB) rider that is meant to further alleviate the financial strain of a family member dying. The rider pays a yearly benefit from the time the policyholder dies to the end of the original contract term . There are a few special kinds of term insurance: Decreasing Term Insurance : The death benefit decreases over the term of the policy, typically set to match how an outstanding loan would decrease over time. This ensures that the loan will be fully paid in the event of untimely death of the person paying the loan. Renewable Term Insurance : The policy can be renewed at the end of the initial term, regardless of the health of the insured . Convertible Term Insurance : The policy can be converted to a whole life policy after the initial term, regardless of the health of the insured . Both Renewable and Convertible typically have some age limit for when they can be exercised. Similarly, the new policy will always charge a higher premium due to the increased age & risk of the insured. Info There is no general use case for renewable and convertible term insurance; it very much depends on the specific needs of individuals. Whole Life Insurance \u00b6 As its name suggests, Whole Life covers the insured for their entire life. Since death is inevitable, the policy will make a payout whenever the insured dies . Regular premium payments must be made up till a certain age (typically 80 or 90), otherwise the policy will lapse. Note The point of whole life insurance is that it will cover the insured no matter what. There is a high chance that the poliycholder will not be able to make payment in their older ages due to the financial strain or inability to manage their own affairs at an older age. Thus, if premiums must be payable throughout the entire term of the policy, then most policyholders would be forced to lapse their policy in their later years, which defeats the purpose of the policy in the first place. Since the policy will inevitably make a payout, there is no subsidizing effect on the premiums, causing premiums to be much higher than term insurance . Instead, the insurer invests the premium such that the combined amount of the premiums and investment income is sufficient to cover the benefit of the policy. However, it is hard to predict investment returns over an extremely long period of time. Thus, most insurers use a conservative estimate (low) for interest, resulting in relatively higher premiums . Info The eventual difference between the actual interest rate and the assumed interest rate is known as the Interest Spread , which forms the profit and a buffer for adverse experience . Whole Life insurance is mainly used by people who want to leave a bequest to their children, especially so for wealthier households as life insurance is not subject to inheritence tax . Another fringe use case is for people who want to cover the expenses related to their deaths through a whole life policy with relatively low sum insured. Cash Values \u00b6 Most whole life policies also provides a payout when the policy is lapsed, known as its Cash Value . They typically start out small and grow to a substantial amount over time, but is still much less than the sum insured . This is intuitive, as if the cash values were larger than the death benefit, the policyholder would get more value from surrendering the policy and would thus do so. Note Some legislations require guaranteed cash values which are computed using specific assumptions and interest rates. For those that do not, the amount can be non-guaranteed , which is up to the insurer's discretion. In these cases, the policyholder can pay an extra premium to make the cash values guaranteed. If the policy does NOT offer cash values, then the premiums from policies that were lapsed can be used to subsidize the benefits of those still in force, resulting in relatively lower premiums . This is known as Lapse Supported Insurance . Stranger Owned Insurance \u00b6 Instead of lapsing the policy, policyholders can instead sell the policy to a third party , known as Stranger Owned Life Insurance . Most commonly, the third party is an investment firm who will purchase the policy for an amount greater than the cash value, but smaller than sum insured . The original policyholder remains the insured while the investment firm is now the policyholder. They are now responsible for making premium payments and will receive the death benefit when the insured dies. The hope is that the eventual death benefit will outweigh the cost of buying the policy and all future premiums , allowing them to make a profit. If the insured survives longer than expected, the additional premium payments needed decreases the chance of the company making money. Participating Insurance \u00b6 Insurers typically face the following dilemna: Assume Relatively high interest rate : Premiums are low but chance for profits are low (high investment risk) Assume Relatively low interest rate : Premiums are high but chance for profits are high (low investment risk) The solution that the industry came up was to charge the higher premiums, but share a portion of the profits with policyholders if the investments do well. However, the reverse is not true - losses are NOT shared with policyholders! This is known as a Participating Policy as the policyholder can participate in the profits of the insurer. The share of the profits are known as Dividends , which can be passed on to the policyholder in two main ways, based on their choice: Cash Benefits (It is sometimes automatically used to offset premiums, known as Premium Reductions ) Increased Death Benefits Info The share of the profits are known as Bonuses in the UK. Similarly, in the UK, the policyholder has no choice on how to receive their bonuses; it will always be in the form of increased death benefits . There are two kinds of Bonuses - Reversionary Bonuses (RB) and Terminal Bonuses (TB). RBs increase the death benefit by a specified percentage each year, which has three variations: Simple RB : Applied to the original sum insured only Compound RB : Applied to sum insured and accumulated previous RB Super Compound : One rate applied to the original sum and another rate applied to the accumulated previous RB RBs are typically credited conservatively; thus the remainder of the bonus is distributed when the policy is terminated in the form of TBs. There are several considerations when deciding between the two methods: Cash Benefits Increasing Death Benefit Easy for PH to understand Hard for PH to understand Flexible Inflexible Taxable Not taxable Need to liquidate assets; harms long term performance No need to liquidate; does not harm performance Expensive to liquidate and disburse Cheap to maintain Another consideration is fairness . If increasing death benefit was chosen, only a small portion of the increased death benefit will go to the surrender value. However, if cash was chosen, then the policyholder would have received the full dividend amount. Regardless of the method chosen, the dividend amount is non-guaranteed . When dividends inevitably fluctuate over the years, policyholders may get upset that the amount was lesser than what was projected . Thus, dividends are smoothed ; the excess from good performing years is saved to cover the shortfall in poor performing years, limiting the variation in dividend amounts. Participating Insurance is most often used as a passive long term investment vehicle with the added benefit of insurance cover. Endowment Insurance \u00b6 Endowment Insurance is a special type of policy that will make a payout either on the death of the policyholder or on survival till a specified term , known as the Maturity Date , whichever is first. They generally offer cash values and can be participating. Essentially, it is a hybrid of a term insurance and fixed term investment and is meant to be an investment product . However, it could not compete with the pure investment options that became popular in the recent decades due to its low returns and flexibility. As such, it has fallen out of favour and has evolved into the modern life insurance contracts that will be discussed later on. Note In many developing nations, Endowment Insurance is still popular as access to other financial products may be limited. It is particularly popular for microinsurance and microfinance. Variations \u00b6 All the points discussed above are for the barebones version of the contracts. There are several variations that can be made: Joint/Multiple life insurance insures multiple people, paying out on either the first death, last death or every death of insured members. It is most commonly used to insure parents or business partners. Policies that offer cash values can also offer Policy Loans which allow the policyholders to borrow money from the policy's cash value . If the insured dies, then the death benefit is reduced by the outstanding loan. Policy loans can be used to help the policyholder pay for premiums when they are unable to raise the funds during times of hardship. This increases the chance that the policyholder will continue with the policy, which generally speaking, creates more opportunities for the insurer to make a profit . GCV Accidental DB Modern Life Insurance \u00b6 The design of life insurance has developed in the past few decades resulting in what we know today as Modern Life Insurance products. There are several reasons for this: Increased Competition : Low rates from traditional products cannot keep up with pure investment options Increased Financial Literacy : People avoid products that are profitable for insurers but not very suitable for themselves Changing Demographic : Jobs have become increasingly short term; thus insurance products need to be more flexible Universal Life \u00b6 Universal Life (UL) contracts are flexible whole life contracts with transparent cash values . Premiums paid are deposited into a notional account and earn a credited interest rate based on investment performance. Note The funds are not actually seperated into different account per policyholder; they are still pooled together into the general account of the insurer. The insurer keeps track of how much was deposited and earned by each policyholder and discloses that amount to them. The credited rate cannot be 0 , thus the account will never lose value this way. Some products may also offer a guaranteed minimum rate , ensuring that the account will always at least grow by a specified amount. Each period, charges are posted into the account to cover the cost of insurance and policy expenses . The balance in the account is known as the Account Value , which is the cash value that the poliyholder would receive upon lapsing the policy. Note If the policyholder lapses their policy in the first few years of the policy, a surrender charge may be applied that reduces the amount received. This is to discourage poliycholders from lapsing their policy too early , ensuring that the insurer can recover the high expenses from setting up the policy. Another key feature is that it is extremely flexible : Death Benefit : Choose a base amount & whether fixed or increasing Premium : Free to pay any amount; shortfall is deducted from notional account The intention is to treat the UL policy as a sort of savings account with built in life insurance coverage. It is expected that they will eventually surrender the policy for its significant cash value, thus it can be viewed as an updated version of endowment insurance. Info One variation is known as a Unitized With Profits (UWP) policy, which is essentially a UL policy with the account value being expressed in terms of Units , which are shares in the notional fund, rather than the actual value. The investment performance is passed on via an increase the value of the units . These policies can also be participating, in which the dividends are passed on in the form of additional units . Equity Linked Insurance \u00b6 Equity Linked Insurance is the true evolution of endowment insurance as it has the same payment structure (earlier of a fixed term and death). It has similar features to a UL policy, with some key differences: Premiums are invested in an actual fund Account value can decrease if the fund does not perform well All benefits are tied to the account value Due to the more volatile nature of this product, the death and maturity benefit are typically floored at the value of the accumulated premiums at the time, ensuring that no loss was made from the policy. Note Some policies can also offer a Guaranteed Minimum Death Benefit (GMDB) or Guaranteed Minimum Maturity Benefit (GMMB) to cover situations where the investments have performed poorly. Policyholders are given a choice on these amounts, which allows them to choose if they would like a more investment or protection oriented policy . The main goal of equity linked insurance is to serve as an investment vehicle with built in life insurance coverage that can reasonably compete with other pure investment options. The main form of Equity Linked Insurance sold in the US is known as a Variable Annuity . Despite its name, it offers a lump sum benefit with the option to convert into an annuity at maturity . Outside of the US, equity linked insurance is often sold as Unit Linked Insurance , where the account value is expressed in terms of Units of an underlying fund. Warning Although both UWP and ULIP use units, the former is from a notional fund while the latter's units are from an actual fund . Long Term Health Insurance \u00b6 Disability Income Insurance \u00b6 Disability Income Insurance makes regular payments to individuals who can no longer work due to contracting sickness or disability. Premiums are also suspended during this period. The benefit payable will always be lesser than their original income (50 - 70%) in order to encourage the poliycholder to return to work as soon as possible . The amount depends on the extent of the disability: Total Disability : Unable to work at all; higher benefits Partial Disability : Able to perform some work; lower benefits Amount can also be reduced if receiving other forms of disability income Amount can also be increased to account for inflation Note The definition of disability is important - it could be the inability to peform any job or the policyholder's own job . If it is based on any job , then they must be unable to peform any job that that they are reasonably expected to be able to carry out given their qualifications. Since the bar is significantly higher for any job, it will make payouts less frequently and hence is cheaper. The benefit is usually payable for a fixed term (2 or 5 years) or up till retirement . Once the payment starts, it will continue till the earlier of the recovery of the policyholder or the end of the term . Regardless, payments only start after an initial Elimination Period (30 - 365 days) has finished since being afflicted with the disability. This is to allow the policyholder to first seek treatment. If the disability persists after that, then the policy will begin payments. Note For shorter terms, the policy can make another series of payouts if the policyholder becomes afflicted with another disability again later on. However, if the second disability occurs too soon to the first, then they will be considered as the same and will cumulatively only make payouts equal to the benefit term. The threshold for what consitutes as being \"too soon\" is known as the Off Period , which defines how long two disabilities must be to be considered seperate. Long Term Care Insurance \u00b6 Long Term Care Insurance makes regular payments to individuals who require long term care (LTC). Premiums are also suspended during this period. The definition of requiring LTC usually refers to the inability to perform two or more Activities of Daily Living (ADL): Bathing Dressing Eating (NOT COOKING!) Toileting Continence Transferring The inability to perform any of the above must be certified by a medical professional Similar to disability income, the payment term could be fixed or indefinite, subject to an initial elimintation period and off period for subsequent sickness . LTC insurance can also be combined with insurance to create interesting payout mechanisms: Return Of Premium : If the LTC benefits paid out are lesser than the total premiums, then the remainder is added to the death benefit of the policy Accelerated Benefit : The death benefit is accelerated and paid under the LTC instead OR the LTC benefit is accelerated and paid under the death benefit instead, whichever happens first Critical Illness Insurance \u00b6 Critical Illness Insurance pays a lump sum on diagnosis of a critical illness (CI). They can either be whole life or for a fixed term. Each insurer has a different definition for CI; but most include heart attack, stroke, major organ failure and most forms of cancer. Unlike the previous two insurance contracts, the policy expires once a payout is made ; a second critical illness will not be covered. CI insurance can also be combined with life insurance via a rider to offer an accelerated death benefit , which pays out all or a portion of the death benefit when diagnosed with a critical illness, on top of the CI claims. Intuitively, if all of the death benefit is accelerated, then the policy terminates. If only partially accelerated, then the policy persists and the remainder is paid out on the eventual death of the insured. Note When diagnosed with a critical illness, the policyholder may need extra funds to support their treatment. They may be incentivised to sell their policy to a third party to extract greater value from it. This sale is known as Viatical Settlement . Investment companies are willing to buy these policies as the probability of death is extremely high, ensuring that they earn a profit. This rider helps to alleviate that problem by paying out the full amount to the policyholder, reducing the need for these viatical settlement. Pensions \u00b6 Defined Benefit Pensions \u00b6 Defined Benefit (DB) Pensions work similar to annuities that pay a fixed amount each period based on a formula: \\[ \\text{Benefit} = \\text{Accrual Rate} \\cdot \\text{Salary} \\cdot \\text{Years of employment} \\] Employers and employees contribute into an internal fund which earns interest. The fund should, on average, be sufficient to cover the above defined benefits of their employees when they retire. Warning This does NOT mean that the benefit is dependent on the performance of the fund. If the fund performs well, then the amount that the employee and employers contribute is reduced, but the benefit remains the same . Some employers may allow employees who leave the company to withdraw their pension at a deferred amount to either use as a lump sum or invest in the pension plan of the new employer. Defined Contribution Pensions \u00b6 Defined Contribution (DC) Pensions work similar to a bank account where the employers and employees pay a pre-determined amount into an external fund which earns interest (EG. 401k). When the employee leaves the company or retires, they fund value is available to them in a lump sum. They have the flexibility to live directly off the fund amount or purchase an annuity. Special Arrangements \u00b6 Continuting Care Retirement Communities \u00b6 Continuing Care Retirement Communities (CCRC) are residential facilities that provide personal and medical support to the elderly (~80). Info There are four types of CCRCs: Independent Living Units : Minimal support Assisted Living Units : Non-medical support for ADLs Skilled Nursing Facility : Medical support Memory Care Units : For dementia or other cognitive impairments The cost of enrolling in a CCRC is a combination of an upfront entry fee and a monthly charge . In exchange, the CCRC provides all support without any future cost. This is essentially a form of insurance as the risk of healthcare cost is being transferred from the eldery to the CCRC. Full Life Care Modified Life Care Fee for Service High upfront fee Medium upfront fee Low upfront fee High monthly fee Medium monthly fee Low monthly fee No further charges Charges on select services Charges on all services Healthy elderly Less healthy elderly Sick elderly Structured Settlements \u00b6 Structured Settlements Other Insurance Concepts \u00b6 Types of Insurers \u00b6 There are two main types of insurance companies: Mutual Insurer Proprietary Insurer No shareholders Has shareholders Only with-profit policyholders Has with-profit policyholders Profits distributed entirely to PH Profits split in some pre-determined proportion Mutual insurers have the advantage in marketing as it is easy to gain new customers by advertising that they will gain 100% of the profits. However, propietary insurers have an easier access to capital and higher operating efficiency, due to their clearer corporate structures. As such, most mutual insurers have demutualized to become propietary ones by issuing shares or cash to the with profit policyholders. Distribution \u00b6 Intermediaries play an important role in the distribution Paid through commissions, percentage of the premium paid some are fee abased or salary based F2F sales focus on higher wealth individuals Trend towards direct marketing, online sales or TV less wealthy Focus is pre need insurance or straightforward insurance Underwriting \u00b6 The premium charged depends on the risk level of the insured, which is determined based on a few rating factors (EG. Age, Gender, Smoking, Occupation etc). The process of collecting and evaluating this information is known as Underwriting . Prospective policyholders are typically split into four categories based on the information given: Preferred Lives : Low Mortality risk; insurable at standard rates Normal Lives : Normal mortality risk; insurable at standard rates Rated Lives : Higher mortality risk; insurable at higher rates Uninsurable : Too high risk; uninsurable at any price Naturally, the higher the risk of the individual, the higher their expected loss and hence higher their premiums. Info Generally speaking, 95% of people are normal or preferred while the remaining 5% are split between rated & insurable (2-3% each). Underwriting is a necessary process to prevent Adverse Selection , which occurs when the policyholders use information about themselves (that the insurer does not know) to make decisions at the expense of the insurer. For instance, high risk individuals may attempt to purchase insurance at lower rates by witholding information. Underwriting allows the insurer to gain more information about the policyholder, preventing them from being taken advantage of. This is why the strictness of underwriting depends on the risk of adverse selection -- policies with higher benefits tend to attract higher risk individuals, which results in stricter underwriting. When all else fails, most insurance policies have a clause of Utmost Good Faith , where the insurer can withold payout if any of the information declared during the underwriting process is found to be false . Rigour of the process depends on the type of insurance being purchasedd Term more strictly udnerwritten because risk taken by insurer is greater Info There are many other factors such as type of product or distribution that can also affect the strictness. High risk individuals to apply for insurance with higher DB than lower risk > Typically apply for high risk DB ,thus it scales according distribution also affects > no medical UW needed, attracts ppl who are unhealthy, which assume heavier mortality","title":"Long Term Insurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#long-term-insurance","text":"Sourced directly from tb because SOA crafts question based on it","title":"Long Term Insurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#history","text":"Life Insurance is a contract (Policy) in which the entity providing the insurance (Insurer) agrees to pay out a pre-determined lump sum (Sum Insured) to the contract owner (Policyholder) in the event that a nominated individual (Insured) dies while the contract is valid (In Force). Info In the UK, \"assurance\" is used to refer to contracts involving lives while \"insurance\" is used to refer to contract involving property. The earliest form of life insurance were annual policies that provided coverage one year at a time. The insurer charges premiums such that the premium inflow received each year should approximately match the expected benefit outgo each year. This process is known as Assessmentism and is essentially a \"pay as you go\" form of pricing. As the insured ages , the probability of death and hence the expected outgo each year increases, increasing the premiums . Info Assessmentism is still used today to price Group Life Insurance . It is a year to year life insurance policy that an employer purchases for their employees. Unlike individual lives whose probability of making a claim inevitably increases each year, the influx of new members into the group helps to offset and stabilize the cost of insurance, keeping it affordable.","title":"History"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#level-premium","text":"One major problem with this is that the annual increases in premium discouraged policyholders from renewing their contracts. This is counter-intuitive, as they would be priced out of the insurance contract when it is most needed . This led to the development of Level Premium contracts, where premiums are fixed over the term of the contract , which could now be for multiple years at a time. This led to a win-win situation: People remain insured at older ages and insurers receive more premiums over a longer period. However, these contracts resulted in a higher risks for the insurer , resulting in the need to develop techniques to model mortality over multiple years and incorporate the time value of money into their calculations, which led to the traditional actuarial techniques we know today.","title":"Level Premium"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#insurable-interest","text":"Another problem was that people were purchasing life insurance on people that they had no connection to (often public figures), as a form of gambling on when they would die. In some cases, this also incentivised people to hasten the death of the insured person, which would cause the mortality of the person to be higher than what the insurer expects. Thus, life insurance the concept of Insurable Interest to solve these problems. It states that policyholders should face a financial loss in the event of the death of the insured. In other words, life insurance should NOT leave the insured better off than if the insured had not died. Note This is similar to the concept of Indemnification of short term insurance. For the purposes of this exam, it is usually assumed that the policyholder and the insured are the same person , thus insurable interest is always fulfilled.","title":"Insurable Interest"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#traditional-life-insurance","text":"There are three traditional forms of life insurance that has existed since its inception: Term, Whole Life & Endowment Insurance.","title":"Traditional Life Insurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#term-insurance","text":"Term Insurance covers the insured for a specified term , which usually ranges from 10 to 30 years. The policy will make a payout only if the insured dies within this period . Regular premium payments must be made throughout the policy term , otherwise the policy will cease (lapse). Premiums for term insurance are very small relative to the sum insured . This is because the insurer only expects to make payouts for a small proportion of the people who bought the policy; the remaining large proportion will expire without having a payout, thus their premiums can be used to subsidize the benefits for the small proportion , allowing premiums to remain low for all. Term Insurance is mainly used at an individual level for family protection . Less commonly, companies can also purchase term insurance for individual employees . However, they must be able to prove that the business has insurable interest in this employee; thus they are usually key persons that are vital to running the company. Note There is a special rider known as the Family Income Benefit (FIB) rider that is meant to further alleviate the financial strain of a family member dying. The rider pays a yearly benefit from the time the policyholder dies to the end of the original contract term . There are a few special kinds of term insurance: Decreasing Term Insurance : The death benefit decreases over the term of the policy, typically set to match how an outstanding loan would decrease over time. This ensures that the loan will be fully paid in the event of untimely death of the person paying the loan. Renewable Term Insurance : The policy can be renewed at the end of the initial term, regardless of the health of the insured . Convertible Term Insurance : The policy can be converted to a whole life policy after the initial term, regardless of the health of the insured . Both Renewable and Convertible typically have some age limit for when they can be exercised. Similarly, the new policy will always charge a higher premium due to the increased age & risk of the insured. Info There is no general use case for renewable and convertible term insurance; it very much depends on the specific needs of individuals.","title":"Term Insurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#whole-life-insurance","text":"As its name suggests, Whole Life covers the insured for their entire life. Since death is inevitable, the policy will make a payout whenever the insured dies . Regular premium payments must be made up till a certain age (typically 80 or 90), otherwise the policy will lapse. Note The point of whole life insurance is that it will cover the insured no matter what. There is a high chance that the poliycholder will not be able to make payment in their older ages due to the financial strain or inability to manage their own affairs at an older age. Thus, if premiums must be payable throughout the entire term of the policy, then most policyholders would be forced to lapse their policy in their later years, which defeats the purpose of the policy in the first place. Since the policy will inevitably make a payout, there is no subsidizing effect on the premiums, causing premiums to be much higher than term insurance . Instead, the insurer invests the premium such that the combined amount of the premiums and investment income is sufficient to cover the benefit of the policy. However, it is hard to predict investment returns over an extremely long period of time. Thus, most insurers use a conservative estimate (low) for interest, resulting in relatively higher premiums . Info The eventual difference between the actual interest rate and the assumed interest rate is known as the Interest Spread , which forms the profit and a buffer for adverse experience . Whole Life insurance is mainly used by people who want to leave a bequest to their children, especially so for wealthier households as life insurance is not subject to inheritence tax . Another fringe use case is for people who want to cover the expenses related to their deaths through a whole life policy with relatively low sum insured.","title":"Whole Life Insurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#cash-values","text":"Most whole life policies also provides a payout when the policy is lapsed, known as its Cash Value . They typically start out small and grow to a substantial amount over time, but is still much less than the sum insured . This is intuitive, as if the cash values were larger than the death benefit, the policyholder would get more value from surrendering the policy and would thus do so. Note Some legislations require guaranteed cash values which are computed using specific assumptions and interest rates. For those that do not, the amount can be non-guaranteed , which is up to the insurer's discretion. In these cases, the policyholder can pay an extra premium to make the cash values guaranteed. If the policy does NOT offer cash values, then the premiums from policies that were lapsed can be used to subsidize the benefits of those still in force, resulting in relatively lower premiums . This is known as Lapse Supported Insurance .","title":"Cash Values"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#stranger-owned-insurance","text":"Instead of lapsing the policy, policyholders can instead sell the policy to a third party , known as Stranger Owned Life Insurance . Most commonly, the third party is an investment firm who will purchase the policy for an amount greater than the cash value, but smaller than sum insured . The original policyholder remains the insured while the investment firm is now the policyholder. They are now responsible for making premium payments and will receive the death benefit when the insured dies. The hope is that the eventual death benefit will outweigh the cost of buying the policy and all future premiums , allowing them to make a profit. If the insured survives longer than expected, the additional premium payments needed decreases the chance of the company making money.","title":"Stranger Owned Insurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#participating-insurance","text":"Insurers typically face the following dilemna: Assume Relatively high interest rate : Premiums are low but chance for profits are low (high investment risk) Assume Relatively low interest rate : Premiums are high but chance for profits are high (low investment risk) The solution that the industry came up was to charge the higher premiums, but share a portion of the profits with policyholders if the investments do well. However, the reverse is not true - losses are NOT shared with policyholders! This is known as a Participating Policy as the policyholder can participate in the profits of the insurer. The share of the profits are known as Dividends , which can be passed on to the policyholder in two main ways, based on their choice: Cash Benefits (It is sometimes automatically used to offset premiums, known as Premium Reductions ) Increased Death Benefits Info The share of the profits are known as Bonuses in the UK. Similarly, in the UK, the policyholder has no choice on how to receive their bonuses; it will always be in the form of increased death benefits . There are two kinds of Bonuses - Reversionary Bonuses (RB) and Terminal Bonuses (TB). RBs increase the death benefit by a specified percentage each year, which has three variations: Simple RB : Applied to the original sum insured only Compound RB : Applied to sum insured and accumulated previous RB Super Compound : One rate applied to the original sum and another rate applied to the accumulated previous RB RBs are typically credited conservatively; thus the remainder of the bonus is distributed when the policy is terminated in the form of TBs. There are several considerations when deciding between the two methods: Cash Benefits Increasing Death Benefit Easy for PH to understand Hard for PH to understand Flexible Inflexible Taxable Not taxable Need to liquidate assets; harms long term performance No need to liquidate; does not harm performance Expensive to liquidate and disburse Cheap to maintain Another consideration is fairness . If increasing death benefit was chosen, only a small portion of the increased death benefit will go to the surrender value. However, if cash was chosen, then the policyholder would have received the full dividend amount. Regardless of the method chosen, the dividend amount is non-guaranteed . When dividends inevitably fluctuate over the years, policyholders may get upset that the amount was lesser than what was projected . Thus, dividends are smoothed ; the excess from good performing years is saved to cover the shortfall in poor performing years, limiting the variation in dividend amounts. Participating Insurance is most often used as a passive long term investment vehicle with the added benefit of insurance cover.","title":"Participating Insurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#endowment-insurance","text":"Endowment Insurance is a special type of policy that will make a payout either on the death of the policyholder or on survival till a specified term , known as the Maturity Date , whichever is first. They generally offer cash values and can be participating. Essentially, it is a hybrid of a term insurance and fixed term investment and is meant to be an investment product . However, it could not compete with the pure investment options that became popular in the recent decades due to its low returns and flexibility. As such, it has fallen out of favour and has evolved into the modern life insurance contracts that will be discussed later on. Note In many developing nations, Endowment Insurance is still popular as access to other financial products may be limited. It is particularly popular for microinsurance and microfinance.","title":"Endowment Insurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#variations","text":"All the points discussed above are for the barebones version of the contracts. There are several variations that can be made: Joint/Multiple life insurance insures multiple people, paying out on either the first death, last death or every death of insured members. It is most commonly used to insure parents or business partners. Policies that offer cash values can also offer Policy Loans which allow the policyholders to borrow money from the policy's cash value . If the insured dies, then the death benefit is reduced by the outstanding loan. Policy loans can be used to help the policyholder pay for premiums when they are unable to raise the funds during times of hardship. This increases the chance that the policyholder will continue with the policy, which generally speaking, creates more opportunities for the insurer to make a profit . GCV Accidental DB","title":"Variations"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#modern-life-insurance","text":"The design of life insurance has developed in the past few decades resulting in what we know today as Modern Life Insurance products. There are several reasons for this: Increased Competition : Low rates from traditional products cannot keep up with pure investment options Increased Financial Literacy : People avoid products that are profitable for insurers but not very suitable for themselves Changing Demographic : Jobs have become increasingly short term; thus insurance products need to be more flexible","title":"Modern Life Insurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#universal-life","text":"Universal Life (UL) contracts are flexible whole life contracts with transparent cash values . Premiums paid are deposited into a notional account and earn a credited interest rate based on investment performance. Note The funds are not actually seperated into different account per policyholder; they are still pooled together into the general account of the insurer. The insurer keeps track of how much was deposited and earned by each policyholder and discloses that amount to them. The credited rate cannot be 0 , thus the account will never lose value this way. Some products may also offer a guaranteed minimum rate , ensuring that the account will always at least grow by a specified amount. Each period, charges are posted into the account to cover the cost of insurance and policy expenses . The balance in the account is known as the Account Value , which is the cash value that the poliyholder would receive upon lapsing the policy. Note If the policyholder lapses their policy in the first few years of the policy, a surrender charge may be applied that reduces the amount received. This is to discourage poliycholders from lapsing their policy too early , ensuring that the insurer can recover the high expenses from setting up the policy. Another key feature is that it is extremely flexible : Death Benefit : Choose a base amount & whether fixed or increasing Premium : Free to pay any amount; shortfall is deducted from notional account The intention is to treat the UL policy as a sort of savings account with built in life insurance coverage. It is expected that they will eventually surrender the policy for its significant cash value, thus it can be viewed as an updated version of endowment insurance. Info One variation is known as a Unitized With Profits (UWP) policy, which is essentially a UL policy with the account value being expressed in terms of Units , which are shares in the notional fund, rather than the actual value. The investment performance is passed on via an increase the value of the units . These policies can also be participating, in which the dividends are passed on in the form of additional units .","title":"Universal Life"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#equity-linked-insurance","text":"Equity Linked Insurance is the true evolution of endowment insurance as it has the same payment structure (earlier of a fixed term and death). It has similar features to a UL policy, with some key differences: Premiums are invested in an actual fund Account value can decrease if the fund does not perform well All benefits are tied to the account value Due to the more volatile nature of this product, the death and maturity benefit are typically floored at the value of the accumulated premiums at the time, ensuring that no loss was made from the policy. Note Some policies can also offer a Guaranteed Minimum Death Benefit (GMDB) or Guaranteed Minimum Maturity Benefit (GMMB) to cover situations where the investments have performed poorly. Policyholders are given a choice on these amounts, which allows them to choose if they would like a more investment or protection oriented policy . The main goal of equity linked insurance is to serve as an investment vehicle with built in life insurance coverage that can reasonably compete with other pure investment options. The main form of Equity Linked Insurance sold in the US is known as a Variable Annuity . Despite its name, it offers a lump sum benefit with the option to convert into an annuity at maturity . Outside of the US, equity linked insurance is often sold as Unit Linked Insurance , where the account value is expressed in terms of Units of an underlying fund. Warning Although both UWP and ULIP use units, the former is from a notional fund while the latter's units are from an actual fund .","title":"Equity Linked Insurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#long-term-health-insurance","text":"","title":"Long Term Health Insurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#disability-income-insurance","text":"Disability Income Insurance makes regular payments to individuals who can no longer work due to contracting sickness or disability. Premiums are also suspended during this period. The benefit payable will always be lesser than their original income (50 - 70%) in order to encourage the poliycholder to return to work as soon as possible . The amount depends on the extent of the disability: Total Disability : Unable to work at all; higher benefits Partial Disability : Able to perform some work; lower benefits Amount can also be reduced if receiving other forms of disability income Amount can also be increased to account for inflation Note The definition of disability is important - it could be the inability to peform any job or the policyholder's own job . If it is based on any job , then they must be unable to peform any job that that they are reasonably expected to be able to carry out given their qualifications. Since the bar is significantly higher for any job, it will make payouts less frequently and hence is cheaper. The benefit is usually payable for a fixed term (2 or 5 years) or up till retirement . Once the payment starts, it will continue till the earlier of the recovery of the policyholder or the end of the term . Regardless, payments only start after an initial Elimination Period (30 - 365 days) has finished since being afflicted with the disability. This is to allow the policyholder to first seek treatment. If the disability persists after that, then the policy will begin payments. Note For shorter terms, the policy can make another series of payouts if the policyholder becomes afflicted with another disability again later on. However, if the second disability occurs too soon to the first, then they will be considered as the same and will cumulatively only make payouts equal to the benefit term. The threshold for what consitutes as being \"too soon\" is known as the Off Period , which defines how long two disabilities must be to be considered seperate.","title":"Disability Income Insurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#long-term-care-insurance","text":"Long Term Care Insurance makes regular payments to individuals who require long term care (LTC). Premiums are also suspended during this period. The definition of requiring LTC usually refers to the inability to perform two or more Activities of Daily Living (ADL): Bathing Dressing Eating (NOT COOKING!) Toileting Continence Transferring The inability to perform any of the above must be certified by a medical professional Similar to disability income, the payment term could be fixed or indefinite, subject to an initial elimintation period and off period for subsequent sickness . LTC insurance can also be combined with insurance to create interesting payout mechanisms: Return Of Premium : If the LTC benefits paid out are lesser than the total premiums, then the remainder is added to the death benefit of the policy Accelerated Benefit : The death benefit is accelerated and paid under the LTC instead OR the LTC benefit is accelerated and paid under the death benefit instead, whichever happens first","title":"Long Term Care Insurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#critical-illness-insurance","text":"Critical Illness Insurance pays a lump sum on diagnosis of a critical illness (CI). They can either be whole life or for a fixed term. Each insurer has a different definition for CI; but most include heart attack, stroke, major organ failure and most forms of cancer. Unlike the previous two insurance contracts, the policy expires once a payout is made ; a second critical illness will not be covered. CI insurance can also be combined with life insurance via a rider to offer an accelerated death benefit , which pays out all or a portion of the death benefit when diagnosed with a critical illness, on top of the CI claims. Intuitively, if all of the death benefit is accelerated, then the policy terminates. If only partially accelerated, then the policy persists and the remainder is paid out on the eventual death of the insured. Note When diagnosed with a critical illness, the policyholder may need extra funds to support their treatment. They may be incentivised to sell their policy to a third party to extract greater value from it. This sale is known as Viatical Settlement . Investment companies are willing to buy these policies as the probability of death is extremely high, ensuring that they earn a profit. This rider helps to alleviate that problem by paying out the full amount to the policyholder, reducing the need for these viatical settlement.","title":"Critical Illness Insurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#pensions","text":"","title":"Pensions"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#defined-benefit-pensions","text":"Defined Benefit (DB) Pensions work similar to annuities that pay a fixed amount each period based on a formula: \\[ \\text{Benefit} = \\text{Accrual Rate} \\cdot \\text{Salary} \\cdot \\text{Years of employment} \\] Employers and employees contribute into an internal fund which earns interest. The fund should, on average, be sufficient to cover the above defined benefits of their employees when they retire. Warning This does NOT mean that the benefit is dependent on the performance of the fund. If the fund performs well, then the amount that the employee and employers contribute is reduced, but the benefit remains the same . Some employers may allow employees who leave the company to withdraw their pension at a deferred amount to either use as a lump sum or invest in the pension plan of the new employer.","title":"Defined Benefit Pensions"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#defined-contribution-pensions","text":"Defined Contribution (DC) Pensions work similar to a bank account where the employers and employees pay a pre-determined amount into an external fund which earns interest (EG. 401k). When the employee leaves the company or retires, they fund value is available to them in a lump sum. They have the flexibility to live directly off the fund amount or purchase an annuity.","title":"Defined Contribution Pensions"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#special-arrangements","text":"","title":"Special Arrangements"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#continuting-care-retirement-communities","text":"Continuing Care Retirement Communities (CCRC) are residential facilities that provide personal and medical support to the elderly (~80). Info There are four types of CCRCs: Independent Living Units : Minimal support Assisted Living Units : Non-medical support for ADLs Skilled Nursing Facility : Medical support Memory Care Units : For dementia or other cognitive impairments The cost of enrolling in a CCRC is a combination of an upfront entry fee and a monthly charge . In exchange, the CCRC provides all support without any future cost. This is essentially a form of insurance as the risk of healthcare cost is being transferred from the eldery to the CCRC. Full Life Care Modified Life Care Fee for Service High upfront fee Medium upfront fee Low upfront fee High monthly fee Medium monthly fee Low monthly fee No further charges Charges on select services Charges on all services Healthy elderly Less healthy elderly Sick elderly","title":"Continuting Care Retirement Communities"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#structured-settlements","text":"Structured Settlements","title":"Structured Settlements"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#other-insurance-concepts","text":"","title":"Other Insurance Concepts"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#types-of-insurers","text":"There are two main types of insurance companies: Mutual Insurer Proprietary Insurer No shareholders Has shareholders Only with-profit policyholders Has with-profit policyholders Profits distributed entirely to PH Profits split in some pre-determined proportion Mutual insurers have the advantage in marketing as it is easy to gain new customers by advertising that they will gain 100% of the profits. However, propietary insurers have an easier access to capital and higher operating efficiency, due to their clearer corporate structures. As such, most mutual insurers have demutualized to become propietary ones by issuing shares or cash to the with profit policyholders.","title":"Types of Insurers"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#distribution","text":"Intermediaries play an important role in the distribution Paid through commissions, percentage of the premium paid some are fee abased or salary based F2F sales focus on higher wealth individuals Trend towards direct marketing, online sales or TV less wealthy Focus is pre need insurance or straightforward insurance","title":"Distribution"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/0.%20Long%20Term%20Insurance/#underwriting","text":"The premium charged depends on the risk level of the insured, which is determined based on a few rating factors (EG. Age, Gender, Smoking, Occupation etc). The process of collecting and evaluating this information is known as Underwriting . Prospective policyholders are typically split into four categories based on the information given: Preferred Lives : Low Mortality risk; insurable at standard rates Normal Lives : Normal mortality risk; insurable at standard rates Rated Lives : Higher mortality risk; insurable at higher rates Uninsurable : Too high risk; uninsurable at any price Naturally, the higher the risk of the individual, the higher their expected loss and hence higher their premiums. Info Generally speaking, 95% of people are normal or preferred while the remaining 5% are split between rated & insurable (2-3% each). Underwriting is a necessary process to prevent Adverse Selection , which occurs when the policyholders use information about themselves (that the insurer does not know) to make decisions at the expense of the insurer. For instance, high risk individuals may attempt to purchase insurance at lower rates by witholding information. Underwriting allows the insurer to gain more information about the policyholder, preventing them from being taken advantage of. This is why the strictness of underwriting depends on the risk of adverse selection -- policies with higher benefits tend to attract higher risk individuals, which results in stricter underwriting. When all else fails, most insurance policies have a clause of Utmost Good Faith , where the insurer can withold payout if any of the information declared during the underwriting process is found to be false . Rigour of the process depends on the type of insurance being purchasedd Term more strictly udnerwritten because risk taken by insurer is greater Info There are many other factors such as type of product or distribution that can also affect the strictness. High risk individuals to apply for insurance with higher DB than lower risk > Typically apply for high risk DB ,thus it scales according distribution also affects > no medical UW needed, attracts ppl who are unhealthy, which assume heavier mortality","title":"Underwriting"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/","text":"Survival Models \u00b6 This section assumes some basic knowledge on Probability Theory , which can be found under another set of notes covering a Review of Probability Theory . Overview \u00b6 Survival Models are probability distributions that measure the time to failure of an entity. In an actuarial context, it measures the time to death of a person. From another perspective, it also measures the future lifetime of that person. Note If someone is going to die in 5 years, then the person only has 5 years left to live (future lifetime). Intuitively, the distribution depends heavily on the age of the person, which is denoted using \\((x)\\) , which denotes a life aged \\(x\\) . Continuous Lifetime \u00b6 The distribution of the time to death ( in years ) is represented using the Continuous Random Variable \\(T_{x}\\) , where \\(x\\) represents the age of the person. Note Although we often use integer values for time, it can be expressed in continuous form as well: 1 Year = 12 Months 0.5 Years = 6 Months 0.541 Years = 6 Months & 15 Days Since \\(T_{x}\\) represents the time to death, a person aged \\(x\\) will live for another \\(T_{x}\\) years and then die before age \\(x + T_{x}\\) : The probability that the person dies DURING the next \\(t\\) years before age \\(x+t\\) is given by the CDF of the distribution: \\[ F_{x}(t) = P(T_{x} \\le t) \\] Note The implicit assumption is that the person aged \\(x\\) just turned age \\(x\\) (on their birthday). Thus, dying within the next \\(t\\) years means that they will not live to turn age \\(x+t\\) . The complement of the CDF is the probability that the person dies AFTER the next \\(t\\) years . From another perspective, this also means that the person survives the next \\(t\\) years to age \\(x+t\\) , which is why it is known as the Survival Function . \\[ \\begin{aligned} S_{x}(t) &= P(T_{x} \\gt t) \\\\ &= 1 - P(T_{x} \\lt t) \\\\ &= 1 - F_{x}(t) \\\\ \\end{aligned} \\] Tip The red squiggly lines in the above diagrams represent the range of times where the individual can die . For survival probabilities, it is the entire time AFTER the specified survival period. Since they are complements of one another, they share similar properties: CDF Survival Function Intuition \\(F_{x}(0) = 0\\) \\(S_{x}(0) = 1\\) Humans are currently alive \\(F_{x}(\\infty) = 1\\) \\(S_{x}(\\infty) = 0\\) Humans will inevitably die Non-decreasing function Non-increasing function Probability of death increases with age Warning The first property is hard to intepret. The probability that a life aged \\(x\\) dies within 0 years before age \\(x+0\\) must be 0, since the person is already aged \\(x\\) . The above two can also be combined into the probability of surviving \\(u\\) years till age \\(x+u\\) and then dying within the following \\(t\\) years before age \\(x+u+t\\) , known as the Probability of Deferred Death . \\[ \\begin{aligned} P(u \\lt T_x \\lt t+u) &= P(T_x \\lt t+u) - P(T_x \\lt u) \\\\ &= [1 - P(T_x \\gt t+u)] - [1 - P(T_x \\gt u)] \\\\ &= P(T_x \\gt u) - P(T_x \\gt t+u) \\\\ &= P(T_x \\gt u) - P(T_x \\gt u) \\cdot P(T_{x+u} \\gt t) \\\\ &= P(T_x \\gt u) [1 - P(T_{x+u} \\gt t)] \\\\ &= P(T_x \\gt u) \\cdot P(T_{x+u} \\lt t) \\\\ &= S_{x}(u) \\cdot F_{x+t}(t) \\end{aligned} \\] Warning Remember that the second term reflects the \"new\" age of the person after surviving \\(u\\) years till age \\(x+u\\) . Using Only Survival Functions , it is represented as the probability that the person survives \\(u\\) years but does not survive past \\(u+t\\) years : \\[ \\begin{aligned} P(u \\lt T_x \\lt t+u) &= P(T_x \\lt t+u) - P(T_x \\lt u) \\\\ &= [1 - P(T_x \\gt t+u)] - [1 - P(T_x \\gt u)] \\\\ &= P(T_x \\gt u) - P(T_x \\gt t+u) \\\\ &= S_{x}(u) - S_{x}(t+u) \\end{aligned} \\] Using Only Death Functions , it is represented as the probability of dying within \\(u+t\\) years but NOT \\(u\\) years: \\[ \\begin{aligned} P(u \\lt T_{x} \\lt u+t) &= P(T_x \\lt u+t) - P(T_x \\lt u) \\\\ &= F_{x} (u+t) - F_{x}(u) \\end{aligned} \\] Tip An easy way to remember the different formulations (especially the ones involving only death or survival) is to visualize them. The goal is to find the combination of probabilities that result in the individual dying in the specified time. Newborn Lifetime \u00b6 Note that one implicit assumption made so far is that the person has already survived till age \\(x\\) . In order to further analyze this, we must consider a special case of the distribution for a newborn aged 0 , represented by \\(T_{0}\\) . The future lifetime variable is essentially conditioned on the newborn having survived \\(x\\) years, then dying within the following \\(t\\) years before age \\(x+t\\) : \\[ \\begin{aligned} T_{x} &= T_{0} - x \\mid T_{0} \\gt x \\\\ \\\\ P(T_x \\le t) &= P(T_0 \\le x + t \\mid T_0 \\gt x) \\\\ &= \\frac{P(x \\lt T_0 \\lt x +t)}{P(T_0 \\gt x)} \\\\ &= \\frac{P(T_0 \\lt x +t)-P(T_0 \\lt x)}{P(T_0 \\gt x)} \\end{aligned} \\] When written in terms of the survival function, it can be simplified even further: \\[ \\begin{aligned} P(T_x \\ge t) &= P(T_0 \\ge x + t | T_0 \\gt x) \\\\ &= \\frac{P(T_0 \\gt x + t)}{P(T_0 \\gt x)} \\\\ &= \\frac{S_0(x+t)}{S_0(x)} \\end{aligned} \\] Note Since the surviving till age \\(x\\) is a subset of surviving till age \\(x+t\\) , the numerator can be simplified to just the probability of surviving till age \\(x+t\\) . This leads to the Multiplication Rule of survival probabilities: \\[ \\begin{aligned} S_x(t) &= \\frac{S_0(x+t)}{S_0(x)} \\\\ S_0(x+t) &= S_0(x)S_x(t) \\\\ \\\\ \\therefore S_x(t+u) &= S_x(u) S_{x+u}(t) \\end{aligned} \\] The survival probability can be decomposed into two components: The probability of a person surviving \\(u\\) years The conditional probability of person who has survived \\(u\\) years (new age) surviving another \\(t\\) years Warning This result ONLY applies to the survival probabilities ! Actuarial Notation \u00b6 Given how often the survival function is used, they are often abbreviated using the International Actuarial Notation : \\[ \\begin{aligned} P(T_{x} \\lt t) &= {}_{t}q_{x} \\\\ P(T_{x} \\gt u) &= {}_{t}p_{x} \\end{aligned} \\] Note If \\(t=1\\) , it is omitted - EG. \\({}_{1}q_{x} = q_{x}\\) . Deferred Deaths are expressed using the pipe symbol : \\[ \\begin{aligned} P(u \\lt T_{x} \\lt t+u) &= {}_{u \\mid t}q_{x} \\\\ &= {}_{u}p_{x} \\cdot {}_{t}q_{x+t} \\\\ &= {}_{u}p_{x} - {}_{u+t}p_{x} \\\\ &= {}_{t+u}q_{x} - {}_{u}q_{x} \\end{aligned} \\] Other key results: \\[ \\begin{aligned} {}_{t+u}p_{x} &= {}_{u}p_{x} \\cdot {}_{t}p_{x+u} \\\\ {}_{t}p_{x} &= \\sum^{t}_{u = t} {}_{u|}q_{x} \\end{aligned} \\] Percentiles \u00b6 Recall that the definition of a percentile is the value of the random variable at which \\(100p\\) of the values fall below. \\[ \\begin{aligned} P(T_{x} \\le t) &\\le p \\\\ P(T_{x} \\ge t) &\\le 1 - p \\\\ {}_{t}P_{x} &\\le 1 - p \\\\ \\frac{\\ell_{x+t}}{\\ell_{x}} &\\le 1 - p \\\\ \\ell_{x+t} &\\le \\ell_{x} \\cdot (1-p) \\\\ \\end{aligned} \\] By comparing values in the life table, we can find the smallest age that fulfils the above inequality, \\(y\\) . Since it is unlikely that \\(\\ell_{x} \\cdot (1-p)\\) exactly matches a value, the true percentile is at a fractional age . \\[ y - 1 \\lt x + t \\lt y \\] Thus, one of the typical assumptions must be used to determine the exact median of the distribution. \\[ \\text{Continuous Percentile} = y - 1 + t \\] However, if only interested in the curtate lifetime, then we can simply take the floor of \\(x+t\\) , which is always \\(y-1\\) . \\[ \\text{Curtate Percentile} = y - 1 \\] Note We traditionall learn percentiles using increasing random variables . We usually think of percentiles as values from the beginning to the percentile . However, future lifetime is a decreasing variable . This means that all values that are smaller than the percentile come AFTER it. This means that the values are from the percentile to its upper limit: Discrete Lifetime \u00b6 Similarly, the distribution of the time to death ( in years ) can also be represented using a Discrete Random Variable \\(K_{x}\\) , where \\(x\\) represents the age of the person. It is also known as the Curtate Future Lifetime as it only contains the integer components of the continuous future lifetime. Mathematically, it is represented as a continuous lifetime variable that has been floored . Thus, given the continuous distribution, the curtate one can be determined: \\[ K_x = \\lfloor T_x \\rfloor \\] Info The definition of Curtate is \"shortened\"; it represents how the continuous distribution has been reduced to just its integer components. Another way to remember is that \\(K\\) stands for Kurtate . The intepretation of \\(K_{x}\\) has changed slightly - A person aged \\(x\\) will live for another \\(K_x\\) FULL years and then die before age \\(x + K_{x} + 1\\) : Warning It is a very common mistake to misintepret \\(K_x\\) , thus consider the following examples: A person aged \\(x\\) dies within the same year . They have lived for 0 full years since turning \\(x\\) , thus \\(K_x = 0\\) . A person aged \\(x\\) lives till their \\(x+1\\) birthday but dies in that year. They have lived for 1 full year since turning \\(x\\) , thus \\(K_x = 1\\) . \\(K_x = k\\) can also be intepreted more intuitively as the deferred probability of living \\(k\\) years and then dying the following year : \\[ \\begin{aligned} P(K_x = k) &= P(k \\le T_x \\lt k+1) \\\\ &= P(T_x \\lt k+1) - P(T_x \\le k) \\\\ &= {}_{k+1}q_{x} - {}_{k}q_{x} \\\\ &= {}_{k}p_{x} - {}_{k+1}p_{x} \\\\ &= {}_{k}p_{x} - {}_{k}p_{x} \\cdot {}_{1}p_{x+k} \\\\ &= {}_{k}p_{x} \\cdot (1 - {}_{1}p_{x+k}) \\\\ &= {}_{k}p_{x} \\cdot {}_{1}q_{x+k} \\\\ &= {}_{k \\mid 1}q_{x} \\end{aligned} \\] Tip Recall that the probability of deferred death can be represented in three different ways, thus the PDF may take on different formulations: \\[ \\begin{aligned} {}_{u \\mid 1}q_{x} &= {}_{u}p_{x} \\cdot {}_{}q_{x+t} \\\\ &= {}_{u}p_{x} - {}_{u+1}p_{x} \\\\ &= {}_{u+1}q_{x} - {}_{u}q_{x} \\end{aligned} \\] Force of Mortality \u00b6 \\({}_{h}q_{x}\\) is the probability of dying within \\(h\\) years: \\[ \\begin{aligned} {}_{h}q_{x} &= 1 - {}_{h}q_{x} \\\\ &= 1 - \\frac{S_0(x+h)}{S_0(x)} \\\\ &= \\frac{S_0(x) - S_0(x+h)}{S_0(x)} \\end{aligned} \\] The Force of Mortality \\((\\mu_{x})\\) is the probability that (x) dies within an infinitely small \\(h\\) ; in other words, the probability of dying instantly . We consider the rate of death per unit time (this is NOT a probability!): \\[ \\frac{{}_{h}q_{x}}{h} = \\frac{S_0(x) - S_0(x+h)}{h \\cdot S_0(x)} \\] Note The proof uses the first principles of differentiation, which measures the rate of change of \\(y\\) over an infinitely small unit of \\(x\\) , which is why the rate of death had to be considered. This also implies that the following is true, ONLY for SMALL values of \\(h\\) : \\[ \\begin{aligned} \\mu_{x} &= \\frac{{}_{h}q_{x}}{h} \\\\ {}_{h}q_{x} &= h \\cdot \\mu_{x} \\end{aligned} \\] As the period of time becomes infinitely small, \\[ \\begin{aligned} \\mu_{x} &= \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{S_0(x)h} \\\\ &= \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{h} \\\\ &= - \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x+h) - S_0(x)}{h} \\\\ &= - \\frac{1}{S_0(x)} \\cdot \\left(\\frac{d}{dx} S_0(x) \\right) \\\\ &= - \\frac{S'_0(x)}{S_0(x)} \\\\ &= - \\frac{d}{dx} \\ln S_0(x) \\end{aligned} \\] Warning It is an extremely common mistake to forget the minus sign in front of the expression. Another common mistake is forgetting how to differentiate a fraction within a logarithm: \\[ \\begin{aligned} \\frac{d}{dx} \\ln \\frac{4}{4+x^2} &= \\frac{4+x^2}{4} \\cdot - 4(4+x)^{-2} \\cdot 2x \\\\ &= - \\frac{2x}{4+x^2} \\end{aligned} \\] Using the PDF of the newborn distribution, the force of mortality can be rewritten accordingly: \\[ \\begin{aligned} f_0(x) &= \\frac{d}{dx} F_0(x) \\\\ &= \\frac{d}{dx} (1 - S_0(x)) \\\\ &= -S'_0(x) \\\\ \\\\ \\therefore \\mu_{x} &= \\frac{f_0(x)}{S_0(x)} \\end{aligned} \\] Tip The force of mortality can also be expressed as the following: \\[ \\mu_{x}(t) = \\frac{\\frac{d}{dt} {}_{t}q_{x}}{{}_{t}p_{x}} \\] We can think of this as a conditional probability : \\[ P(\\text{A} \\mid \\text{B}) = \\frac{P(\\text{A} \\cap \\text{B})}{P(\\text{B})} \\] Thus, the force of mortality can be understood as the \"probability\" that \\(x\\) dies instantly after \\(t\\) years, given that they survive \\(t\\) years . Generalization \u00b6 The force of mortality is currently defined using the newborn distribution: \\[ \\mu_{y} = - \\frac{\\frac{d}{dy} S_0(y)}{S_0(y)} \\] For (x), \\(y = x + t\\) , where t is a random: \\[ \\begin{aligned} dy &= dx + dt \\\\ &= 0 + dt \\\\ &= dt \\\\ \\\\ \\therefore \\mu_{x+t} &= - \\frac{\\frac{d}{dt} S_0(x+t)}{S_0(x+t)} \\\\ &= - \\frac{d}{dt} \\ln S_0(x+t) \\end{aligned} \\] Since integration is the reverse of differentiation, \\[ \\begin{aligned} \\int^{n}_{0} \\frac{d}{dt} \\ln S_0(x+t) &= - \\int^{n}_{0} \\mu_{x+t} \\\\ [\\ln S_0(x+t)]^{n}_{0} &= - \\int^{n}_{0} \\mu_{x+t} \\\\ \\ln S_0(x+n) - \\ln S_0(x) &= - \\int^{n}_{0} \\mu_{x+t} \\\\ \\ln \\frac{S_0(x+n)}{\\ln S_0(x)} &= - \\int^{n}_{0} \\mu_{x+t} \\\\ \\ln S_{x}(n) &= - \\int^{n}_{0} \\mu_{x+t} \\\\ S_{x}(n) &= e^{- \\int^{n}_{0} \\mu_{x+t}} \\\\ {}_{n}p_{x} &= e^{- \\int^{n}_{0} \\mu_{x+t}} \\end{aligned} \\] Note The exponent is also known as the Cumulative Force of Mortality . Outside of human survival, the force of mortality is more generally known as the Hazard Function , denoted by \\(h(x)\\) . Its integral is known as the Cumulative Hazard Function , denoted by \\(H(x)\\) . \\[ H(x) = \\int^{n}_{0} h(x) \\] Although rare, a question could directly provide the cumulative hazard function and ask to compute the survival probability. In such cases, DO NOT integrate it once more! Info This expression can be intuitively understood as the following: Consider the forces trying to kill the individual. Negate these forces; only consider the scenario that the person lives. Take the exponent to compute the probability. Following a similar generalization (proof not shown), the PDF can also be expressed using the force of mortality: \\[ \\begin{aligned} \\mu_{x} &= \\frac{f_0(x)}{S_0(x)} \\\\ f_0(x) &= S_0(x) \\cdot \\mu_{x} \\\\ f_x(t) &= S_x(t) \\cdot \\mu_{x+t} \\\\ f_x(t) &= {}_{t}p_{x} \\cdot \\mu_{x+t} \\end{aligned} \\] This can be intepreted as the probability of dying instantly at age \\(x+t\\) . Risk Adjustments \u00b6 There are people who have a higher risk of dying than the average person - EG. Smokers. The first method to account for their increased risk is to treat them as an older person (higher risk), known as an Age Rating . An \\(n\\) year age rating means that a person aged \\(x\\) should be treated as if they had the mortality of a person aged \\(x+n\\) . Another method would be to simply scale the mortality rates by a factor to increase it: \\[ {}_{}q'_{x} = c \\cdot {}_{}q'_{x} \\] The last method would be to adjust the force of mortality instead, by either adding or multiplying a constant to it: \\[ \\begin{aligned} {}_{}p'_{x} &= e^{- (\\int^{n}_{0} \\mu_{x+t} + k)} \\\\ &= e^{- \\int^{n}_{0} \\mu_{x+t} - kn} \\\\ &= e^{- \\int^{n}_{0} \\mu_{x+t}} \\cdot e^{-kn} \\\\ &= {}_{}p'_{x} \\cdot e^{-kn} \\\\ \\\\ {}_{}p'_{x} &= e^{- k\\int^{n}_{0} \\mu_{x+t}} \\\\ &= (e^{- \\int^{n}_{0} \\mu_{x+t}})^k \\\\ &= ({}_{}p_{x})^k \\end{aligned} \\] Survival Summary \u00b6 Moments \u00b6 Continuous Moments \u00b6 The first raw moment of the distribution is known as the Complete Expectation of Life , which can be calculated via first principles: \\[ E(T_{x}) = \\int_{0}^{\\infty} t \\cdot f_x(t) \\\\ \\] Note that integrating this expression directly requires integration by parts , which can be time consuming, this it can be simplified using the Survival Function Method . Recall that in the previous section we found that the PDF and the survival function are related: \\[ \\begin{aligned} f_{x}(t) &= - \\frac{d}{dt} S_{x}(t) \\\\ \\therefore \\int f_{x}(t) &= -S_{x}(t) \\end{aligned} \\] This result can be used to simplify the integration by parts: \\[ \\begin{aligned} E(T_{x}) &= \\Bigl[t \\cdot \\int f_x(t) \\Bigr]_0^\\infty - \\int_{0}^{\\infty} \\int f_x(t) \\cdot 1 \\\\ &= \\Bigl[t \\cdot (-S_X(t)) \\Bigr]_0^\\infty - \\int_{0}^{\\infty} (-S_X(x)) \\\\ &= tS_X(\\infty) - tS_X(0) + \\int_{0}^{\\infty} S_X(x) \\\\ &= \\infty \\cdot (0) - 0 \\cdot 1 + \\int_{0}^{\\infty} S_X(x) \\\\ &= \\int_{0}^{\\infty} S_X(x) \\\\ &= \\int_0^\\infty {}_{t}p_x \\end{aligned} \\] The same logic can be applied to the second raw moment as well: \\[ \\begin{aligned} E(T^2_x) &= \\int_{0}^{\\infty} t^2 \\cdot f_x(t) \\\\ &= \\Bigl[t^2 \\cdot \\int f_x(t) \\Bigr]_0^\\infty - \\int_{0}^{\\infty} \\int f_x(t) \\cdot 2t \\\\ &= \\Bigl[t^2 \\cdot S_x(t)\\Bigr]_0^\\infty - \\int_{0}^{\\infty} 2t \\cdot S_x(t) \\\\ &= t^2 \\cdot S_x(\\infty) - t^2 \\cdot S_x(0) - \\int_{0}^{\\infty} 2t \\cdot S_x(t) \\\\ &= (\\infty)^2 \\cdot 0 - 0^2 \\cdot 1 - \\int_{0}^{\\infty} 2t \\cdot S_x(t) \\\\ &= \\int_{0}^{\\infty} 2t \\cdot {}_{t}p_x \\end{aligned} \\] Tip More generally, the survival function method can be expressed as the following: \\[ \\begin{aligned} \\int_{0}^{\\infty} g(x) \\cdot f_x(t) &= \\int_{0}^{\\infty} g'(x) \\cdot S_x(t) \\end{aligned} \\] The Variance is simply the difference of the first two raw moments with no special simplification : \\[ \\begin{aligned} Var(T_x) &= E(T^2_x) - [E(T_x)]^2 \\\\ &= 2 \\int_{0}^{\\infty} t \\cdot {}_{t}p_x - \\left(\\int_0^\\infty {}_{t}p_x \\right)^2 \\end{aligned} \\] Discrete Moments \u00b6 Similarly, the expectation of the discrete distribution is known as the Expectation of Curtate Lifetime . \\[ \\begin{aligned} E(K_x) &= \\sum_{k=0}^{\\infty} k \\cdot {}_{k \\mid 1}q_{x} \\\\ &= \\sum_{k=0}^{\\infty} k ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 2 ({}_{2}p_{x} - {}_{3}p_{x}) + 3 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 2 {}_{2}p_{x} - 2{}_{3}p_{x} + 3 {}_{3}p_{x} - 3 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + {}_{2}p_{x} + {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\end{aligned} \\] Note Summation starts from 1 as the first term is cancelled Probability tree perspective The second moment can be calculated in a similar fashion: \\[ \\begin{aligned} E(K^2_x) &= \\sum_{k=0}^{\\infty} k^2 ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 4 ({}_{2}p_{x} - {}_{3}p_{x}) + 9 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 4{}_{2}p_{x} - 4{}_{3}p_{x} + 9 {}_{3}p_{x} - 9 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 3{}_{2}p_{x} + 5 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} (2k-1) {}_{k}p_{x} \\end{aligned} \\] Similarly, the variance has no special simplifications : \\[ \\begin{aligned} Var(K_x) &= E(K_x^2) - [E(K_x)]^2 \\\\ &= \\sum_{k=1}^{\\infty} (2k-1) {}_{k}p_{x} - \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\end{aligned} \\] Temporary Expectation \u00b6 If the future lifetime variable is artifically limited to \\(n\\) years, then the expectation is known as the n-year temporary complete/curtate expectation of life . It is intepreted as the number of years that the person is expected to live out of the next \\(n\\) years ONLY. If the person lives past \\(n\\) years, then only \\(n\\) years are recorded. Thus, it can be represented using a minimum function : \\[ \\min (T_x, n) = \\begin{cases} T_x,& T_x \\lt n \\\\ n,& T_x \\gt n \\end{cases} \\] In order to easily represent and distinguish it from the full counterpart, the following actuarial notation is used: Complete Curtate Full \\(\\mathring{e}_{x}\\) \\(e_{x}\\) Temporary \\(\\mathring{e}_{x:\\enclose{actuarial}{n}}\\) \\(e_{x:\\enclose{actuarial}{n}}\\) \\[ \\begin{aligned} \\mathring{e}_{x} &= \\int^{\\infty}_0 {}_{t}p_x \\\\ \\mathring{e}_{x:\\enclose{actuarial}{n}} &= \\int^{n}_0 {}_{t}p_x \\\\ \\\\ e_{x} &= \\sum^{\\infty}_0 {}_{k}p_x \\\\ e_{x:\\enclose{actuarial}{n}} &= \\sum^{n}_0 {}_{k}p_x \\end{aligned} \\] Decomposition \u00b6 The complete expectation can be decomposed into two components: Term Expectation at the current age representing the \"early\" years Complete Expectation at a future age represenging the \"later\" years \\[ \\begin{aligned} \\mathring{e}_x &\\approx \\mathring{e}_{x:\\enclose{actuarial}{n}} + \\mathring{e}_{x+n} \\\\ e_x &\\approx e_{x:\\enclose{actuarial}{n}} + e_{x+n} \\end{aligned} \\] However, the above makes an implicit assumption that the person will survive the first n years . Thus, the second term needs to account for the probability of surviving those n years: \\[ \\begin{aligned} \\mathring{e}_x &= \\mathring{e}_{x:\\enclose{actuarial}{n}} + {}_{n}p_{x} \\cdot \\mathring{e}_{x+n} \\\\ e_x &= e_{x:\\enclose{actuarial}{n}} + {}_{n}p_{x} \\cdot e_{x+n} \\end{aligned} \\] Tip Many questions require us to use this equation, but may not directly provide the values at \\(x+n\\) , requiring us to go through other steps to calculate it. This can sometimes confuse us into thinking that the decomposition is different. For instance, if \\(x+n\\) is 60, but the questions provide 61, we may inadvertently think that \\(x+n\\) is 61 under stress. Similarly, a temporary expectation can be further decomposed into more temporary expectations: \\[ \\begin{aligned} \\mathring{e}_{x:\\enclose{actuarial}{m+n}} &= \\mathring{e}_{x:\\enclose{actuarial}{m}} + {}_{m}p_{x} \\cdot \\mathring{e}_{x+m:\\enclose{actuarial}{n}} \\\\ e_{x:\\enclose{actuarial}{m+n}} &= e_{x:\\enclose{actuarial}{m}} + {}_{m}p_{x} \\cdot e_{x+m:\\enclose{actuarial}{n}} \\end{aligned} \\] For the curtate expectation only , if \\(n=1\\) , it leads to a recursion : \\[ \\begin{aligned} e_x &= e_{x:\\enclose{actuarial}{1}} + {}_{}p_{x} \\cdot e_{x+1} \\\\ &= {}_{}p_{x} + {}_{}p_x \\cdot e_{x+1} \\\\ &= {}_{}p_{x} (1 + e_{x+1}) \\end{aligned} \\] Note Consider the future lifetime variable for just the coming year: \\[ \\begin{aligned} e_{x:\\enclose{actuarial}{1}} &= \\begin{cases} 1, {}_{}p_{x} \\\\ 0, {}_{}q_{x} \\end{cases} \\\\ \\\\ \\therefore e_{x:\\enclose{actuarial}{1}} &= 1 \\cdot {}_{}p_{x} + 0 \\cdot {}_{}q_{x} \\\\ &= {}_{}p_{x} \\end{aligned} \\] Discrete Continuous Link \u00b6 Note that the two expectations are similar to one another: Continuous Expectation - Area under survival function Discrete Expectation - Right Riemann Sum of the area under the survival function Recall from Calculus that the area under a curve can be estimated through the Trapezium Rule , which states that the area is approximately equal to the sum of the area of discrete trapeziums formed under the curve. \\[ \\begin{aligned} \\int^b_a f(x) & \\approx \\sum \\text{Area of Trapezium} \\\\ & \\approx \\sum \\frac{1}{2} h \\left[f(a+kh) + f(a+(k+1)h) \\right] \\\\ & \\approx \\frac{h}{2}[f(a) + f(a+h)] + \\frac{h}{2}[f(a+h) + f(a+2h)] + \\dots + \\frac{h}{2}[f(b-h) + f(b)] \\\\ & \\approx \\frac{h}{2} f(a) + h[f(a+h) + f(a+2h) + \\dots + f(b-h)] + \\frac{h}{2} f(b) \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b-h)] + \\frac{h}{2} [f(a)+f(b)] \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b)] - \\frac{h}{2} [f(a)+f(b)] \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b-h)] - \\frac{h}{2} [f(a)-f(b)] \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b)] + \\frac{h}{2} [f(a)-f(b)] \\end{aligned} \\] The proof relies on the fact that other than \\(f(a)\\) and \\(f(b)\\) , all other terms are repeated twice . The last four lines are just different variations that showcase this property: Both \\(f(a)\\) and \\(f(b)\\) excluded from main expression Both \\(f(a)\\) and \\(f(b)\\) included in main expression but subtracted Only \\(f(a)\\) included in main expression; Main expression is a Left Riemman Sum Only \\(f(b)\\) included in main expression; Main expression is a Right Riemman Sum Since the discrete expectation is a right riemann sum, the last expression of the trapezoidal rule should be used. This allows the continuous expectation to be expressed using the discrete expectation, assuming \\(h=1\\) : \\[ \\begin{aligned} \\mathring{e}_x &= \\int^{\\infty}_{0} S_x(t) \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b)] + \\frac{h}{2} [f(a)-f(b)] \\\\ & \\approx (1) [S_x(1) + S_x(2) + \\dots + S_x(\\infty)] + \\frac{1}{2} [S_x(0)-S_x(\\infty)] \\\\ & \\approx [S_x(1) + S_x(2) + \\dots + S_x(\\infty)] + \\frac{1}{2} [1-0] \\\\ & \\approx e_x + \\frac{1}{2} \\end{aligned} \\] An alternative way to view the above is that the area of the trapezoid is the sum of the riemann rectangles and a triangle . Euler Maclaurin Formula \u00b6 Note that the Trapezoidal Rule is not perfect - there is an inherent error in trying to approximate a curve using a line. \\[ \\begin{aligned} \\int^b_a f(x) & \\approx \\sum (\\text{Area of Trapezium} + \\text{Error})\\\\ \\end{aligned} \\] The error can be positive or negative, depending on the shape of the curve in that interval: The error term is calculated by taking the difference between the integral and the trapezium, and is generalized using the taylor series. The final result is known as the Euler Maclaurin Formula : \\[ \\sum \\text{Error} = \\frac{h^2}{12} [f'(a)-f'(b)] \\] \\gt Given that a taylor series was used, the error can be expressed as the sum of many different terms, but higher powers are ignored . Thus, the trapezoidal approximation for the expectation can be made more precise: \\[ \\begin{aligned} \\mathring{e}_x & \\approx e_x + \\frac{1}{2} + \\frac{1^2}{12} [S'(0)-S'(\\infty)] \\\\ & \\approx e_x + \\frac{1}{2} + \\frac{1}{12} [S'(0)] \\end{aligned} \\] The above will rarely be used in this manner - it is more than sufficient to know the relationship between the two expectations of life. However, it sets as a strong foundation to understand the Woolhouse Approximation in the life annuities section. Moment Summary \u00b6 Parametric Survival Models \u00b6 Given the importance of \\(\\mu_x\\) , several mathematical functions have been made to describe the force of mortality, known as a Parametric Survival Model . Exponential Distribution \u00b6 When the force of mortality is constant \\(\\mu_{x} = \\mu\\) , the PDF can be shown to be exponential: \\[ \\begin{aligned} {}_{t}p_x &= e^{- \\int^{t}_{0} \\mu} \\\\ &= e^{- \\mu t}\\\\ \\\\ f(x) &= {}_{}p_x \\cdot \\mu \\\\ &= \\mu \\cdot e^{- \\mu t} \\\\ \\\\ \\therefore T_{x} &\\sim \\text{Exponential}(\\mu) \\\\ E(T_x) &= \\frac{1}{\\mu} \\\\ Var (T_x) &= \\frac{1}{\\mu^2} \\end{aligned} \\] The key is to remember that distribution is Memoryless , which means that the age of the person does not matter: \\[ \\begin{aligned} {}_{n}p_{x} &= {}_{n}p_{x+m} \\\\ \\\\ \\therefore {}_{n}p_{x} &= {}_{}p_{x} \\cdot {}_{}p_{x+1} \\cdot {}_{}p_{x+2} \\cdot \\dots \\\\ &= {}_{}p_{x} \\cdot {}_{}p_{x} \\cdot {}_{}p_{x} \\cdot \\dots \\\\ &= ({}_{}p_{x})^n \\end{aligned} \\] This leads to an interesting relationship between the full and temporary expectation: \\[ \\begin{aligned} \\mathring{e}_{x} &= \\mathring{e}_{x+m} \\\\ \\\\ \\therefore \\mathring{e}_x &= \\mathring{e}_{x:\\enclose{actuarial}{n}} + {}_{n}p_{x} \\cdot \\mathring{e}_{x+n} \\\\ \\mathring{e}_x &= \\mathring{e}_{x:\\enclose{actuarial}{n}} + {}_{n}p_{x} \\cdot \\mathring{e}_{x} \\\\ \\mathring{e}_{x:\\enclose{actuarial}{n}} &= \\mathring{e}_x - {}_{n}p_{x} \\cdot \\mathring{e}_{x} \\\\ \\mathring{e}_{x:\\enclose{actuarial}{n}} &= \\mathring{e}_x \\cdot (1 - {}_{n}p_{x}) \\end{aligned} \\] De Moivre's Model \u00b6 De Moivre's Law assumes that the distribution of a NEWBORN is uniformly distributed between 0 and \\(\\omega\\) , where \\(\\omega\\) is known as the Limiting Age . It can be intepreted as starting with \\(k\\omega\\) people at age 0, where \\(k\\) people die every year till everybody dies by age \\(w\\) . \\[ \\ell_{x} = k(\\omega - x) \\] The PDF can be determined using the survival model or the uniform distribution: \\[ \\begin{aligned} {}_{t}p_x &= \\frac{\\ell_{x+t}}{\\ell_{x}} \\\\ &= \\frac{w-(x+t)}{w-x} \\\\ \\\\ \\mu_{x+t} &= \\frac{1}{w-(x+t)} \\\\ \\\\ \\therefore f(x) &= \\frac{w-(x+t)}{w-x} \\cdot \\frac{1}{w-(x+t)} \\\\ &= \\frac{1}{\\omega-x} \\end{aligned} \\] \\[ \\begin{aligned} T_{0} &\\sim \\text{Uniform}(0, \\omega) \\\\ T_{x} &= T_{0} - x \\mid T_{0} \\gt x \\\\ \\therefore T_{x} &\\sim \\text{Uniform}(0, \\omega-x) \\\\ \\\\ E(T_{x}) &= \\frac{\\omega-x}{2} \\\\ Var(T_{x}) &= \\frac{(\\omega-x)^2}{12} \\end{aligned} \\] Gompertz Model \u00b6 Both the above distributions make unrealistic assumptions: Exponential Distribution : Assume age does not matter Uniform Distribution : Assume equal number of people die eacg tear A more realistic model is the Gompertz Model , which suggests that mortality increases with age : \\[ \\mu_{x} = Bc^{x} \\\\ \\] Thus, the corresponding survival function can be calculated: \\[ \\begin{aligned} {}_{t}p_{x} &= e^{-\\int^{n}_{0} Bc^{x+s}} \\\\ &= e^{-Bc^{x} \\int^{n}_{0} c^{s}} \\\\ &= e^{-Bc^{x} [\\frac{c^s}{\\ln c}]^t_0} \\\\ &= e^{-Bc^{x} [\\frac{c^t}{\\ln c} - \\frac{c^0}{\\ln c}]} \\\\ &= e^{\\frac{-Bc^{x}}{\\ln c} (c^t - 1)} \\end{aligned} \\] Makeham Gompertz Model \u00b6 Makeham added a constant \\(A\\) into the Gompertz model, resulting in the Makeham Gompertz Model : \\[ \\mu_{x} = A + B \\cdot c^{x} \\] \\(A\\) represents the age independent mortality - dying from reasons unrelated to age, such as from accidents or natural catastrophes. \\[ \\begin{aligned} S_x(t) &= e^{- \\int^{t}_{0} A + Bc^{x+t}} \\\\ &= e^{\\int^{t}_{0} -A - Bc^{x+t}} \\\\ &= e^{\\left[-At - \\frac{B}{\\ln c} c^{x+t} \\right]^{t}_{0}} \\\\ &= e^{\\left[-At - \\frac{B}{\\ln c} c^{x+t} \\right] - \\left[-\\frac{B}{\\ln c} c^{x} \\right]} \\\\ &= e^{-At - \\frac{B}{\\ln c} c^{x+t} + \\frac{B}{\\ln c} c^{x}} \\\\ &= e^{-At - \\frac{B}{\\ln c} c^{x} [c^t - 1]} \\\\ &= e^{-At} \\cdot e^{- \\frac{B}{\\ln c} c^{x} [c^{t} - 1]} \\end{aligned} \\] Tip If the force of mortality provided follow a similar expression as above, immediately recognize that this is the makeham gompertz model. Do NOT waste time attempting to integrate the force; simply use the formula provided in the formula sheet to calculate the probabilities. Any modification made to the force should be expressed as a change in the Makeham parameters: \\[ \\begin{aligned} \\mu_{x} &= A + B \\cdot c^{x} \\\\ \\\\ \\mu_{y} &= \\mu_{x} + Z \\\\ \\therefore \\mu_{y} &= (A+Z) + B \\cdot c^{x} \\\\ \\\\ \\mu_{Z} &= \\mu_{x} + Z \\cdot B \\cdot c^{x} \\\\ \\end{aligned} \\] Population Mortality \u00b6 The Makeham Gompertz Model is ideal because it depicts human mortality fairly accurately: There are a few key aspects to take note of: Gender Difference : Males have higher mortality Perinatal Mortality : Infants with birth issues likely to die, mortality decreases after Accident Hump : Mortality increases past age 10, more sharply in males due to accidents","title":"Survival Models"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#survival-models","text":"This section assumes some basic knowledge on Probability Theory , which can be found under another set of notes covering a Review of Probability Theory .","title":"Survival Models"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#overview","text":"Survival Models are probability distributions that measure the time to failure of an entity. In an actuarial context, it measures the time to death of a person. From another perspective, it also measures the future lifetime of that person. Note If someone is going to die in 5 years, then the person only has 5 years left to live (future lifetime). Intuitively, the distribution depends heavily on the age of the person, which is denoted using \\((x)\\) , which denotes a life aged \\(x\\) .","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#continuous-lifetime","text":"The distribution of the time to death ( in years ) is represented using the Continuous Random Variable \\(T_{x}\\) , where \\(x\\) represents the age of the person. Note Although we often use integer values for time, it can be expressed in continuous form as well: 1 Year = 12 Months 0.5 Years = 6 Months 0.541 Years = 6 Months & 15 Days Since \\(T_{x}\\) represents the time to death, a person aged \\(x\\) will live for another \\(T_{x}\\) years and then die before age \\(x + T_{x}\\) : The probability that the person dies DURING the next \\(t\\) years before age \\(x+t\\) is given by the CDF of the distribution: \\[ F_{x}(t) = P(T_{x} \\le t) \\] Note The implicit assumption is that the person aged \\(x\\) just turned age \\(x\\) (on their birthday). Thus, dying within the next \\(t\\) years means that they will not live to turn age \\(x+t\\) . The complement of the CDF is the probability that the person dies AFTER the next \\(t\\) years . From another perspective, this also means that the person survives the next \\(t\\) years to age \\(x+t\\) , which is why it is known as the Survival Function . \\[ \\begin{aligned} S_{x}(t) &= P(T_{x} \\gt t) \\\\ &= 1 - P(T_{x} \\lt t) \\\\ &= 1 - F_{x}(t) \\\\ \\end{aligned} \\] Tip The red squiggly lines in the above diagrams represent the range of times where the individual can die . For survival probabilities, it is the entire time AFTER the specified survival period. Since they are complements of one another, they share similar properties: CDF Survival Function Intuition \\(F_{x}(0) = 0\\) \\(S_{x}(0) = 1\\) Humans are currently alive \\(F_{x}(\\infty) = 1\\) \\(S_{x}(\\infty) = 0\\) Humans will inevitably die Non-decreasing function Non-increasing function Probability of death increases with age Warning The first property is hard to intepret. The probability that a life aged \\(x\\) dies within 0 years before age \\(x+0\\) must be 0, since the person is already aged \\(x\\) . The above two can also be combined into the probability of surviving \\(u\\) years till age \\(x+u\\) and then dying within the following \\(t\\) years before age \\(x+u+t\\) , known as the Probability of Deferred Death . \\[ \\begin{aligned} P(u \\lt T_x \\lt t+u) &= P(T_x \\lt t+u) - P(T_x \\lt u) \\\\ &= [1 - P(T_x \\gt t+u)] - [1 - P(T_x \\gt u)] \\\\ &= P(T_x \\gt u) - P(T_x \\gt t+u) \\\\ &= P(T_x \\gt u) - P(T_x \\gt u) \\cdot P(T_{x+u} \\gt t) \\\\ &= P(T_x \\gt u) [1 - P(T_{x+u} \\gt t)] \\\\ &= P(T_x \\gt u) \\cdot P(T_{x+u} \\lt t) \\\\ &= S_{x}(u) \\cdot F_{x+t}(t) \\end{aligned} \\] Warning Remember that the second term reflects the \"new\" age of the person after surviving \\(u\\) years till age \\(x+u\\) . Using Only Survival Functions , it is represented as the probability that the person survives \\(u\\) years but does not survive past \\(u+t\\) years : \\[ \\begin{aligned} P(u \\lt T_x \\lt t+u) &= P(T_x \\lt t+u) - P(T_x \\lt u) \\\\ &= [1 - P(T_x \\gt t+u)] - [1 - P(T_x \\gt u)] \\\\ &= P(T_x \\gt u) - P(T_x \\gt t+u) \\\\ &= S_{x}(u) - S_{x}(t+u) \\end{aligned} \\] Using Only Death Functions , it is represented as the probability of dying within \\(u+t\\) years but NOT \\(u\\) years: \\[ \\begin{aligned} P(u \\lt T_{x} \\lt u+t) &= P(T_x \\lt u+t) - P(T_x \\lt u) \\\\ &= F_{x} (u+t) - F_{x}(u) \\end{aligned} \\] Tip An easy way to remember the different formulations (especially the ones involving only death or survival) is to visualize them. The goal is to find the combination of probabilities that result in the individual dying in the specified time.","title":"Continuous Lifetime"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#newborn-lifetime","text":"Note that one implicit assumption made so far is that the person has already survived till age \\(x\\) . In order to further analyze this, we must consider a special case of the distribution for a newborn aged 0 , represented by \\(T_{0}\\) . The future lifetime variable is essentially conditioned on the newborn having survived \\(x\\) years, then dying within the following \\(t\\) years before age \\(x+t\\) : \\[ \\begin{aligned} T_{x} &= T_{0} - x \\mid T_{0} \\gt x \\\\ \\\\ P(T_x \\le t) &= P(T_0 \\le x + t \\mid T_0 \\gt x) \\\\ &= \\frac{P(x \\lt T_0 \\lt x +t)}{P(T_0 \\gt x)} \\\\ &= \\frac{P(T_0 \\lt x +t)-P(T_0 \\lt x)}{P(T_0 \\gt x)} \\end{aligned} \\] When written in terms of the survival function, it can be simplified even further: \\[ \\begin{aligned} P(T_x \\ge t) &= P(T_0 \\ge x + t | T_0 \\gt x) \\\\ &= \\frac{P(T_0 \\gt x + t)}{P(T_0 \\gt x)} \\\\ &= \\frac{S_0(x+t)}{S_0(x)} \\end{aligned} \\] Note Since the surviving till age \\(x\\) is a subset of surviving till age \\(x+t\\) , the numerator can be simplified to just the probability of surviving till age \\(x+t\\) . This leads to the Multiplication Rule of survival probabilities: \\[ \\begin{aligned} S_x(t) &= \\frac{S_0(x+t)}{S_0(x)} \\\\ S_0(x+t) &= S_0(x)S_x(t) \\\\ \\\\ \\therefore S_x(t+u) &= S_x(u) S_{x+u}(t) \\end{aligned} \\] The survival probability can be decomposed into two components: The probability of a person surviving \\(u\\) years The conditional probability of person who has survived \\(u\\) years (new age) surviving another \\(t\\) years Warning This result ONLY applies to the survival probabilities !","title":"Newborn Lifetime"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#actuarial-notation","text":"Given how often the survival function is used, they are often abbreviated using the International Actuarial Notation : \\[ \\begin{aligned} P(T_{x} \\lt t) &= {}_{t}q_{x} \\\\ P(T_{x} \\gt u) &= {}_{t}p_{x} \\end{aligned} \\] Note If \\(t=1\\) , it is omitted - EG. \\({}_{1}q_{x} = q_{x}\\) . Deferred Deaths are expressed using the pipe symbol : \\[ \\begin{aligned} P(u \\lt T_{x} \\lt t+u) &= {}_{u \\mid t}q_{x} \\\\ &= {}_{u}p_{x} \\cdot {}_{t}q_{x+t} \\\\ &= {}_{u}p_{x} - {}_{u+t}p_{x} \\\\ &= {}_{t+u}q_{x} - {}_{u}q_{x} \\end{aligned} \\] Other key results: \\[ \\begin{aligned} {}_{t+u}p_{x} &= {}_{u}p_{x} \\cdot {}_{t}p_{x+u} \\\\ {}_{t}p_{x} &= \\sum^{t}_{u = t} {}_{u|}q_{x} \\end{aligned} \\]","title":"Actuarial Notation"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#percentiles","text":"Recall that the definition of a percentile is the value of the random variable at which \\(100p\\) of the values fall below. \\[ \\begin{aligned} P(T_{x} \\le t) &\\le p \\\\ P(T_{x} \\ge t) &\\le 1 - p \\\\ {}_{t}P_{x} &\\le 1 - p \\\\ \\frac{\\ell_{x+t}}{\\ell_{x}} &\\le 1 - p \\\\ \\ell_{x+t} &\\le \\ell_{x} \\cdot (1-p) \\\\ \\end{aligned} \\] By comparing values in the life table, we can find the smallest age that fulfils the above inequality, \\(y\\) . Since it is unlikely that \\(\\ell_{x} \\cdot (1-p)\\) exactly matches a value, the true percentile is at a fractional age . \\[ y - 1 \\lt x + t \\lt y \\] Thus, one of the typical assumptions must be used to determine the exact median of the distribution. \\[ \\text{Continuous Percentile} = y - 1 + t \\] However, if only interested in the curtate lifetime, then we can simply take the floor of \\(x+t\\) , which is always \\(y-1\\) . \\[ \\text{Curtate Percentile} = y - 1 \\] Note We traditionall learn percentiles using increasing random variables . We usually think of percentiles as values from the beginning to the percentile . However, future lifetime is a decreasing variable . This means that all values that are smaller than the percentile come AFTER it. This means that the values are from the percentile to its upper limit:","title":"Percentiles"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#discrete-lifetime","text":"Similarly, the distribution of the time to death ( in years ) can also be represented using a Discrete Random Variable \\(K_{x}\\) , where \\(x\\) represents the age of the person. It is also known as the Curtate Future Lifetime as it only contains the integer components of the continuous future lifetime. Mathematically, it is represented as a continuous lifetime variable that has been floored . Thus, given the continuous distribution, the curtate one can be determined: \\[ K_x = \\lfloor T_x \\rfloor \\] Info The definition of Curtate is \"shortened\"; it represents how the continuous distribution has been reduced to just its integer components. Another way to remember is that \\(K\\) stands for Kurtate . The intepretation of \\(K_{x}\\) has changed slightly - A person aged \\(x\\) will live for another \\(K_x\\) FULL years and then die before age \\(x + K_{x} + 1\\) : Warning It is a very common mistake to misintepret \\(K_x\\) , thus consider the following examples: A person aged \\(x\\) dies within the same year . They have lived for 0 full years since turning \\(x\\) , thus \\(K_x = 0\\) . A person aged \\(x\\) lives till their \\(x+1\\) birthday but dies in that year. They have lived for 1 full year since turning \\(x\\) , thus \\(K_x = 1\\) . \\(K_x = k\\) can also be intepreted more intuitively as the deferred probability of living \\(k\\) years and then dying the following year : \\[ \\begin{aligned} P(K_x = k) &= P(k \\le T_x \\lt k+1) \\\\ &= P(T_x \\lt k+1) - P(T_x \\le k) \\\\ &= {}_{k+1}q_{x} - {}_{k}q_{x} \\\\ &= {}_{k}p_{x} - {}_{k+1}p_{x} \\\\ &= {}_{k}p_{x} - {}_{k}p_{x} \\cdot {}_{1}p_{x+k} \\\\ &= {}_{k}p_{x} \\cdot (1 - {}_{1}p_{x+k}) \\\\ &= {}_{k}p_{x} \\cdot {}_{1}q_{x+k} \\\\ &= {}_{k \\mid 1}q_{x} \\end{aligned} \\] Tip Recall that the probability of deferred death can be represented in three different ways, thus the PDF may take on different formulations: \\[ \\begin{aligned} {}_{u \\mid 1}q_{x} &= {}_{u}p_{x} \\cdot {}_{}q_{x+t} \\\\ &= {}_{u}p_{x} - {}_{u+1}p_{x} \\\\ &= {}_{u+1}q_{x} - {}_{u}q_{x} \\end{aligned} \\]","title":"Discrete Lifetime"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#force-of-mortality","text":"\\({}_{h}q_{x}\\) is the probability of dying within \\(h\\) years: \\[ \\begin{aligned} {}_{h}q_{x} &= 1 - {}_{h}q_{x} \\\\ &= 1 - \\frac{S_0(x+h)}{S_0(x)} \\\\ &= \\frac{S_0(x) - S_0(x+h)}{S_0(x)} \\end{aligned} \\] The Force of Mortality \\((\\mu_{x})\\) is the probability that (x) dies within an infinitely small \\(h\\) ; in other words, the probability of dying instantly . We consider the rate of death per unit time (this is NOT a probability!): \\[ \\frac{{}_{h}q_{x}}{h} = \\frac{S_0(x) - S_0(x+h)}{h \\cdot S_0(x)} \\] Note The proof uses the first principles of differentiation, which measures the rate of change of \\(y\\) over an infinitely small unit of \\(x\\) , which is why the rate of death had to be considered. This also implies that the following is true, ONLY for SMALL values of \\(h\\) : \\[ \\begin{aligned} \\mu_{x} &= \\frac{{}_{h}q_{x}}{h} \\\\ {}_{h}q_{x} &= h \\cdot \\mu_{x} \\end{aligned} \\] As the period of time becomes infinitely small, \\[ \\begin{aligned} \\mu_{x} &= \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{S_0(x)h} \\\\ &= \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x) - S_0(x+h)}{h} \\\\ &= - \\frac{1}{S_0(x)} \\lim_{h \\to 0} \\frac{S_0(x+h) - S_0(x)}{h} \\\\ &= - \\frac{1}{S_0(x)} \\cdot \\left(\\frac{d}{dx} S_0(x) \\right) \\\\ &= - \\frac{S'_0(x)}{S_0(x)} \\\\ &= - \\frac{d}{dx} \\ln S_0(x) \\end{aligned} \\] Warning It is an extremely common mistake to forget the minus sign in front of the expression. Another common mistake is forgetting how to differentiate a fraction within a logarithm: \\[ \\begin{aligned} \\frac{d}{dx} \\ln \\frac{4}{4+x^2} &= \\frac{4+x^2}{4} \\cdot - 4(4+x)^{-2} \\cdot 2x \\\\ &= - \\frac{2x}{4+x^2} \\end{aligned} \\] Using the PDF of the newborn distribution, the force of mortality can be rewritten accordingly: \\[ \\begin{aligned} f_0(x) &= \\frac{d}{dx} F_0(x) \\\\ &= \\frac{d}{dx} (1 - S_0(x)) \\\\ &= -S'_0(x) \\\\ \\\\ \\therefore \\mu_{x} &= \\frac{f_0(x)}{S_0(x)} \\end{aligned} \\] Tip The force of mortality can also be expressed as the following: \\[ \\mu_{x}(t) = \\frac{\\frac{d}{dt} {}_{t}q_{x}}{{}_{t}p_{x}} \\] We can think of this as a conditional probability : \\[ P(\\text{A} \\mid \\text{B}) = \\frac{P(\\text{A} \\cap \\text{B})}{P(\\text{B})} \\] Thus, the force of mortality can be understood as the \"probability\" that \\(x\\) dies instantly after \\(t\\) years, given that they survive \\(t\\) years .","title":"Force of Mortality"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#generalization","text":"The force of mortality is currently defined using the newborn distribution: \\[ \\mu_{y} = - \\frac{\\frac{d}{dy} S_0(y)}{S_0(y)} \\] For (x), \\(y = x + t\\) , where t is a random: \\[ \\begin{aligned} dy &= dx + dt \\\\ &= 0 + dt \\\\ &= dt \\\\ \\\\ \\therefore \\mu_{x+t} &= - \\frac{\\frac{d}{dt} S_0(x+t)}{S_0(x+t)} \\\\ &= - \\frac{d}{dt} \\ln S_0(x+t) \\end{aligned} \\] Since integration is the reverse of differentiation, \\[ \\begin{aligned} \\int^{n}_{0} \\frac{d}{dt} \\ln S_0(x+t) &= - \\int^{n}_{0} \\mu_{x+t} \\\\ [\\ln S_0(x+t)]^{n}_{0} &= - \\int^{n}_{0} \\mu_{x+t} \\\\ \\ln S_0(x+n) - \\ln S_0(x) &= - \\int^{n}_{0} \\mu_{x+t} \\\\ \\ln \\frac{S_0(x+n)}{\\ln S_0(x)} &= - \\int^{n}_{0} \\mu_{x+t} \\\\ \\ln S_{x}(n) &= - \\int^{n}_{0} \\mu_{x+t} \\\\ S_{x}(n) &= e^{- \\int^{n}_{0} \\mu_{x+t}} \\\\ {}_{n}p_{x} &= e^{- \\int^{n}_{0} \\mu_{x+t}} \\end{aligned} \\] Note The exponent is also known as the Cumulative Force of Mortality . Outside of human survival, the force of mortality is more generally known as the Hazard Function , denoted by \\(h(x)\\) . Its integral is known as the Cumulative Hazard Function , denoted by \\(H(x)\\) . \\[ H(x) = \\int^{n}_{0} h(x) \\] Although rare, a question could directly provide the cumulative hazard function and ask to compute the survival probability. In such cases, DO NOT integrate it once more! Info This expression can be intuitively understood as the following: Consider the forces trying to kill the individual. Negate these forces; only consider the scenario that the person lives. Take the exponent to compute the probability. Following a similar generalization (proof not shown), the PDF can also be expressed using the force of mortality: \\[ \\begin{aligned} \\mu_{x} &= \\frac{f_0(x)}{S_0(x)} \\\\ f_0(x) &= S_0(x) \\cdot \\mu_{x} \\\\ f_x(t) &= S_x(t) \\cdot \\mu_{x+t} \\\\ f_x(t) &= {}_{t}p_{x} \\cdot \\mu_{x+t} \\end{aligned} \\] This can be intepreted as the probability of dying instantly at age \\(x+t\\) .","title":"Generalization"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#risk-adjustments","text":"There are people who have a higher risk of dying than the average person - EG. Smokers. The first method to account for their increased risk is to treat them as an older person (higher risk), known as an Age Rating . An \\(n\\) year age rating means that a person aged \\(x\\) should be treated as if they had the mortality of a person aged \\(x+n\\) . Another method would be to simply scale the mortality rates by a factor to increase it: \\[ {}_{}q'_{x} = c \\cdot {}_{}q'_{x} \\] The last method would be to adjust the force of mortality instead, by either adding or multiplying a constant to it: \\[ \\begin{aligned} {}_{}p'_{x} &= e^{- (\\int^{n}_{0} \\mu_{x+t} + k)} \\\\ &= e^{- \\int^{n}_{0} \\mu_{x+t} - kn} \\\\ &= e^{- \\int^{n}_{0} \\mu_{x+t}} \\cdot e^{-kn} \\\\ &= {}_{}p'_{x} \\cdot e^{-kn} \\\\ \\\\ {}_{}p'_{x} &= e^{- k\\int^{n}_{0} \\mu_{x+t}} \\\\ &= (e^{- \\int^{n}_{0} \\mu_{x+t}})^k \\\\ &= ({}_{}p_{x})^k \\end{aligned} \\]","title":"Risk Adjustments"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#survival-summary","text":"","title":"Survival Summary"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#moments","text":"","title":"Moments"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#continuous-moments","text":"The first raw moment of the distribution is known as the Complete Expectation of Life , which can be calculated via first principles: \\[ E(T_{x}) = \\int_{0}^{\\infty} t \\cdot f_x(t) \\\\ \\] Note that integrating this expression directly requires integration by parts , which can be time consuming, this it can be simplified using the Survival Function Method . Recall that in the previous section we found that the PDF and the survival function are related: \\[ \\begin{aligned} f_{x}(t) &= - \\frac{d}{dt} S_{x}(t) \\\\ \\therefore \\int f_{x}(t) &= -S_{x}(t) \\end{aligned} \\] This result can be used to simplify the integration by parts: \\[ \\begin{aligned} E(T_{x}) &= \\Bigl[t \\cdot \\int f_x(t) \\Bigr]_0^\\infty - \\int_{0}^{\\infty} \\int f_x(t) \\cdot 1 \\\\ &= \\Bigl[t \\cdot (-S_X(t)) \\Bigr]_0^\\infty - \\int_{0}^{\\infty} (-S_X(x)) \\\\ &= tS_X(\\infty) - tS_X(0) + \\int_{0}^{\\infty} S_X(x) \\\\ &= \\infty \\cdot (0) - 0 \\cdot 1 + \\int_{0}^{\\infty} S_X(x) \\\\ &= \\int_{0}^{\\infty} S_X(x) \\\\ &= \\int_0^\\infty {}_{t}p_x \\end{aligned} \\] The same logic can be applied to the second raw moment as well: \\[ \\begin{aligned} E(T^2_x) &= \\int_{0}^{\\infty} t^2 \\cdot f_x(t) \\\\ &= \\Bigl[t^2 \\cdot \\int f_x(t) \\Bigr]_0^\\infty - \\int_{0}^{\\infty} \\int f_x(t) \\cdot 2t \\\\ &= \\Bigl[t^2 \\cdot S_x(t)\\Bigr]_0^\\infty - \\int_{0}^{\\infty} 2t \\cdot S_x(t) \\\\ &= t^2 \\cdot S_x(\\infty) - t^2 \\cdot S_x(0) - \\int_{0}^{\\infty} 2t \\cdot S_x(t) \\\\ &= (\\infty)^2 \\cdot 0 - 0^2 \\cdot 1 - \\int_{0}^{\\infty} 2t \\cdot S_x(t) \\\\ &= \\int_{0}^{\\infty} 2t \\cdot {}_{t}p_x \\end{aligned} \\] Tip More generally, the survival function method can be expressed as the following: \\[ \\begin{aligned} \\int_{0}^{\\infty} g(x) \\cdot f_x(t) &= \\int_{0}^{\\infty} g'(x) \\cdot S_x(t) \\end{aligned} \\] The Variance is simply the difference of the first two raw moments with no special simplification : \\[ \\begin{aligned} Var(T_x) &= E(T^2_x) - [E(T_x)]^2 \\\\ &= 2 \\int_{0}^{\\infty} t \\cdot {}_{t}p_x - \\left(\\int_0^\\infty {}_{t}p_x \\right)^2 \\end{aligned} \\]","title":"Continuous Moments"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#discrete-moments","text":"Similarly, the expectation of the discrete distribution is known as the Expectation of Curtate Lifetime . \\[ \\begin{aligned} E(K_x) &= \\sum_{k=0}^{\\infty} k \\cdot {}_{k \\mid 1}q_{x} \\\\ &= \\sum_{k=0}^{\\infty} k ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 2 ({}_{2}p_{x} - {}_{3}p_{x}) + 3 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 2 {}_{2}p_{x} - 2{}_{3}p_{x} + 3 {}_{3}p_{x} - 3 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + {}_{2}p_{x} + {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\end{aligned} \\] Note Summation starts from 1 as the first term is cancelled Probability tree perspective The second moment can be calculated in a similar fashion: \\[ \\begin{aligned} E(K^2_x) &= \\sum_{k=0}^{\\infty} k^2 ({}_{k}p_{x} - {}_{k+1}p_{x}) \\\\ &= 1 ({}_{1}p_{x} - {}_{2}p_{x}) + 4 ({}_{2}p_{x} - {}_{3}p_{x}) + 9 ({}_{3}p_{x} - {}_{4}p_{x}) + ... \\\\ &= {}_{1}p_{x} - {}_{2}p_{x} + 4{}_{2}p_{x} - 4{}_{3}p_{x} + 9 {}_{3}p_{x} - 9 {}_{4}p_{x} \\\\ &= {}_{1}p_{x} + 3{}_{2}p_{x} + 5 {}_{3}p_{x} + ... \\\\ &= \\sum_{k=1}^{\\infty} (2k-1) {}_{k}p_{x} \\end{aligned} \\] Similarly, the variance has no special simplifications : \\[ \\begin{aligned} Var(K_x) &= E(K_x^2) - [E(K_x)]^2 \\\\ &= \\sum_{k=1}^{\\infty} (2k-1) {}_{k}p_{x} - \\sum_{k=1}^{\\infty} {}_{k}p_{x} \\end{aligned} \\]","title":"Discrete Moments"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#temporary-expectation","text":"If the future lifetime variable is artifically limited to \\(n\\) years, then the expectation is known as the n-year temporary complete/curtate expectation of life . It is intepreted as the number of years that the person is expected to live out of the next \\(n\\) years ONLY. If the person lives past \\(n\\) years, then only \\(n\\) years are recorded. Thus, it can be represented using a minimum function : \\[ \\min (T_x, n) = \\begin{cases} T_x,& T_x \\lt n \\\\ n,& T_x \\gt n \\end{cases} \\] In order to easily represent and distinguish it from the full counterpart, the following actuarial notation is used: Complete Curtate Full \\(\\mathring{e}_{x}\\) \\(e_{x}\\) Temporary \\(\\mathring{e}_{x:\\enclose{actuarial}{n}}\\) \\(e_{x:\\enclose{actuarial}{n}}\\) \\[ \\begin{aligned} \\mathring{e}_{x} &= \\int^{\\infty}_0 {}_{t}p_x \\\\ \\mathring{e}_{x:\\enclose{actuarial}{n}} &= \\int^{n}_0 {}_{t}p_x \\\\ \\\\ e_{x} &= \\sum^{\\infty}_0 {}_{k}p_x \\\\ e_{x:\\enclose{actuarial}{n}} &= \\sum^{n}_0 {}_{k}p_x \\end{aligned} \\]","title":"Temporary Expectation"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#decomposition","text":"The complete expectation can be decomposed into two components: Term Expectation at the current age representing the \"early\" years Complete Expectation at a future age represenging the \"later\" years \\[ \\begin{aligned} \\mathring{e}_x &\\approx \\mathring{e}_{x:\\enclose{actuarial}{n}} + \\mathring{e}_{x+n} \\\\ e_x &\\approx e_{x:\\enclose{actuarial}{n}} + e_{x+n} \\end{aligned} \\] However, the above makes an implicit assumption that the person will survive the first n years . Thus, the second term needs to account for the probability of surviving those n years: \\[ \\begin{aligned} \\mathring{e}_x &= \\mathring{e}_{x:\\enclose{actuarial}{n}} + {}_{n}p_{x} \\cdot \\mathring{e}_{x+n} \\\\ e_x &= e_{x:\\enclose{actuarial}{n}} + {}_{n}p_{x} \\cdot e_{x+n} \\end{aligned} \\] Tip Many questions require us to use this equation, but may not directly provide the values at \\(x+n\\) , requiring us to go through other steps to calculate it. This can sometimes confuse us into thinking that the decomposition is different. For instance, if \\(x+n\\) is 60, but the questions provide 61, we may inadvertently think that \\(x+n\\) is 61 under stress. Similarly, a temporary expectation can be further decomposed into more temporary expectations: \\[ \\begin{aligned} \\mathring{e}_{x:\\enclose{actuarial}{m+n}} &= \\mathring{e}_{x:\\enclose{actuarial}{m}} + {}_{m}p_{x} \\cdot \\mathring{e}_{x+m:\\enclose{actuarial}{n}} \\\\ e_{x:\\enclose{actuarial}{m+n}} &= e_{x:\\enclose{actuarial}{m}} + {}_{m}p_{x} \\cdot e_{x+m:\\enclose{actuarial}{n}} \\end{aligned} \\] For the curtate expectation only , if \\(n=1\\) , it leads to a recursion : \\[ \\begin{aligned} e_x &= e_{x:\\enclose{actuarial}{1}} + {}_{}p_{x} \\cdot e_{x+1} \\\\ &= {}_{}p_{x} + {}_{}p_x \\cdot e_{x+1} \\\\ &= {}_{}p_{x} (1 + e_{x+1}) \\end{aligned} \\] Note Consider the future lifetime variable for just the coming year: \\[ \\begin{aligned} e_{x:\\enclose{actuarial}{1}} &= \\begin{cases} 1, {}_{}p_{x} \\\\ 0, {}_{}q_{x} \\end{cases} \\\\ \\\\ \\therefore e_{x:\\enclose{actuarial}{1}} &= 1 \\cdot {}_{}p_{x} + 0 \\cdot {}_{}q_{x} \\\\ &= {}_{}p_{x} \\end{aligned} \\]","title":"Decomposition"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#discrete-continuous-link","text":"Note that the two expectations are similar to one another: Continuous Expectation - Area under survival function Discrete Expectation - Right Riemann Sum of the area under the survival function Recall from Calculus that the area under a curve can be estimated through the Trapezium Rule , which states that the area is approximately equal to the sum of the area of discrete trapeziums formed under the curve. \\[ \\begin{aligned} \\int^b_a f(x) & \\approx \\sum \\text{Area of Trapezium} \\\\ & \\approx \\sum \\frac{1}{2} h \\left[f(a+kh) + f(a+(k+1)h) \\right] \\\\ & \\approx \\frac{h}{2}[f(a) + f(a+h)] + \\frac{h}{2}[f(a+h) + f(a+2h)] + \\dots + \\frac{h}{2}[f(b-h) + f(b)] \\\\ & \\approx \\frac{h}{2} f(a) + h[f(a+h) + f(a+2h) + \\dots + f(b-h)] + \\frac{h}{2} f(b) \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b-h)] + \\frac{h}{2} [f(a)+f(b)] \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b)] - \\frac{h}{2} [f(a)+f(b)] \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b-h)] - \\frac{h}{2} [f(a)-f(b)] \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b)] + \\frac{h}{2} [f(a)-f(b)] \\end{aligned} \\] The proof relies on the fact that other than \\(f(a)\\) and \\(f(b)\\) , all other terms are repeated twice . The last four lines are just different variations that showcase this property: Both \\(f(a)\\) and \\(f(b)\\) excluded from main expression Both \\(f(a)\\) and \\(f(b)\\) included in main expression but subtracted Only \\(f(a)\\) included in main expression; Main expression is a Left Riemman Sum Only \\(f(b)\\) included in main expression; Main expression is a Right Riemman Sum Since the discrete expectation is a right riemann sum, the last expression of the trapezoidal rule should be used. This allows the continuous expectation to be expressed using the discrete expectation, assuming \\(h=1\\) : \\[ \\begin{aligned} \\mathring{e}_x &= \\int^{\\infty}_{0} S_x(t) \\\\ & \\approx h[f(a+h) + f(a+2h) + \\dots + f(b)] + \\frac{h}{2} [f(a)-f(b)] \\\\ & \\approx (1) [S_x(1) + S_x(2) + \\dots + S_x(\\infty)] + \\frac{1}{2} [S_x(0)-S_x(\\infty)] \\\\ & \\approx [S_x(1) + S_x(2) + \\dots + S_x(\\infty)] + \\frac{1}{2} [1-0] \\\\ & \\approx e_x + \\frac{1}{2} \\end{aligned} \\] An alternative way to view the above is that the area of the trapezoid is the sum of the riemann rectangles and a triangle .","title":"Discrete Continuous Link"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#euler-maclaurin-formula","text":"Note that the Trapezoidal Rule is not perfect - there is an inherent error in trying to approximate a curve using a line. \\[ \\begin{aligned} \\int^b_a f(x) & \\approx \\sum (\\text{Area of Trapezium} + \\text{Error})\\\\ \\end{aligned} \\] The error can be positive or negative, depending on the shape of the curve in that interval: The error term is calculated by taking the difference between the integral and the trapezium, and is generalized using the taylor series. The final result is known as the Euler Maclaurin Formula : \\[ \\sum \\text{Error} = \\frac{h^2}{12} [f'(a)-f'(b)] \\] \\gt Given that a taylor series was used, the error can be expressed as the sum of many different terms, but higher powers are ignored . Thus, the trapezoidal approximation for the expectation can be made more precise: \\[ \\begin{aligned} \\mathring{e}_x & \\approx e_x + \\frac{1}{2} + \\frac{1^2}{12} [S'(0)-S'(\\infty)] \\\\ & \\approx e_x + \\frac{1}{2} + \\frac{1}{12} [S'(0)] \\end{aligned} \\] The above will rarely be used in this manner - it is more than sufficient to know the relationship between the two expectations of life. However, it sets as a strong foundation to understand the Woolhouse Approximation in the life annuities section.","title":"Euler Maclaurin Formula"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#moment-summary","text":"","title":"Moment Summary"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#parametric-survival-models","text":"Given the importance of \\(\\mu_x\\) , several mathematical functions have been made to describe the force of mortality, known as a Parametric Survival Model .","title":"Parametric Survival Models"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#exponential-distribution","text":"When the force of mortality is constant \\(\\mu_{x} = \\mu\\) , the PDF can be shown to be exponential: \\[ \\begin{aligned} {}_{t}p_x &= e^{- \\int^{t}_{0} \\mu} \\\\ &= e^{- \\mu t}\\\\ \\\\ f(x) &= {}_{}p_x \\cdot \\mu \\\\ &= \\mu \\cdot e^{- \\mu t} \\\\ \\\\ \\therefore T_{x} &\\sim \\text{Exponential}(\\mu) \\\\ E(T_x) &= \\frac{1}{\\mu} \\\\ Var (T_x) &= \\frac{1}{\\mu^2} \\end{aligned} \\] The key is to remember that distribution is Memoryless , which means that the age of the person does not matter: \\[ \\begin{aligned} {}_{n}p_{x} &= {}_{n}p_{x+m} \\\\ \\\\ \\therefore {}_{n}p_{x} &= {}_{}p_{x} \\cdot {}_{}p_{x+1} \\cdot {}_{}p_{x+2} \\cdot \\dots \\\\ &= {}_{}p_{x} \\cdot {}_{}p_{x} \\cdot {}_{}p_{x} \\cdot \\dots \\\\ &= ({}_{}p_{x})^n \\end{aligned} \\] This leads to an interesting relationship between the full and temporary expectation: \\[ \\begin{aligned} \\mathring{e}_{x} &= \\mathring{e}_{x+m} \\\\ \\\\ \\therefore \\mathring{e}_x &= \\mathring{e}_{x:\\enclose{actuarial}{n}} + {}_{n}p_{x} \\cdot \\mathring{e}_{x+n} \\\\ \\mathring{e}_x &= \\mathring{e}_{x:\\enclose{actuarial}{n}} + {}_{n}p_{x} \\cdot \\mathring{e}_{x} \\\\ \\mathring{e}_{x:\\enclose{actuarial}{n}} &= \\mathring{e}_x - {}_{n}p_{x} \\cdot \\mathring{e}_{x} \\\\ \\mathring{e}_{x:\\enclose{actuarial}{n}} &= \\mathring{e}_x \\cdot (1 - {}_{n}p_{x}) \\end{aligned} \\]","title":"Exponential Distribution"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#de-moivres-model","text":"De Moivre's Law assumes that the distribution of a NEWBORN is uniformly distributed between 0 and \\(\\omega\\) , where \\(\\omega\\) is known as the Limiting Age . It can be intepreted as starting with \\(k\\omega\\) people at age 0, where \\(k\\) people die every year till everybody dies by age \\(w\\) . \\[ \\ell_{x} = k(\\omega - x) \\] The PDF can be determined using the survival model or the uniform distribution: \\[ \\begin{aligned} {}_{t}p_x &= \\frac{\\ell_{x+t}}{\\ell_{x}} \\\\ &= \\frac{w-(x+t)}{w-x} \\\\ \\\\ \\mu_{x+t} &= \\frac{1}{w-(x+t)} \\\\ \\\\ \\therefore f(x) &= \\frac{w-(x+t)}{w-x} \\cdot \\frac{1}{w-(x+t)} \\\\ &= \\frac{1}{\\omega-x} \\end{aligned} \\] \\[ \\begin{aligned} T_{0} &\\sim \\text{Uniform}(0, \\omega) \\\\ T_{x} &= T_{0} - x \\mid T_{0} \\gt x \\\\ \\therefore T_{x} &\\sim \\text{Uniform}(0, \\omega-x) \\\\ \\\\ E(T_{x}) &= \\frac{\\omega-x}{2} \\\\ Var(T_{x}) &= \\frac{(\\omega-x)^2}{12} \\end{aligned} \\]","title":"De Moivre's Model"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#gompertz-model","text":"Both the above distributions make unrealistic assumptions: Exponential Distribution : Assume age does not matter Uniform Distribution : Assume equal number of people die eacg tear A more realistic model is the Gompertz Model , which suggests that mortality increases with age : \\[ \\mu_{x} = Bc^{x} \\\\ \\] Thus, the corresponding survival function can be calculated: \\[ \\begin{aligned} {}_{t}p_{x} &= e^{-\\int^{n}_{0} Bc^{x+s}} \\\\ &= e^{-Bc^{x} \\int^{n}_{0} c^{s}} \\\\ &= e^{-Bc^{x} [\\frac{c^s}{\\ln c}]^t_0} \\\\ &= e^{-Bc^{x} [\\frac{c^t}{\\ln c} - \\frac{c^0}{\\ln c}]} \\\\ &= e^{\\frac{-Bc^{x}}{\\ln c} (c^t - 1)} \\end{aligned} \\]","title":"Gompertz Model"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#makeham-gompertz-model","text":"Makeham added a constant \\(A\\) into the Gompertz model, resulting in the Makeham Gompertz Model : \\[ \\mu_{x} = A + B \\cdot c^{x} \\] \\(A\\) represents the age independent mortality - dying from reasons unrelated to age, such as from accidents or natural catastrophes. \\[ \\begin{aligned} S_x(t) &= e^{- \\int^{t}_{0} A + Bc^{x+t}} \\\\ &= e^{\\int^{t}_{0} -A - Bc^{x+t}} \\\\ &= e^{\\left[-At - \\frac{B}{\\ln c} c^{x+t} \\right]^{t}_{0}} \\\\ &= e^{\\left[-At - \\frac{B}{\\ln c} c^{x+t} \\right] - \\left[-\\frac{B}{\\ln c} c^{x} \\right]} \\\\ &= e^{-At - \\frac{B}{\\ln c} c^{x+t} + \\frac{B}{\\ln c} c^{x}} \\\\ &= e^{-At - \\frac{B}{\\ln c} c^{x} [c^t - 1]} \\\\ &= e^{-At} \\cdot e^{- \\frac{B}{\\ln c} c^{x} [c^{t} - 1]} \\end{aligned} \\] Tip If the force of mortality provided follow a similar expression as above, immediately recognize that this is the makeham gompertz model. Do NOT waste time attempting to integrate the force; simply use the formula provided in the formula sheet to calculate the probabilities. Any modification made to the force should be expressed as a change in the Makeham parameters: \\[ \\begin{aligned} \\mu_{x} &= A + B \\cdot c^{x} \\\\ \\\\ \\mu_{y} &= \\mu_{x} + Z \\\\ \\therefore \\mu_{y} &= (A+Z) + B \\cdot c^{x} \\\\ \\\\ \\mu_{Z} &= \\mu_{x} + Z \\cdot B \\cdot c^{x} \\\\ \\end{aligned} \\]","title":"Makeham Gompertz Model"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/1.%20Survival%20Models/#population-mortality","text":"The Makeham Gompertz Model is ideal because it depicts human mortality fairly accurately: There are a few key aspects to take note of: Gender Difference : Males have higher mortality Perinatal Mortality : Infants with birth issues likely to die, mortality decreases after Accident Hump : Mortality increases past age 10, more sharply in males due to accidents","title":"Population Mortality"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/2.%20Life%20Tables/","text":"Life Tables \u00b6 Overview \u00b6 A Life Table is a convenient way of summarizing the results of the survival model. As such, they are equivalent ways of obtaining the same results. Basic Functions \u00b6 The life table is constructed based on the mortality of a homogenous group of people known as the Cohort . Info In practice, multiple life tables are created for different groups of individuals with different risks (Male vs Female, Smoker vs Non Smoker). The initial age of the group people is known as the Starting Age , denoted by \\(\\alpha\\) . The initial number of people in the cohort is known as the Radix , which is usually a large round number (EG. 1000). Conversely, the maximum age is known as the Limiting Age , denoted by \\(\\omega\\) . Everybody in the cohort is expected to gradually die by the terminal age . The expected number of people who are alive at age \\(x\\) is denoted as \\(\\ell_x\\) : \\[ \\begin{aligned} \\ell_0 &= 1000 \\\\ \\ell_{\\omega} &= 0 \\\\ \\\\ \\therefore \\ell_{x+t} &< \\ell_{x} \\end{aligned} \\] From this basic life table, several important values can be calculated: The expected number of deaths over a period, \\({}_{n}d_{x}\\) The probability of survival past a period, \\({}_{n}p_{x}\\) The probability of death within a period, \\({}_{n}q_{x}\\) The deferred probability of death within a period, \\({}_{s|t}q_{x}\\) \\[ \\begin{aligned} {}_{n}d_{x} &= \\ell_{x} - \\ell_{x+n} \\\\ {}_{n}p_{x} &= \\frac{\\ell_{x+t}}{\\ell_{x}} \\\\ {}_{n}q_{x} &= \\frac{{}_{n}d_{x}}{\\ell_{x}} \\\\ {}_{s|t}q_{x} &= \\frac{{}_{t}d_{x+s}}{l_{x}} \\\\ \\end{aligned} \\] Thus, it can be seen that the probabilities are calculated from first principles as the proportion of people who died/survived over the a given period. Warning It is a common mistake when calculating the number of deaths to take the number of survivors at the higher age minus the lower age. This is because we intuitively think \"high minus low\". However, since the number of survivors is a decreasing function, it should be the other way around! Binomial Distribution \u00b6 Intuitively, each person in the cohort can either be Alive or Dead . Thus, their status can be represented by a Bernoulli Variable : \\[ \\begin{aligned} \\text{Status} &= \\begin{cases} 0 \\ (\\text{Dead}),& {}_{n}q_{x} \\\\ 1 \\ (\\text{Alive}),& {}_{n}p_{x} \\end{cases} \\end{aligned} \\] Assuming that the lifetimes of each person in the cohort is independent of one another, the total number of people alive is the sum of independent bernoulli variables , thus is binomially distributed : \\[ \\begin{aligned} \\ell_{t} &\\sim \\text{Binomial}(\\ell_0, {}_{n}p_{x}) \\\\ \\\\ E(N) &= \\ell_0 \\cdot {}_{n}p_{x} \\\\ Var(N) &= \\ell_0 \\cdot {}_{n}p_{x} \\cdot (1-{}_{n}p_{x}) \\\\ \\\\ \\therefore \\ell_{x} &= E(N) \\end{aligned} \\] Given a sufficiently large population, the binomial distributed can be approximated using the normal distribution , with continuity correction applied. For more information on CC, refer to the FAM-S section of the notes. \\[ N \\sim N(E(\\ell_{t}), Var(\\ell_{t})) \\] Note This further emphasizes that the number of survivors in the life table are the expected number of survivors; there is no guarantee that a given population will exactly follow the life table. Thus, questions involving \\(\\ell_{x}\\) can ask for probabilities and variance , which must be solved via this approach. Non-Homogenous Cohort \u00b6 Some questions may provide non-homogenous cohorts where there are different groups of people with different mortality rates, but ask for the overall number of people. Each group is modelled using their own binomial distribution based on their respective mortalities and are assumed to be independent of one another. Usually, the sum of independent binomial variables with the same probabilities will be binomially distributed. However, note that since each group has different survival probabilities , the combined distribution is NOT binomial ! Thus, the normal approximation is usually used for these cases: \\[ \\begin{aligned} N &= N_1 + N_2 \\\\ \\\\ E(N) &= E(N_1) + E(N_2) \\\\ Var(N) &= Var(N_1) + Var (N_2) \\\\ \\\\ N &\\sim N(E(N), Var(N)) \\end{aligned} \\] Fractional Age Assumptions \u00b6 One limitation of the life table is that it is computed at discrete ages while many problems require probabilities for non-discrete ages. Thus, several assumptions about the life table that allows non-discrete values to be interpolated from the discrete ones, known as Fractional Age Assumptions . Uniform Distribution of Deaths \u00b6 The Uniform Distribution of Deaths (UDD) is an assumption that allows for Linear Interpolation between discrete ages. It assumes that there is a uniform distribution between integer ages, such that people are dying at a constant rate over the year; the number of survivors decreases linearly over the year. Note UDD is a valid assumption becomes the probability of dying in one month is not significantly different from another month. However, it becomes less appropriate at older ages as the probability death increases exponentially, even from month to month. The number of survivors can thus be calculated via linear interpolation : \\[ \\ell_{x+s} = (1-s) \\cdot \\ell_{x} + s \\cdot \\ell_{x+1} \\\\ \\] Warning The weights of the interpolation are seemingly counter-intuitive and are thus easily mixed up. Most people would take \\(s\\) for \\(\\ell_{x}\\) because they are on the same side of the interpolation. However, a more intuitive way to think about it is that if the person is closer to age \\(x\\) , then they should have a larger weight applied to that age ! This means that the larger weight should be applied to the boundary age that the estimated age is nearest to. Firstly, we calculate the probability of surviving a fractional amount of time from a discrete age : \\[ \\begin{aligned} {}_{s}p_{x} &= \\frac{\\ell_{x+t}}{\\ell_{x}} \\\\ &= \\frac{(1-s) \\cdot \\ell_{x} + s \\cdot \\ell_{x}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}-s\\ell_{x} + s \\ell_{x+1}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s(\\ell_{x}-\\ell_{x+1})}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s d_{x}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}}{\\ell_{x}} - s\\frac{d_{x}}{\\ell_{x}} \\\\ &= 1 - s q_{x} \\\\ \\\\ \\therefore {}_{s}q_{x} &= s \\cdot q_x \\end{aligned} \\] Tip For most questions, it is sufficient to know the final result in terms of probabilities. However, if the question requires the fractional probability at a fractional age , it is much easier to use the first principles method based on the number of lives . Thus, make a conscious effort to remember both the probability and number of lives approximations. Following the same logic, we calculate the probability of surviving a fractional amount of time from a fractional age : \\[ \\begin{aligned} {}_{t}p_{x+s} &= \\frac{\\ell_{x+s+t}}{\\ell_{x+s}} \\\\ &= \\frac{[1-(s+t)] \\cdot \\ell_{x} + (s+t) \\cdot \\ell_{x+1}}{(1-s) \\cdot \\ell_{x} + s \\cdot \\ell_{x}} \\end{aligned} \\] Finally, the PDF and Force of Mortality can also be determined: \\[ \\begin{aligned} F_x(s) &= s \\cdot q_x \\\\ \\\\ f_x(s) &= \\frac{d}{ds} (s \\cdot q_x) \\\\ &= q_x \\\\ \\\\ \\mu_{x+t} &= \\frac{f_x(s)}{S_x(s)} \\\\ &= \\frac{q_x}{1-sq_x} \\end{aligned} \\] Impact on Expectation \u00b6 We can decompose the continuous lifetime into an integer and a fractional component \\((R_x)\\) : \\[ T_x = K_x + R_x \\] Under UDD, we know that the fractional component is uniformly distributed between 0 and 1 : \\[ \\begin{aligned} R_x &\\sim \\text{Uniform}(0, 1) \\\\ E(R_x) &= \\frac{0 + 1}{2} \\\\ &= \\frac{1}{2} \\end{aligned} \\] Thus, we can derive a formula for the continuous expectation: \\[ \\begin{aligned} E(T_x) &= E(K_x) + E(R_x) \\\\ \\mathring{e}_{x} &= e_{x} + \\frac{1}{2} \\end{aligned} \\] Tip In the previous section, we showed that the above relationship was approximately true using the trapezium rule. Under UDD, this relationship is exact . Thus, if a question requires the above formula, they should explicitly state that UDD holds. Similarly, this impacts the discrete recursion formula. The intuition is that since the person could die uniformly throughout the year, on average , they would die in the middle of the year and thus would have had half a year of future lifetime : \\[ \\begin{aligned} e_{x:\\enclose{actuarial}{1}} &= \\begin{cases} 1, {}_{}p_{x} \\\\ 0.5, {}_{}q_{x} \\end{cases} \\\\ \\\\ e_{x:\\enclose{actuarial}{1}} &= 1 \\cdot {}_{}p_{x} + 0.5 \\cdot {}_{}q_{x} \\\\ &= {}_{}p_{x} + 0.5 \\cdot {}_{}q_{x} \\\\ \\\\ \\therefore e_x &= e_{x:\\enclose{actuarial}{1}} + {}_{}p_{x} \\cdot e_{x+1} \\\\ &= {}_{}p_{x} + 0.5 \\cdot {}_{}q_{x} + {}_{}p_x \\cdot e_{x+1} \\\\ &= 0.5 \\cdot {}_{}q_{x} + {}_{}p_{x} (1 + e_{x+1}) \\end{aligned} \\] Constant Force of Mortality \u00b6 The Constant Force of Mortality is an assumption that allows for Exponential Interpolation between discrete ages. It assumes that there is a constant force of mortality between integer ages , such that the number of survivors decreases exponentially over the year. The number of survivors can thus be calculated via exponential interpolation: \\[ \\ell_{x+s} = \\ell_{x}^{1-s} \\cdot \\ell_{x+1}^{s} \\\\ \\] Warning Similar to before, the larger weight should be applied to the boundary age that the estimated age is nearest to. Firstly, we calculate the probability of surviving a fractional amount of time from a discrete age : \\[ \\begin{aligned} {}_{s}p_{x} &= \\frac{\\ell_{x+s}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}^{1-s} \\cdot \\ell_{x+1}^{s}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x+1}^t}{\\ell_{x}^t} \\\\ &= \\left(\\frac{\\ell_{x+1}}{\\ell_{x}} \\right)^t \\\\ &= ({}_{}p_{x})^t \\\\ \\\\ \\therefore {}_{s}q_{x} &= 1 - ({}_{}p_{x})^t \\end{aligned} \\] Warning This result should not be surprising, as having a constant force means that survival within the year is exponentialy distributed . However, note that ONLY survival WITHIN the year is exponentially distributed - it is similar but NOT the same as assuming that the entire survival is exponentially distributed. Following the same logic, we calculate the probability of surviving a fractional amount of time from a fractional age : \\[ \\begin{aligned} {}_{t}p_{x+s} &= \\frac{\\ell_{x+s+t}}{\\ell_{x+s}} \\\\ &= \\frac{\\ell_{x}^{1-(s+t)} \\cdot \\ell_{x}^{s+t}}{\\ell_{x}^{1-s} \\cdot \\ell_{x+1}^{s}} \\\\ &= \\frac{\\ell_{x+1}^t}{\\ell_{x}^t} \\\\ &= \\left(\\frac{\\ell_{x+1}}{\\ell_{x}} \\right)^t \\\\ &= ({}_{}p_{x})^t \\end{aligned} \\] Note This further drives home the fact age does not matter , even at fractional ages, due to the memoryless property. Given Probabilities \u00b6 If mortality follows the SULT, then the number of survivors can easily be obtained from there, allowing us to easily perform the above calculations. However, some questions might provide their own mortality table. Some of these mortality tables will be provided in terms of number of survivors but sometimes they are provided in terms of probabilities. In such cases, we can convert the probabilities to the number of survivors by choosing an arbitrary number of survivors for the first age provided and then multiplying the probabilities accordingly. This way, we only need to remember the formulas using the number of survivors. Other Assumptions \u00b6 The two assumptions are above are the typical fractional age assumptions. In practice, questions could use a variety of assumptions. Instead of a completely constant force, some questions might provide a linearly increasing force between two different constant forces. Essentially, this combines the key aspects of the above two methods: \\[ \\mu_{x+t} = \\mu_1 \\cdot (1-t) + \\mu_2 \\cdot t \\] Thus, the survival probability during this period can be calculated via first principles: \\[ \\begin{aligned} S_X(t) &= e^{\\int \\mu_{x+t}} \\\\ &= e^{\\int \\mu_1 \\cdot (1-t) + \\mu_2 \\cdot t} \\end{aligned} \\] Another assumption is that a certain proportion of deaths occur in a certain portion of the year. For instance, \"Between integer ages, two-thirds of deaths occur during the last six months of the year\". \\[ \\begin{aligned} {}_{0.5}q_{x} &= \\frac{1}{3} \\cdot q_{x} \\\\ {}_{0.5}q_{x+0.5} &= \\frac{2}{3} \\cdot q_{x} \\end{aligned} \\] Select & Ultimate Mortality \u00b6 Mortality rates for the general population versus individuals who buy life insurance tend to be different. Generally speaking, people who purchase insurance tend to be richer and thus have better mortality rates than the general population . Within the individuals who purchase life insurance, those who have recently purchased a policy tend to have better mortality . This is because these individuals would have gone through Medical Underwriting and thus is expected to be in better health. Tip \"Better\" mortality can be understood as having a higher chance to survive or lower chance to die. The extent of the better mortality decreases over time and after a few years, they should experience the same mortality as the rest of the individuals who purchased life insurance. The duration of time is dependent on the rigorousness of the underwriting process . Formally speaking, the individuals who purchased life insurance were selected by the underwriting process and thus the better mortality experienced is known as Select Mortality . Similarly, the time whereby the select mortality is better is known as the Select Period . The select mortality ultimately converges with the non-select group, known as the Ultimate Mortality . Warning The selection age is tied to when they went through underwriting, NOT when the policy is ultimately written. Intepretation \u00b6 The subscript \\([x]\\) is used to denote the age that the person was selected . The years following that are represented by \\([x]+1, [x]+2, \\dots\\) . Note the difference in intepretation: \\({}_{n}p_{x}\\) is the probability that a person aged \\(x\\) survives \\(n\\) years to age \\(x+n\\) . \\({}_{n}p_{[x]}\\) is the probability that a person aged \\(x\\) , that was selected at age \\(x\\) , survives \\(n\\) years to age \\(x+n\\) . \\({}_{n}p_{[x]+m}\\) is the probability that a person aged \\(x+m\\) , that was selected age \\(x\\) , survives \\(n\\) years to age \\(x+m+n\\) . The key properties of select and ultimate mortality described previously can also be put into mathematical form. Let the select period be denoted by \\(d\\) . Firstly, before the select period finishes \\((t \\lt d)\\) , select mortality is always better than ultimate mortality: \\[ \\begin{aligned} q_{[x]+t} &\\lt q_{x+t} \\\\ p_{[x]+t} &\\gt p_{x+t} \\end{aligned} \\] Next, the effect of selection decreases over time; more recently selected people have better mortality: \\[ \\begin{aligned} {}_{}q_{[x]} &\\lt {}_{}q_{[x]+t} \\\\ {}_{}p_{[x]} &\\gt {}_{}p_{[x]+t} \\end{aligned} \\] Lastly, once the select period finishes \\((t \\gt d)\\) , it is the same as ultimate mortality: \\[ \\begin{aligned} q_{[x]+t} &= q_{x+t} \\\\ p_{[x]+t} &= p_{x+t} \\end{aligned} \\] Select & Ultimate mortality is usually given in tabular form. Thus, the proper way to read the table would be left to right during the select period and then downwards after the select period : Alternative Intepretation \u00b6 Although the above method of denoting select and ultimate mortality is more common, an alternative intepretation is for \\([x]\\) to denote the ultimate age instead . The years before are represented by \\([x], [x-1] + 1, [x-2] + 2, \\dots\\) . Warning Given this possible alternative intepretation, always remember to check which notation the question specified. Thus, the alternative way to read the table would be diagonally to the bottom right during the select period and then downwards after the select period :","title":"Life Tables"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/2.%20Life%20Tables/#life-tables","text":"","title":"Life Tables"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/2.%20Life%20Tables/#overview","text":"A Life Table is a convenient way of summarizing the results of the survival model. As such, they are equivalent ways of obtaining the same results.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/2.%20Life%20Tables/#basic-functions","text":"The life table is constructed based on the mortality of a homogenous group of people known as the Cohort . Info In practice, multiple life tables are created for different groups of individuals with different risks (Male vs Female, Smoker vs Non Smoker). The initial age of the group people is known as the Starting Age , denoted by \\(\\alpha\\) . The initial number of people in the cohort is known as the Radix , which is usually a large round number (EG. 1000). Conversely, the maximum age is known as the Limiting Age , denoted by \\(\\omega\\) . Everybody in the cohort is expected to gradually die by the terminal age . The expected number of people who are alive at age \\(x\\) is denoted as \\(\\ell_x\\) : \\[ \\begin{aligned} \\ell_0 &= 1000 \\\\ \\ell_{\\omega} &= 0 \\\\ \\\\ \\therefore \\ell_{x+t} &< \\ell_{x} \\end{aligned} \\] From this basic life table, several important values can be calculated: The expected number of deaths over a period, \\({}_{n}d_{x}\\) The probability of survival past a period, \\({}_{n}p_{x}\\) The probability of death within a period, \\({}_{n}q_{x}\\) The deferred probability of death within a period, \\({}_{s|t}q_{x}\\) \\[ \\begin{aligned} {}_{n}d_{x} &= \\ell_{x} - \\ell_{x+n} \\\\ {}_{n}p_{x} &= \\frac{\\ell_{x+t}}{\\ell_{x}} \\\\ {}_{n}q_{x} &= \\frac{{}_{n}d_{x}}{\\ell_{x}} \\\\ {}_{s|t}q_{x} &= \\frac{{}_{t}d_{x+s}}{l_{x}} \\\\ \\end{aligned} \\] Thus, it can be seen that the probabilities are calculated from first principles as the proportion of people who died/survived over the a given period. Warning It is a common mistake when calculating the number of deaths to take the number of survivors at the higher age minus the lower age. This is because we intuitively think \"high minus low\". However, since the number of survivors is a decreasing function, it should be the other way around!","title":"Basic Functions"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/2.%20Life%20Tables/#binomial-distribution","text":"Intuitively, each person in the cohort can either be Alive or Dead . Thus, their status can be represented by a Bernoulli Variable : \\[ \\begin{aligned} \\text{Status} &= \\begin{cases} 0 \\ (\\text{Dead}),& {}_{n}q_{x} \\\\ 1 \\ (\\text{Alive}),& {}_{n}p_{x} \\end{cases} \\end{aligned} \\] Assuming that the lifetimes of each person in the cohort is independent of one another, the total number of people alive is the sum of independent bernoulli variables , thus is binomially distributed : \\[ \\begin{aligned} \\ell_{t} &\\sim \\text{Binomial}(\\ell_0, {}_{n}p_{x}) \\\\ \\\\ E(N) &= \\ell_0 \\cdot {}_{n}p_{x} \\\\ Var(N) &= \\ell_0 \\cdot {}_{n}p_{x} \\cdot (1-{}_{n}p_{x}) \\\\ \\\\ \\therefore \\ell_{x} &= E(N) \\end{aligned} \\] Given a sufficiently large population, the binomial distributed can be approximated using the normal distribution , with continuity correction applied. For more information on CC, refer to the FAM-S section of the notes. \\[ N \\sim N(E(\\ell_{t}), Var(\\ell_{t})) \\] Note This further emphasizes that the number of survivors in the life table are the expected number of survivors; there is no guarantee that a given population will exactly follow the life table. Thus, questions involving \\(\\ell_{x}\\) can ask for probabilities and variance , which must be solved via this approach.","title":"Binomial Distribution"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/2.%20Life%20Tables/#non-homogenous-cohort","text":"Some questions may provide non-homogenous cohorts where there are different groups of people with different mortality rates, but ask for the overall number of people. Each group is modelled using their own binomial distribution based on their respective mortalities and are assumed to be independent of one another. Usually, the sum of independent binomial variables with the same probabilities will be binomially distributed. However, note that since each group has different survival probabilities , the combined distribution is NOT binomial ! Thus, the normal approximation is usually used for these cases: \\[ \\begin{aligned} N &= N_1 + N_2 \\\\ \\\\ E(N) &= E(N_1) + E(N_2) \\\\ Var(N) &= Var(N_1) + Var (N_2) \\\\ \\\\ N &\\sim N(E(N), Var(N)) \\end{aligned} \\]","title":"Non-Homogenous Cohort"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/2.%20Life%20Tables/#fractional-age-assumptions","text":"One limitation of the life table is that it is computed at discrete ages while many problems require probabilities for non-discrete ages. Thus, several assumptions about the life table that allows non-discrete values to be interpolated from the discrete ones, known as Fractional Age Assumptions .","title":"Fractional Age Assumptions"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/2.%20Life%20Tables/#uniform-distribution-of-deaths","text":"The Uniform Distribution of Deaths (UDD) is an assumption that allows for Linear Interpolation between discrete ages. It assumes that there is a uniform distribution between integer ages, such that people are dying at a constant rate over the year; the number of survivors decreases linearly over the year. Note UDD is a valid assumption becomes the probability of dying in one month is not significantly different from another month. However, it becomes less appropriate at older ages as the probability death increases exponentially, even from month to month. The number of survivors can thus be calculated via linear interpolation : \\[ \\ell_{x+s} = (1-s) \\cdot \\ell_{x} + s \\cdot \\ell_{x+1} \\\\ \\] Warning The weights of the interpolation are seemingly counter-intuitive and are thus easily mixed up. Most people would take \\(s\\) for \\(\\ell_{x}\\) because they are on the same side of the interpolation. However, a more intuitive way to think about it is that if the person is closer to age \\(x\\) , then they should have a larger weight applied to that age ! This means that the larger weight should be applied to the boundary age that the estimated age is nearest to. Firstly, we calculate the probability of surviving a fractional amount of time from a discrete age : \\[ \\begin{aligned} {}_{s}p_{x} &= \\frac{\\ell_{x+t}}{\\ell_{x}} \\\\ &= \\frac{(1-s) \\cdot \\ell_{x} + s \\cdot \\ell_{x}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}-s\\ell_{x} + s \\ell_{x+1}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s(\\ell_{x}-\\ell_{x+1})}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x} - s d_{x}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}}{\\ell_{x}} - s\\frac{d_{x}}{\\ell_{x}} \\\\ &= 1 - s q_{x} \\\\ \\\\ \\therefore {}_{s}q_{x} &= s \\cdot q_x \\end{aligned} \\] Tip For most questions, it is sufficient to know the final result in terms of probabilities. However, if the question requires the fractional probability at a fractional age , it is much easier to use the first principles method based on the number of lives . Thus, make a conscious effort to remember both the probability and number of lives approximations. Following the same logic, we calculate the probability of surviving a fractional amount of time from a fractional age : \\[ \\begin{aligned} {}_{t}p_{x+s} &= \\frac{\\ell_{x+s+t}}{\\ell_{x+s}} \\\\ &= \\frac{[1-(s+t)] \\cdot \\ell_{x} + (s+t) \\cdot \\ell_{x+1}}{(1-s) \\cdot \\ell_{x} + s \\cdot \\ell_{x}} \\end{aligned} \\] Finally, the PDF and Force of Mortality can also be determined: \\[ \\begin{aligned} F_x(s) &= s \\cdot q_x \\\\ \\\\ f_x(s) &= \\frac{d}{ds} (s \\cdot q_x) \\\\ &= q_x \\\\ \\\\ \\mu_{x+t} &= \\frac{f_x(s)}{S_x(s)} \\\\ &= \\frac{q_x}{1-sq_x} \\end{aligned} \\]","title":"Uniform Distribution of Deaths"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/2.%20Life%20Tables/#impact-on-expectation","text":"We can decompose the continuous lifetime into an integer and a fractional component \\((R_x)\\) : \\[ T_x = K_x + R_x \\] Under UDD, we know that the fractional component is uniformly distributed between 0 and 1 : \\[ \\begin{aligned} R_x &\\sim \\text{Uniform}(0, 1) \\\\ E(R_x) &= \\frac{0 + 1}{2} \\\\ &= \\frac{1}{2} \\end{aligned} \\] Thus, we can derive a formula for the continuous expectation: \\[ \\begin{aligned} E(T_x) &= E(K_x) + E(R_x) \\\\ \\mathring{e}_{x} &= e_{x} + \\frac{1}{2} \\end{aligned} \\] Tip In the previous section, we showed that the above relationship was approximately true using the trapezium rule. Under UDD, this relationship is exact . Thus, if a question requires the above formula, they should explicitly state that UDD holds. Similarly, this impacts the discrete recursion formula. The intuition is that since the person could die uniformly throughout the year, on average , they would die in the middle of the year and thus would have had half a year of future lifetime : \\[ \\begin{aligned} e_{x:\\enclose{actuarial}{1}} &= \\begin{cases} 1, {}_{}p_{x} \\\\ 0.5, {}_{}q_{x} \\end{cases} \\\\ \\\\ e_{x:\\enclose{actuarial}{1}} &= 1 \\cdot {}_{}p_{x} + 0.5 \\cdot {}_{}q_{x} \\\\ &= {}_{}p_{x} + 0.5 \\cdot {}_{}q_{x} \\\\ \\\\ \\therefore e_x &= e_{x:\\enclose{actuarial}{1}} + {}_{}p_{x} \\cdot e_{x+1} \\\\ &= {}_{}p_{x} + 0.5 \\cdot {}_{}q_{x} + {}_{}p_x \\cdot e_{x+1} \\\\ &= 0.5 \\cdot {}_{}q_{x} + {}_{}p_{x} (1 + e_{x+1}) \\end{aligned} \\]","title":"Impact on Expectation"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/2.%20Life%20Tables/#constant-force-of-mortality","text":"The Constant Force of Mortality is an assumption that allows for Exponential Interpolation between discrete ages. It assumes that there is a constant force of mortality between integer ages , such that the number of survivors decreases exponentially over the year. The number of survivors can thus be calculated via exponential interpolation: \\[ \\ell_{x+s} = \\ell_{x}^{1-s} \\cdot \\ell_{x+1}^{s} \\\\ \\] Warning Similar to before, the larger weight should be applied to the boundary age that the estimated age is nearest to. Firstly, we calculate the probability of surviving a fractional amount of time from a discrete age : \\[ \\begin{aligned} {}_{s}p_{x} &= \\frac{\\ell_{x+s}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x}^{1-s} \\cdot \\ell_{x+1}^{s}}{\\ell_{x}} \\\\ &= \\frac{\\ell_{x+1}^t}{\\ell_{x}^t} \\\\ &= \\left(\\frac{\\ell_{x+1}}{\\ell_{x}} \\right)^t \\\\ &= ({}_{}p_{x})^t \\\\ \\\\ \\therefore {}_{s}q_{x} &= 1 - ({}_{}p_{x})^t \\end{aligned} \\] Warning This result should not be surprising, as having a constant force means that survival within the year is exponentialy distributed . However, note that ONLY survival WITHIN the year is exponentially distributed - it is similar but NOT the same as assuming that the entire survival is exponentially distributed. Following the same logic, we calculate the probability of surviving a fractional amount of time from a fractional age : \\[ \\begin{aligned} {}_{t}p_{x+s} &= \\frac{\\ell_{x+s+t}}{\\ell_{x+s}} \\\\ &= \\frac{\\ell_{x}^{1-(s+t)} \\cdot \\ell_{x}^{s+t}}{\\ell_{x}^{1-s} \\cdot \\ell_{x+1}^{s}} \\\\ &= \\frac{\\ell_{x+1}^t}{\\ell_{x}^t} \\\\ &= \\left(\\frac{\\ell_{x+1}}{\\ell_{x}} \\right)^t \\\\ &= ({}_{}p_{x})^t \\end{aligned} \\] Note This further drives home the fact age does not matter , even at fractional ages, due to the memoryless property.","title":"Constant Force of Mortality"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/2.%20Life%20Tables/#given-probabilities","text":"If mortality follows the SULT, then the number of survivors can easily be obtained from there, allowing us to easily perform the above calculations. However, some questions might provide their own mortality table. Some of these mortality tables will be provided in terms of number of survivors but sometimes they are provided in terms of probabilities. In such cases, we can convert the probabilities to the number of survivors by choosing an arbitrary number of survivors for the first age provided and then multiplying the probabilities accordingly. This way, we only need to remember the formulas using the number of survivors.","title":"Given Probabilities"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/2.%20Life%20Tables/#other-assumptions","text":"The two assumptions are above are the typical fractional age assumptions. In practice, questions could use a variety of assumptions. Instead of a completely constant force, some questions might provide a linearly increasing force between two different constant forces. Essentially, this combines the key aspects of the above two methods: \\[ \\mu_{x+t} = \\mu_1 \\cdot (1-t) + \\mu_2 \\cdot t \\] Thus, the survival probability during this period can be calculated via first principles: \\[ \\begin{aligned} S_X(t) &= e^{\\int \\mu_{x+t}} \\\\ &= e^{\\int \\mu_1 \\cdot (1-t) + \\mu_2 \\cdot t} \\end{aligned} \\] Another assumption is that a certain proportion of deaths occur in a certain portion of the year. For instance, \"Between integer ages, two-thirds of deaths occur during the last six months of the year\". \\[ \\begin{aligned} {}_{0.5}q_{x} &= \\frac{1}{3} \\cdot q_{x} \\\\ {}_{0.5}q_{x+0.5} &= \\frac{2}{3} \\cdot q_{x} \\end{aligned} \\]","title":"Other Assumptions"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/2.%20Life%20Tables/#select-ultimate-mortality","text":"Mortality rates for the general population versus individuals who buy life insurance tend to be different. Generally speaking, people who purchase insurance tend to be richer and thus have better mortality rates than the general population . Within the individuals who purchase life insurance, those who have recently purchased a policy tend to have better mortality . This is because these individuals would have gone through Medical Underwriting and thus is expected to be in better health. Tip \"Better\" mortality can be understood as having a higher chance to survive or lower chance to die. The extent of the better mortality decreases over time and after a few years, they should experience the same mortality as the rest of the individuals who purchased life insurance. The duration of time is dependent on the rigorousness of the underwriting process . Formally speaking, the individuals who purchased life insurance were selected by the underwriting process and thus the better mortality experienced is known as Select Mortality . Similarly, the time whereby the select mortality is better is known as the Select Period . The select mortality ultimately converges with the non-select group, known as the Ultimate Mortality . Warning The selection age is tied to when they went through underwriting, NOT when the policy is ultimately written.","title":"Select &amp; Ultimate Mortality"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/2.%20Life%20Tables/#intepretation","text":"The subscript \\([x]\\) is used to denote the age that the person was selected . The years following that are represented by \\([x]+1, [x]+2, \\dots\\) . Note the difference in intepretation: \\({}_{n}p_{x}\\) is the probability that a person aged \\(x\\) survives \\(n\\) years to age \\(x+n\\) . \\({}_{n}p_{[x]}\\) is the probability that a person aged \\(x\\) , that was selected at age \\(x\\) , survives \\(n\\) years to age \\(x+n\\) . \\({}_{n}p_{[x]+m}\\) is the probability that a person aged \\(x+m\\) , that was selected age \\(x\\) , survives \\(n\\) years to age \\(x+m+n\\) . The key properties of select and ultimate mortality described previously can also be put into mathematical form. Let the select period be denoted by \\(d\\) . Firstly, before the select period finishes \\((t \\lt d)\\) , select mortality is always better than ultimate mortality: \\[ \\begin{aligned} q_{[x]+t} &\\lt q_{x+t} \\\\ p_{[x]+t} &\\gt p_{x+t} \\end{aligned} \\] Next, the effect of selection decreases over time; more recently selected people have better mortality: \\[ \\begin{aligned} {}_{}q_{[x]} &\\lt {}_{}q_{[x]+t} \\\\ {}_{}p_{[x]} &\\gt {}_{}p_{[x]+t} \\end{aligned} \\] Lastly, once the select period finishes \\((t \\gt d)\\) , it is the same as ultimate mortality: \\[ \\begin{aligned} q_{[x]+t} &= q_{x+t} \\\\ p_{[x]+t} &= p_{x+t} \\end{aligned} \\] Select & Ultimate mortality is usually given in tabular form. Thus, the proper way to read the table would be left to right during the select period and then downwards after the select period :","title":"Intepretation"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/2.%20Life%20Tables/#alternative-intepretation","text":"Although the above method of denoting select and ultimate mortality is more common, an alternative intepretation is for \\([x]\\) to denote the ultimate age instead . The years before are represented by \\([x], [x-1] + 1, [x-2] + 2, \\dots\\) . Warning Given this possible alternative intepretation, always remember to check which notation the question specified. Thus, the alternative way to read the table would be diagonally to the bottom right during the select period and then downwards after the select period :","title":"Alternative Intepretation"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/","text":"Life Assurances \u00b6 Overview \u00b6 Life Assurances are contracts that promise to pay out a benefit \\(B\\) on either the death or survival of the policyholder. The value of a life assurance must reflect these two aspects: Uncertainty of payments - Expected Value , based on Survival Models Time value of money - Present Value , based on Interest Theory Thus, the value of a life assurance is best quantified through the Expected Present Value (EPV) of the promised payouts. Payable Discretely \u00b6 The simplest form of life assurance pays the benefits at the end of the year that the insured event occurs. Since we are only concerned with the status of the insured at discrete integer ages , \\(K_x\\) is used as the survival model. Let \\(Z\\) be the random variable denoting present value of the benefit payable: \\[ Z = B \\cdot v^{K_x + 1} \\] Thus, the EPV is the expectation of \\(Z\\) ; it is the sumproduct of ALL possible present values and their associated probabilities: \\[ \\begin{aligned} \\text{EPV} &= \\sum B \\cdot v^{k + 1} \\cdot {}_{k}p_{x} {}_{}q_{x + k} \\\\ E(Z) &= \\sum B \\cdot v^{k + 1} \\cdot {}_{k \\mid}q_{x} \\end{aligned} \\] Note The associated probability is the probability of dying in each period, given that they survived till that period . It is essentially the deferred probability of death for each period. For simplicity, the benefits are usually assumed to be 1 ( \\(B = 1\\) ). This allows the EPVs to be easily scaled for any level of \\(B\\) required. Actuarial Notation \u00b6 Similar to the survival model, given how often these values are calculated, they are abbreviated using the International Actuarial Notation as well. \\(A\\) represents the first moment (expectation) of the present value of a contract where a benefit of 1 is payable discretely. The subscript \\(x\\) represents the age of the policyholder while the additional subscript \\(:\\enclose{actuarial}{n}\\) represents the duration the contract. Note To be more precise, the above subscripts are known as a Status . They indicate whether a certain condition is fulfulled : \\(x\\) is known as the Life Status which is active as long as they are alive and fails when they die . \\(n\\) is known as the Duration Status which is active during the term and fails when the term is finished . \\(A\\) is an assurance that pays benefits upon the failure of the contract's status . Thus, \\(T_{x}\\) more generally measures the time to failure of an entity. If a contract has multiple statuses, then the overall status fails as long as one of the underlying statuses fails . However, if the order of the failure matters, then a \\(1\\) can be placed above a particular status to indicate that the overall status only fails if that status fails FIRST . Thus, putting everything together, the EPV of each assurance can be denoted as follows: Whole Life Assurance - Payable whenever policyholder dies; \\(A_x\\) Term Assurance - Payable only if policyholder dies during assurance period; \\(A^{1}_{x:\\enclose{actuarial}{n}}\\) Pure Endowment - Payable only if policyholder survives past assurance period; \\(A^{\\>\\>\\> 1}_{x:\\enclose{actuarial}{n}}\\) Endowment Assurance - Payable whichever of the above two occur first; \\(A_{x:\\enclose{actuarial}{n}}\\) Deferred Whole Life Assurance - Payable only if policyholder dies after \\(n\\) years; \\({}_{n|}A_x\\) Tip Pure Endowments can also be expressed as \\({}_{n}E_x\\) for easier typesetting and is the method that SOA usually uses to denote PEs. Whole Life Assurance \u00b6 Whole Life Assurances cover the insured indefinitely and thus will pay out whenever the insured dies. Let WL be the RV denoting the PV of the death benefit: \\[ \\text{WL} = v^{K_x + 1} \\] Thus, its EPV can be calculated: \\[ \\begin{aligned} E(\\text{WL}) &= \\sum^\\infty_{k = 0} v^{k + 1} \\cdot {}_{k|}q_{x} \\\\ A_{x} &= \\sum^\\infty_{k = 0} v^{k + 1} \\cdot {}_{k|}q_{x} \\end{aligned} \\] Another commonly used metric is its Variance . In order to get it, the Second Moment must first be determined: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{k = 0} \\left(v^{k + 1}\\right)^2 \\cdot {}_{k|}q_{x} \\\\ &= \\sum^\\infty_{k = 0} \\left[(v^2)^{k + 1} \\right] \\cdot {}_{k|}q_{x} \\end{aligned} \\] Notice that the expression can be simplified to a form that looks almost identical to the first moment - with the only difference being that is uses \\(v^2\\) instead of \\(v\\) . Thus, the second moment is simply the first moment evaluated at a higher interest rate \\(i^*=(1+i)^2-1\\) , such that \\(v^* = v^2\\) . Tip Using actuarial notation , it is denoted as \\({}^{2}A_x\\) , where \\(2\\) is known as the multiplier on the interest rate used to evaluate the moment, \\(i^*=(1+i)^2-1\\) . It can also be more generally denoted as \\(\\left. A_x \\right|_{i = (1+i)^2-1}\\) , which will come in handy for more complicated expressions. The Second Moment and hence Variance can be shown as: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ {}^{2} A_{x} &= \\sum^\\infty_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ \\\\ \\therefore Var(\\text{WL}) &= {}^{2} A_{x} - (A_{x})^2 \\end{aligned} \\] Warning In practice, we have to scale the EPV/Variance accordingly based on the actual benefit: \\[ \\begin{aligned} \\text{Actual PV} &= B \\cdot WL \\\\ \\\\ \\text{Actual EPV} &= E(B \\cdot \\text{WL}) \\\\ &= B \\cdot E(\\text{WL}) \\\\ \\\\ \\text{Actual Second Moment} &= E(B^2 \\cdot \\text{WL}^2) \\\\ &= B^2 \\cdot E(\\text{WL}^2) \\\\ \\\\ \\text{Actual Variance} &= \\text{Var}(B \\cdot \\text{WL}) \\\\ &= B^2 \\cdot \\text{Var}(\\text{WL}) \\end{aligned} \\] This may lead to an extremely large value for the variance , thus do not be alarmed when performing such calculations. Term Assurance \u00b6 Term Assurance covers the insured for a specified period \\(n\\) and only pays out if the insured dies within that period. NOTHING is paid out if the insured survives beyond that. Let TA be the RV denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{TA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ 0 ,& K_x \\ge n \\end{cases} \\\\ &= {}_{\\{K_x \\le n-1\\}} v^{K_x+1} \\end{aligned} \\] \\(\\{K_x \\le n-1\\}\\) is known as an Indicator Function , which is a binary variable that takes 1 if the condition is true and 0 if the condition if false . It provides a concise way to express a piecewise function in a single expression. Thus, the EPV and Variance can be determined: \\[ \\begin{aligned} E(\\text{TA}) &= \\sum^{n-1}_{k = 0} v^{k + 1} \\cdot {}_{k|}q_{x} + 0 \\cdot {}_{n}p_{x} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k = 0} v^{k + 1} \\cdot {}_{k|}q_{x} \\\\ \\\\ E({\\text{TA}}^2) &= \\sum^{n-1}_{k = 0} (v^2)^{k + 1} \\cdot {}_{k|}q_{x} + 0^2 \\cdot {}_{n}p_{x} \\\\ {}^{2} A^1_{x:\\enclose{actuarial}{n}} &= \\sum^\\infty_{k = 0} (v^2)^{k + 1} \\cdot {}_{k|}q_{x} \\\\ \\\\ \\therefore Var(\\text{TA}) &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 \\end{aligned} \\] Pure Endowment \u00b6 Pure Endowments are a special kind of contract that instead only pays out if the insured survives past a specified period \\(n\\) . NOTHING is paid out if the insured dies before that. Let PE be the RV denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{PE} &= \\begin{cases} 0 ,& K_x = 0, 1, 2 \\dots, n \\\\ v^n ,& K_x \\ge n \\end{cases} \\end{aligned} \\] Note Unlike the other assurances, the discounting factor for PE does NOT depend on \\(K_x\\) because it is paid out at a fixed time at maturity . No matter how long the policyholder survives past maturity, they are ONLY going to receive that amount. Thus, the EPV and Variance can be determined: \\[ \\begin{aligned} E(\\text{PE}) &= 0 \\cdot {}_{n}q_{x} + v^n {}_{n}p_{x} \\\\ {}_{n}E_x &= v^n {}_{n}p_{x} \\\\ \\\\ E({\\text{PE}}^2) &= 0^2 \\cdot {}_{n}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2}_{n}E_x &= (v^*)^n {}_{n}p_{x} \\\\ \\\\ \\therefore Var(\\text{PE}) &= {}^{2}_{n}E_x - ({}_{n}E_x)^2 \\end{aligned} \\] Note Note that unlike all other assurances, a summation was not involved, since the values do not depend on \\(K_x\\) . The probability of receiving the payout is simply the single probability of surviving till maturity, since it does not matter what happens after that . Since both discount factors and survival probabilities follow the multiplication rule , PEs follow them as well: \\[ \\begin{aligned} v^{m+n} &= v^{m} \\cdot v^{n} \\\\ {}_{n+m}p_{x} &= {}_{m}p_{x} \\cdot {}_{n}p_{x+m} \\\\ \\\\ \\therefore {}_{n+m}E_{x} &= v^{m+n} \\cdot {}_{n+m}p_{x}\\\\ &= (v^{m} \\cdot {}_{m}p_{x}) \\cdot (v^{n} \\cdot {}_{n}p_{x+m}) \\\\ &= {}_{m}E_{x} \\cdot {}_{n}E_{x+m} \\end{aligned} \\] Tip This allows us to use the table values for a wide range of PEs, reducing the chance of miscalculation: \\[ {}_{30}E_{x} = {}_{20}E_{x} \\cdot {}_{10}E_{x+20} \\] However, special care must be taken to ensure that the age in the second term is updated. Endowment Assurance \u00b6 Endowment Assurances are a combination of term assurances and pure endowments: Term Assurance - Pays out if the insured dies within the period Pure Endowment - Pays out if the insured survives past the period Thus, endowment assurances WILL pay out in a relatively shorter time compared to WL Let EA be the RV denoting the PV of its benefits: \\[ \\begin{aligned} \\text{EA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ v^n ,& K_x \\ge n \\end{cases} \\\\ &= v^{\\min(K_x + 1, n)} \\end{aligned} \\] Thus, the EPV and Variance can be determined: \\[ \\begin{aligned} E(\\text{EA}) &= \\sum^{n-1}_{k = 0} v^{k + 1} \\cdot {}_{k|}q_{x} + v^n {}_{n}p_{x} \\\\ A_{x:\\enclose{actuarial}{n}} &= A^1_{x:\\enclose{actuarial}{n}} + {}_{n}E_{x} \\end{aligned} \\] Tip The SULT table provides values for the EPVs EA and PE with terms of 10 or 20 years. Thus, the difference of these values can also be used to calculate the EPV of a TA with a term of 10 or 20 years . \\[ \\begin{aligned} E(\\text{EA}^2) &= \\sum^{n-1}_{k = 0} (v^2)^{k + 1} \\cdot {}_{k|}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k = 0} (v^*)^{k + 1} \\cdot {}_{k|}q_{x} + (v^*)^n {}_{n}p_{x} \\\\ &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\\\ \\\\ \\therefore Var(\\text{EA}) &= {}^{2} A_{x:\\enclose{actuarial}{n}} - \\left(A_{x:\\enclose{actuarial}{n}}\\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left(A^1_{x:\\enclose{actuarial}{n}} - ({}_{n}E_x) \\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left[(A^1_{x:\\enclose{actuarial}{n}})^2 + 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) + ({}_{n}E_x)^2 \\right] \\\\ &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 + {}^{2}_{n}E_x - ({}_{n}E_x)^2 - 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x)\\\\ &= \\text{Var (TA)} + \\text{Var (PE)} - 2 \\cdot E(\\text{TA}) \\cdot E(\\text{PE}) \\end{aligned} \\] Warning It is an extremely common mistake to think that the variance is the combination of the two variances: \\[ \\text{Var(EA)} \\ne \\text{Var(TA)} + \\text{Var(PE)} \\] This is because the Covariance needs to be accounted for: \\[ \\begin{aligned} \\text{Var(EA)} &= \\text{Var (TA + PE)} \\\\ &= \\text{Var (TA)} + \\text{Var (PE)} + 2 \\cdot \\text{Cov(TA, PE)} \\end{aligned} \\] Another common mistake comes from thinking that they are independent, because they seem to be unrelated: \\[ \\begin{aligned} \\text{Cov(TA, PE)} &= E(\\text{TA, PE}) - E(\\text{TA}) \\cdot E(\\text{PE}) \\\\ \\\\ TA \\cdot PE &= \\begin{cases} v^{K_x + 1} \\cdot 0, K_x \\lt n \\\\ 0 \\cdot v^n, K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} 0, K_x \\lt n \\\\ 0, K_x \\ge n \\end{cases} \\\\ \\\\ \\therefore \\text{Cov(TA, PE)} &= 0 - E(\\text{TA}) \\cdot E(\\text{PE}) \\\\ &= - E(\\text{TA}) \\cdot E(\\text{PE}) \\end{aligned} \\] In the final year of the contract, the assurance will pay \\(v^n\\) regardless of the outcome - TA pays if they die while PE pays if they survive. Thus, a simplification can be made to the EPV: \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k = 0} v^{k + 1} \\cdot {}_{k|}q_{x} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{k = 0} v^{k + 1} \\cdot {}_{k}p_{x} {}_{}q_{x + k} + v^n {}_{n-1}p_{x} {}_{}q_{x + n - 1} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{k = 0} v^{k + 1} \\cdot {}_{k|}q_{x} + v^n ({}_{n-1}p_{x} {}_{}q_{x + n - 1} + {}_{n}p_{x}) \\\\ &= \\sum^{n-2}_{k = 0} v^{k + 1} \\cdot {}_{k|}q_{x} + v^n {}_{n-1}p_{x} \\end{aligned} \\] Tip This formula might seem redundant because it is not that much simpler than the original, as it only requires one less probability : \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{3}} &= v \\cdot q_{x} + v^2 \\cdot p_{x}q_{x+1} + v^3 \\cdot p_{x}p_{x+1}q_{x+2} + v^3 + {}_{3}p_{x} \\\\ &= v \\cdot q_{x} + v^2 \\cdot p_{x}q_{x+1} + v^3 \\cdot {}_{2}p_{x} \\\\ &= v \\cdot q_{x} + v^2 \\cdot p_{x}q_{x+1} + v^3 \\cdot p_{x} \\cdot p_{x+1} \\\\ &= v \\cdot q_{x} + v^2 \\cdot p_{x}q_{x+1} + v^3 \\cdot p_{x} \\cdot (1 - q_{x+1}) \\end{aligned} \\] However, this is extremely useful when limited information is provided, allowing us to still be able to calculate the EPV. Deferred Assurances \u00b6 Deferred Assurances are variations of any of the above assurances, where the assurance starts \\(n\\) years later rather than immediately. Let DWL be the RV denoting the PV of the benefits of the death benefit: \\[ \\begin{aligned} \\text{DWL} &= \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ v^{K_x + 1} ,& K_x \\ge n \\end{cases} \\end{aligned} \\] It can be shown to be difference of a WL and TA: \\[ \\begin{aligned} \\text{DWL} &= \\begin{cases} 0,& K_x \\lt n \\\\ v^{K_x + 1} ,& K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} v^{K_x + 1},& K_x \\lt n \\\\ v^{K_x + 1} ,& K_x \\ge n \\end{cases} - \\begin{cases} v^{K_x + 1},& K_x \\lt n \\\\ 0,& K_x \\ge n \\end{cases} \\\\ &= \\text{WL} - \\text{TA} \\end{aligned} \\] Thus, the EPV and Variance can be determined: \\[ \\begin{aligned} E(\\text{DWL}) &= E(\\text{WL}) - E(\\text{TA}) \\\\ {}_{n|}A_x &= A_x - A^1_{x:\\enclose{actuarial}{n}} \\\\ \\\\ Var (\\text{DWL}) &= {}^{2}_{n|}A_x - ({}_{n|}A_x)^2 \\\\ &= \\left({}^{2}A_x - {}^{2}A^1_{x:\\enclose{actuarial}{n}}\\right) - \\left(A_x - A^1_{x:\\enclose{actuarial}{n}}\\right)^2 \\end{aligned} \\] Alternatively, since a DWL is simply a WL assurance issued \\(n\\) years later, the EPV of the DWL is equivalent to the EPV of a WL issued at age \\(x+n\\) after adjusting for interest and survival : \\[ \\begin{aligned} {}_{n|} A_{x} &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x}p_{x} \\cdot q_{x+K_x} \\\\ &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1 + n} \\cdot {}_{K_x+n}p_{x} \\cdot q_{x+K_x+n} \\\\ &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} v^{n} \\cdot {}_{n}p_{x} {}_{K_x}p_{x+n} \\cdot q_{x+K_x+n} \\\\ &= v^n {}_{n} p_{x} \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x+n} \\\\ &= {}_{n}E_{x} \\cdot A_{x+n} \\\\ \\end{aligned} \\] Note Recall in the previous section that we had to account for the person surviving till a certain age. However, now the amount must also be discounted to its present value. Multiplying PEs be used as a factor that that both discounts accounts for survival . Conversely, it can be divided to be used as a factor that accumulates and accounts for death . This allows a TA to be expressed as the difference of two WL assurances issued at different times : \\[ \\begin{aligned} {}_{n|} A_{x} &= {}_{n}E_{x} * A_{x+n} \\\\ A_x - A^1_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} * A_{x+n} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= A_{x} - {}_{n}E_{x} * A_{x+n} \\\\ \\end{aligned} \\] Tip This result is extremely important as this is the main method of calculating the EPV of a TA of any duration as the values for the WL can be found in the SULT. For TAs of 10 or 20 years, the EA and PE method is preferred as it requires fewer values to calculate. Note that this result also applies to the second moment , allowing us to easily compute the variance of a TA: \\[ \\begin{aligned} {}^{2} A^1_{x:\\enclose{actuarial}{n}} &= {}^{2} A_{x} - {}^{2}_{n} E_{x} \\cdot {}^{2} A_{x+n} \\end{aligned} \\] The key intuition is understanding that the discounting factor should be squared , which is why \\({}^{2}_{n} E_{x}\\) is also used. Tip Using the above, it can be shown that the sum of TAs with consecutive policy terms is equivalent to a TA with a long policy term: \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n+m}} &= A_{x:\\enclose{actuarial}{n}} + {}_{n}E_{x} A_{x+n:\\enclose{actuarial}{m}} \\end{aligned} \\] However, if the interest is not 0.05 or if mortality does NOT follow the SULT, then the EPV of a TA must be calculated manually. If so, there is usually a catch that allows the EPV to be easily calculated: Different Interest : Term of the contract is short (EG. 3 Years) Different Mortality : Mortality can be simplified (EG. Becomes a constant) It is rare to have problems that have both different interest and mortality. In such cases, it is likely that an adjusted mortality table is provided. Another less commonly used deferred assurance is the Deferred Term Assurance . As both a deferred and term assurance, both results apply to it: \\[ \\begin{aligned} {}_{k|}A^1_{x:\\enclose{actuarial}{n}} &= {}_{k}E_{x} \\cdot A^1_{x+k:\\enclose{actuarial}{n}} \\\\ &= {}_{k}E_{x} \\cdot (A_{x+k} - {}_{n}E_{x+k} \\cdot A_{x+k+n}) \\end{aligned} \\] Warning It is an extremely common mistake to use the same PE inside and outside. They are completely different : Outer PE : Surviving the deferment period from policy inception Inner PE : Surviving the policy term from the starting age Other Calculations \u00b6 Recursion \u00b6 The EPV of each contract can also be expressed through backwards recursion , where it is calculated as a function of itself one year later. Consider the WL random variable: If the policyholder dies in the year, a benefit of 1 is paid at the end of the year . If the policyholder survives the year, they \"receive\" the PV of the future benefits at the end of the year . Since both amounts are at the end of the year, they must be discounted to the start of the year. \\[ \\begin{aligned} WL &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A_{x+1} ,& {}_{}p_{x} \\end{cases} \\\\ \\\\ \\therefore A_x &= v \\cdot {}_{}q_{x} + v \\cdot {}_{}p_{x}A_{x+1} \\end{aligned} \\] Tip The same recursion can be applied to the second moment as well, since it is simply the first moment evaluated at a higher interest. The only change is that the discounting factor used must be calculated using the higher interest . The same exercise can be shown for the TA random variable. The main difference is understanding how the age & duration changes: \\(x+1\\) reflects the new age of the policyholder (same as WL) \\(n-1\\) reflects that one year of coverage has passed (not applicable for WL) \\[ \\begin{aligned} TA &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A^1_{x+1:\\enclose{actuarial}{n-1}} ,& {}_{}p_{x} \\end{cases} \\\\ \\\\ \\therefore A^1_{x:\\enclose{actuarial}{n}} &= v{}_{}q_{x} + v \\cdot {}_{}p_{x} A^1_{x+1:\\enclose{actuarial}{n-1}} \\end{aligned} \\] Note Note that since it requires a term policy with a reduced term , this recursion is not particularly useful as it is difficult to obtain it. The PE variable is similar, with the main difference being that the policyholder will receive nothing if the policyholder dies . Thus, only the second component of the recursion remains: \\[ \\begin{aligned} PE &= \\begin{cases} 0 ,& {}_{}q_{x} \\\\ v \\cdot {}_{n-1}E_{x+1} ,& {}_{}p_{x} \\end{cases} \\\\ \\\\ \\therefore {}_{n}E_{x} &= v \\cdot {}_{}p_{x} \\cdot {}_{n-1}E_{x+1} \\end{aligned} \\] EA recursions are omitted from this section as it is simply the combination of a TA and PE. Probabilities \u00b6 Questions could also ask for the probability that the present value of the benefits takes on some value \\(u\\) . \\[ \\begin{aligned} P(\\text{WL} \\le u) &= P(v^{K_x+1} \\le u) \\\\ &= P \\left((K_x+1) \\cdot \\ln v \\le \\ln u \\right) \\\\ &= P \\left(K_x+1 \\ge \\frac{\\ln u}{\\ln v} \\right) \\\\ &= P \\left(K_x \\ge \\frac{\\ln u}{\\ln v} - 1 \\right) \\end{aligned} \\] Warning It is a common mistake to forget to flip the inequality sign as \\(\\ln v \\lt 0\\) . Questions will not be as straightforward as above . Sometimes, the required value of \\(K_x\\) has to be determined intuitively from the given scenario . Percentiles \u00b6 Similarly, questions could ask for the percentile of the PV . Since the PV is a transformation of the future lifetime variable, the percentile of the PV uses the percentile of the future lifetime variable . Thus, we can find the percentile of future lifetime and then substitute that into the PV random variable to obtain its percentile. Payable M-thly \u00b6 A slightly more complicated life assurance contract is one that pays benefits \\(m\\) times a year: \\(m=1\\) ; Paid at the end of the year that insured event occurs \\(m=4\\) ; Paid at the end of the quarter that insured event occurs \\(m=12\\) ; Paid at the end of the month that insured event occurs This logic can be applied to all other assurances, EXCEPT for PEs . This is because they pay out at a fixed time , thus there is NO such thing as a payable \\(m\\) times PE. We are now interested in the life status of the policyholder at intervals of \\(\\frac{1}{m}\\) years. This can be found by using an adjusted curtate variable , which rounds down to the nearest \\(\\frac{1}{m}\\) rather than the nearest integer: \\[ K^{(m)}_{x} = \\frac{1}{m} \\cdot \\lfloor m \\cdot T_x \\rfloor \\] It can be intepreted as the probability of surviving \\(K^{(m)}_{x}\\) full years and then dying within the next \\(\\frac{1}{m}\\) years: \\[ \\begin{aligned} P \\left(K^{(m)}_{x} = k \\right) &= {}_{k}p_{x} \\cdot {}_{\\frac{1}{m}}q_{x+k} \\\\ &= {}_{k \\mid \\frac{1}{m}}q_{x} \\end{aligned} \\] Thus, the PV and EPV of the benefit can then be re-expressed as the following: \\[ \\begin{aligned} Z &= v^{K^{(m)}_{x} + \\frac{1}{m}} \\\\ \\\\ \\text{EPV} &= \\sum B \\cdot v^{K^{(m)}_{x} + \\frac{1}{m}} \\cdot {}_{k}p_{x} \\cdot {}_{\\frac{1}{m}}q_{x+k} \\\\ E(Z) &= \\sum B \\cdot v^{K^{(m)}_{x} + \\frac{1}{m}} \\cdot {}_{k \\mid \\frac{1}{m}}q_{x} \\end{aligned} \\] The EPVs of the various contracts follow the same actuarial notation as before, but with an additional \\((m)\\) superscript to distinguish them: \\[ A^{(m)}_{x}, A^{1, (m)}_{x:\\enclose{actuarial}{n}}, A^{(m)}_{x:\\enclose{actuarial}{n}} \\] Recall previously that we were able to approximate fractional probabilities from discrete ones. Thus, by applying these approximations, we are also to approximate the m-thly EPVs from the discrete ones using an appropriate assumption. Uniform Distribution of Deaths \u00b6 We can decompose a WL assurance as the sum of a series of one year deferred term assurances : \\[ \\begin{aligned} A^{(m)}_{x} &= \\sum {}_{k \\mid}A^1_{x:\\enclose{actuarial}{1}} \\\\ &= \\sum {}_{k}E_{x} \\cdot A^1_{x+k:\\enclose{actuarial}{1}} \\end{aligned} \\] Assuming UDD between integer ages, the deferred one year TAs can be expressed as the following: \\[ \\begin{aligned} A^1_{x+k:\\enclose{actuarial}{1}} &= \\sum^{m}_{j=1} v^{\\frac{j}{m}} \\cdot {}_{\\frac{j}{m} \\mid \\frac{1}{m}}q_{x+k} \\\\ &= \\sum^{m}_{j=1} v^{\\frac{j}{m}} \\cdot {}_{\\frac{j+1}{m}}q_{x+k} - {}_{\\frac{j}{m}}q_{x+k} \\\\ &= \\sum^{m}_{j=1} v^{\\frac{j}{m}} \\cdot \\left(\\frac{j+1}{m} - \\frac{j}{m} \\right)q_{x+k} \\\\ &= \\sum^{m}_{j=1} v^{\\frac{j}{m}} \\cdot \\frac{1}{m} q_{x+k} \\\\ &= q_{x+k} \\cdot \\frac{1}{m} \\sum^{m}_{j=1} v^{\\frac{j}{m}} \\\\ &= q_{x+k} \\cdot \\frac{1}{m} \\cdot \\frac{v^{\\frac{1}{m}}(1-v)}{1-v^{\\frac{1}{m}}} \\\\ &= q_{x+k} \\cdot \\frac{1}{m} \\cdot \\frac{v^{\\frac{1}{m}}(1-v)}{v^{\\frac{1}{m}}[(1+i)^{m}-1]} \\\\ &= q_{x+k} \\cdot \\frac{1}{m} \\frac{1-v}{(1+i)^{m}-1} \\\\ &= q_{x+k} \\cdot \\frac{\\frac{1+i-1}{1+i}}{m[(1+i)^{m}-1]} \\\\ &= q_{x+k} \\cdot \\frac{iv}{i^{(m)}} \\end{aligned} \\] Thus, the m-thly EPV can be expressed as a function of the discrete EPV, known as the UDD Approximation : \\[ \\begin{aligned} A^{(m)}_{x} &= \\sum {}_{k}E_{x} \\cdot A^1_{x+k:\\enclose{actuarial}{1}} \\\\ &= \\sum v^{k} {}_{k}p_{x} \\cdot q_{x+k} \\frac{iv}{i^{(m)}} \\\\ &= \\frac{i}{i^{(m)}} \\cdot \\sum v^{k+1} {}_{k}p_{x} q_{x+k} \\\\ &= \\frac{i}{i^{(m)}} \\cdot A_{x} \\end{aligned} \\] Claims Acceleration Approach \u00b6 Given that claims occur every \\(\\frac{1}{m}\\) period, then on average , the claims within a year occur at \\(\\frac{m+1}{2m}\\) : \\[ \\begin{aligned} \\text{Average Time of Death} &= \\frac{\\frac{1}{m} + \\frac{m}{m}}{2} \\\\ &= \\frac{\\frac{1+m}{m}}{2} \\\\ &= \\frac{m+1}{2m} \\\\ \\\\ \\therefore Z &= v^{K_{x} + \\frac{m+1}{2m}} \\end{aligned} \\] Thus, the m-thly EPV can be expressed as a function of the discrete EPV: \\[ \\begin{aligned} A^{(m)}_{x} & \\approx \\sum^{\\infty}_{k = 0} v^{k + \\frac{m+1}{2m}} {}_{k|} q_{x} \\\\ & \\approx v^{\\frac{m+1}{2m}-1} \\sum^{\\infty}_{k = 0} v^{k + 1} {}_{k|} q_{x} \\\\ & \\approx v^{\\frac{m+1-2m}{2m}} A_x \\\\ & \\approx v^{\\frac{-m+1}{2m}} A_x \\\\ & \\approx v^{-\\frac{m-1}{2m}} A_x \\\\ & \\approx (1+i)^{\\frac{m-1}{2m}} A_x \\end{aligned} \\] This is known as the Claims Acceleration Approach as the claims are paid out earlier on in the year as compared to the end. Generally speaking, this approach is preferred as it only has one parameter compared to UDD which has two. Tip Generally speaking, the two approaches should produce similar results , thus can be used to sense check one another. However, these differences are enlarged when dealing with large benefits as these decimal places will be brought forward, resulting in seemingly large differences ; thus do not be alarmed! Common Errors \u00b6 The errors covered in this section apply to both kinds of approximation. The most common mistake is approximating the EPV of an EA. Since there are no continuous PEs, the approximation must be applied to ONLY the TA component : \\[ \\begin{aligned} A^{(m)}_{x:\\enclose{actuarial}{n}} & \\ne \\frac{i}{i^{(m)}} \\cdot A_{x:\\enclose{actuarial}{n}} \\\\ \\\\ A^{(m)}_{x:\\enclose{actuarial}{n}} &= A^{1, (m)}_{x:\\enclose{actuarial}{n}} + {}_{n}E_x \\\\ &= \\frac{i}{i^{(m)}} A^{1}_{x:\\enclose{actuarial}{n}} + {}_{n}E_x \\end{aligned} \\] Info This is why continuous EAs are more appropriately referred to as Semi-Continuous EAs, as they contain both a continuous TA and a discrete PE. Another common mistake occurs when approximating the second moment . Since the second moment is evaluated at a higher interest rate, the approximation factors must reflect this higher interest : \\[ {}^{2} A^{(m)}_x = \\frac{i^*}{{i^*}^{(m)}} \\cdot {}^{2} A_x \\] Fractional Recursion \u00b6 Recursions follow the same logic as the discrete case. However, since the benefits are not paid out at JUST the end of the year , the the extent that the benefit have to be discounted changes . If benefits are paid out every half a year : \\[ A^{(2)}_{x} = v^{0.5} \\cdot {}_{0.5}q_{x} + v^{0.5} \\cdot {}_{0.5}p_{x} A^{(2)}_{x+0.5} \\] There are two key differences to take note of: The extent of discounting changes to half a year The age of the future EPV is at a fractional age Thus, by re-arranging the equation, we can solve for the fractional age EPV . Payable Continuously \u00b6 In practice, life assurances pay out immediately upon the insured event occuring; thus is akin to paying out continuously throughout the year. Tip Some questions may not be very explicit about paying immediately upon death. \"Death benefit paid out exactly 4 months later\" -- implies that the death benefits are paid out continuously, 4 months from whatever time the insured dies . Since we are interested in the life status of the insured at every possible time, \\(T_{x}\\) is used as the survival model. The PV and EPV can be re-expressed as the following: \\[ \\begin{aligned} Z &= v^{T_x} \\\\ &= e^{-\\delta \\cdot T_x} \\end{aligned} \\] However, since the distribution is now continuous, the expectation is instead computed via integration : \\[ \\begin{aligned} \\text{EPV} &= \\int^{\\infty}_{0} B \\cdot v^{t} \\cdot f_x(t) \\\\ E(Z) &= \\int^{\\infty}_{0} B \\cdot e^{-\\delta \\cdot t} \\cdot {}_{t}p_{x} \\mu_{x+t} \\end{aligned} \\] Warning When computing the second moment, remember that BOTH the Benefit and PV must be squared : \\[ E(Z^2) = \\int^{\\infty}_{0} (B \\cdot e^{-\\delta \\cdot t})^2 \\cdot {}_{t}p_{x} \\mu_{x+t} \\] It is a common mistake to forget to square the benefit since the benefit is usually assumed to be 1. Although this applies for all kinds of assurances, the questions that test this concept typically use continuous assurances, which is why this warning is placed here. Fully continuous assurances follow the same actuarial notation , but with an additional bar accent to distinguish them: \\[ \\bar{A}_{x}, \\bar{A}^1_{x:\\enclose{actuarial}{n}}, \\bar{A}_{x:\\enclose{actuarial}{n}} \\] Approximations \u00b6 An assurance that pays out continuously can be thought of as a special case of an assurance that pays out m-thly, when m tends to infinity . \\[ \\begin{aligned} m &\\to \\infty \\\\ \\\\ i^{(m)} &\\to \\delta, \\\\ A^{(m)}_{x} &\\to \\bar{A}_x, \\\\ \\therefore \\bar{A}_x &= \\frac{i}{\\delta} A_x \\\\ \\\\ \\frac{m-1}{2m} &\\to \\frac{1}{2}, \\\\ A^{(m)}_{x} &\\to \\bar{A}_x, \\\\ \\therefore \\bar{A}_x &= (1+i)^{\\frac{1}{2}} A_x \\end{aligned} \\] Apart from approximations, if the survival distribution is known , then they can be calculated directly as well. Exponential Distribution \u00b6 Recall that given a constant force of mortality , the lifetime of the policyholder follows an exponential distribution: \\[ \\begin{aligned} {}_{t}p_{x} &= e^{-\\mu t} \\\\ \\therefore f(x) &= \\mu \\cdot e^{-\\mu t} \\end{aligned} \\] Thus, the EPVs can be calculated via first principles: \\[ \\begin{aligned} \\bar{A}_x &= \\int^{\\infty}_{0} v^{t} \\cdot f_x(t) \\\\ &= \\int^{\\infty}_{0} e^{-\\delta t} \\cdot \\mu \\cdot e^{-\\mu t} \\\\ &= \\mu \\int^{\\infty}_{0} e^{-(\\mu + \\delta)t} \\\\ &= \\mu \\left[\\frac{e^{-(\\mu + \\delta)t}}{-(\\mu + \\delta)t} \\right]^{\\infty}_{0} \\\\ &= \\mu \\left[0 - -\\frac{1}{\\mu + \\delta} \\right] \\\\ &= \\frac{\\mu}{\\mu + \\delta} \\\\ \\\\ {}_{n}E_{x} &= v^{n} \\cdot {}_{n}p_{x} \\\\ &= e^{-\\delta n} \\cdot e^{-\\mu n} \\\\ &= e^{-(\\mu + \\delta)n} \\end{aligned} \\] Notice that the EPV for a WL assurance is independent of the age of the policyholder ; thus the EPV for a person aged \\(x\\) and aged \\(x+n\\) is the same. This is consistent with the memoryless property of the exponential distribution. Thus, these two values can be used to calculate the EPV of any continuous TA and hence EA: \\[ \\begin{aligned} \\bar{A}^{1}_{x:\\enclose{actuarial}{n}} &= \\bar{A}_{x} - {}_{n}E_{x} \\cdot \\bar{A}_{x+n} \\\\ &= \\bar{A}_{x} - {}_{n}E_{x} \\cdot \\bar{A}_{x} \\\\ &= \\bar{A}_{x} \\cdot (1 - {}_{n}E_{x}) \\end{aligned} \\] Note For brevity, the second moment is not included in this section. However, it can be easily derived via first principles. Uniform Distribution \u00b6 Similarly, following De Moivres law, the survival distribution follows a uniform distribtion: \\[ \\begin{aligned} {}_{t}p_x &= \\frac{w-(x+t)}{w-x} \\\\ \\mu_{x+t} &= \\frac{1}{w-(x+t)} \\\\ f(x) &= \\frac{1}{\\omega-x} \\end{aligned} \\] Thus, the EPV can be calculated via first principles: \\[ \\begin{aligned} \\bar{A}_x &= \\int^{\\omega - x}_{0} v^{t} \\cdot f_x(t) \\\\ &= \\int^{\\omega - x}_{0} e^{-\\delta t} \\cdot \\frac{1}{\\omega - x} \\\\ &= \\frac{1}{\\omega - x} \\int^{\\omega - x}_{0} e^{-\\delta t} \\\\ &= \\frac{1}{\\omega - x} \\cdot [\\bar{a}_\\enclose{actuarial}{t}]^{\\omega - x}_{0} \\\\ &= \\frac{\\bar{a}_\\enclose{actuarial}{\\omega - x}}{\\omega - x} \\\\ &= \\frac{1 - v^{\\omega - x}}{\\delta} \\cdot \\frac{1}{\\omega - x} \\\\ \\\\ {}_{n}E_{x} &= v^{n} \\cdot {}_{n}p_{x} \\\\ &= e^{-\\delta n} \\cdot \\frac{\\omega - (x + n)}{\\omega - x} \\end{aligned} \\] Warning Since the uniform distribution has a limiting age, the upper limit of the integral is \\(\\omega - x\\) , NOT \\(\\infty\\) ! Note that the above formulas ONLY apply to continuous assurances. Some questions might provide an exponential or uniform distribution for discrete assurances to confuse us into using them. In this case, the distribution are only useful in computing the probabilities at every age. The EPV must still be computed via first principles.","title":"Life Assurances"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#life-assurances","text":"","title":"Life Assurances"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#overview","text":"Life Assurances are contracts that promise to pay out a benefit \\(B\\) on either the death or survival of the policyholder. The value of a life assurance must reflect these two aspects: Uncertainty of payments - Expected Value , based on Survival Models Time value of money - Present Value , based on Interest Theory Thus, the value of a life assurance is best quantified through the Expected Present Value (EPV) of the promised payouts.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#payable-discretely","text":"The simplest form of life assurance pays the benefits at the end of the year that the insured event occurs. Since we are only concerned with the status of the insured at discrete integer ages , \\(K_x\\) is used as the survival model. Let \\(Z\\) be the random variable denoting present value of the benefit payable: \\[ Z = B \\cdot v^{K_x + 1} \\] Thus, the EPV is the expectation of \\(Z\\) ; it is the sumproduct of ALL possible present values and their associated probabilities: \\[ \\begin{aligned} \\text{EPV} &= \\sum B \\cdot v^{k + 1} \\cdot {}_{k}p_{x} {}_{}q_{x + k} \\\\ E(Z) &= \\sum B \\cdot v^{k + 1} \\cdot {}_{k \\mid}q_{x} \\end{aligned} \\] Note The associated probability is the probability of dying in each period, given that they survived till that period . It is essentially the deferred probability of death for each period. For simplicity, the benefits are usually assumed to be 1 ( \\(B = 1\\) ). This allows the EPVs to be easily scaled for any level of \\(B\\) required.","title":"Payable Discretely"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#actuarial-notation","text":"Similar to the survival model, given how often these values are calculated, they are abbreviated using the International Actuarial Notation as well. \\(A\\) represents the first moment (expectation) of the present value of a contract where a benefit of 1 is payable discretely. The subscript \\(x\\) represents the age of the policyholder while the additional subscript \\(:\\enclose{actuarial}{n}\\) represents the duration the contract. Note To be more precise, the above subscripts are known as a Status . They indicate whether a certain condition is fulfulled : \\(x\\) is known as the Life Status which is active as long as they are alive and fails when they die . \\(n\\) is known as the Duration Status which is active during the term and fails when the term is finished . \\(A\\) is an assurance that pays benefits upon the failure of the contract's status . Thus, \\(T_{x}\\) more generally measures the time to failure of an entity. If a contract has multiple statuses, then the overall status fails as long as one of the underlying statuses fails . However, if the order of the failure matters, then a \\(1\\) can be placed above a particular status to indicate that the overall status only fails if that status fails FIRST . Thus, putting everything together, the EPV of each assurance can be denoted as follows: Whole Life Assurance - Payable whenever policyholder dies; \\(A_x\\) Term Assurance - Payable only if policyholder dies during assurance period; \\(A^{1}_{x:\\enclose{actuarial}{n}}\\) Pure Endowment - Payable only if policyholder survives past assurance period; \\(A^{\\>\\>\\> 1}_{x:\\enclose{actuarial}{n}}\\) Endowment Assurance - Payable whichever of the above two occur first; \\(A_{x:\\enclose{actuarial}{n}}\\) Deferred Whole Life Assurance - Payable only if policyholder dies after \\(n\\) years; \\({}_{n|}A_x\\) Tip Pure Endowments can also be expressed as \\({}_{n}E_x\\) for easier typesetting and is the method that SOA usually uses to denote PEs.","title":"Actuarial Notation"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#whole-life-assurance","text":"Whole Life Assurances cover the insured indefinitely and thus will pay out whenever the insured dies. Let WL be the RV denoting the PV of the death benefit: \\[ \\text{WL} = v^{K_x + 1} \\] Thus, its EPV can be calculated: \\[ \\begin{aligned} E(\\text{WL}) &= \\sum^\\infty_{k = 0} v^{k + 1} \\cdot {}_{k|}q_{x} \\\\ A_{x} &= \\sum^\\infty_{k = 0} v^{k + 1} \\cdot {}_{k|}q_{x} \\end{aligned} \\] Another commonly used metric is its Variance . In order to get it, the Second Moment must first be determined: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{k = 0} \\left(v^{k + 1}\\right)^2 \\cdot {}_{k|}q_{x} \\\\ &= \\sum^\\infty_{k = 0} \\left[(v^2)^{k + 1} \\right] \\cdot {}_{k|}q_{x} \\end{aligned} \\] Notice that the expression can be simplified to a form that looks almost identical to the first moment - with the only difference being that is uses \\(v^2\\) instead of \\(v\\) . Thus, the second moment is simply the first moment evaluated at a higher interest rate \\(i^*=(1+i)^2-1\\) , such that \\(v^* = v^2\\) . Tip Using actuarial notation , it is denoted as \\({}^{2}A_x\\) , where \\(2\\) is known as the multiplier on the interest rate used to evaluate the moment, \\(i^*=(1+i)^2-1\\) . It can also be more generally denoted as \\(\\left. A_x \\right|_{i = (1+i)^2-1}\\) , which will come in handy for more complicated expressions. The Second Moment and hence Variance can be shown as: \\[ \\begin{aligned} E({\\text{WL}}^2) &= \\sum^\\infty_{K_x = 0} (v^2)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ {}^{2} A_{x} &= \\sum^\\infty_{K_x = 0} (v^*)^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ \\\\ \\therefore Var(\\text{WL}) &= {}^{2} A_{x} - (A_{x})^2 \\end{aligned} \\] Warning In practice, we have to scale the EPV/Variance accordingly based on the actual benefit: \\[ \\begin{aligned} \\text{Actual PV} &= B \\cdot WL \\\\ \\\\ \\text{Actual EPV} &= E(B \\cdot \\text{WL}) \\\\ &= B \\cdot E(\\text{WL}) \\\\ \\\\ \\text{Actual Second Moment} &= E(B^2 \\cdot \\text{WL}^2) \\\\ &= B^2 \\cdot E(\\text{WL}^2) \\\\ \\\\ \\text{Actual Variance} &= \\text{Var}(B \\cdot \\text{WL}) \\\\ &= B^2 \\cdot \\text{Var}(\\text{WL}) \\end{aligned} \\] This may lead to an extremely large value for the variance , thus do not be alarmed when performing such calculations.","title":"Whole Life Assurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#term-assurance","text":"Term Assurance covers the insured for a specified period \\(n\\) and only pays out if the insured dies within that period. NOTHING is paid out if the insured survives beyond that. Let TA be the RV denoting the PV of the death benefit: \\[ \\begin{aligned} \\text{TA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ 0 ,& K_x \\ge n \\end{cases} \\\\ &= {}_{\\{K_x \\le n-1\\}} v^{K_x+1} \\end{aligned} \\] \\(\\{K_x \\le n-1\\}\\) is known as an Indicator Function , which is a binary variable that takes 1 if the condition is true and 0 if the condition if false . It provides a concise way to express a piecewise function in a single expression. Thus, the EPV and Variance can be determined: \\[ \\begin{aligned} E(\\text{TA}) &= \\sum^{n-1}_{k = 0} v^{k + 1} \\cdot {}_{k|}q_{x} + 0 \\cdot {}_{n}p_{x} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k = 0} v^{k + 1} \\cdot {}_{k|}q_{x} \\\\ \\\\ E({\\text{TA}}^2) &= \\sum^{n-1}_{k = 0} (v^2)^{k + 1} \\cdot {}_{k|}q_{x} + 0^2 \\cdot {}_{n}p_{x} \\\\ {}^{2} A^1_{x:\\enclose{actuarial}{n}} &= \\sum^\\infty_{k = 0} (v^2)^{k + 1} \\cdot {}_{k|}q_{x} \\\\ \\\\ \\therefore Var(\\text{TA}) &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 \\end{aligned} \\]","title":"Term Assurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#pure-endowment","text":"Pure Endowments are a special kind of contract that instead only pays out if the insured survives past a specified period \\(n\\) . NOTHING is paid out if the insured dies before that. Let PE be the RV denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{PE} &= \\begin{cases} 0 ,& K_x = 0, 1, 2 \\dots, n \\\\ v^n ,& K_x \\ge n \\end{cases} \\end{aligned} \\] Note Unlike the other assurances, the discounting factor for PE does NOT depend on \\(K_x\\) because it is paid out at a fixed time at maturity . No matter how long the policyholder survives past maturity, they are ONLY going to receive that amount. Thus, the EPV and Variance can be determined: \\[ \\begin{aligned} E(\\text{PE}) &= 0 \\cdot {}_{n}q_{x} + v^n {}_{n}p_{x} \\\\ {}_{n}E_x &= v^n {}_{n}p_{x} \\\\ \\\\ E({\\text{PE}}^2) &= 0^2 \\cdot {}_{n}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2}_{n}E_x &= (v^*)^n {}_{n}p_{x} \\\\ \\\\ \\therefore Var(\\text{PE}) &= {}^{2}_{n}E_x - ({}_{n}E_x)^2 \\end{aligned} \\] Note Note that unlike all other assurances, a summation was not involved, since the values do not depend on \\(K_x\\) . The probability of receiving the payout is simply the single probability of surviving till maturity, since it does not matter what happens after that . Since both discount factors and survival probabilities follow the multiplication rule , PEs follow them as well: \\[ \\begin{aligned} v^{m+n} &= v^{m} \\cdot v^{n} \\\\ {}_{n+m}p_{x} &= {}_{m}p_{x} \\cdot {}_{n}p_{x+m} \\\\ \\\\ \\therefore {}_{n+m}E_{x} &= v^{m+n} \\cdot {}_{n+m}p_{x}\\\\ &= (v^{m} \\cdot {}_{m}p_{x}) \\cdot (v^{n} \\cdot {}_{n}p_{x+m}) \\\\ &= {}_{m}E_{x} \\cdot {}_{n}E_{x+m} \\end{aligned} \\] Tip This allows us to use the table values for a wide range of PEs, reducing the chance of miscalculation: \\[ {}_{30}E_{x} = {}_{20}E_{x} \\cdot {}_{10}E_{x+20} \\] However, special care must be taken to ensure that the age in the second term is updated.","title":"Pure Endowment"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#endowment-assurance","text":"Endowment Assurances are a combination of term assurances and pure endowments: Term Assurance - Pays out if the insured dies within the period Pure Endowment - Pays out if the insured survives past the period Thus, endowment assurances WILL pay out in a relatively shorter time compared to WL Let EA be the RV denoting the PV of its benefits: \\[ \\begin{aligned} \\text{EA} &= \\begin{cases} v^{K_x + 1} ,& K_x = 0, 1, \\dots, n-1 \\\\ v^n ,& K_x \\ge n \\end{cases} \\\\ &= v^{\\min(K_x + 1, n)} \\end{aligned} \\] Thus, the EPV and Variance can be determined: \\[ \\begin{aligned} E(\\text{EA}) &= \\sum^{n-1}_{k = 0} v^{k + 1} \\cdot {}_{k|}q_{x} + v^n {}_{n}p_{x} \\\\ A_{x:\\enclose{actuarial}{n}} &= A^1_{x:\\enclose{actuarial}{n}} + {}_{n}E_{x} \\end{aligned} \\] Tip The SULT table provides values for the EPVs EA and PE with terms of 10 or 20 years. Thus, the difference of these values can also be used to calculate the EPV of a TA with a term of 10 or 20 years . \\[ \\begin{aligned} E(\\text{EA}^2) &= \\sum^{n-1}_{k = 0} (v^2)^{k + 1} \\cdot {}_{k|}q_{x} + (v^2)^n {}_{n}p_{x} \\\\ {}^{2} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k = 0} (v^*)^{k + 1} \\cdot {}_{k|}q_{x} + (v^*)^n {}_{n}p_{x} \\\\ &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\\\ \\\\ \\therefore Var(\\text{EA}) &= {}^{2} A_{x:\\enclose{actuarial}{n}} - \\left(A_{x:\\enclose{actuarial}{n}}\\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left(A^1_{x:\\enclose{actuarial}{n}} - ({}_{n}E_x) \\right)^2 \\\\ &= \\left({}^{2} A^1_{x:\\enclose{actuarial}{n}} + {}^{2}_{n}E_x \\right) - \\left[(A^1_{x:\\enclose{actuarial}{n}})^2 + 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x) + ({}_{n}E_x)^2 \\right] \\\\ &= {}^{2} A^1_{x:\\enclose{actuarial}{n}} - (A^1_{x:\\enclose{actuarial}{n}})^2 + {}^{2}_{n}E_x - ({}_{n}E_x)^2 - 2(A^1_{x:\\enclose{actuarial}{n}} \\cdot {}_{n}E_x)\\\\ &= \\text{Var (TA)} + \\text{Var (PE)} - 2 \\cdot E(\\text{TA}) \\cdot E(\\text{PE}) \\end{aligned} \\] Warning It is an extremely common mistake to think that the variance is the combination of the two variances: \\[ \\text{Var(EA)} \\ne \\text{Var(TA)} + \\text{Var(PE)} \\] This is because the Covariance needs to be accounted for: \\[ \\begin{aligned} \\text{Var(EA)} &= \\text{Var (TA + PE)} \\\\ &= \\text{Var (TA)} + \\text{Var (PE)} + 2 \\cdot \\text{Cov(TA, PE)} \\end{aligned} \\] Another common mistake comes from thinking that they are independent, because they seem to be unrelated: \\[ \\begin{aligned} \\text{Cov(TA, PE)} &= E(\\text{TA, PE}) - E(\\text{TA}) \\cdot E(\\text{PE}) \\\\ \\\\ TA \\cdot PE &= \\begin{cases} v^{K_x + 1} \\cdot 0, K_x \\lt n \\\\ 0 \\cdot v^n, K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} 0, K_x \\lt n \\\\ 0, K_x \\ge n \\end{cases} \\\\ \\\\ \\therefore \\text{Cov(TA, PE)} &= 0 - E(\\text{TA}) \\cdot E(\\text{PE}) \\\\ &= - E(\\text{TA}) \\cdot E(\\text{PE}) \\end{aligned} \\] In the final year of the contract, the assurance will pay \\(v^n\\) regardless of the outcome - TA pays if they die while PE pays if they survive. Thus, a simplification can be made to the EPV: \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k = 0} v^{k + 1} \\cdot {}_{k|}q_{x} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{k = 0} v^{k + 1} \\cdot {}_{k}p_{x} {}_{}q_{x + k} + v^n {}_{n-1}p_{x} {}_{}q_{x + n - 1} + v^n {}_{n}p_{x} \\\\ &= \\sum^{n-2}_{k = 0} v^{k + 1} \\cdot {}_{k|}q_{x} + v^n ({}_{n-1}p_{x} {}_{}q_{x + n - 1} + {}_{n}p_{x}) \\\\ &= \\sum^{n-2}_{k = 0} v^{k + 1} \\cdot {}_{k|}q_{x} + v^n {}_{n-1}p_{x} \\end{aligned} \\] Tip This formula might seem redundant because it is not that much simpler than the original, as it only requires one less probability : \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{3}} &= v \\cdot q_{x} + v^2 \\cdot p_{x}q_{x+1} + v^3 \\cdot p_{x}p_{x+1}q_{x+2} + v^3 + {}_{3}p_{x} \\\\ &= v \\cdot q_{x} + v^2 \\cdot p_{x}q_{x+1} + v^3 \\cdot {}_{2}p_{x} \\\\ &= v \\cdot q_{x} + v^2 \\cdot p_{x}q_{x+1} + v^3 \\cdot p_{x} \\cdot p_{x+1} \\\\ &= v \\cdot q_{x} + v^2 \\cdot p_{x}q_{x+1} + v^3 \\cdot p_{x} \\cdot (1 - q_{x+1}) \\end{aligned} \\] However, this is extremely useful when limited information is provided, allowing us to still be able to calculate the EPV.","title":"Endowment Assurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#deferred-assurances","text":"Deferred Assurances are variations of any of the above assurances, where the assurance starts \\(n\\) years later rather than immediately. Let DWL be the RV denoting the PV of the benefits of the death benefit: \\[ \\begin{aligned} \\text{DWL} &= \\begin{cases} 0,& K_x = 0, 1, 2, \\dots, n-1 \\\\ v^{K_x + 1} ,& K_x \\ge n \\end{cases} \\end{aligned} \\] It can be shown to be difference of a WL and TA: \\[ \\begin{aligned} \\text{DWL} &= \\begin{cases} 0,& K_x \\lt n \\\\ v^{K_x + 1} ,& K_x \\ge n \\end{cases} \\\\ &= \\begin{cases} v^{K_x + 1},& K_x \\lt n \\\\ v^{K_x + 1} ,& K_x \\ge n \\end{cases} - \\begin{cases} v^{K_x + 1},& K_x \\lt n \\\\ 0,& K_x \\ge n \\end{cases} \\\\ &= \\text{WL} - \\text{TA} \\end{aligned} \\] Thus, the EPV and Variance can be determined: \\[ \\begin{aligned} E(\\text{DWL}) &= E(\\text{WL}) - E(\\text{TA}) \\\\ {}_{n|}A_x &= A_x - A^1_{x:\\enclose{actuarial}{n}} \\\\ \\\\ Var (\\text{DWL}) &= {}^{2}_{n|}A_x - ({}_{n|}A_x)^2 \\\\ &= \\left({}^{2}A_x - {}^{2}A^1_{x:\\enclose{actuarial}{n}}\\right) - \\left(A_x - A^1_{x:\\enclose{actuarial}{n}}\\right)^2 \\end{aligned} \\] Alternatively, since a DWL is simply a WL assurance issued \\(n\\) years later, the EPV of the DWL is equivalent to the EPV of a WL issued at age \\(x+n\\) after adjusting for interest and survival : \\[ \\begin{aligned} {}_{n|} A_{x} &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x|}q_{x} \\\\ &= \\sum^{\\infty}_{K_x = n} v^{K_x + 1} \\cdot {}_{K_x}p_{x} \\cdot q_{x+K_x} \\\\ &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1 + n} \\cdot {}_{K_x+n}p_{x} \\cdot q_{x+K_x+n} \\\\ &= \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} v^{n} \\cdot {}_{n}p_{x} {}_{K_x}p_{x+n} \\cdot q_{x+K_x+n} \\\\ &= v^n {}_{n} p_{x} \\sum^{\\infty}_{K_x = 0} v^{K_x + 1} \\cdot {}_{K_x|}q_{x+n} \\\\ &= {}_{n}E_{x} \\cdot A_{x+n} \\\\ \\end{aligned} \\] Note Recall in the previous section that we had to account for the person surviving till a certain age. However, now the amount must also be discounted to its present value. Multiplying PEs be used as a factor that that both discounts accounts for survival . Conversely, it can be divided to be used as a factor that accumulates and accounts for death . This allows a TA to be expressed as the difference of two WL assurances issued at different times : \\[ \\begin{aligned} {}_{n|} A_{x} &= {}_{n}E_{x} * A_{x+n} \\\\ A_x - A^1_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} * A_{x+n} \\\\ A^1_{x:\\enclose{actuarial}{n}} &= A_{x} - {}_{n}E_{x} * A_{x+n} \\\\ \\end{aligned} \\] Tip This result is extremely important as this is the main method of calculating the EPV of a TA of any duration as the values for the WL can be found in the SULT. For TAs of 10 or 20 years, the EA and PE method is preferred as it requires fewer values to calculate. Note that this result also applies to the second moment , allowing us to easily compute the variance of a TA: \\[ \\begin{aligned} {}^{2} A^1_{x:\\enclose{actuarial}{n}} &= {}^{2} A_{x} - {}^{2}_{n} E_{x} \\cdot {}^{2} A_{x+n} \\end{aligned} \\] The key intuition is understanding that the discounting factor should be squared , which is why \\({}^{2}_{n} E_{x}\\) is also used. Tip Using the above, it can be shown that the sum of TAs with consecutive policy terms is equivalent to a TA with a long policy term: \\[ \\begin{aligned} A_{x:\\enclose{actuarial}{n+m}} &= A_{x:\\enclose{actuarial}{n}} + {}_{n}E_{x} A_{x+n:\\enclose{actuarial}{m}} \\end{aligned} \\] However, if the interest is not 0.05 or if mortality does NOT follow the SULT, then the EPV of a TA must be calculated manually. If so, there is usually a catch that allows the EPV to be easily calculated: Different Interest : Term of the contract is short (EG. 3 Years) Different Mortality : Mortality can be simplified (EG. Becomes a constant) It is rare to have problems that have both different interest and mortality. In such cases, it is likely that an adjusted mortality table is provided. Another less commonly used deferred assurance is the Deferred Term Assurance . As both a deferred and term assurance, both results apply to it: \\[ \\begin{aligned} {}_{k|}A^1_{x:\\enclose{actuarial}{n}} &= {}_{k}E_{x} \\cdot A^1_{x+k:\\enclose{actuarial}{n}} \\\\ &= {}_{k}E_{x} \\cdot (A_{x+k} - {}_{n}E_{x+k} \\cdot A_{x+k+n}) \\end{aligned} \\] Warning It is an extremely common mistake to use the same PE inside and outside. They are completely different : Outer PE : Surviving the deferment period from policy inception Inner PE : Surviving the policy term from the starting age","title":"Deferred Assurances"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#other-calculations","text":"","title":"Other Calculations"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#recursion","text":"The EPV of each contract can also be expressed through backwards recursion , where it is calculated as a function of itself one year later. Consider the WL random variable: If the policyholder dies in the year, a benefit of 1 is paid at the end of the year . If the policyholder survives the year, they \"receive\" the PV of the future benefits at the end of the year . Since both amounts are at the end of the year, they must be discounted to the start of the year. \\[ \\begin{aligned} WL &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A_{x+1} ,& {}_{}p_{x} \\end{cases} \\\\ \\\\ \\therefore A_x &= v \\cdot {}_{}q_{x} + v \\cdot {}_{}p_{x}A_{x+1} \\end{aligned} \\] Tip The same recursion can be applied to the second moment as well, since it is simply the first moment evaluated at a higher interest. The only change is that the discounting factor used must be calculated using the higher interest . The same exercise can be shown for the TA random variable. The main difference is understanding how the age & duration changes: \\(x+1\\) reflects the new age of the policyholder (same as WL) \\(n-1\\) reflects that one year of coverage has passed (not applicable for WL) \\[ \\begin{aligned} TA &= \\begin{cases} v \\cdot 1 ,& {}_{}q_{x} \\\\ v \\cdot A^1_{x+1:\\enclose{actuarial}{n-1}} ,& {}_{}p_{x} \\end{cases} \\\\ \\\\ \\therefore A^1_{x:\\enclose{actuarial}{n}} &= v{}_{}q_{x} + v \\cdot {}_{}p_{x} A^1_{x+1:\\enclose{actuarial}{n-1}} \\end{aligned} \\] Note Note that since it requires a term policy with a reduced term , this recursion is not particularly useful as it is difficult to obtain it. The PE variable is similar, with the main difference being that the policyholder will receive nothing if the policyholder dies . Thus, only the second component of the recursion remains: \\[ \\begin{aligned} PE &= \\begin{cases} 0 ,& {}_{}q_{x} \\\\ v \\cdot {}_{n-1}E_{x+1} ,& {}_{}p_{x} \\end{cases} \\\\ \\\\ \\therefore {}_{n}E_{x} &= v \\cdot {}_{}p_{x} \\cdot {}_{n-1}E_{x+1} \\end{aligned} \\] EA recursions are omitted from this section as it is simply the combination of a TA and PE.","title":"Recursion"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#probabilities","text":"Questions could also ask for the probability that the present value of the benefits takes on some value \\(u\\) . \\[ \\begin{aligned} P(\\text{WL} \\le u) &= P(v^{K_x+1} \\le u) \\\\ &= P \\left((K_x+1) \\cdot \\ln v \\le \\ln u \\right) \\\\ &= P \\left(K_x+1 \\ge \\frac{\\ln u}{\\ln v} \\right) \\\\ &= P \\left(K_x \\ge \\frac{\\ln u}{\\ln v} - 1 \\right) \\end{aligned} \\] Warning It is a common mistake to forget to flip the inequality sign as \\(\\ln v \\lt 0\\) . Questions will not be as straightforward as above . Sometimes, the required value of \\(K_x\\) has to be determined intuitively from the given scenario .","title":"Probabilities"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#percentiles","text":"Similarly, questions could ask for the percentile of the PV . Since the PV is a transformation of the future lifetime variable, the percentile of the PV uses the percentile of the future lifetime variable . Thus, we can find the percentile of future lifetime and then substitute that into the PV random variable to obtain its percentile.","title":"Percentiles"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#payable-m-thly","text":"A slightly more complicated life assurance contract is one that pays benefits \\(m\\) times a year: \\(m=1\\) ; Paid at the end of the year that insured event occurs \\(m=4\\) ; Paid at the end of the quarter that insured event occurs \\(m=12\\) ; Paid at the end of the month that insured event occurs This logic can be applied to all other assurances, EXCEPT for PEs . This is because they pay out at a fixed time , thus there is NO such thing as a payable \\(m\\) times PE. We are now interested in the life status of the policyholder at intervals of \\(\\frac{1}{m}\\) years. This can be found by using an adjusted curtate variable , which rounds down to the nearest \\(\\frac{1}{m}\\) rather than the nearest integer: \\[ K^{(m)}_{x} = \\frac{1}{m} \\cdot \\lfloor m \\cdot T_x \\rfloor \\] It can be intepreted as the probability of surviving \\(K^{(m)}_{x}\\) full years and then dying within the next \\(\\frac{1}{m}\\) years: \\[ \\begin{aligned} P \\left(K^{(m)}_{x} = k \\right) &= {}_{k}p_{x} \\cdot {}_{\\frac{1}{m}}q_{x+k} \\\\ &= {}_{k \\mid \\frac{1}{m}}q_{x} \\end{aligned} \\] Thus, the PV and EPV of the benefit can then be re-expressed as the following: \\[ \\begin{aligned} Z &= v^{K^{(m)}_{x} + \\frac{1}{m}} \\\\ \\\\ \\text{EPV} &= \\sum B \\cdot v^{K^{(m)}_{x} + \\frac{1}{m}} \\cdot {}_{k}p_{x} \\cdot {}_{\\frac{1}{m}}q_{x+k} \\\\ E(Z) &= \\sum B \\cdot v^{K^{(m)}_{x} + \\frac{1}{m}} \\cdot {}_{k \\mid \\frac{1}{m}}q_{x} \\end{aligned} \\] The EPVs of the various contracts follow the same actuarial notation as before, but with an additional \\((m)\\) superscript to distinguish them: \\[ A^{(m)}_{x}, A^{1, (m)}_{x:\\enclose{actuarial}{n}}, A^{(m)}_{x:\\enclose{actuarial}{n}} \\] Recall previously that we were able to approximate fractional probabilities from discrete ones. Thus, by applying these approximations, we are also to approximate the m-thly EPVs from the discrete ones using an appropriate assumption.","title":"Payable M-thly"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#uniform-distribution-of-deaths","text":"We can decompose a WL assurance as the sum of a series of one year deferred term assurances : \\[ \\begin{aligned} A^{(m)}_{x} &= \\sum {}_{k \\mid}A^1_{x:\\enclose{actuarial}{1}} \\\\ &= \\sum {}_{k}E_{x} \\cdot A^1_{x+k:\\enclose{actuarial}{1}} \\end{aligned} \\] Assuming UDD between integer ages, the deferred one year TAs can be expressed as the following: \\[ \\begin{aligned} A^1_{x+k:\\enclose{actuarial}{1}} &= \\sum^{m}_{j=1} v^{\\frac{j}{m}} \\cdot {}_{\\frac{j}{m} \\mid \\frac{1}{m}}q_{x+k} \\\\ &= \\sum^{m}_{j=1} v^{\\frac{j}{m}} \\cdot {}_{\\frac{j+1}{m}}q_{x+k} - {}_{\\frac{j}{m}}q_{x+k} \\\\ &= \\sum^{m}_{j=1} v^{\\frac{j}{m}} \\cdot \\left(\\frac{j+1}{m} - \\frac{j}{m} \\right)q_{x+k} \\\\ &= \\sum^{m}_{j=1} v^{\\frac{j}{m}} \\cdot \\frac{1}{m} q_{x+k} \\\\ &= q_{x+k} \\cdot \\frac{1}{m} \\sum^{m}_{j=1} v^{\\frac{j}{m}} \\\\ &= q_{x+k} \\cdot \\frac{1}{m} \\cdot \\frac{v^{\\frac{1}{m}}(1-v)}{1-v^{\\frac{1}{m}}} \\\\ &= q_{x+k} \\cdot \\frac{1}{m} \\cdot \\frac{v^{\\frac{1}{m}}(1-v)}{v^{\\frac{1}{m}}[(1+i)^{m}-1]} \\\\ &= q_{x+k} \\cdot \\frac{1}{m} \\frac{1-v}{(1+i)^{m}-1} \\\\ &= q_{x+k} \\cdot \\frac{\\frac{1+i-1}{1+i}}{m[(1+i)^{m}-1]} \\\\ &= q_{x+k} \\cdot \\frac{iv}{i^{(m)}} \\end{aligned} \\] Thus, the m-thly EPV can be expressed as a function of the discrete EPV, known as the UDD Approximation : \\[ \\begin{aligned} A^{(m)}_{x} &= \\sum {}_{k}E_{x} \\cdot A^1_{x+k:\\enclose{actuarial}{1}} \\\\ &= \\sum v^{k} {}_{k}p_{x} \\cdot q_{x+k} \\frac{iv}{i^{(m)}} \\\\ &= \\frac{i}{i^{(m)}} \\cdot \\sum v^{k+1} {}_{k}p_{x} q_{x+k} \\\\ &= \\frac{i}{i^{(m)}} \\cdot A_{x} \\end{aligned} \\]","title":"Uniform Distribution of Deaths"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#claims-acceleration-approach","text":"Given that claims occur every \\(\\frac{1}{m}\\) period, then on average , the claims within a year occur at \\(\\frac{m+1}{2m}\\) : \\[ \\begin{aligned} \\text{Average Time of Death} &= \\frac{\\frac{1}{m} + \\frac{m}{m}}{2} \\\\ &= \\frac{\\frac{1+m}{m}}{2} \\\\ &= \\frac{m+1}{2m} \\\\ \\\\ \\therefore Z &= v^{K_{x} + \\frac{m+1}{2m}} \\end{aligned} \\] Thus, the m-thly EPV can be expressed as a function of the discrete EPV: \\[ \\begin{aligned} A^{(m)}_{x} & \\approx \\sum^{\\infty}_{k = 0} v^{k + \\frac{m+1}{2m}} {}_{k|} q_{x} \\\\ & \\approx v^{\\frac{m+1}{2m}-1} \\sum^{\\infty}_{k = 0} v^{k + 1} {}_{k|} q_{x} \\\\ & \\approx v^{\\frac{m+1-2m}{2m}} A_x \\\\ & \\approx v^{\\frac{-m+1}{2m}} A_x \\\\ & \\approx v^{-\\frac{m-1}{2m}} A_x \\\\ & \\approx (1+i)^{\\frac{m-1}{2m}} A_x \\end{aligned} \\] This is known as the Claims Acceleration Approach as the claims are paid out earlier on in the year as compared to the end. Generally speaking, this approach is preferred as it only has one parameter compared to UDD which has two. Tip Generally speaking, the two approaches should produce similar results , thus can be used to sense check one another. However, these differences are enlarged when dealing with large benefits as these decimal places will be brought forward, resulting in seemingly large differences ; thus do not be alarmed!","title":"Claims Acceleration Approach"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#common-errors","text":"The errors covered in this section apply to both kinds of approximation. The most common mistake is approximating the EPV of an EA. Since there are no continuous PEs, the approximation must be applied to ONLY the TA component : \\[ \\begin{aligned} A^{(m)}_{x:\\enclose{actuarial}{n}} & \\ne \\frac{i}{i^{(m)}} \\cdot A_{x:\\enclose{actuarial}{n}} \\\\ \\\\ A^{(m)}_{x:\\enclose{actuarial}{n}} &= A^{1, (m)}_{x:\\enclose{actuarial}{n}} + {}_{n}E_x \\\\ &= \\frac{i}{i^{(m)}} A^{1}_{x:\\enclose{actuarial}{n}} + {}_{n}E_x \\end{aligned} \\] Info This is why continuous EAs are more appropriately referred to as Semi-Continuous EAs, as they contain both a continuous TA and a discrete PE. Another common mistake occurs when approximating the second moment . Since the second moment is evaluated at a higher interest rate, the approximation factors must reflect this higher interest : \\[ {}^{2} A^{(m)}_x = \\frac{i^*}{{i^*}^{(m)}} \\cdot {}^{2} A_x \\]","title":"Common Errors"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#fractional-recursion","text":"Recursions follow the same logic as the discrete case. However, since the benefits are not paid out at JUST the end of the year , the the extent that the benefit have to be discounted changes . If benefits are paid out every half a year : \\[ A^{(2)}_{x} = v^{0.5} \\cdot {}_{0.5}q_{x} + v^{0.5} \\cdot {}_{0.5}p_{x} A^{(2)}_{x+0.5} \\] There are two key differences to take note of: The extent of discounting changes to half a year The age of the future EPV is at a fractional age Thus, by re-arranging the equation, we can solve for the fractional age EPV .","title":"Fractional Recursion"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#payable-continuously","text":"In practice, life assurances pay out immediately upon the insured event occuring; thus is akin to paying out continuously throughout the year. Tip Some questions may not be very explicit about paying immediately upon death. \"Death benefit paid out exactly 4 months later\" -- implies that the death benefits are paid out continuously, 4 months from whatever time the insured dies . Since we are interested in the life status of the insured at every possible time, \\(T_{x}\\) is used as the survival model. The PV and EPV can be re-expressed as the following: \\[ \\begin{aligned} Z &= v^{T_x} \\\\ &= e^{-\\delta \\cdot T_x} \\end{aligned} \\] However, since the distribution is now continuous, the expectation is instead computed via integration : \\[ \\begin{aligned} \\text{EPV} &= \\int^{\\infty}_{0} B \\cdot v^{t} \\cdot f_x(t) \\\\ E(Z) &= \\int^{\\infty}_{0} B \\cdot e^{-\\delta \\cdot t} \\cdot {}_{t}p_{x} \\mu_{x+t} \\end{aligned} \\] Warning When computing the second moment, remember that BOTH the Benefit and PV must be squared : \\[ E(Z^2) = \\int^{\\infty}_{0} (B \\cdot e^{-\\delta \\cdot t})^2 \\cdot {}_{t}p_{x} \\mu_{x+t} \\] It is a common mistake to forget to square the benefit since the benefit is usually assumed to be 1. Although this applies for all kinds of assurances, the questions that test this concept typically use continuous assurances, which is why this warning is placed here. Fully continuous assurances follow the same actuarial notation , but with an additional bar accent to distinguish them: \\[ \\bar{A}_{x}, \\bar{A}^1_{x:\\enclose{actuarial}{n}}, \\bar{A}_{x:\\enclose{actuarial}{n}} \\]","title":"Payable Continuously"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#approximations","text":"An assurance that pays out continuously can be thought of as a special case of an assurance that pays out m-thly, when m tends to infinity . \\[ \\begin{aligned} m &\\to \\infty \\\\ \\\\ i^{(m)} &\\to \\delta, \\\\ A^{(m)}_{x} &\\to \\bar{A}_x, \\\\ \\therefore \\bar{A}_x &= \\frac{i}{\\delta} A_x \\\\ \\\\ \\frac{m-1}{2m} &\\to \\frac{1}{2}, \\\\ A^{(m)}_{x} &\\to \\bar{A}_x, \\\\ \\therefore \\bar{A}_x &= (1+i)^{\\frac{1}{2}} A_x \\end{aligned} \\] Apart from approximations, if the survival distribution is known , then they can be calculated directly as well.","title":"Approximations"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#exponential-distribution","text":"Recall that given a constant force of mortality , the lifetime of the policyholder follows an exponential distribution: \\[ \\begin{aligned} {}_{t}p_{x} &= e^{-\\mu t} \\\\ \\therefore f(x) &= \\mu \\cdot e^{-\\mu t} \\end{aligned} \\] Thus, the EPVs can be calculated via first principles: \\[ \\begin{aligned} \\bar{A}_x &= \\int^{\\infty}_{0} v^{t} \\cdot f_x(t) \\\\ &= \\int^{\\infty}_{0} e^{-\\delta t} \\cdot \\mu \\cdot e^{-\\mu t} \\\\ &= \\mu \\int^{\\infty}_{0} e^{-(\\mu + \\delta)t} \\\\ &= \\mu \\left[\\frac{e^{-(\\mu + \\delta)t}}{-(\\mu + \\delta)t} \\right]^{\\infty}_{0} \\\\ &= \\mu \\left[0 - -\\frac{1}{\\mu + \\delta} \\right] \\\\ &= \\frac{\\mu}{\\mu + \\delta} \\\\ \\\\ {}_{n}E_{x} &= v^{n} \\cdot {}_{n}p_{x} \\\\ &= e^{-\\delta n} \\cdot e^{-\\mu n} \\\\ &= e^{-(\\mu + \\delta)n} \\end{aligned} \\] Notice that the EPV for a WL assurance is independent of the age of the policyholder ; thus the EPV for a person aged \\(x\\) and aged \\(x+n\\) is the same. This is consistent with the memoryless property of the exponential distribution. Thus, these two values can be used to calculate the EPV of any continuous TA and hence EA: \\[ \\begin{aligned} \\bar{A}^{1}_{x:\\enclose{actuarial}{n}} &= \\bar{A}_{x} - {}_{n}E_{x} \\cdot \\bar{A}_{x+n} \\\\ &= \\bar{A}_{x} - {}_{n}E_{x} \\cdot \\bar{A}_{x} \\\\ &= \\bar{A}_{x} \\cdot (1 - {}_{n}E_{x}) \\end{aligned} \\] Note For brevity, the second moment is not included in this section. However, it can be easily derived via first principles.","title":"Exponential Distribution"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/3.%20Life%20Assurances/#uniform-distribution","text":"Similarly, following De Moivres law, the survival distribution follows a uniform distribtion: \\[ \\begin{aligned} {}_{t}p_x &= \\frac{w-(x+t)}{w-x} \\\\ \\mu_{x+t} &= \\frac{1}{w-(x+t)} \\\\ f(x) &= \\frac{1}{\\omega-x} \\end{aligned} \\] Thus, the EPV can be calculated via first principles: \\[ \\begin{aligned} \\bar{A}_x &= \\int^{\\omega - x}_{0} v^{t} \\cdot f_x(t) \\\\ &= \\int^{\\omega - x}_{0} e^{-\\delta t} \\cdot \\frac{1}{\\omega - x} \\\\ &= \\frac{1}{\\omega - x} \\int^{\\omega - x}_{0} e^{-\\delta t} \\\\ &= \\frac{1}{\\omega - x} \\cdot [\\bar{a}_\\enclose{actuarial}{t}]^{\\omega - x}_{0} \\\\ &= \\frac{\\bar{a}_\\enclose{actuarial}{\\omega - x}}{\\omega - x} \\\\ &= \\frac{1 - v^{\\omega - x}}{\\delta} \\cdot \\frac{1}{\\omega - x} \\\\ \\\\ {}_{n}E_{x} &= v^{n} \\cdot {}_{n}p_{x} \\\\ &= e^{-\\delta n} \\cdot \\frac{\\omega - (x + n)}{\\omega - x} \\end{aligned} \\] Warning Since the uniform distribution has a limiting age, the upper limit of the integral is \\(\\omega - x\\) , NOT \\(\\infty\\) ! Note that the above formulas ONLY apply to continuous assurances. Some questions might provide an exponential or uniform distribution for discrete assurances to confuse us into using them. In this case, the distribution are only useful in computing the probabilities at every age. The EPV must still be computed via first principles.","title":"Uniform Distribution"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/","text":"Life Annuities \u00b6 Overview \u00b6 Life Annuity contracts promise to pay out a stream of benefits in the future for as long the policyholder remains alive. Similar to life insurance, the benefits can be paid discretely or continuously and can be valued through their EPV. Note that these are different from the annuities covered in Exam FM. The payments for those annuities are made regardless of the life of the policyholder, known as Annuity Certain . They serve as the foundation to understanding Life Annuities. Similar to certain annuities, many of the concepts that apply to a life annuity due also apply to a life annuity immediate. Thus, for brevity, the entirety of this section will only cover life annuities due , unless explicitly stated otherwise. Review: Annuities Certain \u00b6 There are two types of payment structures: Period Start Period End Paid in Advance Paid in Arrears Annuity Due Annuity Immediate \\(\\ddot{a}_{\\enclose{actuarial}{n}}\\) \\(a_{\\enclose{actuarial}{n}}\\) The overall PV of the annuity is the sum of the PV of the stream of payments: \\[ \\begin{aligned} a_{\\enclose{actuarial}{n}} &= v + v^2 + v^3 + \\dots + v^n \\\\ &= \\frac{v-v^{n+1}}{1-v} \\\\ &= \\sum^n_{k=1} v^k \\\\ &= \\frac{v(1-v^n)}{1-v} \\\\ &= \\frac{1-v^n}{i} \\\\ \\\\ \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + \\dots + v^n \\\\ &= \\sum^{n-1}_{k=0} v^k \\\\ &= \\frac{1- v^{n}}{1-v} \\\\ &= \\frac{1- v^{n}}{d} \\end{aligned} \\] Notice that the payments simply differ by one period and hence one discounting factor: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + v^3 + \\dots + v^n \\\\ &= (1+i) (v + v^2 + v^3 + \\dots + v^n) \\\\ &= (1+i) \\sum^{n}_{k=1} v^k \\\\ &= (1+i) a_{\\enclose{actuarial}{n}} \\end{aligned} \\] From another perspective, only the payments at the end points \\(t=0\\) and \\(t=n\\) are different: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= \\sum^{n-1}_0 v^k \\\\ &= v^0 + \\sum^{n-1}_{k=1} v^k \\\\ &= 1 + a_{\\enclose{actuarial}{n-1}} \\\\ &= 1 + a_{\\enclose{actuarial}{n}} - v^n \\\\ \\\\ \\therefore a_{\\enclose{actuarial}{n}} &= \\ddot{a}_{\\enclose{actuarial}{n}} - 1 + v^n \\end{aligned} \\] Payable Discretely \u00b6 The simplest form of life annuity due contract pays the benefits at the beginning of the year , provided that the insured is alive at that time. Since we are only interested in the status of the insured at discrete integer ages , \\(K_{x}\\) is used as the survival model. Let \\(Y\\) be the random variable denoting the present value of the benefit payable. Since the policyholder receives benefits across multiple years, the PV is sum of these benefits and is represented by an annuity certain . \\[ Y = B \\cdot \\ddot{a}_{\\enclose{actuarial}{K_{x} + 1}} \\] Note The PV can be a bit confusing as it is very different from an assurance. They key is understanding that we must look backwards to determine how many payments the policyholder has received at that age in order to determine the PV. Thus, the EPV is the expectation of \\(Y\\) ; it is the sumproduct of ALL possible PVs and their associated probabilities: \\[ \\begin{aligned} \\text{EPV} &= \\sum B \\cdot \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k}P_{x} {}_{}q_{x+k} \\\\ E(Y) &= \\sum B \\cdot \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\end{aligned} \\] Note The associated probability might seem counter-intuitive, since a life annuity makes payment for as long as the insured is alive , so why are we using the probability of death? The key is understanding that the random variable was defined as the sum of payments up till that point . In other words, it assumes that the policyholder dies at that age that the PV is calculated. For simplicity, the benefits are usually assumed to be 1 \\((B=1)\\) . This allows the EPVs to be easily scaled for any level of \\(B\\) required. Actuarial Notation \u00b6 Most of the notation for Life Assurances applies to Life Annuities as well. Instead of \\(A\\) , \\(\\ddot{a}\\) is used to represent annuities. It is intepreted as the present value of a contract where a benefit of 1 is paid at the beginning of the year for as long as the status is active . Whole Life Annuity \u00b6 Whole Life Annuities covers the insured indefinitely and thus will pay out for as long as the insured survives . Let \\(\\text{WL}_{\\text{Due}}\\) be the RV denoting the PV of the stream of benefits: \\[ \\begin{aligned} \\text{WL}_{\\text{Due}} &=\\ddot{a}_{\\enclose{actuarial}{K_{x}+1}} \\end{aligned} \\] Thus, its EPV can be calculated: \\[ \\begin{aligned} E(\\text{WLA}_{\\text{Due}}) &= \\sum^{\\infty}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\\\ \\ddot{a}_{x} &= \\sum^{\\infty}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\end{aligned} \\] The EPV can be furthered simplified, allowing a life annuity to be viewed as the sum of a series of pure endowments : \\[ \\begin{aligned} \\ddot{a}_{x} &= \\sum^{\\infty}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k}} \\cdot {}_{k|}q_{x} \\\\ &=\\sum^{\\infty}_{k = 0} \\left(\\sum^{n-1}_{j=0} v^j \\right) \\cdot {}_{k|}q_{x} \\\\ &= v^0 \\cdot {}_{0|}q_{x} + (v^0 + v^1) \\cdot {}_{1|}q_{x} + (v^0 + v^1 + v^2) \\cdot {}_{2|}q_{x} + \\dots \\\\ &= v^0 ({}_{0|}q_{x} + {}_{1|}q_{x} + \\dots) + v^1 ({}_{1|}q_{x} + {}_{2|}q_{x} + \\dots) + v^2 ({}_{2|}q_{x} + {}_{3|}q_{x} + \\dots) + \\dots \\\\ &= \\sum^{\\infty}_{j=0} v^j \\cdot \\left(\\sum^{\\infty}_{k = j} {}_{k|}q_{x} \\right) \\\\ &= \\sum^{\\infty}_{j=0} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] Warning Note that the above formula can be used to easily prove the relationships between different kinds of annuities, but is NOT a proper definition of an annuity . It hence cannot be adjusted to compute the variance or other \"special\" annuities. Tip Notice that the expression is similar to the kurtate expectation of life. The key difference is that the annuity contains the discounting term . Thus, if the interest was 0 (no discounting), then the two are equal. The second moment and variance for all life annuities will be covered in a later section. Temporary Annuity \u00b6 Temporary Life Annuities covers the insured for a specified period \\(n\\) and thus pays out for as long as the insured survives during that period only . Let \\(\\text{TA}_\\text{Due}\\) be the RV denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{TA}_\\text{Due} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{K_{x}+1}},& k = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{n}},& k \\ge n \\\\ \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{\\min(K_{x} + 1, n)}} \\end{aligned} \\] Thus, the EPV can be determined and simplified using the same approach as before: \\[ \\begin{aligned} E(\\text{TA}_\\text{Due}) &= \\sum^{n-1}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k}} \\cdot {}_{k|}q_x + \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{n}p_x \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{j=0} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] Warning It is a common mistake to use \\(n\\) as the upper limit of the summation as there are \\(n\\) payments. While this is true, the summation starts from 0 . Thus, for there to be \\(n\\) payments,the summation must end at \\(n-1\\) . Tip Intuitively, the EPV of an \\(n\\) year temporary annuity should NOT be larger than \\(n\\) as there are \\(n\\) payments of 1 which are discounted. This can be used as a simple sense check when calculating the EPV. Deferred Annuities \u00b6 Deferred Annuities are variations of the above contracts where the assurance starts \\(n\\) years later rather than immediately. Let \\(\\text{DWL}_{\\text{Due}}\\) be the RV denoting the PV of a Deferred Whole Life Annuity Due : \\[ \\begin{aligned} \\text{DWL}_{\\text{Due}} &= \\begin{cases} 0, & k = 0, 1, 2, \\dots, n-1 \\\\ v^n \\cdot \\ddot{a}_{\\enclose{actuarial}{K_{x}+1-n}}, & k \\ge n \\end{cases} \\end{aligned} \\] It can be expressed as the difference between a \\(\\text{WL}_{\\text{Due}}\\) and a certain annuity for the guaranteed period: \\[ \\begin{aligned} \\text{DWL}_{\\text{Due}} &= v^n \\cdot \\ddot{a}_{\\enclose{actuarial}{k+1-n}} \\\\ &= v^n \\cdot \\frac{1-v^{k+1-n}}{d} \\\\ &= \\frac{v^{n}-v^{k+1}}{d} \\\\ &= \\frac{(1-v^{k+1}) - (1-v^{n})}{d} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{k+1}} -\\ddot{a}_{\\enclose{actuarial}{n}} \\\\ \\\\ \\therefore \\text{DWL}_{\\text{Due}} &= \\begin{cases} 0, & k = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{k+1}} -\\ddot{a}_{\\enclose{actuarial}{n}} , & k \\ge n \\end{cases} \\end{aligned} \\] Thus, its EPV can be determined: \\[ \\begin{aligned} E(\\text{DWL}_{\\text{Due}}) &= \\sum^{\\infty}_{k=n} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x} \\\\ {}_{n|}\\ddot{a}_{x} &= \\sum^{\\infty}_{k=0} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x} - \\sum^{n-1}_{k=0} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x} \\\\ &= \\sum^{\\infty}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} - \\sum^{\\infty}_{k=0} \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_{x} - \\sum^{n-1}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} + \\sum^{n-1}_{k=0} \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_{x} \\\\ &= \\sum^{\\infty}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} - \\sum^{n-1}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\\\ &= \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Similar to assurances, its EPV its effectively a \\(\\text{DWL}_{\\text{Due}}\\) issued at age \\(x+n\\) , after adjusting for interest and survival : \\[ \\begin{aligned} {}_{n|}\\ddot{a}_{x} &= \\sum^{\\infty}_{j=n} v^j \\cdot {}_{j}p_{x} \\\\ &= \\sum^{\\infty}_{j=0} v^{j+n} \\cdot {}_{j+n}p_{x} \\\\ &= \\sum^{\\infty}_{j=0} v^{j} v^{n} \\cdot {}_{n}p_{x} {}_{j}p_{x+n} \\\\ &= v^{n} \\cdot {}_{n}p_{x} \\sum^{\\infty}_{j=0} v^{j} \\cdot {}_{j}p_{x+n} \\\\ &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\] Similarly, this allows a \\(\\text{TA}_{\\text{Due}}\\) to be expressed as the difference of two \\(\\text{WL}_{\\text{Due}}\\) annuities issued at different times : \\[ \\begin{aligned} {}_{n|}\\ddot{a}_{x} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\ddot{a}_x - {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\] For brevity, Deferred Temporary Annuities will NOT be covered. The same concepts and warnings from its assurance counterpart apply to it as well. Guaranteed Annuities \u00b6 Guaranteed Life Annuities are whole life annuities with a guarantee in the first \\(n\\) years. The benefit will be paid out even if the insured dies during this period . Let \\(\\text{GA}_{\\text{Due}}\\) be the RV denoting the PV of the stream of benefits: \\[ \\begin{aligned} \\text{GA}_{\\text{Due}} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& k = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_{x}+1}},& k \\ge n \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{max(K_{x}+1,n)}} \\end{aligned} \\] Note Since the first payments are paid out regardless, they can be represented using a certain annuity . It can be manipulated so that it becomes easier to work with: \\[ \\begin{aligned} \\text{GA}_{\\text{Due}} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_{x} = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_{x}+1}},& K_{x} \\ge n \\end{cases} \\\\ &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_{x} = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_{x}+1}} - \\ddot{a}_{\\enclose{actuarial}{n}} + \\ddot{a}_{\\enclose{actuarial}{n}},& K_{x} \\ge n \\end{cases} \\\\ &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}} \\\\ \\ddot{a}_{\\enclose{actuarial}{n}} \\end{cases} + \\begin{cases} 0,& K_{x} = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_{x}+1}} - \\ddot{a}_{\\enclose{actuarial}{n}},& K_{x} \\ge n \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{n}} + \\begin{cases} 0,& K_{x} = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_{x}+1}} - \\ddot{a}_{\\enclose{actuarial}{n}},& K_{x} \\ge n \\end{cases} \\end{aligned} \\] Note Because of this, they are sometimes referred to as \\(\\text{WL}_{\\text{Due}}\\) annuities that are certain for \\(n\\) years . Thus, the EPV can be determined: $$ \\begin{aligned} E(\\text{GA} {\\text{Due}}) &= \\ddot{a} {n}} + 0 \\cdot {} {n}q_x + \\sum^{\\infty} (\\ddot{a} {\\enclose{actuarial}{k+1}} - \\ddot{a} {n}}) \\cdot {} {k|}q \\ \\ddot{a}_{\\overline{x:\\enclose{actuarial}{n}}} &= \\ddot{a}_{\\enclose{actuarial}{n}} + {}_{n|}\\ddot{a}_{x} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{n}} + {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} $$ Warning It is an extremely common mistake to forget to update the age of the policyholder in the second term. To minimize the chance of this mistake, it is advised to write the expression in terms of the deferred WL first and then convert it. Relationships \u00b6 Assurances & Annuities \u00b6 Consider the RV for a whole life assurance and annuity due. Notice that the annuity contains the assurance: \\[ \\begin{aligned} \\text{WL} &= v^{K_{x} + 1} \\\\ \\text{WLA}_{\\text{Due}} &= \\ddot{a}_{\\enclose{actuarial}{K_{x}+1}} \\\\ &= \\frac{1-v^{K_{x} + 1}}{d} \\end{aligned} \\] This relationship can be applied to their EPVs as well: \\[ \\begin{aligned} E \\left(\\ddot{a}_{\\enclose{actuarial}{K_{x}+1}} \\right) &= E \\left(\\frac{1-v^{K_{x} + 1}}{d} \\right) \\\\ \\ddot{a}_{x} &= \\frac{1- E \\left(v^{K_{x} + 1} \\right)}{d} \\\\ \\ddot{a}_{x} &= \\frac{1 - A_{x}}{d} \\\\ A_{x} &= 1 - d \\ddot{a}_{x} \\end{aligned} \\] The same relationship can be shown for a temporary annuity and an Endowment Assurance, NOT a term assurance : \\[ \\begin{aligned} \\text{TA}_{\\text{Due}} &= \\ddot{a}_{\\enclose{actuarial}{\\min(K_{x} + 1, n)}} \\\\ \\text{TA} &= {}_{\\{K_{x} \\le n-1\\}} v^{K_{x}+1} \\\\ \\text{EA} &= v^{\\min(K_{x} + 1, n)} \\\\ \\\\ \\therefore \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\frac{1 - A_{x:\\enclose{actuarial}{n}}}{d} \\\\ A_{x:\\enclose{actuarial}{n}} &= 1 - d \\ddot{a}_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Warning It is a very common mistake to think that term assurances and temporary annuities are related, as both share similar roles. However, it becomes clear when considering the random variables and actuarial notation that they are not related. The same logic can be applied to the Variance as well. In fact, this is the suggested method of calculating the variance of basic life annuities: \\[ \\begin{aligned} \\text{Var} \\left(\\ddot{a}_{\\enclose{actuarial}{K_{x}+1}} \\right) &= \\text{Var} \\left(\\frac{1-v^{K_{x} + 1}}{d} \\right) \\\\ \\text{Var} \\left(\\ddot{a}_{\\enclose{actuarial}{K_{x}+1}} \\right) &= \\frac{1}{d^2} \\cdot \\text{Var} \\left(v^{K_{x} + 1} \\right) \\\\ &= \\frac{1}{d^2} \\left[{}_{}^2 A_{x} - (A_{x})^2 \\right] \\\\ \\\\ \\therefore \\text{Var} \\left(\\ddot{a}_{\\enclose{actuarial}{\\min(K_{x} + 1, n)}} \\right) &= \\frac{1}{d^2} \\cdot \\left[{}_{}^2 A_{x:\\enclose{actuarial}{n}} - (A_{x:\\enclose{actuarial}{n}})^2 \\right] \\end{aligned} \\] Note There is no need to know the variance for a deferred or guaranteed life annuity. It is important to note that \\({}_{}^2 \\ddot{a}_{x}\\) is NOT the second moment of an annuity: \\[ \\begin{aligned} E \\left[(\\ddot{a}_{\\enclose{actuarial}{K_{x}+1}})^2 \\right] &= E \\left[\\left(\\frac{1-v^{K_{x} + 1}}{d} \\right)^2 \\right] \\\\ \\\\ {}_{}^2 \\ddot{a}_{x} &= \\frac{1 - (v^2)^{K_{x}+1}}{\\frac{(1+i)^2 - 1}{1 + (1+i)^2 - 1}} \\\\ &= \\frac{1 - {}_{}^2 A_{x}}{2d - d^2} \\\\ \\\\ \\therefore \\text{Var} \\left(\\ddot{a}_{\\enclose{actuarial}{K_{x}+1}} \\right) &\\ne {}_{}^2 \\ddot{a}_{x} - (\\ddot{a}_{x})^2 \\end{aligned} \\] Tip Despite this, it is still worthwhile to know how \\({}_{}^2 \\ddot{a}_{x}\\) relates to \\({}_{}^2 A_{x}\\) as the question could provide provide \\({}_{}^2 \\ddot{a}_{x}\\) when \\({}_{}^2 A_{x}\\) is needed. The key is understanding that the expression is now evaluated at a higher interest rate . Thus, the denominator changes to: \\[ \\begin{aligned} \\frac{(1+i)^2 - 1}{1 + {(1+i)^2 - 1}} &= \\frac{1 + 2i + i^2 - 1}{(1+i)^2} \\\\ &= \\frac{i^2}{(1+i)^2} + \\frac{2i}{(1+i)^2} \\\\ &= \\left(\\frac{i}{1+i} \\right)^2 + 2 \\left(\\frac{i}{1+i} \\right)\\left(\\frac{1}{1+i} \\right) \\\\ &= d^2 + 2d(1-d) \\\\ &= d^2 + 2d - 2d^2 \\\\ &= 2d - d^2 \\end{aligned} \\] Thus, the second moment of an assurance can be determined: \\[ \\begin{aligned} {}_{}^2 \\ddot{a}_{x} &= \\frac{1 - {}_{}^2 A_{x}}{2d - d^2} \\\\ {}_{}^2 \\ddot{a}_{x} (2d - d^2) &= 1 - {}_{}^2 A_{x} \\\\ {}_{}^2 A_{x} &= 1 - {}_{}^2 \\ddot{a}_{x} (2d - d^2) \\end{aligned} \\] Immediate & Due \u00b6 Recall that for Certain Annuities, the only difference between Due and Immediate are the payments at the start and end times . This relationship can be extended for Life Annuities as well. For \\(\\text{WL}_{\\text{Due}}\\) annuities, since the end point is infinity , the difference in the end points can be ignored . Thus, the only difference is the first payment of 1 , which can be easily accounted for: \\[ a_x = \\ddot{a}_x - 1 \\] Note This section uses \\(a_x\\) as the focus, as it is assumed that if needed, the due versions will be calculated and then converted to immediate , rather than calculating immediate annuities directly. Unfortunately, the difference at the end cannot be ignored for \\(\\text{TA}_{\\text{Due}}\\) annuities: \\[ a_{x:\\enclose{actuarial}{n}} = \\ddot{a}_{x:\\enclose{actuarial}{n}} - 1 + {}_{n}E_x \\] There is no need to memorize this expression as it can be easily derived using the WL conversion: \\[ \\begin{aligned} a_{x:\\enclose{actuarial}{n}} &= a_{x} - {}_{n}E_x a_{x+n} \\\\ &= \\ddot{a}_{x} - 1 - {}_{n}E_x (\\ddot{a}_{x+n} - 1) \\\\ &= \\ddot{a}_{x} - 1 - {}_{n}E_x \\cdot \\ddot{a}_{x+n} + {}_{n}E_x \\\\ &= \\ddot{a}_{x} - {}_{n}E_x \\cdot \\ddot{a}_{x+n} - 1 + {}_{n}E_x \\\\ &= \\ddot{a}_{x:\\enclose{actuarial}{n}} - 1 + {}_{n}E_x \\end{aligned} \\] For variance, the random variables (which are certain annuities) are used instead: \\[ \\begin{aligned} \\text{Var}(\\text{WL}_{\\text{Immediate}}) &= \\text{Var}(a_{\\enclose{actuarial}{k}}) \\\\ &= \\text{Var}(\\ddot{a}_{\\enclose{actuarial}{k+1}}-1) \\\\ &= \\text{Var}(\\ddot{a}_{\\enclose{actuarial}{k+1}}) \\\\ &= \\text{Var}(\\text{WL}_{\\text{Due}}) \\\\ \\\\ \\text{Var}(\\text{TA}_{\\text{Immediate}}) &= \\text{Var}(a_{\\enclose{actuarial}{\\min(k,n)}}) \\\\ &= \\text{Var}(\\ddot{a}_{\\enclose{actuarial}{\\min(k+1,n+1)}}-1) \\\\ &= \\text{Var}(\\ddot{a}_{\\enclose{actuarial}{\\min(k+1,n+1)}}) \\\\ &= \\text{Var} \\left( \\frac{1-v^{\\min(k+1,n+1)}}{d} \\right) \\\\ &= \\frac{1}{d^2} \\text{Var}(v^{\\min(k+1,n+1)}) \\\\ &= \\frac{1}{d^2} \\left({}_{2}A_{x:\\enclose{actuarial}{n+1}} - (A_{x:\\enclose{actuarial}{n+1}})^2 \\right) \\end{aligned} \\] Thus, the variance is the same for a whole life annuity but NOT for a temporary annuity . Other Calculations \u00b6 Recursion \u00b6 Following the same logic as assurances, Annuities can also be recursively expressed as a function of itself: If the policyholder dies , nothing happens If the policyholder survives , they would receive the benefits of the future periods Regardless , the benefit of 1 was already paid at the start of the year \\[ \\begin{aligned} \\text{WL}_\\text{Due} &= 1 + \\begin{cases} 0,& q_x \\\\ v\\ddot{a}_{x+1} ,& p_x \\end{cases} \\\\ \\\\ \\therefore \\ddot{a}_{x} &= 1 + vp_x\\ddot{a}_{x+1} \\end{aligned} \\] The same can be shown for TAs, but remember that the remaining duration of the policy must decrease as well: \\[ \\ddot{a}_{x:\\enclose{actuarial}{n}} = 1 + vp_x \\ddot{a}_{x+1:\\enclose{actuarial}{n-1}} \\] Probability \u00b6 Questions could also ask for the probability that the present value of the benefits takes on some value \\(u\\) . \\[ \\begin{aligned} P(\\text{WL}_\\text{Due} \\le u) &= P \\left(\\frac{1-v^{k+1}}{d} \\le u \\right) \\\\ &= P \\left(1-v^{k+1} \\le ud \\right) \\\\ &= P \\left(v^{k+1} \\ge 1-ud \\right) \\\\ &= P \\left((k+1) \\ln v \\ge \\ln(1-ud) \\right) \\\\ &= P \\left(k+1 \\le \\frac{\\ln(1-ud)}{\\ln v} \\right) \\\\ &= P \\left(k \\le \\frac{\\ln(1-ud)}{\\ln v} - 1 \\right) \\\\ \\end{aligned} \\] Warning Similar to assurances, it is a common mistake to forget to flip the inequality sign as \\(\\ln v \\lt 0\\) . Questions will not be as straightforward as above . Sometimes, the required value of \\(K_x\\) has to be determined intuitively from the given scenario . For instance, a question may ask for the probability that the sum of undiscounted benefits exceeds its EPV. Assuming the EPV is 12.5, the policyholder would have to receive 13 benefit payments . The probability of receiving 13 benefits is equivalent to the probability of suriving 12 years . Warning It is a common mistake to use \\(K_x \\gt 13\\) , since 13 payments must be made. However, since benefits are paid at the start of the year , it doesnt matter whether the policyholder survives the 13 th year or not; as long as they survive 12 years to see the 13 th year and receive the last required payment . The RHS of the expression is unlikely to be an integer. However, \\(k\\) can only take integer values. Thus, the values can rounded DOWN to the nearest whole number (EG. 4): \\[ \\begin{aligned} P(k \\le \\frac{\\ln(1-ud)}{\\ln v} - 1) &= P(k \\le 4) \\\\ &= P(T_x \\le 4) \\\\ &= {}_{4}p_x \\end{aligned} \\] Percentiles \u00b6 Similarly, questions could ask for the percentile of the PV . Since the PV is a transformation of the future lifetime variable, the percentile of the PV uses the percentile of the future lifetime variable . Thus, we can find the percentile of future lifetime and then substitute that into the PV random variable to obtain its percentile. Payable M-thly \u00b6 Similar to assurances, annuities can also be payable \\(m\\) times a year. Most of the key ideas carry over from the assurances, thus this section will will mainly focus on the unique aspects for annuities. Consider the EPV of a payable m-thly whole life annuity due: \\[ \\text{EPV} = B \\cdot \\ddot{a}^{(m)}_{x} \\] \\(B\\) is intepreted as the total benefit paid in the year . In other words, \\(\\frac{B}{m}\\) is paid in each period. This change in the cashflows influences how the annuities are converted from one another: \\[ \\begin{aligned} \\ddot{a}^{(m)}_{x} &= \\ddot{a}^{(m)}_{x} - \\frac{1}{m} \\\\ \\\\ \\ddot{a}^{(m)}_{x} &= \\ddot{a}^{(m)}_{x:\\enclose{actuarial}{n}} - \\frac{1}{m} (1 - {}_{n}E_{x}) \\end{aligned} \\] Another difference is in the interest rates used to convert annuities and assurances: \\[ \\begin{aligned} \\ddot{a}^{(m)}_x &= \\frac{1 - A^{(m)}_x}{d^{(m)}} \\end{aligned} \\] Uniform Distribution of Deaths \u00b6 Since we have already used UDD for assurances and since annuities & assurances are related, we can combine both of them together: \\[ \\begin{aligned} \\ddot{a}^{(m)}_x &= \\frac{1 - A^{(m)}_x}{d^{(m)}} \\\\ &= \\frac{1 - \\frac{i}{i^{(m)}}A_x}{d^{(m)}} \\\\ &= \\frac{\\frac{i^{(m)}-iA_x}{i^{(m)}}}{d^{(m)}} \\\\ &= \\frac{i^{(m)}-iA_x}{i^{(m)}d^{(m)}} \\\\ &= \\frac{i^{(m)}-i(1-d\\ddot{a}_x)}{i^{(m)}d^{(m)}} \\\\ &= \\frac{i^{(m)}-i+id\\ddot{a}_x}{i^{(m)}d^{(m)}} \\\\ &= \\frac{id}{i^{(m)}d^{(m)}} \\ddot{a}_x - \\frac{i-i^{(m)}}{i^{(m)}d^{(m)}} \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) \\\\ \\\\ \\therefore \\alpha(m) &= \\frac{id}{i^{(m)}d^{(m)}} \\\\ \\\\ \\therefore \\beta(m) &= \\frac{i-i^{(m)}}{i^{(m)}d^{(m)}} \\end{aligned} \\] Note Recall that nominal and effective rates are linked using: \\[ \\begin{aligned} i^{(m)} &= m[(1+i)^\\frac{1}{m}-1] \\\\ d^{(m)} &= m[1-(1+d)^\\frac{1}{m}] \\end{aligned} \\] However, for \\(i=0.05\\) , \\(\\alpha\\) and \\(\\beta\\) are provided inside the SULT for common values of \\(m\\) , thus there is no need to waste precious time calculating them. Woolhouse Approximation \u00b6 Since a continuous annuity can be expressed as a sum of discrete pure endowments , the trapezoidal rule and euler-macluarin correction can be used to approximate its EPV. The key difference from before is understanding that the EPV of an annuity due is a LEFT riemann sum as there is a payment at time 0. \\[ \\begin{aligned} \\bar{a}_x &= \\int^{\\infty}_0 v^t {}_{t}p_x \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b-h)] - \\frac{h}{2} [f(a)-f(b)] + \\frac{h^2}{12} [f'(a)-f'(b)] \\end{aligned} \\] The first derivatives can be found below: \\[ \\begin{aligned} f'(n) &= \\frac{d}{dt} (e^{-\\delta n} {}_{n}p_x) \\\\ &= {}_{n}p_x \\delta e^{-\\delta n} - e^{-\\delta n} {}_{n}p_x \\mu_{x+n} \\\\ &= e^{-\\delta n} {}_{n}p_x (\\delta + \\mu_{x+n}) \\\\ &= {}_{n}E_x (\\delta + \\mu_{x+n}) \\\\ \\\\ \\therefore f'(0) &= -(\\delta + \\mu_{x}) \\\\ \\therefore f'(\\infty) &= 0 \\end{aligned} \\] Assuming \\(h=1\\) , \\[ \\begin{aligned} \\bar{a}_x & \\approx (1) [{}_{0}E_x + {}_{1}E_x + \\dots + {}_{\\infty-1}E_x] - \\frac{1}{2} [{}_{0}E_x-{}_{\\infty-1}E_x] + \\frac{1}{12} [-(\\delta + \\mu_{x}) - 0] \\\\ & \\approx [1 + {}_{n}E_1 + \\dots] - \\frac{1}{2} [1-0] - \\frac{1}{12} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) \\end{aligned} \\] Assuming \\(h=\\frac{1}{m}\\) instead, \\[ \\begin{aligned} \\bar{a}_x & \\approx \\left(\\frac{1}{m} \\right) [{}_{0}E_x + {}_{\\frac{1}{m}}E_x + \\dots + {}_{\\infty-\\frac{1}{m}}E_x] - \\frac{1}{2m} [{}_{0}E_x-{}_{\\infty-\\frac{1}{m}}E_x] + \\frac{1}{12m^2} [-(\\delta + \\mu_{x}) - 0] \\\\ & \\approx \\left(\\frac{1}{m} \\right) [1 + {}_{\\frac{1}{m}}E_x + \\dots] - \\frac{1}{2m} [1-0] - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}^{(m)}_x - \\frac{1}{2m} - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\end{aligned} \\] Combining both together, \\[ \\begin{aligned} \\bar{a}_x &= \\bar{a}_x \\\\ \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) & \\approx \\ddot{a}^{(m)}_x - \\frac{1}{2m} - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ \\ddot{a}^{(m)}_x & \\approx \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) + \\frac{1}{2m} + \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - (\\frac{1}{2}-\\frac{1}{2m}) - (\\frac{1}{12} - \\frac{1}{12m^2})(\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - \\frac{m-1}{2m} - \\frac{m^2-1}{12m^2}(\\delta + \\mu_{x}) \\end{aligned} \\] Note Even though not in a continuous setting, both the force of interest and force of mortality are used in the approximation; which may cause confusion for some. This process is known as the Woolhouse Approximation . If the error term is omitted, then it is known as the 2-term Woolhouse Approximation else it is known as the 3-term Woolhouse Approximation . The 3-term woolhouse approximation is extremely accurate due to its mathematical roots. It so much more accurate than UDD such that when calculating a continuous assurance , it is advisable to calculate the corresponding annuity using the 3 term approximation first and then convert it, for the best results. Unfortunately, the drawback is that it requires knowledge about the force of mortality, which is not commonly provided in life tables. Thus, although less accurate, the 2 term approach can be easily used in any situation. Term Approximations \u00b6 Unlike assurances, the approximations for annuities have multiple components. Thus, the approximations for term annuities are slightly different from the ones used for WL. However, there is NO need to remember a seperate expression for it, it can be calculated by converting the WL to a TA: \\[ \\begin{aligned} \\ddot{a}^{(m)}_{x:\\enclose{actuarial}{n}} &= \\ddot{a}^{(m)}_{x} - {}_{n}E_x \\ddot{a}^{(m)}_{x+n} \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) - {}_{n}E_x [\\alpha(m)\\ddot{a}_{x:\\enclose{actuarial}{n}} - \\beta(m)] \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) - {}_{n}E_x \\alpha(m)\\ddot{a}_{x:\\enclose{actuarial}{n}} - {}_{n}E_x \\beta(m) \\\\ &= \\alpha(m) [\\ddot{a}_x - {}_{n}E_x \\ddot{a}_{x:\\enclose{actuarial}{n}}] - \\beta(m) [1-{}_{n}E_x] \\\\ &= \\alpha(m) \\ddot{a}_{x:\\enclose{actuarial}{n}} - \\beta(m) [1-{}_{n}E_x] \\end{aligned} \\] A similar approach can be taken for the woolhouse approximation, resulting in the following: \\[ a^{(m)}_{x:\\enclose{actuarial}{n}} = \\ddot{a}_{x:n} - \\frac{m-1}{2m} (1-{}_{n}E_{x}) \\] Tip The underlying reason for taking this roundabout approach is that the approximation shown above and provided by in the formula sheet is only applicable for WL annuities . Rather than memorizing the approximation for each type of annuity, it is much more efficient to simply convert the annuity into a combination of WL annuities to and approximate from there. For instance, for a guaranteed annuity annuity, \\[ \\begin{aligned} \\ddot{a}^{(m)}_{\\overline{x:\\enclose{actuarial}{n}}} &= \\ddot{a}^{(m)}_{\\enclose{actuarial}{n}} + {}_{n}E_{x} \\cdot \\ddot{a}^{(m)}_{x+n} \\\\ &= \\frac{1 - v^{10}}{d^{(m)}} + {}_{n}E_{x} \\cdot \\left(\\ddot{a}_{x+n} - \\frac{m-1}{2m} \\right) \\end{aligned} \\] Fractional Recursion \u00b6 Recursions follow the same logic as the discrete case. However, since the benefits are not paid out at JUST the start of the year , the the extent that future benefits have to be discounted changes . If benefits are paid out every half a year : \\[ \\ddot{a}^{(2)}_{x} = 1 + v^{0.5} p_x\\ddot{a}^{(2)}_{x+0.5} \\] There are two key differences to take note of: The extent of discounting changes to half a year The age of the future EPV is at a fractional age Thus, by re-arranging the equation, we can solve for the fractional age EPV . Payable Continuously \u00b6 Similarly, annuities can also be paid continuously. All the concepts are exactly the same as the assurances , thus this section will just be stating the final results. Approximations \u00b6 \\[ \\begin{aligned} \\bar{a}_{x} &= \\alpha(\\infty)\\ddot{a}_x - \\beta(\\infty) \\\\ \\\\ \\bar{a}_{x} &= \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12}(\\delta + \\mu_{x}) \\end{aligned} \\] Tip The above expressions were obtained by using the monthly approximations and setting \\(m \\to \\infty\\) . For the woolhouse approximation, the simplification happens via the following process: \\[ \\begin{aligned} \\frac{m-1}{2m} &= \\frac{m \\left(1 - \\frac{1}{m} \\right)}{2m} =\\frac{1 - \\frac{1}{m}}{2} \\\\ \\therefore \\lim_{m \\to \\infty} \\frac{m-1}{2m} &= \\frac{1 - \\frac{1}{\\infty}}{2} = \\frac{1}{2} \\\\ \\\\ \\frac{m^{2} - 1}{12m^{2}} &= \\frac{m^{2} \\left(1 - \\frac{1}{m^{2}} \\right)}{12m^{2}} = \\frac{1 - \\frac{1}{m^{2}}}{12} \\\\ \\therefore \\lim_{m \\to \\infty} \\frac{m^{2} - 1}{12m^{2}} &= \\frac{1 - \\frac{1}{\\infty}^{2}}{12} = \\frac{1}{12} \\end{aligned} \\] Exponential Distribution \u00b6 \\[ \\begin{aligned} \\bar{a}_{x} &= \\int^{\\infty}_{0} v^{t} {}_{t}P_{x} \\\\ &= \\int^{\\infty}_{0} e^{- \\delta t} \\cdot e^{- \\mu t} \\\\ &= \\int^{\\infty}_{0} e^{- (\\mu + \\delta)t} \\\\ &= \\frac{1}{\\mu + \\delta} \\\\ \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\bar{a}_{x} - {}_{n}E_{x} \\bar{a}_{x+n} \\\\ &= \\bar{a}_{x} \\cdot (1 - {}_{n}E_{x}) \\end{aligned} \\] Uniform Distribution \u00b6 Even though the distribution is known, it is difficult to integrate for the annuity EPV. Thus, it is recommended to compute the EPV of a continuous assurance and then convert it into a continuous annuity: \\[ \\bar{a}_{x} = \\frac{1 - \\bar{A}_{x}}{\\delta} \\]","title":"Life Annuities"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#life-annuities","text":"","title":"Life Annuities"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#overview","text":"Life Annuity contracts promise to pay out a stream of benefits in the future for as long the policyholder remains alive. Similar to life insurance, the benefits can be paid discretely or continuously and can be valued through their EPV. Note that these are different from the annuities covered in Exam FM. The payments for those annuities are made regardless of the life of the policyholder, known as Annuity Certain . They serve as the foundation to understanding Life Annuities. Similar to certain annuities, many of the concepts that apply to a life annuity due also apply to a life annuity immediate. Thus, for brevity, the entirety of this section will only cover life annuities due , unless explicitly stated otherwise.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#review-annuities-certain","text":"There are two types of payment structures: Period Start Period End Paid in Advance Paid in Arrears Annuity Due Annuity Immediate \\(\\ddot{a}_{\\enclose{actuarial}{n}}\\) \\(a_{\\enclose{actuarial}{n}}\\) The overall PV of the annuity is the sum of the PV of the stream of payments: \\[ \\begin{aligned} a_{\\enclose{actuarial}{n}} &= v + v^2 + v^3 + \\dots + v^n \\\\ &= \\frac{v-v^{n+1}}{1-v} \\\\ &= \\sum^n_{k=1} v^k \\\\ &= \\frac{v(1-v^n)}{1-v} \\\\ &= \\frac{1-v^n}{i} \\\\ \\\\ \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + \\dots + v^n \\\\ &= \\sum^{n-1}_{k=0} v^k \\\\ &= \\frac{1- v^{n}}{1-v} \\\\ &= \\frac{1- v^{n}}{d} \\end{aligned} \\] Notice that the payments simply differ by one period and hence one discounting factor: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= 1 + v + v^2 + v^3 + \\dots + v^n \\\\ &= (1+i) (v + v^2 + v^3 + \\dots + v^n) \\\\ &= (1+i) \\sum^{n}_{k=1} v^k \\\\ &= (1+i) a_{\\enclose{actuarial}{n}} \\end{aligned} \\] From another perspective, only the payments at the end points \\(t=0\\) and \\(t=n\\) are different: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= \\sum^{n-1}_0 v^k \\\\ &= v^0 + \\sum^{n-1}_{k=1} v^k \\\\ &= 1 + a_{\\enclose{actuarial}{n-1}} \\\\ &= 1 + a_{\\enclose{actuarial}{n}} - v^n \\\\ \\\\ \\therefore a_{\\enclose{actuarial}{n}} &= \\ddot{a}_{\\enclose{actuarial}{n}} - 1 + v^n \\end{aligned} \\]","title":"Review: Annuities Certain"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#payable-discretely","text":"The simplest form of life annuity due contract pays the benefits at the beginning of the year , provided that the insured is alive at that time. Since we are only interested in the status of the insured at discrete integer ages , \\(K_{x}\\) is used as the survival model. Let \\(Y\\) be the random variable denoting the present value of the benefit payable. Since the policyholder receives benefits across multiple years, the PV is sum of these benefits and is represented by an annuity certain . \\[ Y = B \\cdot \\ddot{a}_{\\enclose{actuarial}{K_{x} + 1}} \\] Note The PV can be a bit confusing as it is very different from an assurance. They key is understanding that we must look backwards to determine how many payments the policyholder has received at that age in order to determine the PV. Thus, the EPV is the expectation of \\(Y\\) ; it is the sumproduct of ALL possible PVs and their associated probabilities: \\[ \\begin{aligned} \\text{EPV} &= \\sum B \\cdot \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k}P_{x} {}_{}q_{x+k} \\\\ E(Y) &= \\sum B \\cdot \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\end{aligned} \\] Note The associated probability might seem counter-intuitive, since a life annuity makes payment for as long as the insured is alive , so why are we using the probability of death? The key is understanding that the random variable was defined as the sum of payments up till that point . In other words, it assumes that the policyholder dies at that age that the PV is calculated. For simplicity, the benefits are usually assumed to be 1 \\((B=1)\\) . This allows the EPVs to be easily scaled for any level of \\(B\\) required.","title":"Payable Discretely"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#actuarial-notation","text":"Most of the notation for Life Assurances applies to Life Annuities as well. Instead of \\(A\\) , \\(\\ddot{a}\\) is used to represent annuities. It is intepreted as the present value of a contract where a benefit of 1 is paid at the beginning of the year for as long as the status is active .","title":"Actuarial Notation"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#whole-life-annuity","text":"Whole Life Annuities covers the insured indefinitely and thus will pay out for as long as the insured survives . Let \\(\\text{WL}_{\\text{Due}}\\) be the RV denoting the PV of the stream of benefits: \\[ \\begin{aligned} \\text{WL}_{\\text{Due}} &=\\ddot{a}_{\\enclose{actuarial}{K_{x}+1}} \\end{aligned} \\] Thus, its EPV can be calculated: \\[ \\begin{aligned} E(\\text{WLA}_{\\text{Due}}) &= \\sum^{\\infty}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\\\ \\ddot{a}_{x} &= \\sum^{\\infty}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\end{aligned} \\] The EPV can be furthered simplified, allowing a life annuity to be viewed as the sum of a series of pure endowments : \\[ \\begin{aligned} \\ddot{a}_{x} &= \\sum^{\\infty}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k}} \\cdot {}_{k|}q_{x} \\\\ &=\\sum^{\\infty}_{k = 0} \\left(\\sum^{n-1}_{j=0} v^j \\right) \\cdot {}_{k|}q_{x} \\\\ &= v^0 \\cdot {}_{0|}q_{x} + (v^0 + v^1) \\cdot {}_{1|}q_{x} + (v^0 + v^1 + v^2) \\cdot {}_{2|}q_{x} + \\dots \\\\ &= v^0 ({}_{0|}q_{x} + {}_{1|}q_{x} + \\dots) + v^1 ({}_{1|}q_{x} + {}_{2|}q_{x} + \\dots) + v^2 ({}_{2|}q_{x} + {}_{3|}q_{x} + \\dots) + \\dots \\\\ &= \\sum^{\\infty}_{j=0} v^j \\cdot \\left(\\sum^{\\infty}_{k = j} {}_{k|}q_{x} \\right) \\\\ &= \\sum^{\\infty}_{j=0} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] Warning Note that the above formula can be used to easily prove the relationships between different kinds of annuities, but is NOT a proper definition of an annuity . It hence cannot be adjusted to compute the variance or other \"special\" annuities. Tip Notice that the expression is similar to the kurtate expectation of life. The key difference is that the annuity contains the discounting term . Thus, if the interest was 0 (no discounting), then the two are equal. The second moment and variance for all life annuities will be covered in a later section.","title":"Whole Life Annuity"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#temporary-annuity","text":"Temporary Life Annuities covers the insured for a specified period \\(n\\) and thus pays out for as long as the insured survives during that period only . Let \\(\\text{TA}_\\text{Due}\\) be the RV denoting the PV of the survival benefit: \\[ \\begin{aligned} \\text{TA}_\\text{Due} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{K_{x}+1}},& k = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{n}},& k \\ge n \\\\ \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{\\min(K_{x} + 1, n)}} \\end{aligned} \\] Thus, the EPV can be determined and simplified using the same approach as before: \\[ \\begin{aligned} E(\\text{TA}_\\text{Due}) &= \\sum^{n-1}_{k = 0} \\ddot{a}_{\\enclose{actuarial}{k}} \\cdot {}_{k|}q_x + \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{n}p_x \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{j=0} v^j \\cdot {}_{j}p_{x} \\end{aligned} \\] Warning It is a common mistake to use \\(n\\) as the upper limit of the summation as there are \\(n\\) payments. While this is true, the summation starts from 0 . Thus, for there to be \\(n\\) payments,the summation must end at \\(n-1\\) . Tip Intuitively, the EPV of an \\(n\\) year temporary annuity should NOT be larger than \\(n\\) as there are \\(n\\) payments of 1 which are discounted. This can be used as a simple sense check when calculating the EPV.","title":"Temporary Annuity"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#deferred-annuities","text":"Deferred Annuities are variations of the above contracts where the assurance starts \\(n\\) years later rather than immediately. Let \\(\\text{DWL}_{\\text{Due}}\\) be the RV denoting the PV of a Deferred Whole Life Annuity Due : \\[ \\begin{aligned} \\text{DWL}_{\\text{Due}} &= \\begin{cases} 0, & k = 0, 1, 2, \\dots, n-1 \\\\ v^n \\cdot \\ddot{a}_{\\enclose{actuarial}{K_{x}+1-n}}, & k \\ge n \\end{cases} \\end{aligned} \\] It can be expressed as the difference between a \\(\\text{WL}_{\\text{Due}}\\) and a certain annuity for the guaranteed period: \\[ \\begin{aligned} \\text{DWL}_{\\text{Due}} &= v^n \\cdot \\ddot{a}_{\\enclose{actuarial}{k+1-n}} \\\\ &= v^n \\cdot \\frac{1-v^{k+1-n}}{d} \\\\ &= \\frac{v^{n}-v^{k+1}}{d} \\\\ &= \\frac{(1-v^{k+1}) - (1-v^{n})}{d} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{k+1}} -\\ddot{a}_{\\enclose{actuarial}{n}} \\\\ \\\\ \\therefore \\text{DWL}_{\\text{Due}} &= \\begin{cases} 0, & k = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{k+1}} -\\ddot{a}_{\\enclose{actuarial}{n}} , & k \\ge n \\end{cases} \\end{aligned} \\] Thus, its EPV can be determined: \\[ \\begin{aligned} E(\\text{DWL}_{\\text{Due}}) &= \\sum^{\\infty}_{k=n} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x} \\\\ {}_{n|}\\ddot{a}_{x} &= \\sum^{\\infty}_{k=0} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x} - \\sum^{n-1}_{k=0} (\\ddot{a}_{\\enclose{actuarial}{k+1}} - \\ddot{a}_{\\enclose{actuarial}{n}}) \\cdot {}_{k|}q_{x} \\\\ &= \\sum^{\\infty}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} - \\sum^{\\infty}_{k=0} \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_{x} - \\sum^{n-1}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} + \\sum^{n-1}_{k=0} \\ddot{a}_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_{x} \\\\ &= \\sum^{\\infty}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} - \\sum^{n-1}_{k=0} \\ddot{a}_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_{x} \\\\ &= \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Similar to assurances, its EPV its effectively a \\(\\text{DWL}_{\\text{Due}}\\) issued at age \\(x+n\\) , after adjusting for interest and survival : \\[ \\begin{aligned} {}_{n|}\\ddot{a}_{x} &= \\sum^{\\infty}_{j=n} v^j \\cdot {}_{j}p_{x} \\\\ &= \\sum^{\\infty}_{j=0} v^{j+n} \\cdot {}_{j+n}p_{x} \\\\ &= \\sum^{\\infty}_{j=0} v^{j} v^{n} \\cdot {}_{n}p_{x} {}_{j}p_{x+n} \\\\ &= v^{n} \\cdot {}_{n}p_{x} \\sum^{\\infty}_{j=0} v^{j} \\cdot {}_{j}p_{x+n} \\\\ &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\] Similarly, this allows a \\(\\text{TA}_{\\text{Due}}\\) to be expressed as the difference of two \\(\\text{WL}_{\\text{Due}}\\) annuities issued at different times : \\[ \\begin{aligned} {}_{n|}\\ddot{a}_{x} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_x - \\ddot{a}_{x:\\enclose{actuarial}{n}} &= {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\ddot{a}_x - {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} \\] For brevity, Deferred Temporary Annuities will NOT be covered. The same concepts and warnings from its assurance counterpart apply to it as well.","title":"Deferred Annuities"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#guaranteed-annuities","text":"Guaranteed Life Annuities are whole life annuities with a guarantee in the first \\(n\\) years. The benefit will be paid out even if the insured dies during this period . Let \\(\\text{GA}_{\\text{Due}}\\) be the RV denoting the PV of the stream of benefits: \\[ \\begin{aligned} \\text{GA}_{\\text{Due}} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& k = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_{x}+1}},& k \\ge n \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{max(K_{x}+1,n)}} \\end{aligned} \\] Note Since the first payments are paid out regardless, they can be represented using a certain annuity . It can be manipulated so that it becomes easier to work with: \\[ \\begin{aligned} \\text{GA}_{\\text{Due}} &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_{x} = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_{x}+1}},& K_{x} \\ge n \\end{cases} \\\\ &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}},& K_{x} = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_{x}+1}} - \\ddot{a}_{\\enclose{actuarial}{n}} + \\ddot{a}_{\\enclose{actuarial}{n}},& K_{x} \\ge n \\end{cases} \\\\ &= \\begin{cases} \\ddot{a}_{\\enclose{actuarial}{n}} \\\\ \\ddot{a}_{\\enclose{actuarial}{n}} \\end{cases} + \\begin{cases} 0,& K_{x} = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_{x}+1}} - \\ddot{a}_{\\enclose{actuarial}{n}},& K_{x} \\ge n \\end{cases} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{n}} + \\begin{cases} 0,& K_{x} = 0, 1, 2, \\dots, n-1 \\\\ \\ddot{a}_{\\enclose{actuarial}{K_{x}+1}} - \\ddot{a}_{\\enclose{actuarial}{n}},& K_{x} \\ge n \\end{cases} \\end{aligned} \\] Note Because of this, they are sometimes referred to as \\(\\text{WL}_{\\text{Due}}\\) annuities that are certain for \\(n\\) years . Thus, the EPV can be determined: $$ \\begin{aligned} E(\\text{GA} {\\text{Due}}) &= \\ddot{a} {n}} + 0 \\cdot {} {n}q_x + \\sum^{\\infty} (\\ddot{a} {\\enclose{actuarial}{k+1}} - \\ddot{a} {n}}) \\cdot {} {k|}q \\ \\ddot{a}_{\\overline{x:\\enclose{actuarial}{n}}} &= \\ddot{a}_{\\enclose{actuarial}{n}} + {}_{n|}\\ddot{a}_{x} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{n}} + {}_{n}E_{x} \\cdot \\ddot{a}_{x+n} \\end{aligned} $$ Warning It is an extremely common mistake to forget to update the age of the policyholder in the second term. To minimize the chance of this mistake, it is advised to write the expression in terms of the deferred WL first and then convert it.","title":"Guaranteed Annuities"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#relationships","text":"","title":"Relationships"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#assurances-annuities","text":"Consider the RV for a whole life assurance and annuity due. Notice that the annuity contains the assurance: \\[ \\begin{aligned} \\text{WL} &= v^{K_{x} + 1} \\\\ \\text{WLA}_{\\text{Due}} &= \\ddot{a}_{\\enclose{actuarial}{K_{x}+1}} \\\\ &= \\frac{1-v^{K_{x} + 1}}{d} \\end{aligned} \\] This relationship can be applied to their EPVs as well: \\[ \\begin{aligned} E \\left(\\ddot{a}_{\\enclose{actuarial}{K_{x}+1}} \\right) &= E \\left(\\frac{1-v^{K_{x} + 1}}{d} \\right) \\\\ \\ddot{a}_{x} &= \\frac{1- E \\left(v^{K_{x} + 1} \\right)}{d} \\\\ \\ddot{a}_{x} &= \\frac{1 - A_{x}}{d} \\\\ A_{x} &= 1 - d \\ddot{a}_{x} \\end{aligned} \\] The same relationship can be shown for a temporary annuity and an Endowment Assurance, NOT a term assurance : \\[ \\begin{aligned} \\text{TA}_{\\text{Due}} &= \\ddot{a}_{\\enclose{actuarial}{\\min(K_{x} + 1, n)}} \\\\ \\text{TA} &= {}_{\\{K_{x} \\le n-1\\}} v^{K_{x}+1} \\\\ \\text{EA} &= v^{\\min(K_{x} + 1, n)} \\\\ \\\\ \\therefore \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\frac{1 - A_{x:\\enclose{actuarial}{n}}}{d} \\\\ A_{x:\\enclose{actuarial}{n}} &= 1 - d \\ddot{a}_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Warning It is a very common mistake to think that term assurances and temporary annuities are related, as both share similar roles. However, it becomes clear when considering the random variables and actuarial notation that they are not related. The same logic can be applied to the Variance as well. In fact, this is the suggested method of calculating the variance of basic life annuities: \\[ \\begin{aligned} \\text{Var} \\left(\\ddot{a}_{\\enclose{actuarial}{K_{x}+1}} \\right) &= \\text{Var} \\left(\\frac{1-v^{K_{x} + 1}}{d} \\right) \\\\ \\text{Var} \\left(\\ddot{a}_{\\enclose{actuarial}{K_{x}+1}} \\right) &= \\frac{1}{d^2} \\cdot \\text{Var} \\left(v^{K_{x} + 1} \\right) \\\\ &= \\frac{1}{d^2} \\left[{}_{}^2 A_{x} - (A_{x})^2 \\right] \\\\ \\\\ \\therefore \\text{Var} \\left(\\ddot{a}_{\\enclose{actuarial}{\\min(K_{x} + 1, n)}} \\right) &= \\frac{1}{d^2} \\cdot \\left[{}_{}^2 A_{x:\\enclose{actuarial}{n}} - (A_{x:\\enclose{actuarial}{n}})^2 \\right] \\end{aligned} \\] Note There is no need to know the variance for a deferred or guaranteed life annuity. It is important to note that \\({}_{}^2 \\ddot{a}_{x}\\) is NOT the second moment of an annuity: \\[ \\begin{aligned} E \\left[(\\ddot{a}_{\\enclose{actuarial}{K_{x}+1}})^2 \\right] &= E \\left[\\left(\\frac{1-v^{K_{x} + 1}}{d} \\right)^2 \\right] \\\\ \\\\ {}_{}^2 \\ddot{a}_{x} &= \\frac{1 - (v^2)^{K_{x}+1}}{\\frac{(1+i)^2 - 1}{1 + (1+i)^2 - 1}} \\\\ &= \\frac{1 - {}_{}^2 A_{x}}{2d - d^2} \\\\ \\\\ \\therefore \\text{Var} \\left(\\ddot{a}_{\\enclose{actuarial}{K_{x}+1}} \\right) &\\ne {}_{}^2 \\ddot{a}_{x} - (\\ddot{a}_{x})^2 \\end{aligned} \\] Tip Despite this, it is still worthwhile to know how \\({}_{}^2 \\ddot{a}_{x}\\) relates to \\({}_{}^2 A_{x}\\) as the question could provide provide \\({}_{}^2 \\ddot{a}_{x}\\) when \\({}_{}^2 A_{x}\\) is needed. The key is understanding that the expression is now evaluated at a higher interest rate . Thus, the denominator changes to: \\[ \\begin{aligned} \\frac{(1+i)^2 - 1}{1 + {(1+i)^2 - 1}} &= \\frac{1 + 2i + i^2 - 1}{(1+i)^2} \\\\ &= \\frac{i^2}{(1+i)^2} + \\frac{2i}{(1+i)^2} \\\\ &= \\left(\\frac{i}{1+i} \\right)^2 + 2 \\left(\\frac{i}{1+i} \\right)\\left(\\frac{1}{1+i} \\right) \\\\ &= d^2 + 2d(1-d) \\\\ &= d^2 + 2d - 2d^2 \\\\ &= 2d - d^2 \\end{aligned} \\] Thus, the second moment of an assurance can be determined: \\[ \\begin{aligned} {}_{}^2 \\ddot{a}_{x} &= \\frac{1 - {}_{}^2 A_{x}}{2d - d^2} \\\\ {}_{}^2 \\ddot{a}_{x} (2d - d^2) &= 1 - {}_{}^2 A_{x} \\\\ {}_{}^2 A_{x} &= 1 - {}_{}^2 \\ddot{a}_{x} (2d - d^2) \\end{aligned} \\]","title":"Assurances &amp; Annuities"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#immediate-due","text":"Recall that for Certain Annuities, the only difference between Due and Immediate are the payments at the start and end times . This relationship can be extended for Life Annuities as well. For \\(\\text{WL}_{\\text{Due}}\\) annuities, since the end point is infinity , the difference in the end points can be ignored . Thus, the only difference is the first payment of 1 , which can be easily accounted for: \\[ a_x = \\ddot{a}_x - 1 \\] Note This section uses \\(a_x\\) as the focus, as it is assumed that if needed, the due versions will be calculated and then converted to immediate , rather than calculating immediate annuities directly. Unfortunately, the difference at the end cannot be ignored for \\(\\text{TA}_{\\text{Due}}\\) annuities: \\[ a_{x:\\enclose{actuarial}{n}} = \\ddot{a}_{x:\\enclose{actuarial}{n}} - 1 + {}_{n}E_x \\] There is no need to memorize this expression as it can be easily derived using the WL conversion: \\[ \\begin{aligned} a_{x:\\enclose{actuarial}{n}} &= a_{x} - {}_{n}E_x a_{x+n} \\\\ &= \\ddot{a}_{x} - 1 - {}_{n}E_x (\\ddot{a}_{x+n} - 1) \\\\ &= \\ddot{a}_{x} - 1 - {}_{n}E_x \\cdot \\ddot{a}_{x+n} + {}_{n}E_x \\\\ &= \\ddot{a}_{x} - {}_{n}E_x \\cdot \\ddot{a}_{x+n} - 1 + {}_{n}E_x \\\\ &= \\ddot{a}_{x:\\enclose{actuarial}{n}} - 1 + {}_{n}E_x \\end{aligned} \\] For variance, the random variables (which are certain annuities) are used instead: \\[ \\begin{aligned} \\text{Var}(\\text{WL}_{\\text{Immediate}}) &= \\text{Var}(a_{\\enclose{actuarial}{k}}) \\\\ &= \\text{Var}(\\ddot{a}_{\\enclose{actuarial}{k+1}}-1) \\\\ &= \\text{Var}(\\ddot{a}_{\\enclose{actuarial}{k+1}}) \\\\ &= \\text{Var}(\\text{WL}_{\\text{Due}}) \\\\ \\\\ \\text{Var}(\\text{TA}_{\\text{Immediate}}) &= \\text{Var}(a_{\\enclose{actuarial}{\\min(k,n)}}) \\\\ &= \\text{Var}(\\ddot{a}_{\\enclose{actuarial}{\\min(k+1,n+1)}}-1) \\\\ &= \\text{Var}(\\ddot{a}_{\\enclose{actuarial}{\\min(k+1,n+1)}}) \\\\ &= \\text{Var} \\left( \\frac{1-v^{\\min(k+1,n+1)}}{d} \\right) \\\\ &= \\frac{1}{d^2} \\text{Var}(v^{\\min(k+1,n+1)}) \\\\ &= \\frac{1}{d^2} \\left({}_{2}A_{x:\\enclose{actuarial}{n+1}} - (A_{x:\\enclose{actuarial}{n+1}})^2 \\right) \\end{aligned} \\] Thus, the variance is the same for a whole life annuity but NOT for a temporary annuity .","title":"Immediate &amp; Due"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#other-calculations","text":"","title":"Other Calculations"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#recursion","text":"Following the same logic as assurances, Annuities can also be recursively expressed as a function of itself: If the policyholder dies , nothing happens If the policyholder survives , they would receive the benefits of the future periods Regardless , the benefit of 1 was already paid at the start of the year \\[ \\begin{aligned} \\text{WL}_\\text{Due} &= 1 + \\begin{cases} 0,& q_x \\\\ v\\ddot{a}_{x+1} ,& p_x \\end{cases} \\\\ \\\\ \\therefore \\ddot{a}_{x} &= 1 + vp_x\\ddot{a}_{x+1} \\end{aligned} \\] The same can be shown for TAs, but remember that the remaining duration of the policy must decrease as well: \\[ \\ddot{a}_{x:\\enclose{actuarial}{n}} = 1 + vp_x \\ddot{a}_{x+1:\\enclose{actuarial}{n-1}} \\]","title":"Recursion"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#probability","text":"Questions could also ask for the probability that the present value of the benefits takes on some value \\(u\\) . \\[ \\begin{aligned} P(\\text{WL}_\\text{Due} \\le u) &= P \\left(\\frac{1-v^{k+1}}{d} \\le u \\right) \\\\ &= P \\left(1-v^{k+1} \\le ud \\right) \\\\ &= P \\left(v^{k+1} \\ge 1-ud \\right) \\\\ &= P \\left((k+1) \\ln v \\ge \\ln(1-ud) \\right) \\\\ &= P \\left(k+1 \\le \\frac{\\ln(1-ud)}{\\ln v} \\right) \\\\ &= P \\left(k \\le \\frac{\\ln(1-ud)}{\\ln v} - 1 \\right) \\\\ \\end{aligned} \\] Warning Similar to assurances, it is a common mistake to forget to flip the inequality sign as \\(\\ln v \\lt 0\\) . Questions will not be as straightforward as above . Sometimes, the required value of \\(K_x\\) has to be determined intuitively from the given scenario . For instance, a question may ask for the probability that the sum of undiscounted benefits exceeds its EPV. Assuming the EPV is 12.5, the policyholder would have to receive 13 benefit payments . The probability of receiving 13 benefits is equivalent to the probability of suriving 12 years . Warning It is a common mistake to use \\(K_x \\gt 13\\) , since 13 payments must be made. However, since benefits are paid at the start of the year , it doesnt matter whether the policyholder survives the 13 th year or not; as long as they survive 12 years to see the 13 th year and receive the last required payment . The RHS of the expression is unlikely to be an integer. However, \\(k\\) can only take integer values. Thus, the values can rounded DOWN to the nearest whole number (EG. 4): \\[ \\begin{aligned} P(k \\le \\frac{\\ln(1-ud)}{\\ln v} - 1) &= P(k \\le 4) \\\\ &= P(T_x \\le 4) \\\\ &= {}_{4}p_x \\end{aligned} \\]","title":"Probability"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#percentiles","text":"Similarly, questions could ask for the percentile of the PV . Since the PV is a transformation of the future lifetime variable, the percentile of the PV uses the percentile of the future lifetime variable . Thus, we can find the percentile of future lifetime and then substitute that into the PV random variable to obtain its percentile.","title":"Percentiles"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#payable-m-thly","text":"Similar to assurances, annuities can also be payable \\(m\\) times a year. Most of the key ideas carry over from the assurances, thus this section will will mainly focus on the unique aspects for annuities. Consider the EPV of a payable m-thly whole life annuity due: \\[ \\text{EPV} = B \\cdot \\ddot{a}^{(m)}_{x} \\] \\(B\\) is intepreted as the total benefit paid in the year . In other words, \\(\\frac{B}{m}\\) is paid in each period. This change in the cashflows influences how the annuities are converted from one another: \\[ \\begin{aligned} \\ddot{a}^{(m)}_{x} &= \\ddot{a}^{(m)}_{x} - \\frac{1}{m} \\\\ \\\\ \\ddot{a}^{(m)}_{x} &= \\ddot{a}^{(m)}_{x:\\enclose{actuarial}{n}} - \\frac{1}{m} (1 - {}_{n}E_{x}) \\end{aligned} \\] Another difference is in the interest rates used to convert annuities and assurances: \\[ \\begin{aligned} \\ddot{a}^{(m)}_x &= \\frac{1 - A^{(m)}_x}{d^{(m)}} \\end{aligned} \\]","title":"Payable M-thly"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#uniform-distribution-of-deaths","text":"Since we have already used UDD for assurances and since annuities & assurances are related, we can combine both of them together: \\[ \\begin{aligned} \\ddot{a}^{(m)}_x &= \\frac{1 - A^{(m)}_x}{d^{(m)}} \\\\ &= \\frac{1 - \\frac{i}{i^{(m)}}A_x}{d^{(m)}} \\\\ &= \\frac{\\frac{i^{(m)}-iA_x}{i^{(m)}}}{d^{(m)}} \\\\ &= \\frac{i^{(m)}-iA_x}{i^{(m)}d^{(m)}} \\\\ &= \\frac{i^{(m)}-i(1-d\\ddot{a}_x)}{i^{(m)}d^{(m)}} \\\\ &= \\frac{i^{(m)}-i+id\\ddot{a}_x}{i^{(m)}d^{(m)}} \\\\ &= \\frac{id}{i^{(m)}d^{(m)}} \\ddot{a}_x - \\frac{i-i^{(m)}}{i^{(m)}d^{(m)}} \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) \\\\ \\\\ \\therefore \\alpha(m) &= \\frac{id}{i^{(m)}d^{(m)}} \\\\ \\\\ \\therefore \\beta(m) &= \\frac{i-i^{(m)}}{i^{(m)}d^{(m)}} \\end{aligned} \\] Note Recall that nominal and effective rates are linked using: \\[ \\begin{aligned} i^{(m)} &= m[(1+i)^\\frac{1}{m}-1] \\\\ d^{(m)} &= m[1-(1+d)^\\frac{1}{m}] \\end{aligned} \\] However, for \\(i=0.05\\) , \\(\\alpha\\) and \\(\\beta\\) are provided inside the SULT for common values of \\(m\\) , thus there is no need to waste precious time calculating them.","title":"Uniform Distribution of Deaths"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#woolhouse-approximation","text":"Since a continuous annuity can be expressed as a sum of discrete pure endowments , the trapezoidal rule and euler-macluarin correction can be used to approximate its EPV. The key difference from before is understanding that the EPV of an annuity due is a LEFT riemann sum as there is a payment at time 0. \\[ \\begin{aligned} \\bar{a}_x &= \\int^{\\infty}_0 v^t {}_{t}p_x \\\\ & \\approx h[f(a) + f(a+h) + \\dots + f(b-h)] - \\frac{h}{2} [f(a)-f(b)] + \\frac{h^2}{12} [f'(a)-f'(b)] \\end{aligned} \\] The first derivatives can be found below: \\[ \\begin{aligned} f'(n) &= \\frac{d}{dt} (e^{-\\delta n} {}_{n}p_x) \\\\ &= {}_{n}p_x \\delta e^{-\\delta n} - e^{-\\delta n} {}_{n}p_x \\mu_{x+n} \\\\ &= e^{-\\delta n} {}_{n}p_x (\\delta + \\mu_{x+n}) \\\\ &= {}_{n}E_x (\\delta + \\mu_{x+n}) \\\\ \\\\ \\therefore f'(0) &= -(\\delta + \\mu_{x}) \\\\ \\therefore f'(\\infty) &= 0 \\end{aligned} \\] Assuming \\(h=1\\) , \\[ \\begin{aligned} \\bar{a}_x & \\approx (1) [{}_{0}E_x + {}_{1}E_x + \\dots + {}_{\\infty-1}E_x] - \\frac{1}{2} [{}_{0}E_x-{}_{\\infty-1}E_x] + \\frac{1}{12} [-(\\delta + \\mu_{x}) - 0] \\\\ & \\approx [1 + {}_{n}E_1 + \\dots] - \\frac{1}{2} [1-0] - \\frac{1}{12} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) \\end{aligned} \\] Assuming \\(h=\\frac{1}{m}\\) instead, \\[ \\begin{aligned} \\bar{a}_x & \\approx \\left(\\frac{1}{m} \\right) [{}_{0}E_x + {}_{\\frac{1}{m}}E_x + \\dots + {}_{\\infty-\\frac{1}{m}}E_x] - \\frac{1}{2m} [{}_{0}E_x-{}_{\\infty-\\frac{1}{m}}E_x] + \\frac{1}{12m^2} [-(\\delta + \\mu_{x}) - 0] \\\\ & \\approx \\left(\\frac{1}{m} \\right) [1 + {}_{\\frac{1}{m}}E_x + \\dots] - \\frac{1}{2m} [1-0] - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}^{(m)}_x - \\frac{1}{2m} - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\end{aligned} \\] Combining both together, \\[ \\begin{aligned} \\bar{a}_x &= \\bar{a}_x \\\\ \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) & \\approx \\ddot{a}^{(m)}_x - \\frac{1}{2m} - \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ \\ddot{a}^{(m)}_x & \\approx \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12} (\\delta + \\mu_{x}) + \\frac{1}{2m} + \\frac{1}{12m^2} (\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - (\\frac{1}{2}-\\frac{1}{2m}) - (\\frac{1}{12} - \\frac{1}{12m^2})(\\delta + \\mu_{x}) \\\\ & \\approx \\ddot{a}_x - \\frac{m-1}{2m} - \\frac{m^2-1}{12m^2}(\\delta + \\mu_{x}) \\end{aligned} \\] Note Even though not in a continuous setting, both the force of interest and force of mortality are used in the approximation; which may cause confusion for some. This process is known as the Woolhouse Approximation . If the error term is omitted, then it is known as the 2-term Woolhouse Approximation else it is known as the 3-term Woolhouse Approximation . The 3-term woolhouse approximation is extremely accurate due to its mathematical roots. It so much more accurate than UDD such that when calculating a continuous assurance , it is advisable to calculate the corresponding annuity using the 3 term approximation first and then convert it, for the best results. Unfortunately, the drawback is that it requires knowledge about the force of mortality, which is not commonly provided in life tables. Thus, although less accurate, the 2 term approach can be easily used in any situation.","title":"Woolhouse Approximation"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#term-approximations","text":"Unlike assurances, the approximations for annuities have multiple components. Thus, the approximations for term annuities are slightly different from the ones used for WL. However, there is NO need to remember a seperate expression for it, it can be calculated by converting the WL to a TA: \\[ \\begin{aligned} \\ddot{a}^{(m)}_{x:\\enclose{actuarial}{n}} &= \\ddot{a}^{(m)}_{x} - {}_{n}E_x \\ddot{a}^{(m)}_{x+n} \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) - {}_{n}E_x [\\alpha(m)\\ddot{a}_{x:\\enclose{actuarial}{n}} - \\beta(m)] \\\\ &= \\alpha(m)\\ddot{a}_x - \\beta(m) - {}_{n}E_x \\alpha(m)\\ddot{a}_{x:\\enclose{actuarial}{n}} - {}_{n}E_x \\beta(m) \\\\ &= \\alpha(m) [\\ddot{a}_x - {}_{n}E_x \\ddot{a}_{x:\\enclose{actuarial}{n}}] - \\beta(m) [1-{}_{n}E_x] \\\\ &= \\alpha(m) \\ddot{a}_{x:\\enclose{actuarial}{n}} - \\beta(m) [1-{}_{n}E_x] \\end{aligned} \\] A similar approach can be taken for the woolhouse approximation, resulting in the following: \\[ a^{(m)}_{x:\\enclose{actuarial}{n}} = \\ddot{a}_{x:n} - \\frac{m-1}{2m} (1-{}_{n}E_{x}) \\] Tip The underlying reason for taking this roundabout approach is that the approximation shown above and provided by in the formula sheet is only applicable for WL annuities . Rather than memorizing the approximation for each type of annuity, it is much more efficient to simply convert the annuity into a combination of WL annuities to and approximate from there. For instance, for a guaranteed annuity annuity, \\[ \\begin{aligned} \\ddot{a}^{(m)}_{\\overline{x:\\enclose{actuarial}{n}}} &= \\ddot{a}^{(m)}_{\\enclose{actuarial}{n}} + {}_{n}E_{x} \\cdot \\ddot{a}^{(m)}_{x+n} \\\\ &= \\frac{1 - v^{10}}{d^{(m)}} + {}_{n}E_{x} \\cdot \\left(\\ddot{a}_{x+n} - \\frac{m-1}{2m} \\right) \\end{aligned} \\]","title":"Term Approximations"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#fractional-recursion","text":"Recursions follow the same logic as the discrete case. However, since the benefits are not paid out at JUST the start of the year , the the extent that future benefits have to be discounted changes . If benefits are paid out every half a year : \\[ \\ddot{a}^{(2)}_{x} = 1 + v^{0.5} p_x\\ddot{a}^{(2)}_{x+0.5} \\] There are two key differences to take note of: The extent of discounting changes to half a year The age of the future EPV is at a fractional age Thus, by re-arranging the equation, we can solve for the fractional age EPV .","title":"Fractional Recursion"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#payable-continuously","text":"Similarly, annuities can also be paid continuously. All the concepts are exactly the same as the assurances , thus this section will just be stating the final results.","title":"Payable Continuously"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#approximations","text":"\\[ \\begin{aligned} \\bar{a}_{x} &= \\alpha(\\infty)\\ddot{a}_x - \\beta(\\infty) \\\\ \\\\ \\bar{a}_{x} &= \\ddot{a}_x - \\frac{1}{2} - \\frac{1}{12}(\\delta + \\mu_{x}) \\end{aligned} \\] Tip The above expressions were obtained by using the monthly approximations and setting \\(m \\to \\infty\\) . For the woolhouse approximation, the simplification happens via the following process: \\[ \\begin{aligned} \\frac{m-1}{2m} &= \\frac{m \\left(1 - \\frac{1}{m} \\right)}{2m} =\\frac{1 - \\frac{1}{m}}{2} \\\\ \\therefore \\lim_{m \\to \\infty} \\frac{m-1}{2m} &= \\frac{1 - \\frac{1}{\\infty}}{2} = \\frac{1}{2} \\\\ \\\\ \\frac{m^{2} - 1}{12m^{2}} &= \\frac{m^{2} \\left(1 - \\frac{1}{m^{2}} \\right)}{12m^{2}} = \\frac{1 - \\frac{1}{m^{2}}}{12} \\\\ \\therefore \\lim_{m \\to \\infty} \\frac{m^{2} - 1}{12m^{2}} &= \\frac{1 - \\frac{1}{\\infty}^{2}}{12} = \\frac{1}{12} \\end{aligned} \\]","title":"Approximations"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#exponential-distribution","text":"\\[ \\begin{aligned} \\bar{a}_{x} &= \\int^{\\infty}_{0} v^{t} {}_{t}P_{x} \\\\ &= \\int^{\\infty}_{0} e^{- \\delta t} \\cdot e^{- \\mu t} \\\\ &= \\int^{\\infty}_{0} e^{- (\\mu + \\delta)t} \\\\ &= \\frac{1}{\\mu + \\delta} \\\\ \\\\ \\ddot{a}_{x:\\enclose{actuarial}{n}} &= \\bar{a}_{x} - {}_{n}E_{x} \\bar{a}_{x+n} \\\\ &= \\bar{a}_{x} \\cdot (1 - {}_{n}E_{x}) \\end{aligned} \\]","title":"Exponential Distribution"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/4.%20Life%20Annuities/#uniform-distribution","text":"Even though the distribution is known, it is difficult to integrate for the annuity EPV. Thus, it is recommended to compute the EPV of a continuous assurance and then convert it into a continuous annuity: \\[ \\bar{a}_{x} = \\frac{1 - \\bar{A}_{x}}{\\delta} \\]","title":"Uniform Distribution"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/5.%20Variable%20Benefits/","text":"Variable Benefits \u00b6 Overview \u00b6 Previously, the benefit of the assurances were assumed to be fixed at 1 for simplicity. In practice, it is common to have assurances where the benefits change over time . The content for this section has rarely been tested and thus can be skimmed through for the purposes of this exam. Geometric Contracts \u00b6 If the benefits change by a common factor each period, then it is known as a Geometric contract. The benefit of the policy starts at 1 in the first year and changes by \\((1+b)\\) each period: If \\(b>0\\) , then the benefits are increasing If \\(b<0\\) , then the benefits are decreasing \\[ \\text{Geometric Benefit} = (1+b)^{K_x} \\] Note that the power is \\(K_x\\) and NOT \\(K_x+1\\) to reflect that the benefit starts at 1 when \\(K_x=0\\) . Geometric Assurance \u00b6 Let \\(\\text{Geom WL}\\) be the random variable denoting the PV of a Geometric Assurance. \\[ \\text{Geom WL} = (1+b)^{K_x} v^{K_x+1} \\] Thus, the EPV of a WL Geometric Assurance can be calculated as: \\[ \\begin{aligned} \\text{EPV Geom WL} &= \\sum^{\\infty}_{k=0} (1+b)^{k} v^{k+1} {}_{k|}q_x \\\\ &= (1+b)^{-1} \\sum^{\\infty}_{k=0} (1+b)^{k+1} v^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} \\sum^{\\infty}_{k=0} \\left(\\frac{1+b}{1+i}\\right)^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} \\sum^{\\infty}_{k=0} (v')^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} A_x |_{i=\\frac{1+i}{1+b}-1} \\end{aligned} \\] Note that this means that the term \\(A_x |_{i=\\frac{1+i}{1+b}-1}\\) by itself is a Geometric Assurance that starts with payments of \\((1+b)\\) , which is why \\(\\frac{1}{1+b}\\) is required to scale it down by one factor such that it starts at one. In practice, most questions will provide a value of \\(b\\) such that \\((i=\\frac{1+i}{1+b}-1)\\) simplifies to a nice percentage . However, this usually means that the value of \\(b\\) provided is some complicated number, so do not be taken aback! The expression follows similar intuition to the second moment - it is simply a regular EPV expression evaluated at a different interest rate . Unfortunately, there is no specified actuarial notation for this niche case. One common approach is to denote it using \\(A_x^i\\) . Geometric Annuity \u00b6 Let \\(\\text{EPV Geom WL}_\\text{Due}\\) be the random variable denoting the PV of a Geometric Annuity Due. Thus, the EPV of a Geometric Annuity Due can be calculated as: \\[ \\begin{aligned} \\text{EPV Geom WL}_\\text{Due} &= \\sum^{\\infty}_{k=0} (1+c)^k v^k {}_{k}p_x \\\\ &= \\sum^{\\infty}_{k=0} \\left(\\frac{1+c}{1+i}\\right)^k {}_{k}p_x \\\\ &= \\ddot{a}_x|_{i = \\frac{1+i}{1+c}-1} \\end{aligned} \\] Continuous Geometric \u00b6 The EPV of continuously payable geometric contracts can be calculated using the usual UDD, Claims Acceleration or Woolhouse Approximations. However, despite having a \"new\" interest rate, the original interest rate is used in the continuous approximation parameters. Warning This is different from the second moment where the new interest rate must be reflected in the approximation as well. Take note of the different treatments. Arithmetic Contracts \u00b6 If the benefits change by a fixed constant each period, then it is instead known as an Arithmetic contract. For simplicity, the change each period is assumed to be 1: Arithmetically Increasing - Increases by 1 each period Arithmetically Decreasing - Decreased by 1 each period Since a WL contract lasts forever, the benefits of a decreasing WL contract would inevitably become negative , which do not make sense. Thus, WLs can only be arithmetically increasing contracts . \\[ \\text{Arithmetically Increasing Benefit} = K_x + 1 \\] Arithmetic Assurance \u00b6 Let \\(\\text{Arith WL}\\) be the random variable denoting the PV of an arithmetically increasing WL Assurance: \\[ \\text{Arith WL} = (K_x + 1) v^{K_x+1} \\] Thus, the EPV of a WL Arithmetic Assurance can be shown to be: \\[ \\begin{aligned} \\text{EPV Arith WL} &= \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\\\ (IA)_x &= \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\end{aligned} \\] \\((IA)_x\\) is the actuarial notation for the EPV of a contract with arithmetically increasing benefits starting at 1 and increasing by 1 each period. It can also be shown to be a sum of deferred WLs , each deferred by one period from the previous. This creates a step-like benefit which creates the increasing effect of the assurance: \\[ \\begin{aligned} (IA)_x &= {}_{0|}A_x + {}_{1|}A_x + {}_{2|}A_x + ... \\\\ &= \\sum^{\\infty}_{K_x = 0} {}_{K_x|}A_x \\end{aligned} \\] In practice, the magnitude of the change is unlikely to be one. Thus, the EPV must be scaled to match the actual change. This can be problematic if the starting benefit is NOT the same as the change . Consider a product with benefit \\(B\\) that increases by \\(k\\) each year. Due to the definition of the IWL variable, the starting benefit and change each period are equal - we cannot simply multiply \\(B\\) or \\(k\\) to the IWL EPV. Thus, we must split this product into a fixed component with \\(B-k\\) benefits and a variable component that starts and increases by \\(k\\) each period. The sum of these two components results in a benefit of \\(B, B+k, B+2k, \\dots\\) Arithmetic Annuities \u00b6 Let \\(\\text{Arith WL}_\\text{Due}\\) be the random variable denoting the PV of an arithmetically increasing annuity. The PV can be denoted by an arithmetically increasing annuity certain : \\[ \\begin{aligned} \\text{Arith WL}_\\text{Due} &= (I\\ddot{a})_{\\enclose{actuarial}{n}} \\\\ &= \\sum^{n-1}_{k=0} v^k \\cdot \\ddot{a}_{\\enclose{actuarial}{n-k}} \\\\ &= \\sum^{n-1}_{k=0} v^k \\cdot \\frac{1-v^{n-k}}{d} \\\\ &= \\sum^{n-1}_{k=0} \\frac{v^k - v^n}{d} \\\\ &= \\frac{1}{d} \\left(\\sum^{n-1}_{k=0} v^k - \\sum^{n-1}_{k=0} v^n \\right) \\\\ &= \\frac{1}{d} (\\ddot{a}_{\\enclose{actuarial}{n}} - nv^n) \\\\ &= \\frac{\\ddot{a}_{\\enclose{actuarial}{n}} - nv^n}{d} \\end{aligned} \\] Similarly, the EPV of an arithmetically increasing WL annuity Due can be calculated as: \\[ \\begin{aligned} \\text{EPV Arith WL}_\\text{Due} &= \\sum^{\\infty}_{k=0} (I\\ddot{a})_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_x \\\\ (I\\ddot{a})_x &= \\sum^{\\infty}_{k=0} (k+1) v^k {}_{k}p_x \\\\ \\end{aligned} \\] Alternatively, it can be calculated from an assurance instead: \\[ \\begin{aligned} \\text{Arith WL}_\\text{Due} &= \\frac{\\ddot{a}_{\\enclose{actuarial}{k+1}} - (k+1)v^{k+1}}{d} \\\\ E(\\text{Arith WL}_\\text{Due}) &= \\frac{E(\\ddot{a}_{\\enclose{actuarial}{k+1}}) - E((k+1)v^{k+1})}{d} \\\\ (I\\ddot{a})_x &= \\frac{\\ddot{a}_x - (IA)_x}{d} \\end{aligned} \\] Notice that the main difference between an increasing annuity due and immediate is that the immediate case now lags the due by 1 every period . This difference can be accounted for using a level annuity due: \\[ \\begin{aligned} (I\\ddot{a})_x &= \\sum^{\\infty}_{k=0} (k+1) v^k {}_{k}p_{x} \\\\ &= v^{0} {}_{0}p_{x} + 2v^{1} {}_{1}p_{x} + 3v^{2} {}_{2}p_{x} + \\dots \\\\ &= (v^{1} {}_{1}p_{x} + 2v^{2} {}_{2}p_{x} + \\dots) + (v^{0} {}_{0}p_{x} + v^{1} {}_{1}p_{x} + v^{2} {}_{2}p_{x} + \\dots) \\\\ &= (Ia)_x + \\ddot{a}_x \\\\ (Ia)_x &= (I\\ddot{a})_x - \\ddot{a}_x \\end{aligned} \\] Arithmetic Term/Temporary \u00b6 For contract with level benefits , term/temporary contracts can be expressed as a difference of two WLs issued at different times because the benefits for the time after the specified term would cancel out . For contracts with variable benefits, this is slightly more complicated as the benefits after the specified term do NOT cancel out : The \"earlier\" WL would have increased significantly from its starting value The \"later\" WL would only be at its starting value Thus, an additional expression needs to be subtracted in order to remove the remaining benefit past the specified term. This can be done using a level benefit contract with the a benefit equal to the remaining amount . Thus, the EPV of an Increasing Term Assurance can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith TA} &= \\sum^{n-1}_{K_x=0} (k+1) v^{K_x+1} {}_{k|}q_x \\\\ (IA)^1_{x:\\enclose{actuarial}{n}} &= (IA)_x - {}_{n}E_x [(IA)_{x+n} + nA_{x+n}] \\end{aligned} \\] Thus, the EPV of an Increasing Term Annuity can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith TA}_\\text{Due} &= \\sum^{n-1}_{k=0} (I\\ddot{a})_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_x \\\\ (I\\ddot{a})_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k=0} (k+1) v^k {}_{k}p_x \\\\ &= (I\\ddot{a})_x - {}_{n}E_x[(I\\ddot{a})_{x+n}+n\\ddot{a}_{x+n}] \\end{aligned} \\] Due to Immediate Conversion As mentioned in the Life Annuity section, it is not useful to memorize the conversion from Due to Immediate for TAs. Thus, simply express the ITA in the form of IWLs and then perform the conversion there. Decreasing Benefits \u00b6 As mentioned previously, another key feature of term/temporary contracts is that they can have decreasing benefits . This is because the benefit of the policy can be set such that it would exactly decrease to 0 by the end of the policy term. This is done by setting a starting benefit of \\(n\\) and decreasing by \\(1\\) each period : \\[ \\text{Arithmetically Decreasing Benefit} = n-k \\] Thus, the EPV of a Decreasing Term Assurance can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith Decreasing TA} &= \\sum^{\\infty}_{k=0} (n-k) v^{k+1} {}_{k|}q_x \\\\ (D_{\\enclose{actuarial}{n}}A)_{x:\\enclose{actuarial}{n}} &= \\sum^{\\infty}_{k=0} (n-k+1-1) v^{k+1} {}_{k|}q_x \\\\ &= \\sum^{\\infty}_{k=0} (n+1) v^{k+1} {}_{k|}q_x - \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\\\ &= (n+1)A^1_{x:\\enclose{actuarial}{n}} - (IA)^1_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] \\((DA)\\) is the actuarial notation for an Assurance with Arithmetically decreasing benefits starting at \\(n\\) and decreasing by 1 each period. Similarly, the EPV of a decreasing temporary annuity can be calculated as the following: \\[ (D_{\\enclose{actuarial}{n}}\\ddot{a})_{x:\\enclose{actuarial}{n}} = (n+1)\\ddot{a}_{x:\\enclose{actuarial}{n}} - (I\\ddot{a})_{x:\\enclose{actuarial}{n}} \\] Continuous Arithmetic \u00b6 Similar to Geometric Contracts, the EPV of a continuously payable Arithmetic Contracts can be calculated using the usual approximations. However, take note that for TAs, the added term should also be continuous : \\[ \\begin{aligned} (I\\bar{A})_{x:\\enclose{actuarial}{n}} &= (I\\bar{A})_x - {}_{n}E_x[(I\\bar{A})_{x+n}+ n\\bar{A}_{x+n}] \\\\ (I\\bar{a})_{x:\\enclose{actuarial}{n}} &= (I\\bar{a})_x - {}_{n}E_x[(I\\bar{a})_{x+n}+ n\\bar{a}_{x+n}] \\end{aligned} \\] Increasing Woolhouse Approximation For IWL Annuities, the woolhouse approximation is slightly different: \\[ (I\\bar{a})_{x} = (I\\ddot{a})_{x} - \\frac{1}{2} \\ddot{a}_x \\] Continuous arithmetic contracts could also refer to a contract that not only payable continuously, but changes continuously as well. The benefits change at a constant rate throughout the period , such that the total change is still equal to 1. For Assurances , it can be understood from a graphical approach: \\[ \\begin{aligned} (\\bar{I}\\bar{A})_{x} &= (I\\bar{A})_{x} - \\frac{1}{2} \\bar{A}_x \\end{aligned} \\] For Annuities, it can be derived using the woolhouse approximation : \\[ \\begin{aligned} (\\bar{I}\\bar{a})_x &= (Ia)_x + \\frac{1}{12} \\\\ &= (I\\ddot{a})_x - \\ddot{a}_x + \\frac{1}{12} \\end{aligned} \\] Note that these approximations are ONLY applicable for WL policies . For continously increasing TAs, convert them to WLs first before applying the approximations. Variable Recursions \u00b6 Since the benefit payable at the end of the first year is still 1 , the recursion for variable contracts are similar to the level ones. Variable Assurances \u00b6 Consider the two scenarios: If the policyholder dies in the current year with probability \\(q_x\\) , then a benefit of 1 is paid at the end of the year If the policyholder lives past the current year with probability \\(p_x\\) , then the they will receive the EPV of a policy with benefits \\(2, 3, \\dots\\) or \\((1+b)^1, (1+b)^2, \\dots\\) at some future time The second component must be the EPV of an assurance that STARTS with an increased benefit of \\((1+b)\\) or \\(2\\) . The issue is that the random variable is currently defined as a contract with benefits starting at 1 , which is why the recursive formula is different from the level case . For Geometric Assurances, \\(A_{x+1}|_{i=\\frac{1+i}{1+b}-1}\\) is already an Geometric Assurance that starts at \\((1+b)\\) , thus no adjustment is needed for the RHS: \\[ \\text{EPV Geom WL}_{x} = vq_x + vp_x A_{x+1}|_{i=\\frac{1+i}{1+b}-1} \\] For Arithmetic Assurances, an EPV representing an additional benefit of 1 each period must be added to the second component so that the RHS expression starts from 2 and increases by an additional 1 each period. \\[ (IA)_{x} = vq_x + vp_x[(IA)_{x+1} + A_{x+1}] \\] Variable Annuities \u00b6 Recall that a benefit of 1 is already received at the beginning of the year regardless of whether the policyholder lives or dies. If the policyholder dies with probability \\(q_x\\) , then they receive nothing extra If the policyholder lives past the current year with probability \\(p_x\\) , then they will receive benefits that are larger than 1 at some future time For Geometric Annuities, TBC. \\[ \\begin{aligned} \\text{EPV Geom WL}_{\\text{Due},x} = 1 + vp_x \\cdot (1+c) \\left(\\ddot{a}_{x+1}|_{i = \\frac{1+i}{1+c}-1} \\right) \\end{aligned} \\] For Arithmetic Annuities, the same adjustment as before must be made: \\[ (I\\ddot{a})_x = 1 + vp_x[(I\\ddot{a})_{x+1} + \\ddot{a}_{x+1}] \\] In Force Recursions \u00b6 Previously, we have only considered the EPV for a newly issued policy at age \\(x\\) . However, now we want to consider the EPV of a policy that has already been in force for some period of time \\(t\\) . For policies with level benefits , the EPV of an already in force policy is exactly the same as the EPV of a newly issued policy at the current age with the same expiration date . This means that the same recursion can be used for both newly issued policies and inforce policies. However, this is NOT the case for policies with variable benefits as the inforce policy would have a benefit of \\(t\\) or \\((1+b)^{t-1}\\) while a newly issued policy would have benefit of 1. This means that a seperate recursion must be defined for inforce variable policies. For simplicity, we will consider an in force arithmetically increasing WL Assurance : If the policyholder dies with probability \\(q_{x+t-1}\\) , they will receive a benefit of \\(t\\) at the end of the year If the poliycholder survives with probability \\(p_{x+t-1}\\) , they will receive the EPV of a contract with benefits \\(t+1, t+2, \\dots\\) at some future time \\[ RHS = vq_{x+t-1} \\cdot t + vp_{x+t-1} [(IA)_{x+t} + tA_{x+t}] \\] Since (IA) is used to denote a policy with benefits starting at 1, an additional term must be added to the LHS to show that the in force policy has benefits of t: \\[ LHS = (IA)_{x+t-1} + (t-1) A_{x+t-1} \\] The same exercise can be performed for Annuities as well, resulting in the following: \\[ \\begin{aligned} (IA)_{x+t-1} + (t-1) A_{x+t-1} &= vq_{x+t-1} \\cdot t + vp_{x+t-1} [(IA)_{x+t} + tA_{x+t}] \\\\ (I\\ddot{a})_{x+t-1} + (t-1) \\ddot{a}_{x+t-1} &= t + vp_{x+t-1} [(I\\ddot{a})_{x+t} + t\\ddot{a}_{x+t}] \\end{aligned} \\] Term/Temporary Policies The same can be shown for Variable Term/Temporary policies, but remember that the remaining duration of the policy must decrease as well.","title":"Variable Benefits"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/5.%20Variable%20Benefits/#variable-benefits","text":"","title":"Variable Benefits"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/5.%20Variable%20Benefits/#overview","text":"Previously, the benefit of the assurances were assumed to be fixed at 1 for simplicity. In practice, it is common to have assurances where the benefits change over time . The content for this section has rarely been tested and thus can be skimmed through for the purposes of this exam.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/5.%20Variable%20Benefits/#geometric-contracts","text":"If the benefits change by a common factor each period, then it is known as a Geometric contract. The benefit of the policy starts at 1 in the first year and changes by \\((1+b)\\) each period: If \\(b>0\\) , then the benefits are increasing If \\(b<0\\) , then the benefits are decreasing \\[ \\text{Geometric Benefit} = (1+b)^{K_x} \\] Note that the power is \\(K_x\\) and NOT \\(K_x+1\\) to reflect that the benefit starts at 1 when \\(K_x=0\\) .","title":"Geometric Contracts"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/5.%20Variable%20Benefits/#geometric-assurance","text":"Let \\(\\text{Geom WL}\\) be the random variable denoting the PV of a Geometric Assurance. \\[ \\text{Geom WL} = (1+b)^{K_x} v^{K_x+1} \\] Thus, the EPV of a WL Geometric Assurance can be calculated as: \\[ \\begin{aligned} \\text{EPV Geom WL} &= \\sum^{\\infty}_{k=0} (1+b)^{k} v^{k+1} {}_{k|}q_x \\\\ &= (1+b)^{-1} \\sum^{\\infty}_{k=0} (1+b)^{k+1} v^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} \\sum^{\\infty}_{k=0} \\left(\\frac{1+b}{1+i}\\right)^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} \\sum^{\\infty}_{k=0} (v')^{k+1} {}_{k|}q_x \\\\ &= \\frac{1}{1+b} A_x |_{i=\\frac{1+i}{1+b}-1} \\end{aligned} \\] Note that this means that the term \\(A_x |_{i=\\frac{1+i}{1+b}-1}\\) by itself is a Geometric Assurance that starts with payments of \\((1+b)\\) , which is why \\(\\frac{1}{1+b}\\) is required to scale it down by one factor such that it starts at one. In practice, most questions will provide a value of \\(b\\) such that \\((i=\\frac{1+i}{1+b}-1)\\) simplifies to a nice percentage . However, this usually means that the value of \\(b\\) provided is some complicated number, so do not be taken aback! The expression follows similar intuition to the second moment - it is simply a regular EPV expression evaluated at a different interest rate . Unfortunately, there is no specified actuarial notation for this niche case. One common approach is to denote it using \\(A_x^i\\) .","title":"Geometric Assurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/5.%20Variable%20Benefits/#geometric-annuity","text":"Let \\(\\text{EPV Geom WL}_\\text{Due}\\) be the random variable denoting the PV of a Geometric Annuity Due. Thus, the EPV of a Geometric Annuity Due can be calculated as: \\[ \\begin{aligned} \\text{EPV Geom WL}_\\text{Due} &= \\sum^{\\infty}_{k=0} (1+c)^k v^k {}_{k}p_x \\\\ &= \\sum^{\\infty}_{k=0} \\left(\\frac{1+c}{1+i}\\right)^k {}_{k}p_x \\\\ &= \\ddot{a}_x|_{i = \\frac{1+i}{1+c}-1} \\end{aligned} \\]","title":"Geometric Annuity"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/5.%20Variable%20Benefits/#continuous-geometric","text":"The EPV of continuously payable geometric contracts can be calculated using the usual UDD, Claims Acceleration or Woolhouse Approximations. However, despite having a \"new\" interest rate, the original interest rate is used in the continuous approximation parameters. Warning This is different from the second moment where the new interest rate must be reflected in the approximation as well. Take note of the different treatments.","title":"Continuous Geometric"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/5.%20Variable%20Benefits/#arithmetic-contracts","text":"If the benefits change by a fixed constant each period, then it is instead known as an Arithmetic contract. For simplicity, the change each period is assumed to be 1: Arithmetically Increasing - Increases by 1 each period Arithmetically Decreasing - Decreased by 1 each period Since a WL contract lasts forever, the benefits of a decreasing WL contract would inevitably become negative , which do not make sense. Thus, WLs can only be arithmetically increasing contracts . \\[ \\text{Arithmetically Increasing Benefit} = K_x + 1 \\]","title":"Arithmetic Contracts"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/5.%20Variable%20Benefits/#arithmetic-assurance","text":"Let \\(\\text{Arith WL}\\) be the random variable denoting the PV of an arithmetically increasing WL Assurance: \\[ \\text{Arith WL} = (K_x + 1) v^{K_x+1} \\] Thus, the EPV of a WL Arithmetic Assurance can be shown to be: \\[ \\begin{aligned} \\text{EPV Arith WL} &= \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\\\ (IA)_x &= \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\end{aligned} \\] \\((IA)_x\\) is the actuarial notation for the EPV of a contract with arithmetically increasing benefits starting at 1 and increasing by 1 each period. It can also be shown to be a sum of deferred WLs , each deferred by one period from the previous. This creates a step-like benefit which creates the increasing effect of the assurance: \\[ \\begin{aligned} (IA)_x &= {}_{0|}A_x + {}_{1|}A_x + {}_{2|}A_x + ... \\\\ &= \\sum^{\\infty}_{K_x = 0} {}_{K_x|}A_x \\end{aligned} \\] In practice, the magnitude of the change is unlikely to be one. Thus, the EPV must be scaled to match the actual change. This can be problematic if the starting benefit is NOT the same as the change . Consider a product with benefit \\(B\\) that increases by \\(k\\) each year. Due to the definition of the IWL variable, the starting benefit and change each period are equal - we cannot simply multiply \\(B\\) or \\(k\\) to the IWL EPV. Thus, we must split this product into a fixed component with \\(B-k\\) benefits and a variable component that starts and increases by \\(k\\) each period. The sum of these two components results in a benefit of \\(B, B+k, B+2k, \\dots\\)","title":"Arithmetic Assurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/5.%20Variable%20Benefits/#arithmetic-annuities","text":"Let \\(\\text{Arith WL}_\\text{Due}\\) be the random variable denoting the PV of an arithmetically increasing annuity. The PV can be denoted by an arithmetically increasing annuity certain : \\[ \\begin{aligned} \\text{Arith WL}_\\text{Due} &= (I\\ddot{a})_{\\enclose{actuarial}{n}} \\\\ &= \\sum^{n-1}_{k=0} v^k \\cdot \\ddot{a}_{\\enclose{actuarial}{n-k}} \\\\ &= \\sum^{n-1}_{k=0} v^k \\cdot \\frac{1-v^{n-k}}{d} \\\\ &= \\sum^{n-1}_{k=0} \\frac{v^k - v^n}{d} \\\\ &= \\frac{1}{d} \\left(\\sum^{n-1}_{k=0} v^k - \\sum^{n-1}_{k=0} v^n \\right) \\\\ &= \\frac{1}{d} (\\ddot{a}_{\\enclose{actuarial}{n}} - nv^n) \\\\ &= \\frac{\\ddot{a}_{\\enclose{actuarial}{n}} - nv^n}{d} \\end{aligned} \\] Similarly, the EPV of an arithmetically increasing WL annuity Due can be calculated as: \\[ \\begin{aligned} \\text{EPV Arith WL}_\\text{Due} &= \\sum^{\\infty}_{k=0} (I\\ddot{a})_{\\enclose{actuarial}{k+1}} \\cdot {}_{k|}q_x \\\\ (I\\ddot{a})_x &= \\sum^{\\infty}_{k=0} (k+1) v^k {}_{k}p_x \\\\ \\end{aligned} \\] Alternatively, it can be calculated from an assurance instead: \\[ \\begin{aligned} \\text{Arith WL}_\\text{Due} &= \\frac{\\ddot{a}_{\\enclose{actuarial}{k+1}} - (k+1)v^{k+1}}{d} \\\\ E(\\text{Arith WL}_\\text{Due}) &= \\frac{E(\\ddot{a}_{\\enclose{actuarial}{k+1}}) - E((k+1)v^{k+1})}{d} \\\\ (I\\ddot{a})_x &= \\frac{\\ddot{a}_x - (IA)_x}{d} \\end{aligned} \\] Notice that the main difference between an increasing annuity due and immediate is that the immediate case now lags the due by 1 every period . This difference can be accounted for using a level annuity due: \\[ \\begin{aligned} (I\\ddot{a})_x &= \\sum^{\\infty}_{k=0} (k+1) v^k {}_{k}p_{x} \\\\ &= v^{0} {}_{0}p_{x} + 2v^{1} {}_{1}p_{x} + 3v^{2} {}_{2}p_{x} + \\dots \\\\ &= (v^{1} {}_{1}p_{x} + 2v^{2} {}_{2}p_{x} + \\dots) + (v^{0} {}_{0}p_{x} + v^{1} {}_{1}p_{x} + v^{2} {}_{2}p_{x} + \\dots) \\\\ &= (Ia)_x + \\ddot{a}_x \\\\ (Ia)_x &= (I\\ddot{a})_x - \\ddot{a}_x \\end{aligned} \\]","title":"Arithmetic Annuities"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/5.%20Variable%20Benefits/#arithmetic-termtemporary","text":"For contract with level benefits , term/temporary contracts can be expressed as a difference of two WLs issued at different times because the benefits for the time after the specified term would cancel out . For contracts with variable benefits, this is slightly more complicated as the benefits after the specified term do NOT cancel out : The \"earlier\" WL would have increased significantly from its starting value The \"later\" WL would only be at its starting value Thus, an additional expression needs to be subtracted in order to remove the remaining benefit past the specified term. This can be done using a level benefit contract with the a benefit equal to the remaining amount . Thus, the EPV of an Increasing Term Assurance can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith TA} &= \\sum^{n-1}_{K_x=0} (k+1) v^{K_x+1} {}_{k|}q_x \\\\ (IA)^1_{x:\\enclose{actuarial}{n}} &= (IA)_x - {}_{n}E_x [(IA)_{x+n} + nA_{x+n}] \\end{aligned} \\] Thus, the EPV of an Increasing Term Annuity can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith TA}_\\text{Due} &= \\sum^{n-1}_{k=0} (I\\ddot{a})_{\\enclose{actuarial}{n}} \\cdot {}_{k|}q_x \\\\ (I\\ddot{a})_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k=0} (k+1) v^k {}_{k}p_x \\\\ &= (I\\ddot{a})_x - {}_{n}E_x[(I\\ddot{a})_{x+n}+n\\ddot{a}_{x+n}] \\end{aligned} \\] Due to Immediate Conversion As mentioned in the Life Annuity section, it is not useful to memorize the conversion from Due to Immediate for TAs. Thus, simply express the ITA in the form of IWLs and then perform the conversion there.","title":"Arithmetic Term/Temporary"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/5.%20Variable%20Benefits/#decreasing-benefits","text":"As mentioned previously, another key feature of term/temporary contracts is that they can have decreasing benefits . This is because the benefit of the policy can be set such that it would exactly decrease to 0 by the end of the policy term. This is done by setting a starting benefit of \\(n\\) and decreasing by \\(1\\) each period : \\[ \\text{Arithmetically Decreasing Benefit} = n-k \\] Thus, the EPV of a Decreasing Term Assurance can be calculated as the following: \\[ \\begin{aligned} \\text{EPV Arith Decreasing TA} &= \\sum^{\\infty}_{k=0} (n-k) v^{k+1} {}_{k|}q_x \\\\ (D_{\\enclose{actuarial}{n}}A)_{x:\\enclose{actuarial}{n}} &= \\sum^{\\infty}_{k=0} (n-k+1-1) v^{k+1} {}_{k|}q_x \\\\ &= \\sum^{\\infty}_{k=0} (n+1) v^{k+1} {}_{k|}q_x - \\sum^{\\infty}_{k=0} (k+1) v^{k+1} {}_{k|}q_x \\\\ &= (n+1)A^1_{x:\\enclose{actuarial}{n}} - (IA)^1_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] \\((DA)\\) is the actuarial notation for an Assurance with Arithmetically decreasing benefits starting at \\(n\\) and decreasing by 1 each period. Similarly, the EPV of a decreasing temporary annuity can be calculated as the following: \\[ (D_{\\enclose{actuarial}{n}}\\ddot{a})_{x:\\enclose{actuarial}{n}} = (n+1)\\ddot{a}_{x:\\enclose{actuarial}{n}} - (I\\ddot{a})_{x:\\enclose{actuarial}{n}} \\]","title":"Decreasing Benefits"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/5.%20Variable%20Benefits/#continuous-arithmetic","text":"Similar to Geometric Contracts, the EPV of a continuously payable Arithmetic Contracts can be calculated using the usual approximations. However, take note that for TAs, the added term should also be continuous : \\[ \\begin{aligned} (I\\bar{A})_{x:\\enclose{actuarial}{n}} &= (I\\bar{A})_x - {}_{n}E_x[(I\\bar{A})_{x+n}+ n\\bar{A}_{x+n}] \\\\ (I\\bar{a})_{x:\\enclose{actuarial}{n}} &= (I\\bar{a})_x - {}_{n}E_x[(I\\bar{a})_{x+n}+ n\\bar{a}_{x+n}] \\end{aligned} \\] Increasing Woolhouse Approximation For IWL Annuities, the woolhouse approximation is slightly different: \\[ (I\\bar{a})_{x} = (I\\ddot{a})_{x} - \\frac{1}{2} \\ddot{a}_x \\] Continuous arithmetic contracts could also refer to a contract that not only payable continuously, but changes continuously as well. The benefits change at a constant rate throughout the period , such that the total change is still equal to 1. For Assurances , it can be understood from a graphical approach: \\[ \\begin{aligned} (\\bar{I}\\bar{A})_{x} &= (I\\bar{A})_{x} - \\frac{1}{2} \\bar{A}_x \\end{aligned} \\] For Annuities, it can be derived using the woolhouse approximation : \\[ \\begin{aligned} (\\bar{I}\\bar{a})_x &= (Ia)_x + \\frac{1}{12} \\\\ &= (I\\ddot{a})_x - \\ddot{a}_x + \\frac{1}{12} \\end{aligned} \\] Note that these approximations are ONLY applicable for WL policies . For continously increasing TAs, convert them to WLs first before applying the approximations.","title":"Continuous Arithmetic"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/5.%20Variable%20Benefits/#variable-recursions","text":"Since the benefit payable at the end of the first year is still 1 , the recursion for variable contracts are similar to the level ones.","title":"Variable Recursions"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/5.%20Variable%20Benefits/#variable-assurances","text":"Consider the two scenarios: If the policyholder dies in the current year with probability \\(q_x\\) , then a benefit of 1 is paid at the end of the year If the policyholder lives past the current year with probability \\(p_x\\) , then the they will receive the EPV of a policy with benefits \\(2, 3, \\dots\\) or \\((1+b)^1, (1+b)^2, \\dots\\) at some future time The second component must be the EPV of an assurance that STARTS with an increased benefit of \\((1+b)\\) or \\(2\\) . The issue is that the random variable is currently defined as a contract with benefits starting at 1 , which is why the recursive formula is different from the level case . For Geometric Assurances, \\(A_{x+1}|_{i=\\frac{1+i}{1+b}-1}\\) is already an Geometric Assurance that starts at \\((1+b)\\) , thus no adjustment is needed for the RHS: \\[ \\text{EPV Geom WL}_{x} = vq_x + vp_x A_{x+1}|_{i=\\frac{1+i}{1+b}-1} \\] For Arithmetic Assurances, an EPV representing an additional benefit of 1 each period must be added to the second component so that the RHS expression starts from 2 and increases by an additional 1 each period. \\[ (IA)_{x} = vq_x + vp_x[(IA)_{x+1} + A_{x+1}] \\]","title":"Variable Assurances"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/5.%20Variable%20Benefits/#variable-annuities","text":"Recall that a benefit of 1 is already received at the beginning of the year regardless of whether the policyholder lives or dies. If the policyholder dies with probability \\(q_x\\) , then they receive nothing extra If the policyholder lives past the current year with probability \\(p_x\\) , then they will receive benefits that are larger than 1 at some future time For Geometric Annuities, TBC. \\[ \\begin{aligned} \\text{EPV Geom WL}_{\\text{Due},x} = 1 + vp_x \\cdot (1+c) \\left(\\ddot{a}_{x+1}|_{i = \\frac{1+i}{1+c}-1} \\right) \\end{aligned} \\] For Arithmetic Annuities, the same adjustment as before must be made: \\[ (I\\ddot{a})_x = 1 + vp_x[(I\\ddot{a})_{x+1} + \\ddot{a}_{x+1}] \\]","title":"Variable Annuities"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/5.%20Variable%20Benefits/#in-force-recursions","text":"Previously, we have only considered the EPV for a newly issued policy at age \\(x\\) . However, now we want to consider the EPV of a policy that has already been in force for some period of time \\(t\\) . For policies with level benefits , the EPV of an already in force policy is exactly the same as the EPV of a newly issued policy at the current age with the same expiration date . This means that the same recursion can be used for both newly issued policies and inforce policies. However, this is NOT the case for policies with variable benefits as the inforce policy would have a benefit of \\(t\\) or \\((1+b)^{t-1}\\) while a newly issued policy would have benefit of 1. This means that a seperate recursion must be defined for inforce variable policies. For simplicity, we will consider an in force arithmetically increasing WL Assurance : If the policyholder dies with probability \\(q_{x+t-1}\\) , they will receive a benefit of \\(t\\) at the end of the year If the poliycholder survives with probability \\(p_{x+t-1}\\) , they will receive the EPV of a contract with benefits \\(t+1, t+2, \\dots\\) at some future time \\[ RHS = vq_{x+t-1} \\cdot t + vp_{x+t-1} [(IA)_{x+t} + tA_{x+t}] \\] Since (IA) is used to denote a policy with benefits starting at 1, an additional term must be added to the LHS to show that the in force policy has benefits of t: \\[ LHS = (IA)_{x+t-1} + (t-1) A_{x+t-1} \\] The same exercise can be performed for Annuities as well, resulting in the following: \\[ \\begin{aligned} (IA)_{x+t-1} + (t-1) A_{x+t-1} &= vq_{x+t-1} \\cdot t + vp_{x+t-1} [(IA)_{x+t} + tA_{x+t}] \\\\ (I\\ddot{a})_{x+t-1} + (t-1) \\ddot{a}_{x+t-1} &= t + vp_{x+t-1} [(I\\ddot{a})_{x+t} + t\\ddot{a}_{x+t}] \\end{aligned} \\] Term/Temporary Policies The same can be shown for Variable Term/Temporary policies, but remember that the remaining duration of the policy must decrease as well.","title":"In Force Recursions"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/6.%20Premiums/","text":"Premiums \u00b6 Overview \u00b6 Unlike other products, the cost of a Life Assurance or Annuity are not known when it is issued, as it is impossible to precisely predict when an individual will die or how long they will live for. Premiums are the amount that the insurance company charges for a Life Assurance or Annuity. They aim to charge a sufficiently high premium such that they expect to at least break even on the sale of the policy on an EPV basis . There are three main types of premiums: Single Premium Limited Premium Regular Premium Single Lump Sum Recurring for a fixed period Recurring as long as the contract is valid Use dollar amount Modelled using TA Annuities Modelled using WL Annuities Info Premiums can be paid at any frequency , but monthly is the most common as it matches the frequency of when people receive their income. They are always paid in advance . If they were paid in arrears, policyholders may refuse to pay premiums on the flawed grounds that since they did not die, they did not utilize the coverage and hence should not pay. There are two types of premiums that can be calculated: Net Premiums ( \\(P\\) ) - Excluding expenses; benefits only Gross Premiums ( \\(G\\) ) - Including expenses and other cashflows The key thing to notice is that the components of net premiums are fixed while gross premiums differ from insurer to insurer. Thus, this section will only show formulas for net premiums . Equivalence Principle Approach \u00b6 To determine the breakeven point of the insurer, the Loss Amount must first be defined. It is the net outflow of the policy: \\[ \\text{Loss} = \\text{Outflow} - \\text{Inflow} \\] Let \\({}_{t-1}L_{x}\\) be the random variable denoting the PV of the loss at the START of policy year t : \\[ \\begin{aligned} {}_{t-1}L_{x} &= \\text{PV Outflow} - \\text{PV Inflows} \\\\ &= B \\cdot v^{K_{x+(t-1)} + 1} - P \\cdot \\ddot{a}_{\\enclose{actuarial}{K_{x+(t-1)} + 1}} \\end{aligned} \\] Since L represents an outflow rather than an inflow, its intepretation is reversed : \\({}_{t-1}L_{x} \\gt 0\\) , Negative Profits (Losing money) \\({}_{t-1}L_{x} \\lt 0\\) , Positive Profits (Making money) Intuitively, insurers will charge a premium such that they expect to break even at the time of the sale , time 0. In other words, the expected loss at the start of the policy year 1 is 0 : \\[ \\begin{aligned} {}_{0}L_{x} &= B \\cdot v^{K_{x} + 1} - P \\cdot \\ddot{a}_{\\enclose{actuarial}{K_{x} + 1}} \\\\ \\\\ E \\left({}_{0}L_{x} \\right) &= B \\cdot E \\left(v^{K_{x} + 1} \\right) - P \\cdot E \\left(\\ddot{a}_{\\enclose{actuarial}{K_{x} + 1}} \\right) \\\\ &= B \\cdot A_{x} - P \\cdot \\ddot{a}_{x} \\\\ \\\\ \\therefore E \\left({}_{0}L_{x} \\right) &= 0 \\\\ B \\cdot A_{x} - P \\cdot \\ddot{a}_x &= 0 \\\\ P \\cdot \\ddot{a}_{x} &= B \\cdot A_{x} \\\\ P &= B \\cdot \\frac{A_x}{\\ddot{a}_x} \\end{aligned} \\] Note For the purposes of this exam, premiums are usually paid annually , where \\(P\\) represents the annual net premium . If premiums were paid more frequently, then the corresponding EPVs should be used: \\[ \\text{EPV Inflow} = P \\cdot \\ddot{a}^{(m)}_{x} \\] Regardless of the frequency of payment, the annual net premium is still multiplied the EPVs. Thus, to obtain the premium payable per period (vice-versa) the annual premium must be divided by the number of periods. \\[ P^{(m)} = \\frac{P}{m} \\] Premiums can also be paid continuously. The amount paid continuously is known as the Premium Rate -- it does not follow the above convention. This is known as the Equivalence Principle approach, as it charges premiums such that the EPVs of the inflows and outflows are equal: \\[ \\begin{aligned} \\text{EPV Outflow} &= \\text{EPV Inflow} \\\\ \\text{EPV Premium} &= \\text{EPV Benefit} \\end{aligned} \\] Note If the policy charges a Single Net Premium using the equivalence principle, then that is simply equivalent to the EPV of the benefits. \\[ \\begin{aligned} \\text{EPV Premium} &= \\text{Single Net Premium} \\\\ \\therefore \\text{Single Net Premium} &= \\text{EPV Benefit} \\end{aligned} \\] Portfolio Percentile Approach \u00b6 The equivalence principle calculates the premium for a single policy such that it is expected to break even . Given a portfolio of N homogenous policies , the Portfolio Percentile Approach can be used to calculate the premims such that the probability that the entire portfolio at least breaks even is fixed at some minimum value. Let \\(S\\) be the random variable denoting the aggregate loss of \\(N\\) homogenous policies that were just issued : \\[ \\begin{aligned} S &= {}_{0}L_1 + {}_{0}L_2 + \\dots + {}_{0}L_{N} \\\\ E(S) &= N \\cdot E({}_{0}L) \\\\ Var (S) &= N \\cdot \\text{Var}({}_{0}L) \\end{aligned} \\] The expectation and variance of a single policy can be determined as the following: \\[ \\begin{aligned} {}_{0}L &= B \\cdot v^{K_{x} + 1} - P \\cdot \\ddot{a}_{\\enclose{actuarial}{K_{x} + 1}} \\\\ &= B \\cdot v^{K_{x} + 1} - P \\cdot \\frac{1- v^{K_{x} + 1}}{d} \\\\ &= B \\cdot v^{K_{x} + 1} + \\frac{- P + P v^{K_{x} + 1}}{d} \\\\ &= \\left(B + \\frac{P}{d} \\right)v^{K_{x} + 1} - \\frac{P}{d} \\\\ \\\\ E({}_{0}L) &= E \\left[\\left(B + \\frac{P}{d} \\right)v^{K_{x} + 1} - \\frac{P}{d} \\right] \\\\ &= \\left(B + \\frac{P}{d} \\right) E(v^{K_{x} + 1}) - E \\left(\\frac{P}{d} \\right) \\\\ &= \\left(B + \\frac{P}{d} \\right)A_{x} - \\frac{P}{d} \\\\ \\\\ \\text{Var}({}_{0}L) &= \\text{Var}\\left[\\left(B + \\frac{P}{d} \\right)v^{K_{x} + 1} - \\frac{P}{d} \\right] \\\\ &= \\text{Var} \\left[\\left(B + \\frac{P}{d} \\right)v^{K_{x} + 1} \\right] \\\\ &= \\left(B + \\frac{P}{d} \\right)^2 \\cdot \\text{Var} \\left(v^{K_{x} + 1} \\right) \\\\ &= \\left(B + \\frac{P}{d} \\right)^2 \\cdot \\left[{}^{2}A_{x} - (A_{x})^2 \\right] \\end{aligned} \\] Warning It is a common mistake to mix this up with the variance of the policy benefits. Tip Notice that the EPV components of the mean and variance are only based on the policyholder's age. Thus, different policies (with different benefits and premiums) can be used to solve for these shared EPV components and hence calculate the mean and variance of each policy. If \\(N\\) is sufficiently large, then \\(S\\) is approximately normally distributed via the central limit theorem: \\[ S \\sim N(E(S), \\text{Var}(S)) \\] The premium is then set such the probability of at least breaking even (probability of profit) is set a pre-determined level \\(\\alpha\\) : \\[ \\begin{aligned} P(S < 0) &= \\alpha \\\\ P\\left(\\frac{S - E(S)}{\\sqrt{Var (S)}} \\le \\frac{0 - E(S)}{\\sqrt{Var (S)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- E(S)}{\\sqrt{Var (S)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- n \\cdot E({}_{0}L)}{\\sqrt{n \\cdot Var ({}_{0}L)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- n \\cdot E({}_{0}L)}{\\sqrt{n \\cdot Var ({}_{0}L)}}\\right) &= \\alpha \\\\ \\\\ \\frac{- n \\cdot E({}_{0}L)}{\\sqrt{n \\cdot Var ({}_{0}L)}} = Z_{\\alpha} \\end{aligned} \\] By plugging in the values determined earlier and solving for P, the appropriate premiums can be determined. Loss Probability \u00b6 The above method can also be used in reverse - given a premium, the probability that the aggregate loss taking on some range of values can be computed. For a single policy , the probability must be solved using the survival distribution: \\[ \\begin{aligned} P({{}_{0}L} \\gt k) &= P \\left( \\left(B + \\frac{P}{d} \\right)v^{K_{x} + 1} - \\frac{P}{d} \\gt k \\right) \\\\ &= \\dots \\\\ &= P(K_x \\gt \\dots) \\end{aligned} \\] Gross Premiums \u00b6 What makes gross premiums so much trickier than net premiums is that there are many different types of expenses , where every question can ask for a unique combination of them. Tip Questions will explicitly state if there are expenses in the policy. Just because a question uses \"Gross Premiums\" does not automatically mean that there are expenses. Expenses can be differentiated in terms of when they are charged : Acquisition Expenses Renewal Expenses Termination Expense At policy inception During policy lifetime At policy termination Single payment at time 0 Recurring payments from time 1 onwards Single payment at unknown time Denoted by \\(e_0\\) Denoted by \\(e_t\\) Denoted by \\(E\\) Raw Value Annuity Functions Assurance Function EG. Commissions expenses EG. Administrative Expenses EG. Claims expenses Warning Renewal Expenses is also sometimes known as Maintenance Expenses . Although they are usually charged from policy year 2 onwards, they can be charged in the first year as well . The above table is just a general guide. Acquisition expenses are usually higher than renewal ones as agent commissions are mostly front-loaded in the first year to give agents cash faster, incentivizing them to sell more. Note Renewal Expenses can also inflate over time, where they increase by a factor of \\((1+r)\\) each year. In this case, they are modelled using a geometrically increasing annuity instead. They can also be differentiated in terms of how much they charge : Overhead Expenses Direct Expenses Shared among all policies Borne by a specific policy Fixed amount Percentage of Premium or Benefit EG. Office Rental EG. Sales Commission Tip Generally speaking, bverhead expenses are charged every year while direct expenses are only charged whenever premiums are paid. This is relevant for policies for with a limited payment period . After the premium payment period, only overhead expenses are charged to the policy. Similarly, if premiums are payable \\(m\\) times a year, then expenses are payable \\(m\\) times a year as well . Gross premiums can also include other cashflows , such as a pre-determined profit margin . Since these are highly dependent on context, they will NOT be covered in this section. Level Expenses \u00b6 Acquisition and Renewal Expenses are always assumed to be paid at the beginning of the period , unless stated otherwise: Since the renewal expenses only occur at the beginning of year 2 , they can be equivalently thought of as occuring at the end of year 1 and every subsequent year after. Thus, it can be modelled using an annuity immediate : \\[ \\begin{aligned} \\text{EPV Expenses} &= e_0 + e_t \\cdot a_x \\\\ &= e_0 + e_t \\cdot (\\ddot{a}_{x} - 1) \\end{aligned} \\] Note This simplification does NOT hold true if there is expense inflation - the level annuity immediate CANNOT simply be replaced with the geometric annuity. This is because the expenses only start to inflate from the third payment onwards: If expenses are charged at the end of the year, including the year of death , then an assurance must be used on top of an annuity immediate. This is because an annuity always assumes that the individual is alive at the time that the cashflows occur. Thus, the expense of the year of death can be thought of as a termination expense equal to the renewal expenses : \\[ \\begin{aligned} \\text{EPV Expenses} &= \\text{EPV Renewal Expense} + \\text{EPV Termination Expense} \\\\ &= e_t \\cdot a_{x} + e_t \\cdot A_{x} \\\\ &= e_t \\cdot (a_{x} + A_{x}) \\end{aligned} \\] Non-Level Expenses \u00b6 Expenses may also be non-level , typically decreasing throughout the lifetime of the policy: If mortality follows the SULT, then the various EPVs can be easily calculated. Thus, the EPV of expenses can be directly calculated: \\[ \\begin{aligned} \\text{EPV Expenses} &= 60 + 0.8G \\\\ &+ (30 + 0.2G) (\\ddot{a}_{x:\\enclose{actuarial}{15}} - 1) \\\\ &+ ({}_{15}E_{x})(30 + 0.1G)(\\ddot{a}_{x+15:\\enclose{actuarial}{15}} - 1) \\\\ &+ ({}_{30}E_{x})(30)(\\ddot{a}_{x+30} - 1) \\end{aligned} \\] However, if the various EPVs cannot be obtained, then the cashflows must the simplified by splitting them into shared components : \\[ \\begin{aligned} \\text{EPV Expenses} &= 30 \\cdot \\ddot{a}_{x} \\\\ &+ 0.1G \\cdot \\ddot{a}_{x:\\enclose{actuarial}{30}} \\\\ &+ 0.1G \\cdot \\ddot{a}_{x:\\enclose{actuarial}{15}} \\\\ &+ 0.6G \\end{aligned} \\] Tip This is the preferred method of calculating the EPV of expenses when dealing with multiple expense tiers. Expense Premiums \u00b6 The difference between the gross and net premiums represent the premium needed to cover just the expenses . It is known as the Expense Loading , as it represents the amoount loaded onto the net premiums for expenses . Info A front-end load is an expense load placed at cashflows at inception while a back-end load is placed at termination. Since it is still a premium, the equivalance principle still applies to it: \\[ \\text{EPV Expense Loading} = \\text{EPV Expenses} \\] Intuitively, it is the difference between the gross and net premium : \\[ G^E = G - P \\] Return of Premiums \u00b6 Some policies offer a Return of Premium (ROP) feature where the death benefit is increased by the total amount of premiums paid till that point . These ROP features are usually in effect for a limited period of time or when premiums are paid for a limited period of time. Without Interest \u00b6 If the premiums are paid back without interest , then the ROP benefits are akin to an arithmetically increasing annuity : \\[ \\text{EPV ROP} = P \\cdot (IA)^{1}_{x:\\enclose{actuarial}{n}} \\] Info This is the main way (if at all) that variable benefit assurances and annuities are tested. Thus, it is recommended to memorize just this subsection rather than go through the entire section on variable benefits. Since it not possible to calculate \\((IA)_{x}\\) from the SULT, the question must provide values to help compute it. This can be done in one of two ways: Provide EPV of an increasing whole life assurance at different ages Provide EPV of a decreasing term assurance \\[ \\begin{aligned} (IA)^1_{x:\\enclose{actuarial}{n}} &= (IA)_x - {}_{n}E_x [(IA)_{x+n} + nA_{x+n}] \\\\ &= (n+1)A^1_{x:\\enclose{actuarial}{n}} - (D_{\\enclose{actuarial}{n}}A)^{1}_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Note The EPV of a decreasing term assurance can also be represented by: \\[ (D_{\\enclose{actuarial}{n}}A)^{1}_{x:\\enclose{actuarial}{n}} = \\sum^{n}_{k=1} A^{1}_{x:\\enclose{actuarial}{k}} \\] This is better understood visually: With Interest \u00b6 If the premiums are paid back with interest, then we need to find the accumulated value of the premiums at every point, and then discount them back to their present values . By visualizing the cashflows, we can see that the accumulated value of the premiums can be represented by the accumulated value of an annuity due , \\(\\ddot{s}\\) : Warning It is a common misconception to think that an annuity immediate should be used, because the death cashflows occur at the end of the period. However, it is key to understand that the annuities look at the raw cashflows, not the ones accumulated with interest. The PV of these accumulated annuities can be determined: \\[ \\begin{aligned} \\text{PV ROP} &= P(\\ddot{s}_{\\enclose{actuarial}{1}} \\cdot v^{1} + \\ddot{s}_{\\enclose{actuarial}{2}} \\cdot v^{2} + \\dots + \\ddot{s}_{\\enclose{actuarial}{n}} \\cdot v^{n}) \\\\ &= P \\cdot \\sum^{n-1}_{k = 0} \\ddot{s}_{\\enclose{actuarial}{k+1}} \\cdot v^{k+1} \\end{aligned} \\] Thus, their EPVs can be determined: \\[ \\begin{aligned} \\text{EPV ROP} &= P \\cdot \\sum^{n-1}_{k = 0} \\ddot{s}_{\\enclose{actuarial}{k+1}} \\cdot v^{k+1} {}_{k \\mid}q_{x} \\\\ &= P \\cdot \\sum^{n-1}_{k = 0} \\frac{(1+i)^{k+1} - 1}{d} \\cdot v^{k+1} {}_{k \\mid}q_{x} \\\\ &= \\frac{P}{d} \\cdot \\sum^{n-1}_{k = 0} (1 - v^{k+1}) \\cdot {}_{k \\mid}q_{x} \\\\ &= \\frac{P}{d} \\cdot \\left(\\sum^{n-1}_{k = 0} {}_{k \\mid}q_{x} - \\sum^{n-1}_{k = 0} v^{k+1} {}_{k \\mid}q_{x} \\right) \\\\ &= \\frac{P}{d} \\cdot \\left({}_{n}q_{x} - A^{1}_{x:\\enclose{actuarial}{n}} \\right) \\end{aligned} \\] Warning It is easy to confuse the above term \\((\\sum^{n-1}_{k = 0} {}_{k \\mid}q_{x})\\) with the discrete expectation: \\[ E(X) = \\sum k \\cdot {}_{k \\mid}q_{x} \\] The key difference is that the discrete expectation is multiplied by \\(k\\) , which causes to reduce to the survival probability. Without it, as like above, it simply reduces to the probability of dying during the period.","title":"Premiums"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/6.%20Premiums/#premiums","text":"","title":"Premiums"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/6.%20Premiums/#overview","text":"Unlike other products, the cost of a Life Assurance or Annuity are not known when it is issued, as it is impossible to precisely predict when an individual will die or how long they will live for. Premiums are the amount that the insurance company charges for a Life Assurance or Annuity. They aim to charge a sufficiently high premium such that they expect to at least break even on the sale of the policy on an EPV basis . There are three main types of premiums: Single Premium Limited Premium Regular Premium Single Lump Sum Recurring for a fixed period Recurring as long as the contract is valid Use dollar amount Modelled using TA Annuities Modelled using WL Annuities Info Premiums can be paid at any frequency , but monthly is the most common as it matches the frequency of when people receive their income. They are always paid in advance . If they were paid in arrears, policyholders may refuse to pay premiums on the flawed grounds that since they did not die, they did not utilize the coverage and hence should not pay. There are two types of premiums that can be calculated: Net Premiums ( \\(P\\) ) - Excluding expenses; benefits only Gross Premiums ( \\(G\\) ) - Including expenses and other cashflows The key thing to notice is that the components of net premiums are fixed while gross premiums differ from insurer to insurer. Thus, this section will only show formulas for net premiums .","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/6.%20Premiums/#equivalence-principle-approach","text":"To determine the breakeven point of the insurer, the Loss Amount must first be defined. It is the net outflow of the policy: \\[ \\text{Loss} = \\text{Outflow} - \\text{Inflow} \\] Let \\({}_{t-1}L_{x}\\) be the random variable denoting the PV of the loss at the START of policy year t : \\[ \\begin{aligned} {}_{t-1}L_{x} &= \\text{PV Outflow} - \\text{PV Inflows} \\\\ &= B \\cdot v^{K_{x+(t-1)} + 1} - P \\cdot \\ddot{a}_{\\enclose{actuarial}{K_{x+(t-1)} + 1}} \\end{aligned} \\] Since L represents an outflow rather than an inflow, its intepretation is reversed : \\({}_{t-1}L_{x} \\gt 0\\) , Negative Profits (Losing money) \\({}_{t-1}L_{x} \\lt 0\\) , Positive Profits (Making money) Intuitively, insurers will charge a premium such that they expect to break even at the time of the sale , time 0. In other words, the expected loss at the start of the policy year 1 is 0 : \\[ \\begin{aligned} {}_{0}L_{x} &= B \\cdot v^{K_{x} + 1} - P \\cdot \\ddot{a}_{\\enclose{actuarial}{K_{x} + 1}} \\\\ \\\\ E \\left({}_{0}L_{x} \\right) &= B \\cdot E \\left(v^{K_{x} + 1} \\right) - P \\cdot E \\left(\\ddot{a}_{\\enclose{actuarial}{K_{x} + 1}} \\right) \\\\ &= B \\cdot A_{x} - P \\cdot \\ddot{a}_{x} \\\\ \\\\ \\therefore E \\left({}_{0}L_{x} \\right) &= 0 \\\\ B \\cdot A_{x} - P \\cdot \\ddot{a}_x &= 0 \\\\ P \\cdot \\ddot{a}_{x} &= B \\cdot A_{x} \\\\ P &= B \\cdot \\frac{A_x}{\\ddot{a}_x} \\end{aligned} \\] Note For the purposes of this exam, premiums are usually paid annually , where \\(P\\) represents the annual net premium . If premiums were paid more frequently, then the corresponding EPVs should be used: \\[ \\text{EPV Inflow} = P \\cdot \\ddot{a}^{(m)}_{x} \\] Regardless of the frequency of payment, the annual net premium is still multiplied the EPVs. Thus, to obtain the premium payable per period (vice-versa) the annual premium must be divided by the number of periods. \\[ P^{(m)} = \\frac{P}{m} \\] Premiums can also be paid continuously. The amount paid continuously is known as the Premium Rate -- it does not follow the above convention. This is known as the Equivalence Principle approach, as it charges premiums such that the EPVs of the inflows and outflows are equal: \\[ \\begin{aligned} \\text{EPV Outflow} &= \\text{EPV Inflow} \\\\ \\text{EPV Premium} &= \\text{EPV Benefit} \\end{aligned} \\] Note If the policy charges a Single Net Premium using the equivalence principle, then that is simply equivalent to the EPV of the benefits. \\[ \\begin{aligned} \\text{EPV Premium} &= \\text{Single Net Premium} \\\\ \\therefore \\text{Single Net Premium} &= \\text{EPV Benefit} \\end{aligned} \\]","title":"Equivalence Principle Approach"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/6.%20Premiums/#portfolio-percentile-approach","text":"The equivalence principle calculates the premium for a single policy such that it is expected to break even . Given a portfolio of N homogenous policies , the Portfolio Percentile Approach can be used to calculate the premims such that the probability that the entire portfolio at least breaks even is fixed at some minimum value. Let \\(S\\) be the random variable denoting the aggregate loss of \\(N\\) homogenous policies that were just issued : \\[ \\begin{aligned} S &= {}_{0}L_1 + {}_{0}L_2 + \\dots + {}_{0}L_{N} \\\\ E(S) &= N \\cdot E({}_{0}L) \\\\ Var (S) &= N \\cdot \\text{Var}({}_{0}L) \\end{aligned} \\] The expectation and variance of a single policy can be determined as the following: \\[ \\begin{aligned} {}_{0}L &= B \\cdot v^{K_{x} + 1} - P \\cdot \\ddot{a}_{\\enclose{actuarial}{K_{x} + 1}} \\\\ &= B \\cdot v^{K_{x} + 1} - P \\cdot \\frac{1- v^{K_{x} + 1}}{d} \\\\ &= B \\cdot v^{K_{x} + 1} + \\frac{- P + P v^{K_{x} + 1}}{d} \\\\ &= \\left(B + \\frac{P}{d} \\right)v^{K_{x} + 1} - \\frac{P}{d} \\\\ \\\\ E({}_{0}L) &= E \\left[\\left(B + \\frac{P}{d} \\right)v^{K_{x} + 1} - \\frac{P}{d} \\right] \\\\ &= \\left(B + \\frac{P}{d} \\right) E(v^{K_{x} + 1}) - E \\left(\\frac{P}{d} \\right) \\\\ &= \\left(B + \\frac{P}{d} \\right)A_{x} - \\frac{P}{d} \\\\ \\\\ \\text{Var}({}_{0}L) &= \\text{Var}\\left[\\left(B + \\frac{P}{d} \\right)v^{K_{x} + 1} - \\frac{P}{d} \\right] \\\\ &= \\text{Var} \\left[\\left(B + \\frac{P}{d} \\right)v^{K_{x} + 1} \\right] \\\\ &= \\left(B + \\frac{P}{d} \\right)^2 \\cdot \\text{Var} \\left(v^{K_{x} + 1} \\right) \\\\ &= \\left(B + \\frac{P}{d} \\right)^2 \\cdot \\left[{}^{2}A_{x} - (A_{x})^2 \\right] \\end{aligned} \\] Warning It is a common mistake to mix this up with the variance of the policy benefits. Tip Notice that the EPV components of the mean and variance are only based on the policyholder's age. Thus, different policies (with different benefits and premiums) can be used to solve for these shared EPV components and hence calculate the mean and variance of each policy. If \\(N\\) is sufficiently large, then \\(S\\) is approximately normally distributed via the central limit theorem: \\[ S \\sim N(E(S), \\text{Var}(S)) \\] The premium is then set such the probability of at least breaking even (probability of profit) is set a pre-determined level \\(\\alpha\\) : \\[ \\begin{aligned} P(S < 0) &= \\alpha \\\\ P\\left(\\frac{S - E(S)}{\\sqrt{Var (S)}} \\le \\frac{0 - E(S)}{\\sqrt{Var (S)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- E(S)}{\\sqrt{Var (S)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- n \\cdot E({}_{0}L)}{\\sqrt{n \\cdot Var ({}_{0}L)}}\\right) &= \\alpha \\\\ P\\left(Z \\le \\frac{- n \\cdot E({}_{0}L)}{\\sqrt{n \\cdot Var ({}_{0}L)}}\\right) &= \\alpha \\\\ \\\\ \\frac{- n \\cdot E({}_{0}L)}{\\sqrt{n \\cdot Var ({}_{0}L)}} = Z_{\\alpha} \\end{aligned} \\] By plugging in the values determined earlier and solving for P, the appropriate premiums can be determined.","title":"Portfolio Percentile Approach"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/6.%20Premiums/#loss-probability","text":"The above method can also be used in reverse - given a premium, the probability that the aggregate loss taking on some range of values can be computed. For a single policy , the probability must be solved using the survival distribution: \\[ \\begin{aligned} P({{}_{0}L} \\gt k) &= P \\left( \\left(B + \\frac{P}{d} \\right)v^{K_{x} + 1} - \\frac{P}{d} \\gt k \\right) \\\\ &= \\dots \\\\ &= P(K_x \\gt \\dots) \\end{aligned} \\]","title":"Loss Probability"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/6.%20Premiums/#gross-premiums","text":"What makes gross premiums so much trickier than net premiums is that there are many different types of expenses , where every question can ask for a unique combination of them. Tip Questions will explicitly state if there are expenses in the policy. Just because a question uses \"Gross Premiums\" does not automatically mean that there are expenses. Expenses can be differentiated in terms of when they are charged : Acquisition Expenses Renewal Expenses Termination Expense At policy inception During policy lifetime At policy termination Single payment at time 0 Recurring payments from time 1 onwards Single payment at unknown time Denoted by \\(e_0\\) Denoted by \\(e_t\\) Denoted by \\(E\\) Raw Value Annuity Functions Assurance Function EG. Commissions expenses EG. Administrative Expenses EG. Claims expenses Warning Renewal Expenses is also sometimes known as Maintenance Expenses . Although they are usually charged from policy year 2 onwards, they can be charged in the first year as well . The above table is just a general guide. Acquisition expenses are usually higher than renewal ones as agent commissions are mostly front-loaded in the first year to give agents cash faster, incentivizing them to sell more. Note Renewal Expenses can also inflate over time, where they increase by a factor of \\((1+r)\\) each year. In this case, they are modelled using a geometrically increasing annuity instead. They can also be differentiated in terms of how much they charge : Overhead Expenses Direct Expenses Shared among all policies Borne by a specific policy Fixed amount Percentage of Premium or Benefit EG. Office Rental EG. Sales Commission Tip Generally speaking, bverhead expenses are charged every year while direct expenses are only charged whenever premiums are paid. This is relevant for policies for with a limited payment period . After the premium payment period, only overhead expenses are charged to the policy. Similarly, if premiums are payable \\(m\\) times a year, then expenses are payable \\(m\\) times a year as well . Gross premiums can also include other cashflows , such as a pre-determined profit margin . Since these are highly dependent on context, they will NOT be covered in this section.","title":"Gross Premiums"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/6.%20Premiums/#level-expenses","text":"Acquisition and Renewal Expenses are always assumed to be paid at the beginning of the period , unless stated otherwise: Since the renewal expenses only occur at the beginning of year 2 , they can be equivalently thought of as occuring at the end of year 1 and every subsequent year after. Thus, it can be modelled using an annuity immediate : \\[ \\begin{aligned} \\text{EPV Expenses} &= e_0 + e_t \\cdot a_x \\\\ &= e_0 + e_t \\cdot (\\ddot{a}_{x} - 1) \\end{aligned} \\] Note This simplification does NOT hold true if there is expense inflation - the level annuity immediate CANNOT simply be replaced with the geometric annuity. This is because the expenses only start to inflate from the third payment onwards: If expenses are charged at the end of the year, including the year of death , then an assurance must be used on top of an annuity immediate. This is because an annuity always assumes that the individual is alive at the time that the cashflows occur. Thus, the expense of the year of death can be thought of as a termination expense equal to the renewal expenses : \\[ \\begin{aligned} \\text{EPV Expenses} &= \\text{EPV Renewal Expense} + \\text{EPV Termination Expense} \\\\ &= e_t \\cdot a_{x} + e_t \\cdot A_{x} \\\\ &= e_t \\cdot (a_{x} + A_{x}) \\end{aligned} \\]","title":"Level Expenses"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/6.%20Premiums/#non-level-expenses","text":"Expenses may also be non-level , typically decreasing throughout the lifetime of the policy: If mortality follows the SULT, then the various EPVs can be easily calculated. Thus, the EPV of expenses can be directly calculated: \\[ \\begin{aligned} \\text{EPV Expenses} &= 60 + 0.8G \\\\ &+ (30 + 0.2G) (\\ddot{a}_{x:\\enclose{actuarial}{15}} - 1) \\\\ &+ ({}_{15}E_{x})(30 + 0.1G)(\\ddot{a}_{x+15:\\enclose{actuarial}{15}} - 1) \\\\ &+ ({}_{30}E_{x})(30)(\\ddot{a}_{x+30} - 1) \\end{aligned} \\] However, if the various EPVs cannot be obtained, then the cashflows must the simplified by splitting them into shared components : \\[ \\begin{aligned} \\text{EPV Expenses} &= 30 \\cdot \\ddot{a}_{x} \\\\ &+ 0.1G \\cdot \\ddot{a}_{x:\\enclose{actuarial}{30}} \\\\ &+ 0.1G \\cdot \\ddot{a}_{x:\\enclose{actuarial}{15}} \\\\ &+ 0.6G \\end{aligned} \\] Tip This is the preferred method of calculating the EPV of expenses when dealing with multiple expense tiers.","title":"Non-Level Expenses"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/6.%20Premiums/#expense-premiums","text":"The difference between the gross and net premiums represent the premium needed to cover just the expenses . It is known as the Expense Loading , as it represents the amoount loaded onto the net premiums for expenses . Info A front-end load is an expense load placed at cashflows at inception while a back-end load is placed at termination. Since it is still a premium, the equivalance principle still applies to it: \\[ \\text{EPV Expense Loading} = \\text{EPV Expenses} \\] Intuitively, it is the difference between the gross and net premium : \\[ G^E = G - P \\]","title":"Expense Premiums"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/6.%20Premiums/#return-of-premiums","text":"Some policies offer a Return of Premium (ROP) feature where the death benefit is increased by the total amount of premiums paid till that point . These ROP features are usually in effect for a limited period of time or when premiums are paid for a limited period of time.","title":"Return of Premiums"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/6.%20Premiums/#without-interest","text":"If the premiums are paid back without interest , then the ROP benefits are akin to an arithmetically increasing annuity : \\[ \\text{EPV ROP} = P \\cdot (IA)^{1}_{x:\\enclose{actuarial}{n}} \\] Info This is the main way (if at all) that variable benefit assurances and annuities are tested. Thus, it is recommended to memorize just this subsection rather than go through the entire section on variable benefits. Since it not possible to calculate \\((IA)_{x}\\) from the SULT, the question must provide values to help compute it. This can be done in one of two ways: Provide EPV of an increasing whole life assurance at different ages Provide EPV of a decreasing term assurance \\[ \\begin{aligned} (IA)^1_{x:\\enclose{actuarial}{n}} &= (IA)_x - {}_{n}E_x [(IA)_{x+n} + nA_{x+n}] \\\\ &= (n+1)A^1_{x:\\enclose{actuarial}{n}} - (D_{\\enclose{actuarial}{n}}A)^{1}_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] Note The EPV of a decreasing term assurance can also be represented by: \\[ (D_{\\enclose{actuarial}{n}}A)^{1}_{x:\\enclose{actuarial}{n}} = \\sum^{n}_{k=1} A^{1}_{x:\\enclose{actuarial}{k}} \\] This is better understood visually:","title":"Without Interest"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/6.%20Premiums/#with-interest","text":"If the premiums are paid back with interest, then we need to find the accumulated value of the premiums at every point, and then discount them back to their present values . By visualizing the cashflows, we can see that the accumulated value of the premiums can be represented by the accumulated value of an annuity due , \\(\\ddot{s}\\) : Warning It is a common misconception to think that an annuity immediate should be used, because the death cashflows occur at the end of the period. However, it is key to understand that the annuities look at the raw cashflows, not the ones accumulated with interest. The PV of these accumulated annuities can be determined: \\[ \\begin{aligned} \\text{PV ROP} &= P(\\ddot{s}_{\\enclose{actuarial}{1}} \\cdot v^{1} + \\ddot{s}_{\\enclose{actuarial}{2}} \\cdot v^{2} + \\dots + \\ddot{s}_{\\enclose{actuarial}{n}} \\cdot v^{n}) \\\\ &= P \\cdot \\sum^{n-1}_{k = 0} \\ddot{s}_{\\enclose{actuarial}{k+1}} \\cdot v^{k+1} \\end{aligned} \\] Thus, their EPVs can be determined: \\[ \\begin{aligned} \\text{EPV ROP} &= P \\cdot \\sum^{n-1}_{k = 0} \\ddot{s}_{\\enclose{actuarial}{k+1}} \\cdot v^{k+1} {}_{k \\mid}q_{x} \\\\ &= P \\cdot \\sum^{n-1}_{k = 0} \\frac{(1+i)^{k+1} - 1}{d} \\cdot v^{k+1} {}_{k \\mid}q_{x} \\\\ &= \\frac{P}{d} \\cdot \\sum^{n-1}_{k = 0} (1 - v^{k+1}) \\cdot {}_{k \\mid}q_{x} \\\\ &= \\frac{P}{d} \\cdot \\left(\\sum^{n-1}_{k = 0} {}_{k \\mid}q_{x} - \\sum^{n-1}_{k = 0} v^{k+1} {}_{k \\mid}q_{x} \\right) \\\\ &= \\frac{P}{d} \\cdot \\left({}_{n}q_{x} - A^{1}_{x:\\enclose{actuarial}{n}} \\right) \\end{aligned} \\] Warning It is easy to confuse the above term \\((\\sum^{n-1}_{k = 0} {}_{k \\mid}q_{x})\\) with the discrete expectation: \\[ E(X) = \\sum k \\cdot {}_{k \\mid}q_{x} \\] The key difference is that the discrete expectation is multiplied by \\(k\\) , which causes to reduce to the survival probability. Without it, as like above, it simply reduces to the probability of dying during the period.","title":"With Interest"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/7.%20Reserves/","text":"Reserves \u00b6 Background \u00b6 Most life insurance contracts charge level premiums - charging the same amount throughout the lifetime of the policy. When the policyholder is young, the probability of death is small, thus the premiums collected are larger than the expected outflow in the year, leading to a surplus . When the policyholder is older, the probability of death is high, thus the premiums collected are smaller than the expected outflow in the year, leading to a deficit . Given the inevitable deficit, the surplus in the earlier years cannot be recognized as a profit and are instead pooled together and safely invested into an account, from which the deficit in later years will draw from. Info It is a common misconception to think that each policy has its own account. Following this logic, the insurer would not have enough funds to pay an insured who dies shortly after death. In order to determine the adequacy of the account, insurers will calculate the size of the expected loss of ALL policies and compare it against the (hopefully larger) account value. The expected loss is known as the Reserve of the policy because it represents an amount inside the pooled account that is reserved for paying the benefits of the policy. Overview \u00b6 There are two approaches to calculate the reserves: Prospective Approach - Based on Present Values of future cashflows Retrospective Approach - Based on Future Values of past cashflows (Not covered) Similar to premiums, there are also two types of reserves, depending on what cashflows are being considered: Net Premium Reserves - Net Premiums and Benefits Gross Premium Reserves - Gross Premiums, Benefits and Expenses This exam typically assumes that reserves are calculated at the end of the policy year : AFTER any year end cashflows from the current year (Claims & Claim Expenses) BEFORE any beginning of year cashflows from the next period (Premiums & Renewal Expenses) For the purposes of this exam, only the prospective approach is covered. Similar to premiums, we will only define formulas for the Net Premium Reserves . Net Premium Reserves \u00b6 The prospective approach calculates the reserves as the expected value of the loss variable . Let \\({}_{t}V\\) represent the reserve calculated at the end of policy year \\(t\\) : \\[ \\begin{aligned} {}_{t}L_{x} &= B \\cdot v^{K_{x+t} + 1} - P \\cdot \\ddot{a}_{\\enclose{actuarial}{K_{x+t} + 1}} \\\\ \\\\ {}_{t}V &= E({}_{t}L_{x}) \\\\ &= E \\left(B \\cdot v^{K_{x+t} + 1} - P \\cdot \\ddot{a}_{\\enclose{actuarial}{K_{x+t} + 1}} \\right) \\\\ &= B \\cdot E \\left(v^{K_{x+t} + 1} \\right) - P \\cdot E \\left(\\ddot{a}_{\\enclose{actuarial}{K_{x+t} + 1}} \\right) \\\\ &= B \\cdot A_{x+t} - P \\cdot \\ddot{a}_{x+t} \\end{aligned} \\] Warning Both premiums and reserves use the loss variable. Premiums use the loss variable at issue \\(({}_{0}L_{x})\\) while reserves use the general loss variable \\(({}_{t}L_{x})\\) . Premiums consider the loss variable at the start of the policy year \\(({}_{t-1}L_{x})\\) while reserves typically use the end of the policy year \\(({}_{t}L_{x})\\) . Both are equivalent ways of referring to the loss variable as the start of the current year is the end of the previous year. Reserve questions can also ask for the reserve at the start of the policy year , so be sure to read the question very carefully. Based on this, the reserves can be intepreted as the expected addittional amount on top of premiums to cover claims from that point on. As its name suggests, the prospective approach only considers prospective (future) cashflows . Thus, for single premium or deferred products , since the premium was paid in the past, the reserve only contains the benefit . Shortcuts \u00b6 Net premium reserves can be simplified into an expression involving ONLY the following, which allows it to be easily calculated in situations where limited information is provided: Annuities only Assurances only Annuities and Premiums Assurance and Premiums Case 1: Annuities Only \\[ \\begin{aligned} {}_{t}V &= A_{x+t} - P_{x} \\ddot{a}_{x+t} \\\\ &= A_{x+t} - \\frac{A_x}{\\ddot{a}_x} \\cdot \\ddot{a}_{x+t} \\\\ &= (1-d\\ddot{a}_{x+t}) - \\frac{1-d\\ddot{a}_{x}}{\\ddot{a}_{x}} \\cdot \\ddot{a}_{x+t} \\\\ &= 1-d\\ddot{a}_{x+t} - \\left (\\frac{1}{\\ddot{a}_x}-d \\right) \\cdot \\ddot{a}_{x+t} \\\\ &= 1-d\\ddot{a}_{x+t} - \\frac{\\ddot{a}_{x+t}}{\\ddot{a}_x} + d\\ddot{a}_{x+t} \\\\ &= 1 - \\frac{\\ddot{a}_{x+t}}{\\ddot{a}_x} \\end{aligned} \\] Note \\(P_{x}\\) is used to denote the net premium for a policy that was purchased at age \\(x\\) . Case 2: Assurances Only \\[ \\begin{aligned} {}_{t-1}V &= A_{x+t} - P_{x} \\ddot{a}_{x+t} \\\\ &= A_{x+t} - \\frac{A_x}{\\ddot{a}_x} \\cdot \\ddot{a}_{x+t} \\\\ &= A_{x+t} - \\frac{A_x}{\\frac{1-A_x}{d}} \\cdot \\frac{1-A_{x+t}}{d} \\\\ &= A_{x+t} - \\frac{A_x}{1-A_x} \\cdot (1-A_{x+t}) \\\\ &= \\frac{A_{x+t} \\cdot (1-A_{x}) - A_x(1-A_{x+t})}{1-A_x} \\\\ &= \\frac{A_{x+t} - A_{x}A_{x+t} - A_x + A_x A_{x+t}}{1-A_x} \\\\ &= \\frac{A_{x+t} - A_{x}}{1 - A_{x}} \\end{aligned} \\] Case 3: Annuities & Premiums \\[ \\begin{aligned} {}_{t}V &= A_{x+t} - P_x \\cdot \\ddot{a}_{x+t} \\\\ &= \\ddot{a}_{x+t} \\cdot \\left(\\frac{A_{x+t}}{\\ddot{a}_{x+t}} - P_{x} \\right) \\\\ &= \\ddot{a}_{x+t} \\cdot \\left(P_{x+t} - P_{x} \\right) \\end{aligned} \\] Note Notice that the reserves at time \\(t\\) and a newly issued policy at time \\(x+t\\) both use the same loss variable \\({}_{t}L_{x}\\) . This is why the expressions contain \\(P_{x+t}\\) , which is the premium for this newly issued policy. Case 4: Assurances & Premiums \\[ \\begin{aligned} {}_{t}V &= A_{x+t} - P_x \\cdot \\ddot{a}_{x+t} \\\\ &= A_{x+t} \\left(1 - P_x \\cdot \\frac{\\ddot{a}_{x+t}}{A_{x+t}} \\right) \\\\ &= A_{x+t} \\left(1 - P_x \\cdot \\frac{1}{\\frac{A_{x+t}}{\\ddot{a}_{x+t}}} \\right) \\\\ &= A_{x+t} \\left(1 - \\frac{P_x}{P_{x+t}} \\right) \\end{aligned} \\] Reserve at Issue \u00b6 As alluded to earlier, the calculation of reserves and premiums are similar, which makes it easy to mix them up : Premiums Reserves Loss at issue \\({}_{0}L_{x}\\) General Loss \\({}_{t}L_{x}\\) \\(E(L)\\) set at 0 \\(E(L)\\) is the target \\(P\\) is the target \\(P\\) is given Calculated at age \\(x\\) Calculated at age \\(x+t\\) Note We have implicitly assumed that premiums are calculated using the equivalence principle. For the purposes of the exam, do NOT assume so unless explicitly stated . Putting both together, the reserve at the start of the policy will be calculated using the loss at issue at variable. If premiums are calculated using the equivalence principle , then the expectation of the loss at issue will always be 0. Thus, the reserve at the start of the policy is always 0 . This is intuitive, as premiums (using the equivalence principle) are set such that they expect to exactly cover the losses of the policy at issue. \\[ \\begin{aligned} {}_{0}V &= E({}_{0}L_{x}) \\\\ &= 0 \\end{aligned} \\] Tip This can be generalized -- If the reserves are zero in a particular year, then the equivalence principle can be applied at that timing: \\[ \\text{EPV Inflow} = \\text{EPV Outflow} \\] This applies to both net and gross premium reserves. Recursions \u00b6 Following the same logic as assurances and annuities, net premium reserves can also be recursively expressed as a function of itself: If the policyholder dies , the insurer pays a benefit of 1 at the end of the year If the policyholder survives , the insurer keeps a reserve at the end of the year Regardless , Premiums have already been received at the start of the year \\[ \\begin{aligned} {}_{t}V &= - P + \\begin{cases} v \\cdot 1, & q_{x+t} \\\\ v \\cdot {}_{t+1}V, & p_{x+t} \\end{cases} \\\\ \\\\ \\therefore {}_{t}V &= - P + q_{x+t} \\cdot v + p_{x+t} \\cdot v \\cdot {}_{t+1}V \\end{aligned} \\] Tip Since the reserve at time 0 is always 0, recursion can be used to easily calculate the reserves at time 1 or time 2 . Interim Reserves \u00b6 Since reserves can be calculated at any point in time, they can also be calculated as at a fractional age . They are then known as the Interim Reserves . Similar to EPVs, it can be calculated via recursion: If the policyholder dies within \\(s\\) , then the benefit is paid the end of the year If the policy survives \\(s\\) , then a reserve is set up at that time Regardless , Premiums have already been received at the start of the year \\[ \\begin{aligned} {}_{t}V &= -P + \\begin{cases} v \\cdot 1 ,& {}_{s}q_{x+t} \\\\ v^s \\cdot {}_{t+s}V ,& {}_{s}p_{x+t} \\end{cases} \\\\ \\\\ \\therefore {}_{t}V &= -P + v \\cdot q_{x+t} + v^s \\cdot p_{x+t} \\cdot {}_{t+s}V \\end{aligned} \\] The key difference is that the two events now happen at seperate times, which changes the extent that they are discounted . The benefit is paid at the end of the year \\((x+1)\\) while the interim reserve is set up upon survival \\((x+s)\\) . Warning If the policy is not discrete, the timing and hence discounting factor MUST be changed accordingly. The interim reserve itself can also be directly recursed upon: \\[ \\begin{aligned} {}_{t+s}V &= \\begin{cases} v^{1-s} \\cdot 1 ,& {}_{1-s}q_{x+t+s} \\\\ v^{1-s} \\cdot {}_{t+1}V ,& {}_{1-s}p_{x+t+s} \\end{cases} \\\\ \\\\ \\therefore {}_{t+s}V &= v^{1-s} \\cdot {}_{1-s}q_{x+t+s} + v^{1-s} \\cdot {}_{1-s}p_{x+t+s} \\cdot {}_{t+s}V \\end{aligned} \\] They key difference is that since there are no premium payments between the interim reserve and the next discrete reserve (since reserves are calculated before premiums). The discounting factor has also been flipped because we are discounting from the discrete reserve to the interim , unlike previously where it was the interim to the discrete. Alternative Recursion \u00b6 This section requires basic knowledge of ALTAM content. A more intuitive way to think about recursion is to follow the profit cashflow logic: The insurer starts the month with the beginning reserves Premiums are received Maintenance expenses and any annuity benefits are incurred The net amount is invested to earn interest Assurance benefits and associated expenses are paid New reserve is set up for the surviving policyholders \\[ ({}_{t-1}V + P - e - B^{\\text{Annuity}})(1+i) = q_{x+t} \\cdot (B^{\\text{Assurance}} + E) + p_{x+t} \\cdot {}_{t}V \\] The above logic can be applied for interim reserves as well; simply adjust the interest and probability functions accordingly: \\[ ({}_{t-s}V + P - e - B^{\\text{Annuity}})(1+j)^{m} = {}_{s}q_{x+t} \\cdot (B^{\\text{Assurance}} + E) + {}_{s}p_{x+t} \\cdot {}_{t}V \\] Warning The key difference is that not all of the above cashflows may be present at the fractional time. For instance, if benefits are paid out at the end of the year, then there will be no benefit cashflow in the recursion. Another common mistake is to forget to update the interest rate to the rate corresponding to that time period. Gross Premium Reserves \u00b6 The main difference is that Gross reserves use Gross Premiums and include expenses in the calculation. Warning It is a common mistake to include other cashflows (EG. Profit Margin) in the gross reserve calculations because they included in the gross premium calculations. Remember that gross reserves ONLY contain the above three components. This is intuitive, as reserves should only be to meet essential cashflows . This leads to an interesting result because the cashflows used for premiums and reserves are different: Premium Cashflows : Benefit, Expenses & Others (EG. Profit Margin) Reserve Cashflows : Benefit, Expenses The gross premiums include additional provisions for other cashflows which are not considered in the reserve calculation. This means that the gross premiums are always more than enough to cover expected benefits and reserves at time 0 , leading to a negative reserve : \\[ {}_{0}V^{\\text{Gross}} \\le 0 \\] Note If the gross premiums are calculated using ONLY benefits and expenses , then the Gross Reserves are 0: \\[ {}_{0}V^{\\text{Gross}} = 0 \\] Gross premium reserves are very similar to net premium ones, with just the inclusion of expenses that are paid at the beginning of the year: \\[ {}_{t}V^{\\text{Gross}} = (e_t - G) + q_{x+t} \\cdot v + p_{x+t} \\cdot v \\cdot {}_{t+1}V^{\\text{Gross}} \\] Red Herrings \u00b6 Due to the difference in components for Gross Premiums and Gross Reserves, questions may sometimes provide more information than required to trick us into using the wrong cashflows for gross reserves. For instance, questions may sometimes provide the acquisition epxenses or profit margin, which are not needed for gross reserves. Another red herring would be when the question provides a premium (known as the Office Premium ) and provides the components to calculate the gross premium. The office premium is assumed to be the gross premium unless explicitly stated otherwise and should be used in the reserve calculation. There is no need to be recalculate the gross premium . The reverse is also possible, where gross premiums are provided but the question actually requires us to calculate net premiums, which can cause us to mistakenly use the gross premium instead. Expense Reserves \u00b6 Similar to expense loadings, we can define a reserve for just the expenses , known as the Expense Reserves. It can be determined from first principles using the expense loadings or intuitively as the difference between the gross and net reserves: \\[ \\begin{aligned} {}_{t}V^{\\text{Expense}} &= \\text{EPV(Expenses)} - \\text{EPV(Expense Premium)} \\\\ &= {}_{t}V^{\\text{Gross}} - {}_{t}V^{\\text{Net}} \\end{aligned} \\] If calculated via the equivalence principle, the expense reserve is 0 at time 0 . However, the expense reserve is typically negative for all other years : \\[ \\begin{aligned} {}_{0}V^{\\text{Expense}} &= 0 \\\\ {}_{t}V^{\\text{Expense}} &\\lt 0 \\\\ \\end{aligned} \\] This is because acquisition expenses are usually significantly higher than renewal expenses. This means that the expense loading is usually between the two : \\[ e_{t} \\lt G^{E} \\lt e_{0} \\] This means that there this is an ACTUAL positive loss in the first year, as the expense loading is insufficient to cover the acquisition expenses. This loss is known as the New Business Strain . On the flipside, this means that the expense premiums are higher than the renewal expenses in all other years, making a negative loss (profit) to recover the new business strain. Info This is also known as the Deferred Acquisition Costs (DAC), as the recovery of the acquisition expenses are deferre to all future years. On an EPV basis, this means that at the end of year 1, all future expense loadings are larger than the future renewal premiums, resulting in a negative expense reserve from that time on: \\[ \\begin{aligned} \\text{EPV(Expense Loading)} &\\gt \\text{EPV(Renewal Expense)} \\\\ {}_{t}V^{\\text{Expense}} &< 0 \\end{aligned} \\] Modified Net Premium Reserves \u00b6 Net Premium Reserves are used in the US due to its simplicity, but at the cost of some accuracy loss. In particular, net premium reserves tend to be higher than gross premium reserves . This is due to the negative expense reserves , as the excess expense premiums help to offset some of the future benefits as well. Since net premiums reserves do not recognize expenses, they do not have this benefit! \\[ \\begin{aligned} {}_{t}V^{\\text{Net}} &= {}_{t}V^{\\text{Gross}} + \\underbrace{{}_{t}V^{\\text{Expense}}}_{\\text{-ve}} \\\\ {}_{t}V^{\\text{Net}} &\\gt {}_{t}V^{\\text{Gross}} \\end{aligned} \\] This means that insurers are holding too much reserves under this valuation basis. This is problematic because reserves can only be invested in low risk assets, which typically have lower returns relative to what the capital would have usually be invested in. This incurs an opportunity cost in investment income . To account for this, US regulators allow for insurers to use a modified net premium reserve instead that implicitly accounts for expenses (resulting in a smaller reserve ) while retaining the simplicity of a net premium approach. General Approach \u00b6 Most methods of calculating modified reserves involve using non-level modified premiums to calculate the reserves. They typically consist of multiple components, where the premiums in the early years are smaller than \\(P\\) while the premiums in the later years are larger than \\(P\\) . \\[ P^{\\text{Early}} \\lt P \\lt P^{\\text{Later}} \\] Regardless of the number of components, the EPV of all components must be equal to the EPV of the original level net premium: \\[ \\text{EPV Non Level Components} = \\text{EPV Level Net Premium} \\] In the later years, this allows the insurer to recognise more premiums , resulting in a smaller reserve, allowing the insurer to use their capital more productively. \\[ {}_{t}V^{\\text{Modified}} < {}_{t}V^{\\text{Net}} \\] Full Preliminary Term \u00b6 In the US, the modified premiums are calculated using the Full Preliminary Term (FPT) method. Premiums are split into two components : First Year Premiums , \\(\\alpha\\) Subsequent Premiums , \\(\\beta\\) The policy can be thought of as being split into two components: One year TA with single premiums of \\(\\alpha\\) The original policy issued one year later with one less year of coverage with regular premiums of \\(\\beta\\) Info The first year premiums are sometimes referred to as the Net Cost of Insurance in the year. \\[ \\begin{aligned} {}_{0}V^{\\text{FPT}} &= 0 \\\\ \\alpha &= A^{1}_{x:\\enclose{actuarial}{n}}\\\\ \\\\ {}_{1}V^{\\text{FPT}} &= 0 \\\\ A_{x+1} - \\beta \\ddot{a}_{x+1} &= 0 \\\\ \\beta &= \\frac{A_{x+1}}{\\ddot{a}_{x+1}} \\end{aligned} \\] The modified reserves at all later times follows the same formula as before, simply using \\(\\beta\\) instead: \\[ \\begin{aligned} {}_{t}V^{\\text{FPT}} &= \\begin{cases} 0, & t = 0 \\\\ 0, & t = 1 \\\\ A_{x+t} - \\beta \\ddot{a}_{x+t}, & t > 1 \\end{cases} \\end{aligned} \\] The FPT reserves (third component) can be intepreted as the net premium reserves for a policy that was issues one year later with one less year of coverage : \\[ \\begin{aligned} \\therefore {}_{t}V^{\\text{FPT}} &= {}_{t-1}V^{\\text{Net}}_{\\text{One year later, one less year}} \\\\ &= B \\cdot A_{x+(t-1)} - P_{x+1} \\cdot A_{x+(t-1)} \\end{aligned} \\] Since this is a net premium, the shortcuts earlier can be used to quickly calculate the reserve if the policy is WL . Warning When select and ultimate mortality is involved, the select age does not change . In other words, under FPT, the policyholder is selected at age \\([x]\\) but purchases the policy one year later \\([x]+1\\) . It is a common mistake to take the select age as \\([x+1]\\) , since the policy was issued one year later. However, the policy is not actually issued one year later; the policyholders mortality still follows that of being selected at their original age. If the policy is an EA or TA, then the one less year of coverage must be accounted for: \\[ \\begin{aligned} \\therefore {}_{t}V^{\\text{FPT}} &= B \\cdot A_{x+(t-1):\\enclose{actuarial}{n-1}} - P_{x+1:\\enclose{actuarial}{n-1}} \\cdot A_{x+(t-1):\\enclose{actuarial}{n-1}} \\end{aligned} \\] Note The one less year of coverage also applies to limited pay WL policies: \\[ \\begin{aligned} \\therefore {}_{t}V^{\\text{FPT}} &= B \\cdot A_{x+(t-1)} - P_{x+1:\\enclose{actuarial}{n-1}} \\cdot A_{x+(t-1)} \\end{aligned} \\]","title":"Reserves"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/7.%20Reserves/#reserves","text":"","title":"Reserves"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/7.%20Reserves/#background","text":"Most life insurance contracts charge level premiums - charging the same amount throughout the lifetime of the policy. When the policyholder is young, the probability of death is small, thus the premiums collected are larger than the expected outflow in the year, leading to a surplus . When the policyholder is older, the probability of death is high, thus the premiums collected are smaller than the expected outflow in the year, leading to a deficit . Given the inevitable deficit, the surplus in the earlier years cannot be recognized as a profit and are instead pooled together and safely invested into an account, from which the deficit in later years will draw from. Info It is a common misconception to think that each policy has its own account. Following this logic, the insurer would not have enough funds to pay an insured who dies shortly after death. In order to determine the adequacy of the account, insurers will calculate the size of the expected loss of ALL policies and compare it against the (hopefully larger) account value. The expected loss is known as the Reserve of the policy because it represents an amount inside the pooled account that is reserved for paying the benefits of the policy.","title":"Background"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/7.%20Reserves/#overview","text":"There are two approaches to calculate the reserves: Prospective Approach - Based on Present Values of future cashflows Retrospective Approach - Based on Future Values of past cashflows (Not covered) Similar to premiums, there are also two types of reserves, depending on what cashflows are being considered: Net Premium Reserves - Net Premiums and Benefits Gross Premium Reserves - Gross Premiums, Benefits and Expenses This exam typically assumes that reserves are calculated at the end of the policy year : AFTER any year end cashflows from the current year (Claims & Claim Expenses) BEFORE any beginning of year cashflows from the next period (Premiums & Renewal Expenses) For the purposes of this exam, only the prospective approach is covered. Similar to premiums, we will only define formulas for the Net Premium Reserves .","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/7.%20Reserves/#net-premium-reserves","text":"The prospective approach calculates the reserves as the expected value of the loss variable . Let \\({}_{t}V\\) represent the reserve calculated at the end of policy year \\(t\\) : \\[ \\begin{aligned} {}_{t}L_{x} &= B \\cdot v^{K_{x+t} + 1} - P \\cdot \\ddot{a}_{\\enclose{actuarial}{K_{x+t} + 1}} \\\\ \\\\ {}_{t}V &= E({}_{t}L_{x}) \\\\ &= E \\left(B \\cdot v^{K_{x+t} + 1} - P \\cdot \\ddot{a}_{\\enclose{actuarial}{K_{x+t} + 1}} \\right) \\\\ &= B \\cdot E \\left(v^{K_{x+t} + 1} \\right) - P \\cdot E \\left(\\ddot{a}_{\\enclose{actuarial}{K_{x+t} + 1}} \\right) \\\\ &= B \\cdot A_{x+t} - P \\cdot \\ddot{a}_{x+t} \\end{aligned} \\] Warning Both premiums and reserves use the loss variable. Premiums use the loss variable at issue \\(({}_{0}L_{x})\\) while reserves use the general loss variable \\(({}_{t}L_{x})\\) . Premiums consider the loss variable at the start of the policy year \\(({}_{t-1}L_{x})\\) while reserves typically use the end of the policy year \\(({}_{t}L_{x})\\) . Both are equivalent ways of referring to the loss variable as the start of the current year is the end of the previous year. Reserve questions can also ask for the reserve at the start of the policy year , so be sure to read the question very carefully. Based on this, the reserves can be intepreted as the expected addittional amount on top of premiums to cover claims from that point on. As its name suggests, the prospective approach only considers prospective (future) cashflows . Thus, for single premium or deferred products , since the premium was paid in the past, the reserve only contains the benefit .","title":"Net Premium Reserves"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/7.%20Reserves/#shortcuts","text":"Net premium reserves can be simplified into an expression involving ONLY the following, which allows it to be easily calculated in situations where limited information is provided: Annuities only Assurances only Annuities and Premiums Assurance and Premiums Case 1: Annuities Only \\[ \\begin{aligned} {}_{t}V &= A_{x+t} - P_{x} \\ddot{a}_{x+t} \\\\ &= A_{x+t} - \\frac{A_x}{\\ddot{a}_x} \\cdot \\ddot{a}_{x+t} \\\\ &= (1-d\\ddot{a}_{x+t}) - \\frac{1-d\\ddot{a}_{x}}{\\ddot{a}_{x}} \\cdot \\ddot{a}_{x+t} \\\\ &= 1-d\\ddot{a}_{x+t} - \\left (\\frac{1}{\\ddot{a}_x}-d \\right) \\cdot \\ddot{a}_{x+t} \\\\ &= 1-d\\ddot{a}_{x+t} - \\frac{\\ddot{a}_{x+t}}{\\ddot{a}_x} + d\\ddot{a}_{x+t} \\\\ &= 1 - \\frac{\\ddot{a}_{x+t}}{\\ddot{a}_x} \\end{aligned} \\] Note \\(P_{x}\\) is used to denote the net premium for a policy that was purchased at age \\(x\\) . Case 2: Assurances Only \\[ \\begin{aligned} {}_{t-1}V &= A_{x+t} - P_{x} \\ddot{a}_{x+t} \\\\ &= A_{x+t} - \\frac{A_x}{\\ddot{a}_x} \\cdot \\ddot{a}_{x+t} \\\\ &= A_{x+t} - \\frac{A_x}{\\frac{1-A_x}{d}} \\cdot \\frac{1-A_{x+t}}{d} \\\\ &= A_{x+t} - \\frac{A_x}{1-A_x} \\cdot (1-A_{x+t}) \\\\ &= \\frac{A_{x+t} \\cdot (1-A_{x}) - A_x(1-A_{x+t})}{1-A_x} \\\\ &= \\frac{A_{x+t} - A_{x}A_{x+t} - A_x + A_x A_{x+t}}{1-A_x} \\\\ &= \\frac{A_{x+t} - A_{x}}{1 - A_{x}} \\end{aligned} \\] Case 3: Annuities & Premiums \\[ \\begin{aligned} {}_{t}V &= A_{x+t} - P_x \\cdot \\ddot{a}_{x+t} \\\\ &= \\ddot{a}_{x+t} \\cdot \\left(\\frac{A_{x+t}}{\\ddot{a}_{x+t}} - P_{x} \\right) \\\\ &= \\ddot{a}_{x+t} \\cdot \\left(P_{x+t} - P_{x} \\right) \\end{aligned} \\] Note Notice that the reserves at time \\(t\\) and a newly issued policy at time \\(x+t\\) both use the same loss variable \\({}_{t}L_{x}\\) . This is why the expressions contain \\(P_{x+t}\\) , which is the premium for this newly issued policy. Case 4: Assurances & Premiums \\[ \\begin{aligned} {}_{t}V &= A_{x+t} - P_x \\cdot \\ddot{a}_{x+t} \\\\ &= A_{x+t} \\left(1 - P_x \\cdot \\frac{\\ddot{a}_{x+t}}{A_{x+t}} \\right) \\\\ &= A_{x+t} \\left(1 - P_x \\cdot \\frac{1}{\\frac{A_{x+t}}{\\ddot{a}_{x+t}}} \\right) \\\\ &= A_{x+t} \\left(1 - \\frac{P_x}{P_{x+t}} \\right) \\end{aligned} \\]","title":"Shortcuts"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/7.%20Reserves/#reserve-at-issue","text":"As alluded to earlier, the calculation of reserves and premiums are similar, which makes it easy to mix them up : Premiums Reserves Loss at issue \\({}_{0}L_{x}\\) General Loss \\({}_{t}L_{x}\\) \\(E(L)\\) set at 0 \\(E(L)\\) is the target \\(P\\) is the target \\(P\\) is given Calculated at age \\(x\\) Calculated at age \\(x+t\\) Note We have implicitly assumed that premiums are calculated using the equivalence principle. For the purposes of the exam, do NOT assume so unless explicitly stated . Putting both together, the reserve at the start of the policy will be calculated using the loss at issue at variable. If premiums are calculated using the equivalence principle , then the expectation of the loss at issue will always be 0. Thus, the reserve at the start of the policy is always 0 . This is intuitive, as premiums (using the equivalence principle) are set such that they expect to exactly cover the losses of the policy at issue. \\[ \\begin{aligned} {}_{0}V &= E({}_{0}L_{x}) \\\\ &= 0 \\end{aligned} \\] Tip This can be generalized -- If the reserves are zero in a particular year, then the equivalence principle can be applied at that timing: \\[ \\text{EPV Inflow} = \\text{EPV Outflow} \\] This applies to both net and gross premium reserves.","title":"Reserve at Issue"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/7.%20Reserves/#recursions","text":"Following the same logic as assurances and annuities, net premium reserves can also be recursively expressed as a function of itself: If the policyholder dies , the insurer pays a benefit of 1 at the end of the year If the policyholder survives , the insurer keeps a reserve at the end of the year Regardless , Premiums have already been received at the start of the year \\[ \\begin{aligned} {}_{t}V &= - P + \\begin{cases} v \\cdot 1, & q_{x+t} \\\\ v \\cdot {}_{t+1}V, & p_{x+t} \\end{cases} \\\\ \\\\ \\therefore {}_{t}V &= - P + q_{x+t} \\cdot v + p_{x+t} \\cdot v \\cdot {}_{t+1}V \\end{aligned} \\] Tip Since the reserve at time 0 is always 0, recursion can be used to easily calculate the reserves at time 1 or time 2 .","title":"Recursions"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/7.%20Reserves/#interim-reserves","text":"Since reserves can be calculated at any point in time, they can also be calculated as at a fractional age . They are then known as the Interim Reserves . Similar to EPVs, it can be calculated via recursion: If the policyholder dies within \\(s\\) , then the benefit is paid the end of the year If the policy survives \\(s\\) , then a reserve is set up at that time Regardless , Premiums have already been received at the start of the year \\[ \\begin{aligned} {}_{t}V &= -P + \\begin{cases} v \\cdot 1 ,& {}_{s}q_{x+t} \\\\ v^s \\cdot {}_{t+s}V ,& {}_{s}p_{x+t} \\end{cases} \\\\ \\\\ \\therefore {}_{t}V &= -P + v \\cdot q_{x+t} + v^s \\cdot p_{x+t} \\cdot {}_{t+s}V \\end{aligned} \\] The key difference is that the two events now happen at seperate times, which changes the extent that they are discounted . The benefit is paid at the end of the year \\((x+1)\\) while the interim reserve is set up upon survival \\((x+s)\\) . Warning If the policy is not discrete, the timing and hence discounting factor MUST be changed accordingly. The interim reserve itself can also be directly recursed upon: \\[ \\begin{aligned} {}_{t+s}V &= \\begin{cases} v^{1-s} \\cdot 1 ,& {}_{1-s}q_{x+t+s} \\\\ v^{1-s} \\cdot {}_{t+1}V ,& {}_{1-s}p_{x+t+s} \\end{cases} \\\\ \\\\ \\therefore {}_{t+s}V &= v^{1-s} \\cdot {}_{1-s}q_{x+t+s} + v^{1-s} \\cdot {}_{1-s}p_{x+t+s} \\cdot {}_{t+s}V \\end{aligned} \\] They key difference is that since there are no premium payments between the interim reserve and the next discrete reserve (since reserves are calculated before premiums). The discounting factor has also been flipped because we are discounting from the discrete reserve to the interim , unlike previously where it was the interim to the discrete.","title":"Interim Reserves"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/7.%20Reserves/#alternative-recursion","text":"This section requires basic knowledge of ALTAM content. A more intuitive way to think about recursion is to follow the profit cashflow logic: The insurer starts the month with the beginning reserves Premiums are received Maintenance expenses and any annuity benefits are incurred The net amount is invested to earn interest Assurance benefits and associated expenses are paid New reserve is set up for the surviving policyholders \\[ ({}_{t-1}V + P - e - B^{\\text{Annuity}})(1+i) = q_{x+t} \\cdot (B^{\\text{Assurance}} + E) + p_{x+t} \\cdot {}_{t}V \\] The above logic can be applied for interim reserves as well; simply adjust the interest and probability functions accordingly: \\[ ({}_{t-s}V + P - e - B^{\\text{Annuity}})(1+j)^{m} = {}_{s}q_{x+t} \\cdot (B^{\\text{Assurance}} + E) + {}_{s}p_{x+t} \\cdot {}_{t}V \\] Warning The key difference is that not all of the above cashflows may be present at the fractional time. For instance, if benefits are paid out at the end of the year, then there will be no benefit cashflow in the recursion. Another common mistake is to forget to update the interest rate to the rate corresponding to that time period.","title":"Alternative Recursion"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/7.%20Reserves/#gross-premium-reserves","text":"The main difference is that Gross reserves use Gross Premiums and include expenses in the calculation. Warning It is a common mistake to include other cashflows (EG. Profit Margin) in the gross reserve calculations because they included in the gross premium calculations. Remember that gross reserves ONLY contain the above three components. This is intuitive, as reserves should only be to meet essential cashflows . This leads to an interesting result because the cashflows used for premiums and reserves are different: Premium Cashflows : Benefit, Expenses & Others (EG. Profit Margin) Reserve Cashflows : Benefit, Expenses The gross premiums include additional provisions for other cashflows which are not considered in the reserve calculation. This means that the gross premiums are always more than enough to cover expected benefits and reserves at time 0 , leading to a negative reserve : \\[ {}_{0}V^{\\text{Gross}} \\le 0 \\] Note If the gross premiums are calculated using ONLY benefits and expenses , then the Gross Reserves are 0: \\[ {}_{0}V^{\\text{Gross}} = 0 \\] Gross premium reserves are very similar to net premium ones, with just the inclusion of expenses that are paid at the beginning of the year: \\[ {}_{t}V^{\\text{Gross}} = (e_t - G) + q_{x+t} \\cdot v + p_{x+t} \\cdot v \\cdot {}_{t+1}V^{\\text{Gross}} \\]","title":"Gross Premium Reserves"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/7.%20Reserves/#red-herrings","text":"Due to the difference in components for Gross Premiums and Gross Reserves, questions may sometimes provide more information than required to trick us into using the wrong cashflows for gross reserves. For instance, questions may sometimes provide the acquisition epxenses or profit margin, which are not needed for gross reserves. Another red herring would be when the question provides a premium (known as the Office Premium ) and provides the components to calculate the gross premium. The office premium is assumed to be the gross premium unless explicitly stated otherwise and should be used in the reserve calculation. There is no need to be recalculate the gross premium . The reverse is also possible, where gross premiums are provided but the question actually requires us to calculate net premiums, which can cause us to mistakenly use the gross premium instead.","title":"Red Herrings"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/7.%20Reserves/#expense-reserves","text":"Similar to expense loadings, we can define a reserve for just the expenses , known as the Expense Reserves. It can be determined from first principles using the expense loadings or intuitively as the difference between the gross and net reserves: \\[ \\begin{aligned} {}_{t}V^{\\text{Expense}} &= \\text{EPV(Expenses)} - \\text{EPV(Expense Premium)} \\\\ &= {}_{t}V^{\\text{Gross}} - {}_{t}V^{\\text{Net}} \\end{aligned} \\] If calculated via the equivalence principle, the expense reserve is 0 at time 0 . However, the expense reserve is typically negative for all other years : \\[ \\begin{aligned} {}_{0}V^{\\text{Expense}} &= 0 \\\\ {}_{t}V^{\\text{Expense}} &\\lt 0 \\\\ \\end{aligned} \\] This is because acquisition expenses are usually significantly higher than renewal expenses. This means that the expense loading is usually between the two : \\[ e_{t} \\lt G^{E} \\lt e_{0} \\] This means that there this is an ACTUAL positive loss in the first year, as the expense loading is insufficient to cover the acquisition expenses. This loss is known as the New Business Strain . On the flipside, this means that the expense premiums are higher than the renewal expenses in all other years, making a negative loss (profit) to recover the new business strain. Info This is also known as the Deferred Acquisition Costs (DAC), as the recovery of the acquisition expenses are deferre to all future years. On an EPV basis, this means that at the end of year 1, all future expense loadings are larger than the future renewal premiums, resulting in a negative expense reserve from that time on: \\[ \\begin{aligned} \\text{EPV(Expense Loading)} &\\gt \\text{EPV(Renewal Expense)} \\\\ {}_{t}V^{\\text{Expense}} &< 0 \\end{aligned} \\]","title":"Expense Reserves"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/7.%20Reserves/#modified-net-premium-reserves","text":"Net Premium Reserves are used in the US due to its simplicity, but at the cost of some accuracy loss. In particular, net premium reserves tend to be higher than gross premium reserves . This is due to the negative expense reserves , as the excess expense premiums help to offset some of the future benefits as well. Since net premiums reserves do not recognize expenses, they do not have this benefit! \\[ \\begin{aligned} {}_{t}V^{\\text{Net}} &= {}_{t}V^{\\text{Gross}} + \\underbrace{{}_{t}V^{\\text{Expense}}}_{\\text{-ve}} \\\\ {}_{t}V^{\\text{Net}} &\\gt {}_{t}V^{\\text{Gross}} \\end{aligned} \\] This means that insurers are holding too much reserves under this valuation basis. This is problematic because reserves can only be invested in low risk assets, which typically have lower returns relative to what the capital would have usually be invested in. This incurs an opportunity cost in investment income . To account for this, US regulators allow for insurers to use a modified net premium reserve instead that implicitly accounts for expenses (resulting in a smaller reserve ) while retaining the simplicity of a net premium approach.","title":"Modified Net Premium Reserves"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/7.%20Reserves/#general-approach","text":"Most methods of calculating modified reserves involve using non-level modified premiums to calculate the reserves. They typically consist of multiple components, where the premiums in the early years are smaller than \\(P\\) while the premiums in the later years are larger than \\(P\\) . \\[ P^{\\text{Early}} \\lt P \\lt P^{\\text{Later}} \\] Regardless of the number of components, the EPV of all components must be equal to the EPV of the original level net premium: \\[ \\text{EPV Non Level Components} = \\text{EPV Level Net Premium} \\] In the later years, this allows the insurer to recognise more premiums , resulting in a smaller reserve, allowing the insurer to use their capital more productively. \\[ {}_{t}V^{\\text{Modified}} < {}_{t}V^{\\text{Net}} \\]","title":"General Approach"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/7.%20Reserves/#full-preliminary-term","text":"In the US, the modified premiums are calculated using the Full Preliminary Term (FPT) method. Premiums are split into two components : First Year Premiums , \\(\\alpha\\) Subsequent Premiums , \\(\\beta\\) The policy can be thought of as being split into two components: One year TA with single premiums of \\(\\alpha\\) The original policy issued one year later with one less year of coverage with regular premiums of \\(\\beta\\) Info The first year premiums are sometimes referred to as the Net Cost of Insurance in the year. \\[ \\begin{aligned} {}_{0}V^{\\text{FPT}} &= 0 \\\\ \\alpha &= A^{1}_{x:\\enclose{actuarial}{n}}\\\\ \\\\ {}_{1}V^{\\text{FPT}} &= 0 \\\\ A_{x+1} - \\beta \\ddot{a}_{x+1} &= 0 \\\\ \\beta &= \\frac{A_{x+1}}{\\ddot{a}_{x+1}} \\end{aligned} \\] The modified reserves at all later times follows the same formula as before, simply using \\(\\beta\\) instead: \\[ \\begin{aligned} {}_{t}V^{\\text{FPT}} &= \\begin{cases} 0, & t = 0 \\\\ 0, & t = 1 \\\\ A_{x+t} - \\beta \\ddot{a}_{x+t}, & t > 1 \\end{cases} \\end{aligned} \\] The FPT reserves (third component) can be intepreted as the net premium reserves for a policy that was issues one year later with one less year of coverage : \\[ \\begin{aligned} \\therefore {}_{t}V^{\\text{FPT}} &= {}_{t-1}V^{\\text{Net}}_{\\text{One year later, one less year}} \\\\ &= B \\cdot A_{x+(t-1)} - P_{x+1} \\cdot A_{x+(t-1)} \\end{aligned} \\] Since this is a net premium, the shortcuts earlier can be used to quickly calculate the reserve if the policy is WL . Warning When select and ultimate mortality is involved, the select age does not change . In other words, under FPT, the policyholder is selected at age \\([x]\\) but purchases the policy one year later \\([x]+1\\) . It is a common mistake to take the select age as \\([x+1]\\) , since the policy was issued one year later. However, the policy is not actually issued one year later; the policyholders mortality still follows that of being selected at their original age. If the policy is an EA or TA, then the one less year of coverage must be accounted for: \\[ \\begin{aligned} \\therefore {}_{t}V^{\\text{FPT}} &= B \\cdot A_{x+(t-1):\\enclose{actuarial}{n-1}} - P_{x+1:\\enclose{actuarial}{n-1}} \\cdot A_{x+(t-1):\\enclose{actuarial}{n-1}} \\end{aligned} \\] Note The one less year of coverage also applies to limited pay WL policies: \\[ \\begin{aligned} \\therefore {}_{t}V^{\\text{FPT}} &= B \\cdot A_{x+(t-1)} - P_{x+1:\\enclose{actuarial}{n-1}} \\cdot A_{x+(t-1)} \\end{aligned} \\]","title":"Full Preliminary Term"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/8.%20Model%20Estimation/","text":"Model Estimation \u00b6 This section assumes some basic knowledge on Model Estimation , which can be found under another set of notes covering Model Estimation . Overview \u00b6 This chapter follows a similar motivation to the FAM-S one - rather than making an assumption about the survival distribution, it is better to directly study it instead. All the same principles about data apply, but instead of studying loss amounts, we are now interested in studying the times to death of people. Complete Data \u00b6 Given complete data, the probability of survival is simply the ratio of the number of survivors to the total number of people. However, since the probability of survival can be calculated for different lengths of time \\((k)\\) , the number of survivors is dependent on the length of time chosen. \\[ \\hat{S}_x(t) = \\frac{\\text{# of people who survive past t}}{n} \\] Incomplete Data \u00b6 Grouped Data \u00b6 For grouped data, if the \\(k\\) is at the boundary of the ranges provided, then the number of survivors can be easily determined and the survival probability is calculated the same way as before: \\[ \\hat{S}_x(t) = \\frac{\\text{# of observations larger than k}}{n} \\] However, if \\(k\\) is within the range, by assuming that values within the range are uniformly distributed , then the probability can be linearly interpolated from the probabilities at the boundary: \\[ F(s) = \\frac{b-s}{b-a} \\cdot F(a) + \\frac{s-a}{b-a} \\cdot F(b) \\] Warning Similar to the fractional age interpolation, recall that the larger weight should be applied to the boundary age that the estimated age is nearest to s. The interpolation follows the same intuition as the fractional age assumption, where the uniform assumption results in a linear graph of probabilities: Censored and Truncated data can only be worked with using dedicated methods discussed below. Kaplan Meier Estimation \u00b6 Consider a study of 5 individuals: One individual dies at \\(t=0.8\\) and \\(t=1.4\\) 5 more individuals join the study at \\(t=1\\) ; Left Truncated at \\(t=1\\) 3 individual leave the study at \\(t=0.5\\) ; Right Censored at \\(t=0.5\\) Note If there are people who die and leave the study at the same time, then the people who left are assumed to have left after the people who die . In other words, any death or survival probabilities should include them. Exam questions often present the information in a table format: \\(t\\) represents the time of death \\(d\\) represents the number of deaths \\(r\\) represents the risk set RIGHT BEFORE death; simply known as the sample size Tip Although the table is what is provided, a more effective way to visualize the problem is via the timeline seen above. This is problematic as the sample size changes throughout the study. However, the Kaplan Meier method accounts for this by calculating the survival probability as the product of the survival probabilities for all previous intervals between deaths : \\[ \\hat{S}_x(t) = \\prod \\left(1-\\frac{d_j}{r_j} \\right) \\] Note This means that the ratio between the estimator at two consecutive times of death is the the survival probability at the later time. However, remember that the first estimate provided is the combination of all survival probabilities before it, just that they were not provided . It should NOT be used to directly reverse engineer the sample size etc. Given the above example, the resulting Kaplan Meier estimation would be: \\[ \\begin{aligned} \\hat{S}_0(1.4) &= P(t > 0.8) * P(t > 1.4 | x > 0.8) \\\\ &= \\left (1 - \\frac{1}{2} \\right) * \\left (1 - \\frac{1}{6} \\right) \\\\ &= \\left (\\frac{1}{2} \\right) * \\left (\\frac{5}{6} \\right) \\\\ &= \\frac{5}{12} \\end{aligned} \\] Warning Although this example (and many other questions) assume that there is only one death at a time, it is completely possible to have a different number of deaths . This is especially so when the probabilities are already provided by the question - always solve for the precise number of deaths to be sure. Although counter-intuitive, the number of deaths calculated this way do NOT need to be discrete! This results in a constant probability between deaths . Thus, the focus is on the times of death ; the entries and exits from the study are only used to change the sample size. Nelson Aelen Estimation \u00b6 Recall that the survival probability can be expressed as a function of the Force of Mortality: \\[ \\hat{S}_x(t) = e^{- \\int^t_0 \\mu_{x+t}} \\] The Nelson Aelen method estimates the Cumulative Force of Mortality (the exponent of the expression), which can be then used to compute the survival probability. Warning It is an extremely common mistake to take the result of the Nelson Aelen estimation as the survival probability. The nelson aelen estimator is computed as the sum of the death probabilities for all previous intervals between deaths : \\[ \\begin{aligned} H_{x}(t) &= \\sum \\frac{d_j}{r_j} \\\\ \\int^t_0 \\mu_{x+t} &= \\sum \\frac{d_j}{r_j} \\end{aligned} \\] Warning Similar to the Kaplan Estimator, the difference at two consecutive times is the death probability at the later time. The same warning applies. Given the previous example, the resulting Nelson Aelen estimation would be: \\[ \\begin{aligned} \\int^t_0 \\mu_{x+1.4} &= P(t < 0.8) + P(t < 1.4 | x > 0.8) \\\\ &= \\left (\\frac{1}{2} \\right) + \\left (\\frac{1}{6} \\right) \\\\ &= \\frac{2}{3} \\\\ \\\\ {}_{1.4}\\hat{p}_x &= e^{-\\frac{2}{3}} \\\\ &= 0.51171 \\end{aligned} \\] Note The results for the two methods are not that far off from one another. They can be used as to sense check each other if asked to calculate both. Kaplan Meier Nelson Aelen Estimates Survival Probability Estimates Cumulative Force of Mortality Uses Survival probabilities Uses Death probabilities Ratio of consecutive estimates Difference of consecutive estimates Central Exposed Risk \u00b6 The last method estimates the force of mortality . Warning This is not to be confused with the nelson aelen estimator which estimates the cumulative force of mortality . The key idea is Central Exposed to Risk \\((E^c_x)\\) , which is a measure of how long an individual was alive in the study while at a specified age . Consider a study conducted for individuals aged 50 : If the individual has not reached the target age , then we only start counting from their birthday . If they already have, then we can start counting immediately. In either case, the study ends at the either the death of the invididual or on their birthday (as they are no longer the desired age), whichever comes first. The Central Exposed Risk is the sum of these durations . In the above example, First Person : Not yet aged 50, turned 51 in 1 year from turning 50 Second Person : Already aged 50, died in 0.3 years Third Person : Already agd 50, turned 51 in 0.2 years \\[ \\begin{aligned} E^c_x &= 1 + 0.3 + 0.2 \\\\ &= 1.5 \\end{aligned} \\] Note Some questions may not provide the age of the policyholder so explicitly like above. They may provide the dates that they joined and left the study as well as their birthdays, making it slightly more challenging to compute . The next statistic is the number of individuals who died at that age , \\(d_x\\) . The key thing to note is that if the individual dies before they turn that age or dies after they grew one age, then they are NOT counted in this metric. Putting both together, the force of mortality can be determined: \\[ \\hat{\\mu}_x = \\frac{d_x}{E^c_x} \\] Tip The general idea is that \\(E^c_x\\) represents the total time that individuals were alive in the study . The above examples was only studying mortality at a specified age . Studies could also be for \"any age\", such as in this example for newborns : \\[ \\begin{aligned} E^c_x &= 1586 + 597 = 2183 \\\\ d_x &= 39 \\\\ \\therefore \\hat{\\mu}_x &= \\frac{39}{2183} \\end{aligned} \\] Assuming that this force is constant , the resulting survival probability is calculated as: \\[ \\hat{S}_x(t) = e^{-\\hat{\\mu}_x \\cdot t} \\] Another metric that can be calculated is the Actuarial Estimate of death \\((\\tilde{q}_{x})\\) : \\[ \\tilde{q}_{x} = \\frac{d_x}{E^c_x + 0.5 \\cdot d_x} \\] Info The denominator is also known as the Initial Exposed to Risk . Maximum Likelihood Estimation \u00b6 Given a distribution for future lifetime, MLE can also be used to estimate the parameters of that distribution via first principles. Recall that for a continuous distribution, the PDF is used to form the likelihood function: \\[ f(x) = {}_{t}p_{x} \\cdot \\mu_{x+t} \\] All other key concepts from FAM-S apply: Joining the study late : Left Truncated; divide pdf by survival function Leaving the study early : Right Censored; replace PDF with survival function Tip Most likely, questions of this nature will use the Gompertz Model . Thus, \\(\\mu\\) and \\({}_{t}p_{x}\\) can be easily found from the formula sheet. Statistical Inference \u00b6 Given that all the above methods are estimates of the survival probability, the estimated values will change depending on the underlying sample. Thus, it is important to consider the variance of the estimators as well across various different samples. Complete Variance \u00b6 Recall that the number of survivors follows a Binomial Distribution. Thus, its variance can be computed using the variance of the binomial distribution: \\[ Var(\\text{Number of Survivors}) = n \\cdot {}_{t}p_x \\cdot (1-{}_{t}p_x) \\] Thus, the variance of the complete estimator can be easily computed as the following: \\[ \\begin{aligned} Var(\\hat{S}_x(t)) &= Var \\left(\\frac{\\text{Number of Survivors}}{n} \\right) \\\\ &= \\frac{1}{n^2} Var(\\text{Number of Survivors}) \\\\ &= \\frac{1}{n^2} n \\cdot {}_{t}p_x \\cdot (1-{}_{t}p_x) \\\\ &= \\frac{{}_{t}p_x \\cdot (1-{}_{t}p_x)}{n} \\\\ \\end{aligned} \\] Since the actual survival probability is not known, it is often substituted with the estimate: \\[ Var(\\hat{S}_x(t)) = \\frac{\\hat{S}_x(t) \\cdot (1-\\hat{S}_x(t))}{n} \\] Note that the variance of the estimated death probability is identical to that of the survival probability: \\[ \\begin{aligned} Var({}_{t}\\hat{q}_x) &= Var(1 - \\hat{S}_x(t)) \\\\ &= Var(\\hat{S}_x(t)) \\\\ \\end{aligned} \\] Incomplete Variance \u00b6 Due to the complexity of calculating the variance under these methods, the formula for variances are provided in the formula sheets; no further explanations will be provided. Kaplan Meier Variance : \\[ Var(\\hat{S}_x(t)) = (\\hat{S}_x(t))^2 \\sum \\frac{d_j}{r_j (r_j - d_j)} \\] Nelson Aelen Variance : \\[ \\begin{aligned} \\text{Var}(H_{x}(t)) &= \\sum \\frac{d_j (r_j - d_j)}{r^3_j} \\\\ \\text{Var}(\\hat{S}_x(t)) &= (\\hat{S}_x(t))^2 \\sum \\frac{d_j (r_j - d_j)}{r^3_j} \\end{aligned} \\] The variance of the cumulative force is technically not provided; but it is simply the second term of the nelson aelen variance. Alive Dead Variance : \\[ Var(\\hat{S}_x(t)) = (\\hat{S}_x(t))^2 \\cdot \\frac{d_x}{\\left(E^c_x \\right)^2} \\] Unfortunately, this formula is not provided on the formula sheet and hence must be memorized. Confidence Intervals \u00b6 Assuming that the estimators follow a normal distribution, a standard linear confidence interval can be calculated as the following: \\[ \\hat{S}_x(t) \\pm Z_{\\frac{1+p}{2}} \\sqrt{Var(\\hat{S}_x(t))} \\] However, one problem with a linear confidence interval for probabilities is that upper and lower bound can exceed 0 and 1 respectively , which is unintuitive. Thus, the log-transformed confidence interval can be used instead: \\[ \\begin{aligned} \\text{Confidence Interval} &= \\left(\\hat{S}_x(t)^{\\frac{1}{U^{S}}}, \\hat{S}_x(t)^{U^{S}} \\right) \\\\ \\\\ U^{S} &= e^ \\frac{Z_{\\frac{1+p}{2}} \\cdot \\sqrt{Var[\\hat{S}_x(t)]}}{\\hat{S}_x(t) \\cdot \\ln \\hat{S}_x(t)} \\\\ &= \\exp \\left(\\frac{Z_{\\frac{1+p}{2}} \\cdot \\sqrt{Var[\\hat{S}_x(t)]}}{\\hat{S}_x(t) \\cdot \\ln \\hat{S}_x(t)} \\right) \\end{aligned} \\] Unfortunately, this formula is not provided on the formula sheet and thus must be memorized . Note We can also define a log-confidence interval for the cumulative force of mortality: \\[ \\begin{aligned} \\text{Confidence Interval} &= \\left( \\hat{H}_x(t) \\cdot \\frac{1}{U^{H}}, \\hat{H}_x(t) \\cdot U^{H} \\right) \\\\ \\\\ U^{H} &= \\frac{1}{U^{S}} \\end{aligned} \\] Since both \\(U\\) s are related, it is only necessary to remember the exponent for the survival probability . The key difference is that the survival probabilities use exponents while cumulative forces use multiplication .","title":"Model Estimation"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/8.%20Model%20Estimation/#model-estimation","text":"This section assumes some basic knowledge on Model Estimation , which can be found under another set of notes covering Model Estimation .","title":"Model Estimation"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/8.%20Model%20Estimation/#overview","text":"This chapter follows a similar motivation to the FAM-S one - rather than making an assumption about the survival distribution, it is better to directly study it instead. All the same principles about data apply, but instead of studying loss amounts, we are now interested in studying the times to death of people.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/8.%20Model%20Estimation/#complete-data","text":"Given complete data, the probability of survival is simply the ratio of the number of survivors to the total number of people. However, since the probability of survival can be calculated for different lengths of time \\((k)\\) , the number of survivors is dependent on the length of time chosen. \\[ \\hat{S}_x(t) = \\frac{\\text{# of people who survive past t}}{n} \\]","title":"Complete Data"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/8.%20Model%20Estimation/#incomplete-data","text":"","title":"Incomplete Data"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/8.%20Model%20Estimation/#grouped-data","text":"For grouped data, if the \\(k\\) is at the boundary of the ranges provided, then the number of survivors can be easily determined and the survival probability is calculated the same way as before: \\[ \\hat{S}_x(t) = \\frac{\\text{# of observations larger than k}}{n} \\] However, if \\(k\\) is within the range, by assuming that values within the range are uniformly distributed , then the probability can be linearly interpolated from the probabilities at the boundary: \\[ F(s) = \\frac{b-s}{b-a} \\cdot F(a) + \\frac{s-a}{b-a} \\cdot F(b) \\] Warning Similar to the fractional age interpolation, recall that the larger weight should be applied to the boundary age that the estimated age is nearest to s. The interpolation follows the same intuition as the fractional age assumption, where the uniform assumption results in a linear graph of probabilities: Censored and Truncated data can only be worked with using dedicated methods discussed below.","title":"Grouped Data"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/8.%20Model%20Estimation/#kaplan-meier-estimation","text":"Consider a study of 5 individuals: One individual dies at \\(t=0.8\\) and \\(t=1.4\\) 5 more individuals join the study at \\(t=1\\) ; Left Truncated at \\(t=1\\) 3 individual leave the study at \\(t=0.5\\) ; Right Censored at \\(t=0.5\\) Note If there are people who die and leave the study at the same time, then the people who left are assumed to have left after the people who die . In other words, any death or survival probabilities should include them. Exam questions often present the information in a table format: \\(t\\) represents the time of death \\(d\\) represents the number of deaths \\(r\\) represents the risk set RIGHT BEFORE death; simply known as the sample size Tip Although the table is what is provided, a more effective way to visualize the problem is via the timeline seen above. This is problematic as the sample size changes throughout the study. However, the Kaplan Meier method accounts for this by calculating the survival probability as the product of the survival probabilities for all previous intervals between deaths : \\[ \\hat{S}_x(t) = \\prod \\left(1-\\frac{d_j}{r_j} \\right) \\] Note This means that the ratio between the estimator at two consecutive times of death is the the survival probability at the later time. However, remember that the first estimate provided is the combination of all survival probabilities before it, just that they were not provided . It should NOT be used to directly reverse engineer the sample size etc. Given the above example, the resulting Kaplan Meier estimation would be: \\[ \\begin{aligned} \\hat{S}_0(1.4) &= P(t > 0.8) * P(t > 1.4 | x > 0.8) \\\\ &= \\left (1 - \\frac{1}{2} \\right) * \\left (1 - \\frac{1}{6} \\right) \\\\ &= \\left (\\frac{1}{2} \\right) * \\left (\\frac{5}{6} \\right) \\\\ &= \\frac{5}{12} \\end{aligned} \\] Warning Although this example (and many other questions) assume that there is only one death at a time, it is completely possible to have a different number of deaths . This is especially so when the probabilities are already provided by the question - always solve for the precise number of deaths to be sure. Although counter-intuitive, the number of deaths calculated this way do NOT need to be discrete! This results in a constant probability between deaths . Thus, the focus is on the times of death ; the entries and exits from the study are only used to change the sample size.","title":"Kaplan Meier Estimation"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/8.%20Model%20Estimation/#nelson-aelen-estimation","text":"Recall that the survival probability can be expressed as a function of the Force of Mortality: \\[ \\hat{S}_x(t) = e^{- \\int^t_0 \\mu_{x+t}} \\] The Nelson Aelen method estimates the Cumulative Force of Mortality (the exponent of the expression), which can be then used to compute the survival probability. Warning It is an extremely common mistake to take the result of the Nelson Aelen estimation as the survival probability. The nelson aelen estimator is computed as the sum of the death probabilities for all previous intervals between deaths : \\[ \\begin{aligned} H_{x}(t) &= \\sum \\frac{d_j}{r_j} \\\\ \\int^t_0 \\mu_{x+t} &= \\sum \\frac{d_j}{r_j} \\end{aligned} \\] Warning Similar to the Kaplan Estimator, the difference at two consecutive times is the death probability at the later time. The same warning applies. Given the previous example, the resulting Nelson Aelen estimation would be: \\[ \\begin{aligned} \\int^t_0 \\mu_{x+1.4} &= P(t < 0.8) + P(t < 1.4 | x > 0.8) \\\\ &= \\left (\\frac{1}{2} \\right) + \\left (\\frac{1}{6} \\right) \\\\ &= \\frac{2}{3} \\\\ \\\\ {}_{1.4}\\hat{p}_x &= e^{-\\frac{2}{3}} \\\\ &= 0.51171 \\end{aligned} \\] Note The results for the two methods are not that far off from one another. They can be used as to sense check each other if asked to calculate both. Kaplan Meier Nelson Aelen Estimates Survival Probability Estimates Cumulative Force of Mortality Uses Survival probabilities Uses Death probabilities Ratio of consecutive estimates Difference of consecutive estimates","title":"Nelson Aelen Estimation"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/8.%20Model%20Estimation/#central-exposed-risk","text":"The last method estimates the force of mortality . Warning This is not to be confused with the nelson aelen estimator which estimates the cumulative force of mortality . The key idea is Central Exposed to Risk \\((E^c_x)\\) , which is a measure of how long an individual was alive in the study while at a specified age . Consider a study conducted for individuals aged 50 : If the individual has not reached the target age , then we only start counting from their birthday . If they already have, then we can start counting immediately. In either case, the study ends at the either the death of the invididual or on their birthday (as they are no longer the desired age), whichever comes first. The Central Exposed Risk is the sum of these durations . In the above example, First Person : Not yet aged 50, turned 51 in 1 year from turning 50 Second Person : Already aged 50, died in 0.3 years Third Person : Already agd 50, turned 51 in 0.2 years \\[ \\begin{aligned} E^c_x &= 1 + 0.3 + 0.2 \\\\ &= 1.5 \\end{aligned} \\] Note Some questions may not provide the age of the policyholder so explicitly like above. They may provide the dates that they joined and left the study as well as their birthdays, making it slightly more challenging to compute . The next statistic is the number of individuals who died at that age , \\(d_x\\) . The key thing to note is that if the individual dies before they turn that age or dies after they grew one age, then they are NOT counted in this metric. Putting both together, the force of mortality can be determined: \\[ \\hat{\\mu}_x = \\frac{d_x}{E^c_x} \\] Tip The general idea is that \\(E^c_x\\) represents the total time that individuals were alive in the study . The above examples was only studying mortality at a specified age . Studies could also be for \"any age\", such as in this example for newborns : \\[ \\begin{aligned} E^c_x &= 1586 + 597 = 2183 \\\\ d_x &= 39 \\\\ \\therefore \\hat{\\mu}_x &= \\frac{39}{2183} \\end{aligned} \\] Assuming that this force is constant , the resulting survival probability is calculated as: \\[ \\hat{S}_x(t) = e^{-\\hat{\\mu}_x \\cdot t} \\] Another metric that can be calculated is the Actuarial Estimate of death \\((\\tilde{q}_{x})\\) : \\[ \\tilde{q}_{x} = \\frac{d_x}{E^c_x + 0.5 \\cdot d_x} \\] Info The denominator is also known as the Initial Exposed to Risk .","title":"Central Exposed Risk"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/8.%20Model%20Estimation/#maximum-likelihood-estimation","text":"Given a distribution for future lifetime, MLE can also be used to estimate the parameters of that distribution via first principles. Recall that for a continuous distribution, the PDF is used to form the likelihood function: \\[ f(x) = {}_{t}p_{x} \\cdot \\mu_{x+t} \\] All other key concepts from FAM-S apply: Joining the study late : Left Truncated; divide pdf by survival function Leaving the study early : Right Censored; replace PDF with survival function Tip Most likely, questions of this nature will use the Gompertz Model . Thus, \\(\\mu\\) and \\({}_{t}p_{x}\\) can be easily found from the formula sheet.","title":"Maximum Likelihood Estimation"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/8.%20Model%20Estimation/#statistical-inference","text":"Given that all the above methods are estimates of the survival probability, the estimated values will change depending on the underlying sample. Thus, it is important to consider the variance of the estimators as well across various different samples.","title":"Statistical Inference"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/8.%20Model%20Estimation/#complete-variance","text":"Recall that the number of survivors follows a Binomial Distribution. Thus, its variance can be computed using the variance of the binomial distribution: \\[ Var(\\text{Number of Survivors}) = n \\cdot {}_{t}p_x \\cdot (1-{}_{t}p_x) \\] Thus, the variance of the complete estimator can be easily computed as the following: \\[ \\begin{aligned} Var(\\hat{S}_x(t)) &= Var \\left(\\frac{\\text{Number of Survivors}}{n} \\right) \\\\ &= \\frac{1}{n^2} Var(\\text{Number of Survivors}) \\\\ &= \\frac{1}{n^2} n \\cdot {}_{t}p_x \\cdot (1-{}_{t}p_x) \\\\ &= \\frac{{}_{t}p_x \\cdot (1-{}_{t}p_x)}{n} \\\\ \\end{aligned} \\] Since the actual survival probability is not known, it is often substituted with the estimate: \\[ Var(\\hat{S}_x(t)) = \\frac{\\hat{S}_x(t) \\cdot (1-\\hat{S}_x(t))}{n} \\] Note that the variance of the estimated death probability is identical to that of the survival probability: \\[ \\begin{aligned} Var({}_{t}\\hat{q}_x) &= Var(1 - \\hat{S}_x(t)) \\\\ &= Var(\\hat{S}_x(t)) \\\\ \\end{aligned} \\]","title":"Complete Variance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/8.%20Model%20Estimation/#incomplete-variance","text":"Due to the complexity of calculating the variance under these methods, the formula for variances are provided in the formula sheets; no further explanations will be provided. Kaplan Meier Variance : \\[ Var(\\hat{S}_x(t)) = (\\hat{S}_x(t))^2 \\sum \\frac{d_j}{r_j (r_j - d_j)} \\] Nelson Aelen Variance : \\[ \\begin{aligned} \\text{Var}(H_{x}(t)) &= \\sum \\frac{d_j (r_j - d_j)}{r^3_j} \\\\ \\text{Var}(\\hat{S}_x(t)) &= (\\hat{S}_x(t))^2 \\sum \\frac{d_j (r_j - d_j)}{r^3_j} \\end{aligned} \\] The variance of the cumulative force is technically not provided; but it is simply the second term of the nelson aelen variance. Alive Dead Variance : \\[ Var(\\hat{S}_x(t)) = (\\hat{S}_x(t))^2 \\cdot \\frac{d_x}{\\left(E^c_x \\right)^2} \\] Unfortunately, this formula is not provided on the formula sheet and hence must be memorized.","title":"Incomplete Variance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/8.%20Model%20Estimation/#confidence-intervals","text":"Assuming that the estimators follow a normal distribution, a standard linear confidence interval can be calculated as the following: \\[ \\hat{S}_x(t) \\pm Z_{\\frac{1+p}{2}} \\sqrt{Var(\\hat{S}_x(t))} \\] However, one problem with a linear confidence interval for probabilities is that upper and lower bound can exceed 0 and 1 respectively , which is unintuitive. Thus, the log-transformed confidence interval can be used instead: \\[ \\begin{aligned} \\text{Confidence Interval} &= \\left(\\hat{S}_x(t)^{\\frac{1}{U^{S}}}, \\hat{S}_x(t)^{U^{S}} \\right) \\\\ \\\\ U^{S} &= e^ \\frac{Z_{\\frac{1+p}{2}} \\cdot \\sqrt{Var[\\hat{S}_x(t)]}}{\\hat{S}_x(t) \\cdot \\ln \\hat{S}_x(t)} \\\\ &= \\exp \\left(\\frac{Z_{\\frac{1+p}{2}} \\cdot \\sqrt{Var[\\hat{S}_x(t)]}}{\\hat{S}_x(t) \\cdot \\ln \\hat{S}_x(t)} \\right) \\end{aligned} \\] Unfortunately, this formula is not provided on the formula sheet and thus must be memorized . Note We can also define a log-confidence interval for the cumulative force of mortality: \\[ \\begin{aligned} \\text{Confidence Interval} &= \\left( \\hat{H}_x(t) \\cdot \\frac{1}{U^{H}}, \\hat{H}_x(t) \\cdot U^{H} \\right) \\\\ \\\\ U^{H} &= \\frac{1}{U^{S}} \\end{aligned} \\] Since both \\(U\\) s are related, it is only necessary to remember the exponent for the survival probability . The key difference is that the survival probabilities use exponents while cumulative forces use multiplication .","title":"Confidence Intervals"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/Retired/","text":"Note Since the force of mortality represents the instantaneous rate of death, for extremely small time intervals , it can be used to approximate the probability of death in that interval: \\[ P(T_x \\lt h) \\approx h * \\mu_{x} \\] MOVE TO CURTATE? This result can be better expressed in the form of a probability tree: Thus, it can be shown through recursion that the probability of surviving \\(t\\) years is equal to the sum of the probabilities of deferred deaths for every year after: \\[ \\begin{aligned} S_x(1) &= S_x(1)F_{x+1}(1) + S_x(1)S_{x+1}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(1) [S_{x+1}(1)F_{x+2}(1) + S_{x+1}(1)S_{x+2}(1)] \\\\ &= S_x(1)F_{x+1}(1) + S_x(1) S_{x+1}(1)F_{x+2}(1) + S_x(1) S_{x+1}(1)S_{x+2}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(2)F_{x+2}(1) + S_x(2)S_{x+2}(1) \\\\ &= S_x(1)F_{x+1}(1) + S_x(2)F_{x+2}(1) + \\dots \\\\ \\\\ \\therefore S_x(t) &= \\sum^{t}_{t = 1} S_x(t)F_{x+t}(1) \\end{aligned} \\] Recall that the probability of surviving till a certain age is simply the probability that the person will die sometime after that age. The above expression solidifies this, where the probability of survival is equivalent to the probability of dying in every possible age after . Intuitions \u00b6 Although the exam questions are mostly computational, it is good to have an understanding of how the different EPVs compare against one another to serve as a sort of \"sense check\" against the SULT provided to determine if the answer is in the right direction. Same Assurance \u00b6 Recall that the probability of death is an increasing function with age. The death benefit is more likely to be paid out to an older policyholder - in other words, they receive the death benefit \"sooner\" than a younger policyholder. Thus, an older policyholder has larger expected cashflows that are discounted less (due to receiving it sooner), which results in a higher EPV than a younger policyholder, all else equal: \\[ \\begin{aligned} A_{x+k} & \\gt A_{x} \\\\ A^{\\> \\> 1} _{x+k:\\enclose{actuarial}{n}} & \\gt A^{\\> \\> 1}_{x+k:\\enclose{actuarial}{n}} \\end{aligned} \\] Conversely, the probability of survival is a decreasing function with age. The survival benefit is less likely to be paid out to an older policyholder - smaller expected cashflows. Regardless of the age of the policyholder, the survival benefit is paid at the same time ( same discounting ). Thus, since an older policyholder has smaller expected cashflows , it has a lower EPV than a younger policyholder: \\[ {}_{n}E_{x+k} \\le {}_{n}E_{x} \\] Endowment Assurances have both a death and survival component , thus the comparison is a combination of the two: A younger policyholder is more likely to survive and receive the survival benefit at the end of the term (discounted more) An older policyholder is more likely to die and receive the death benefit during the term (discounted less) Assuming that the difference in expected cashflows are negligible , then an older policyholder would have an higher EPV due to the lower discounting : \\[ A_{x+k:\\enclose{actuarial}{n}} \\gt A_{x:\\enclose{actuarial}{n}} \\] Naturally, all else equal, assurances with a lower interest rate are discounted less and thus have a higher EPV . If a seperate mortality table with EPVs are given, it is likely that the question is not using an interest rate of 5%. Different Assurances \u00b6 At a young age where the probability of death is low, all else equal (where applicable), the EPVs of each assurance rank as follows: TA will have the smallest EPV . Although their benefits are paid out sooner, the expected benefits are small as the probability of death is small. WL has the next largest EPV . They have the same benefits as term in the short run, but have large expected benefits in the future . However, these large benefits are heavily discounted , still resuling in a small EPV. PE has the next largest EPV . Given the high probability of survival, the expected benefits are large . EA has the largest EPV . Since it is a combination of TA and PE, it is naturally the highest. \\[ \\begin{aligned} A^{1}_{30:\\enclose{actuarial}{n}} < A_{30} < {}_{n}E_{30} < A_{30:\\enclose{actuarial}{n}} \\\\ \\end{aligned} \\] At an old age where the probability of death is high, all else equal (where applicable), the EPVs of each assurance rank as follows: PE will have the smallest EPV . Given the low probability of survival, the expected benefits are are small . TA will have the next largest EPV . Given the high probability of death, the expected benefits are high . EA will have the next largest EPV . Since it is combination of TA and PE, it is naturally higher than both of them. WL has the largest EPV . Given the inevitable death of the policyholder, the expected benefits are the highest . \\[ \\begin{aligned} {}_{n}E_{100} < A^{1}_{100:\\enclose{actuarial}{n}} < A_{100:\\enclose{actuarial}{n}} < A_{100} \\\\ \\end{aligned} \\] As the policyholder approaches the terminal age, the EPVs tend to one another: \\[ \\begin{aligned} x &\\to \\omega \\\\ E(\\text{PE}) &\\to 0 \\\\ E(\\text{EA}) &\\to E(\\text{TA}) \\\\ E(\\text{TA}) &\\to E(\\text{WL}) \\end{aligned} \\] TA tends to WL whenever the end of the term exceeds the terminal age - thus the cashflows and hence EPV for both assurances become identical. These identities are useful when there is a starting value given . The most common starting point is the terminal age as the EPVs can be intuitively determined since the policyholder will inevitably die at the end of the year: \\[ \\begin{aligned} A_{\\omega-1} &= v \\\\ A^{\\> \\> 1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\\\ {}_{n}E_{\\omega-1} &= 0 \\\\ A^{1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\end{aligned} \\] Intuitions \u00b6 Similar to assurances, several intuitions can be made about the EPV of various annuities to serve as a sort of \"sense check\" against the SULT provided to determine if the answer is in the right direction. Same Annuity \u00b6 Since annuities are all contingent on the survival of the policyholder, only one case needs to be considered. The probability of survival is a decreasing function with age. The benefits are less likely to be paid out to an older policyholder, resulting in smaller expected cashflows . Since the cashflows are discounted the same amount, an older policyholder will have a lower EPV than a younger one: \\[ \\ddot{a}_{x+n} \\lt \\ddot{a}_{x} \\] As shown previously, Annuity Dues are always smaller than Immediates as the cashflows occur earlier and are hence discounted less . \\[ a_x < \\ddot{a}_x \\] Naturally, all else equal, annuities with a lower interest rate are discounted less and thus have a higher EPV . Different Annuities \u00b6 Since annuities are all contingent on the survival of the policyholder, the age of the policyholder for comparison does not matter. TA has the smallest EPV as it can only pay for a maximum of \\(n\\) years, while WLs and GA can pay indefinitely. WL is always smaller than GA as its payments in the first \\(n\\) years are not guaranteed; the payments after that are identical. \\[ \\ddot{a}_{x:\\enclose{actuarial}{n}} < \\ddot{a}_x < \\ddot{a}_{\\bar{x:\\enclose{actuarial}{n}}} \\] Immediate VS Due \u00b6 Consider two temporary life annuity with a term of \\(n\\) years: Annuity Immediate issued at age \\(x\\) Annuity Due issued at age \\(x+1\\) Both have the same cashflows : However, both of them are valued at different times: Annuity Immediate valued at age \\(x\\) Annuity Due valued at age \\(x+1\\) Thus, the PV of the cashflows are NOT the same : Thus, although they have the same cashflows, the annuity due has a larger EPV : \\[ \\begin{aligned} a_{x:\\enclose{actuarial}{n}} &= \\sum^n_{j=1} v^j {}_{j}p_{x} \\\\ \\\\ \\ddot{a}_{x+1:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{j=0} v^j {}_{j}p_{x+1} \\\\ &= \\frac{1}{vp_{x}} \\sum^{n-1}_{j=0} v^{j+1} {}_{j}p_{x+1} p_{x} \\\\ &= \\frac{1}{vp_{x}} \\sum^{n-1}_{j=0} v^{j+1} {}_{j+1}p_{x} \\\\ &= \\frac{1}{vp_{x}} \\sum^{n}_{j=1} v^{j} {}_{j}p_{x} \\\\ &= \\underbrace{\\frac{1}{vp_{x}}}_{>1} \\cdot a_{x:\\enclose{actuarial}{n}} \\\\ \\\\ \\therefore \\ddot{a}_{x+1:\\enclose{actuarial}{n}} &> a_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] This approach might seem long winded, as it seems that it is sufficient to simply compare the cashflows of \\(1 > v^n\\) . However, that ignores the probabilities , which is properly accounted for in the above approach. The premiums determined this way will always be larger than the premiums determined through the equivalence principle: \\[ \\begin{aligned} P_{\\text{Portfolio Percentile}} &\\ge P_{\\text{Equivalence Principle}} \\\\ \\\\ \\text{As n} &\\to \\infty \\\\ P_{\\text{Portfolio Percentile}} &\\to P_{\\text{Equivalence Principle}} \\end{aligned} \\] Alternatively, it can be used as a building block to decompose a regular assurance. An \\(n\\) year term assurance can be thought of as the sum of \\(n\\) deferred TAs , each with a one year term: \\[ \\begin{aligned} A^1_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k=0} {}_{k|}A^1_{x:\\enclose{actuarial}{1}} \\\\ A_{x} &= \\sum^{\\infty}_{k=0} {}_{k|}A_{x} \\end{aligned} \\] Since WLs are just term assurances with infinite coverage, it can be extended to WLs as well if needed. This result will come in handy in later sections.","title":"Retired"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/Retired/#intuitions","text":"Although the exam questions are mostly computational, it is good to have an understanding of how the different EPVs compare against one another to serve as a sort of \"sense check\" against the SULT provided to determine if the answer is in the right direction.","title":"Intuitions"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/Retired/#same-assurance","text":"Recall that the probability of death is an increasing function with age. The death benefit is more likely to be paid out to an older policyholder - in other words, they receive the death benefit \"sooner\" than a younger policyholder. Thus, an older policyholder has larger expected cashflows that are discounted less (due to receiving it sooner), which results in a higher EPV than a younger policyholder, all else equal: \\[ \\begin{aligned} A_{x+k} & \\gt A_{x} \\\\ A^{\\> \\> 1} _{x+k:\\enclose{actuarial}{n}} & \\gt A^{\\> \\> 1}_{x+k:\\enclose{actuarial}{n}} \\end{aligned} \\] Conversely, the probability of survival is a decreasing function with age. The survival benefit is less likely to be paid out to an older policyholder - smaller expected cashflows. Regardless of the age of the policyholder, the survival benefit is paid at the same time ( same discounting ). Thus, since an older policyholder has smaller expected cashflows , it has a lower EPV than a younger policyholder: \\[ {}_{n}E_{x+k} \\le {}_{n}E_{x} \\] Endowment Assurances have both a death and survival component , thus the comparison is a combination of the two: A younger policyholder is more likely to survive and receive the survival benefit at the end of the term (discounted more) An older policyholder is more likely to die and receive the death benefit during the term (discounted less) Assuming that the difference in expected cashflows are negligible , then an older policyholder would have an higher EPV due to the lower discounting : \\[ A_{x+k:\\enclose{actuarial}{n}} \\gt A_{x:\\enclose{actuarial}{n}} \\] Naturally, all else equal, assurances with a lower interest rate are discounted less and thus have a higher EPV . If a seperate mortality table with EPVs are given, it is likely that the question is not using an interest rate of 5%.","title":"Same Assurance"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/Retired/#different-assurances","text":"At a young age where the probability of death is low, all else equal (where applicable), the EPVs of each assurance rank as follows: TA will have the smallest EPV . Although their benefits are paid out sooner, the expected benefits are small as the probability of death is small. WL has the next largest EPV . They have the same benefits as term in the short run, but have large expected benefits in the future . However, these large benefits are heavily discounted , still resuling in a small EPV. PE has the next largest EPV . Given the high probability of survival, the expected benefits are large . EA has the largest EPV . Since it is a combination of TA and PE, it is naturally the highest. \\[ \\begin{aligned} A^{1}_{30:\\enclose{actuarial}{n}} < A_{30} < {}_{n}E_{30} < A_{30:\\enclose{actuarial}{n}} \\\\ \\end{aligned} \\] At an old age where the probability of death is high, all else equal (where applicable), the EPVs of each assurance rank as follows: PE will have the smallest EPV . Given the low probability of survival, the expected benefits are are small . TA will have the next largest EPV . Given the high probability of death, the expected benefits are high . EA will have the next largest EPV . Since it is combination of TA and PE, it is naturally higher than both of them. WL has the largest EPV . Given the inevitable death of the policyholder, the expected benefits are the highest . \\[ \\begin{aligned} {}_{n}E_{100} < A^{1}_{100:\\enclose{actuarial}{n}} < A_{100:\\enclose{actuarial}{n}} < A_{100} \\\\ \\end{aligned} \\] As the policyholder approaches the terminal age, the EPVs tend to one another: \\[ \\begin{aligned} x &\\to \\omega \\\\ E(\\text{PE}) &\\to 0 \\\\ E(\\text{EA}) &\\to E(\\text{TA}) \\\\ E(\\text{TA}) &\\to E(\\text{WL}) \\end{aligned} \\] TA tends to WL whenever the end of the term exceeds the terminal age - thus the cashflows and hence EPV for both assurances become identical. These identities are useful when there is a starting value given . The most common starting point is the terminal age as the EPVs can be intuitively determined since the policyholder will inevitably die at the end of the year: \\[ \\begin{aligned} A_{\\omega-1} &= v \\\\ A^{\\> \\> 1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\\\ {}_{n}E_{\\omega-1} &= 0 \\\\ A^{1}_{\\omega-1:\\enclose{actuarial}{n}} &= v \\end{aligned} \\]","title":"Different Assurances"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/Retired/#intuitions_1","text":"Similar to assurances, several intuitions can be made about the EPV of various annuities to serve as a sort of \"sense check\" against the SULT provided to determine if the answer is in the right direction.","title":"Intuitions"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/Retired/#same-annuity","text":"Since annuities are all contingent on the survival of the policyholder, only one case needs to be considered. The probability of survival is a decreasing function with age. The benefits are less likely to be paid out to an older policyholder, resulting in smaller expected cashflows . Since the cashflows are discounted the same amount, an older policyholder will have a lower EPV than a younger one: \\[ \\ddot{a}_{x+n} \\lt \\ddot{a}_{x} \\] As shown previously, Annuity Dues are always smaller than Immediates as the cashflows occur earlier and are hence discounted less . \\[ a_x < \\ddot{a}_x \\] Naturally, all else equal, annuities with a lower interest rate are discounted less and thus have a higher EPV .","title":"Same Annuity"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/Retired/#different-annuities","text":"Since annuities are all contingent on the survival of the policyholder, the age of the policyholder for comparison does not matter. TA has the smallest EPV as it can only pay for a maximum of \\(n\\) years, while WLs and GA can pay indefinitely. WL is always smaller than GA as its payments in the first \\(n\\) years are not guaranteed; the payments after that are identical. \\[ \\ddot{a}_{x:\\enclose{actuarial}{n}} < \\ddot{a}_x < \\ddot{a}_{\\bar{x:\\enclose{actuarial}{n}}} \\]","title":"Different Annuities"},{"location":"2.%20Actuarial%20Mathematics/2.%20ASA-FAML/Retired/#immediate-vs-due","text":"Consider two temporary life annuity with a term of \\(n\\) years: Annuity Immediate issued at age \\(x\\) Annuity Due issued at age \\(x+1\\) Both have the same cashflows : However, both of them are valued at different times: Annuity Immediate valued at age \\(x\\) Annuity Due valued at age \\(x+1\\) Thus, the PV of the cashflows are NOT the same : Thus, although they have the same cashflows, the annuity due has a larger EPV : \\[ \\begin{aligned} a_{x:\\enclose{actuarial}{n}} &= \\sum^n_{j=1} v^j {}_{j}p_{x} \\\\ \\\\ \\ddot{a}_{x+1:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{j=0} v^j {}_{j}p_{x+1} \\\\ &= \\frac{1}{vp_{x}} \\sum^{n-1}_{j=0} v^{j+1} {}_{j}p_{x+1} p_{x} \\\\ &= \\frac{1}{vp_{x}} \\sum^{n-1}_{j=0} v^{j+1} {}_{j+1}p_{x} \\\\ &= \\frac{1}{vp_{x}} \\sum^{n}_{j=1} v^{j} {}_{j}p_{x} \\\\ &= \\underbrace{\\frac{1}{vp_{x}}}_{>1} \\cdot a_{x:\\enclose{actuarial}{n}} \\\\ \\\\ \\therefore \\ddot{a}_{x+1:\\enclose{actuarial}{n}} &> a_{x:\\enclose{actuarial}{n}} \\end{aligned} \\] This approach might seem long winded, as it seems that it is sufficient to simply compare the cashflows of \\(1 > v^n\\) . However, that ignores the probabilities , which is properly accounted for in the above approach. The premiums determined this way will always be larger than the premiums determined through the equivalence principle: \\[ \\begin{aligned} P_{\\text{Portfolio Percentile}} &\\ge P_{\\text{Equivalence Principle}} \\\\ \\\\ \\text{As n} &\\to \\infty \\\\ P_{\\text{Portfolio Percentile}} &\\to P_{\\text{Equivalence Principle}} \\end{aligned} \\] Alternatively, it can be used as a building block to decompose a regular assurance. An \\(n\\) year term assurance can be thought of as the sum of \\(n\\) deferred TAs , each with a one year term: \\[ \\begin{aligned} A^1_{x:\\enclose{actuarial}{n}} &= \\sum^{n-1}_{k=0} {}_{k|}A^1_{x:\\enclose{actuarial}{1}} \\\\ A_{x} &= \\sum^{\\infty}_{k=0} {}_{k|}A_{x} \\end{aligned} \\] Since WLs are just term assurances with infinite coverage, it can be extended to WLs as well if needed. This result will come in handy in later sections.","title":"Immediate VS Due"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/","text":"Multiple Life Model \u00b6 Overview \u00b6 In FAM-L, the survival model focused on the future lifetime of an individual, used to model insurance on a single life. In practice, it is possible to have an insurance on multiple lives , thus the survival model must be extended to model such insurance contracts: Life Insurance which pays a benefit on the first death of the couple Annuities which pays benefits until the last of the beneficiaries have died Tip All the concepts and notation from the base survival model also apply to a multi-life setting and thus will not be repeated. For the purposes of simplicity , unless explicitly stated otherwise, there are two assumptions to be made: Only two lives with life status \\(x\\) and \\(y\\) Lives are independent Note Although the most common situation is two lives, it is possible to extend the principles to a case of 3 or more lives , which some questions may ask. Multiple Life Status \u00b6 In order to track when these benefits should be paid, a multiple life status is used. For the purposes of this exam, only the following two are considered: Joint Life Status Last Survivor Status Active as long as ALL are alive Active as long as at least one is alive Fails when the first person dies Fails when the last person dies \\(x:y\\) \\(\\overline{x:y}\\) The names of the statuses are intuitive -- they directly describe the condition which the statuses are active . Tip It is important to have a strong understanding of the concept of Status that was introduced in FAM-L. Please refer to the corresponding section for a quick refresher. Joint Life Status \u00b6 A Joint Life Status fails when the first person dies , thus the future lifetime of the entity is the smaller of the two: \\[ T_{x:y} = \\min (T_{x}, T_{y}) \\] Probability \u00b6 There is only one case that fulfils the joint life status for survival - both individuals surviving past the specified time. Thus, it can be shown to be the probability that both \\(x\\) AND \\(y\\) survive : \\[ \\begin{aligned} {}_{t}p_{x:y} &= P(T_{x:y} \\gt t) \\\\ &= P(\\min (T_{x}, T_{y}) \\gt t) \\\\ &= P(T_{x} \\gt t, T_{y} \\gt t) \\\\ &= P(T_{x} \\gt t) \\cdot P(T_{y} \\gt t), \\text{Independence} \\\\ &= {}_{t}p_{x} \\cdot {}_{t}p_{y} \\end{aligned} \\] Tip The multiplication rule still applies in a joint life context, but note the difference in notation : \\[ {}_{t+u}p_{x:y} = {}_{t}p_{x:y} \\cdot {}_{u}p_{x+t:y+t} \\] Conversely, the death probability has multiple cases to consider: \\(x\\) dies before \\(y\\) \\(y\\) dies before \\(x\\) BOTH die before \\(t\\) In other words, it is the probability that at least one dies (either \\(x\\) OR \\(y\\) ): \\[ \\begin{aligned} {}_{t}q_{x:y} &= 1 - {}_{t}p_{x:y} \\\\ &= 1 - {}_{t}p_{x} \\cdot {}_{t}p_{y} \\\\ &= 1 - (1 - {}_{t}q_{x}) \\cdot (1 - {}_{t}q_{y}) \\\\ &= 1 - (1 - {}_{t}q_{y} - {}_{t}q_{x} + {}_{t}q_{x} \\cdot {}_{t}q_{y}) \\\\ &= {}_{t}q_{y} + {}_{t}q_{x} - {}_{t}q_{x} \\cdot {}_{t}q_{y} \\end{aligned} \\] Note Remember that the intersection must be subtracted in order to avoid double counting of the possibility. The probability of deferred death is analagous to the single life model: \\[ \\begin{aligned} {}_{u \\mid t}q_{x:y} &= {}_{u}p_{x:y} - {}_{u+t}p_{x:y} \\\\ &= {}_{u+t}q_{x:y} - {}_{u}q_{x:y} \\\\ &= {}_{u}p_{x:y} \\cdot {}_{t}q_{x+u:y+u} \\end{aligned} \\] Tip Note that the deferred death probability can be split using the same death probability logic: \\[ \\begin{aligned} {}_{u \\mid t}q_{x:y} &= {}_{u \\mid t}q_{y} + {}_{u \\mid t}q_{x} - {}_{u \\mid t}q_{x} \\cdot {}_{u \\mid t}q_{y} \\end{aligned} \\] Force of Mortality \u00b6 The force of mortality can be derived from first principles : \\[ \\begin{aligned} \\mu_{x+t:y+t} &= \\frac{f_{x:y}(t)}{S_{x:y}(t)} \\\\ &= \\frac{- \\frac{d}{dt} S_{x:y}(t)}{S_{x:y}(t)} \\\\ &= - \\frac{d}{dt} \\ln S_{x:y}(t) \\\\ &= - \\frac{d}{dt} \\ln S_{x}(t) \\cdot S_{y}(t) \\\\ &= - \\frac{d}{dt} (\\ln S_{x}(t) + \\ln S_{y}(t)) \\\\ &= - \\frac{d}{dt} \\ln S_{x}(t) - \\frac{d}{dt} \\ln S_{y}(t) \\\\ &= \\mu_{x+t} + \\mu_{y+t} \\end{aligned} \\] Thus, the two probabilities can be reformulated as: \\[ \\begin{aligned} {}_{t}p_{x:y} &= e^{- \\int \\mu_{x+t:y+t}} \\\\ {}_{t}q_{x:y} &= \\int {}_{t}p_{x:y} \\cdot \\mu_{x+t:y+t} \\end{aligned} \\] Moments \u00b6 The expectation of future life is also analagous to the single life version: \\[ \\begin{aligned} E(T_{x:y}) &= \\mathring{e}_{x:y} \\\\ &= \\int^{\\infty}_{0} t \\cdot f_{x:y}(t) \\\\ &= \\int^{\\infty}_{0} {}_{t}p_{x:y} \\\\ \\end{aligned} \\] Recall that if the individual has a maximum age ( \\(\\omega\\) ), the upper limit of the integration should be set to the maximum age: \\[ \\mathring{e}_{x} = \\int^{\\omega - x}_{0} {}_{t}p_{x} \\] For a joint life model, the limit of the integration should be set to the lower maximum age of the two lives: \\[ \\mathring{e}_{x:y} = \\int^{\\min(w_{x} - x, w_{y} - y)}_{0} {}_{t}p_{x:y} \\] The intuition is as follows - the joint life expectation is the expected time till the first life dies . Since the individuals will inevitably die at their maximum age, the maximum possible time is till the smaller of the two maximum ages . Note Overall, the joint life model is analagous to the that of the single life model. For most items, the single life status ( \\(x\\) ) can simply be replaced with the joint life subscripts ( \\(x:y\\) ). Last Survivor Status \u00b6 A Last Survivor life status fails when the last person dies, thus the future lifetime of the entity is the larger of the two: \\[ T_{\\overline{x:y}} = \\max (T_{x}, T_{y}) \\] Probability \u00b6 Similarly, there is only one case that fulfils the last survivor status for death - both individuals dying by specified time. Thus, it can be shown to be the probability that both \\(x\\) AND \\(y\\) die : \\[ \\begin{aligned} {}_{t}q_{\\overline{x:y}} &= P(T_{\\overline{x:y}} \\lt t) \\\\ &= P(\\max (T_{x}, T_{y}) \\lt t) \\\\ &= P(T_{x} \\lt t, T_{y} \\lt t) \\\\ &= P(T_{x} \\lt t) \\cdot P(T_{y} \\lt t), \\text{Independence} \\\\ &= {}_{t}q_{x} \\cdot {}_{t}q_{y} \\end{aligned} \\] Conversely, the probability of survival has multiple cases to consider: \\(x\\) survives longer than \\(y\\) \\(y\\) survives longer than \\(x\\) Both survive past \\(t\\) Similarly, the survival probability is the probability that at least one survives (either \\(x\\) OR \\(y\\) ): \\[ \\begin{aligned} {}_{t}p_{\\overline{x:y}} &= 1 - {}_{t}q_{\\overline{x:y}} \\\\ &= 1 - {}_{t}q_{x} \\cdot {}_{t}q_{y} \\\\ &= 1 - (1 - {}_{t}p_{x}) \\cdot (1 - {}_{t}p_{y}) \\\\ &= 1 - (1 - {}_{t}p_{y} - {}_{t}p_{x} + {}_{t}p_{x} \\cdot {}_{t}p_{y}) \\\\ &= {}_{t}p_{y} + {}_{t}p_{x} - {}_{t}p_{x} \\cdot {}_{t}p_{y} \\end{aligned} \\] Warning The multiplication rule DOES NOT apply in a last survivor context! \\[ {}_{t+u}p_{\\overline{x:y}} \\ne {}_{t}p_{\\overline{x:y}} \\cdot {}_{u}p_{\\overline{x+t:y+t}} \\] This is because \\({}_{u}p_{\\overline{x+t:y+t}}\\) is the probability that BOTH lives have survived \\(t\\) years. However, only the third case of \\({}_{t}p_{\\overline{x:y}}\\) fulfils this description. The two terms are on a different basis , thus the multiplication rule cannot be applied . Similarly, the probabilitiy of deferred death is analagous to the single life model. However, given that the multiplication rule cannot be applied in a last survivor setting, the last variation cannot be applied : \\[ \\begin{aligned} {}_{u \\mid t}q_{\\overline{x:y}} &= {}_{u}p_{\\overline{x:y}} - {}_{u+t}p_{\\overline{x:y}} \\\\ &= {}_{u+t}q_{\\overline{x:y}} - {}_{u}q_{\\overline{x:y}} \\\\ \\end{aligned} \\] Tip Notice that the Joint Life and Last Survivor probabilities are mirror images of one another: Joint Life Status Last Survivor Status 'AND' Logic (Survival perspective) 'OR' Logic (Survival Perspective) 'OR' Logic (Death perspective) 'AND' Logic (Death Perspective) Force of Mortality \u00b6 Similarly, the force of mortality can be derived from first principles : \\[ \\begin{aligned} \\mu_{\\overline{x+t:y+t}} &= \\frac{f_{\\overline{x:y}}(t)}{S_{\\overline{x:y}}(t)} \\\\ &= \\frac{- \\frac{d}{dt} S_{\\overline{x:y}}(t)}{S_{\\overline{x:y}}(t)} \\\\ \\\\ \\frac{d}{dt} S_{\\overline{x:y}}(t) &= \\frac{d}{dt} S_{x}(t) + S_{y}(t) - S_{x:y}(t) \\\\ &= S_{x}(t) \\mu_{x+t} + S_{y}(t) \\mu_{y+t} - S_{x:y}(t) \\mu_{x+t:y+t} \\\\ &= S_{x}(t) \\mu_{x+t} + S_{y}(t) \\mu_{y+t} - S_{x}(t) S_{y}(t) (\\mu_{x+t} + \\mu_{y+t}) \\\\ &= S_{x}(t) \\mu_{x+t} (1 - S_{y}(t)) + S_{y}(t) \\mu_{y+t} (1 - S_{x}(t)) \\\\ &= {}_{t}p_{x} \\mu_{x+t} \\cdot {}_{t}q_{y} + {}_{t}p_{y} \\mu_{y+t} \\cdot {}_{t}q_{x} \\\\ \\\\ \\therefore \\mu_{\\overline{x+t:y+t}} &= \\frac {{}_{t}p_{x} \\mu_{x+t} \\cdot {}_{t}q_{y} + {}_{t}p_{y} \\mu_{y+t} \\cdot {}_{t}q_{x}} {{}_{t}p_{x} + {}_{t}p_{y} + {}_{t}p_{x:y}} \\end{aligned} \\] Unlike joint lives, the probabilities cannot be reformulated in terms of the force of mortality. Note Recall the intuition for the integration - Life survives till some \\(t\\) , then the force of mortality acts on the life at age \\(x+t\\) , causing them to die. This is calculated for every possible \\(t\\) . The last survivor survival probability is the probability that at least one survives but the last survivor force is supposed to act on both lives . Thus, similar to the multiplication property, there is a mismatch in basis and thus no formulation exists. Moments \u00b6 Similarly, the last survivor expectation of life can be calculated as: \\[ \\begin{aligned} E(T_{\\overline{x:y}}) &= \\mathring{e}_{\\overline{x:y}} \\\\ &= \\int^{\\infty}_{0} t \\cdot f_{\\overline{x:y}}(t) \\\\ &= \\int^{\\infty}_{0} {}_{t}p_{\\overline{x:y}} \\\\ \\end{aligned} \\] Similarly, if there is a maximum age to the lives, exceptional logic must be applied: \\[ \\mathring{e}_{\\overline{x:y}} = \\int^{\\min (\\omega_{x} - x, \\omega_{y} - y)}_{0} {}_{t}p_{\\overline{x:y}} + \\begin{cases} \\int^{\\omega_{x} - x}_{\\omega_{y} - y} {}_{t}p_{x} \\\\ \\int^{\\omega_{y} - y}_{\\omega_{x} - x} {}_{t}p_{y} \\end{cases} \\] The intuition is similar to before - the last survivor expectation measures the expected time till both lives die : The first term calculates the probability that both lives die by the smaller maximum age One of the lives WILL die by this age while the other may survive (different maximum age) Thus, there must be an additional term to account for the remaining lifetime of the surviving life The second term will use the expectation of a single life since only one life remains Note To better illustrate this, consider the following example: \\(x\\) is currently aged 65 and has a maximum lifetime of 75 \\(y\\) is currently aged 60 but has a maximum lifetime of 85 \\[ \\mathring{e}_{\\overline{65:60}} = \\int^{10}_{0} {}_{t}p_{\\overline{60:65}} + \\int^{25}_{10} {}_{t}p_{\\overline{60}} \\] There is no need to update the age of the individual in the second term - it is implied via the limits of the integration. Multi Life Relationship \u00b6 There is an interesting relationship between the two life statuses: \\[ \\begin{aligned} T_{x:y} + T_{\\overline{x:y}} &= \\min (T_{x}, T_{y}) + \\max (T_{x}, T_{y}) \\\\ &= T_{x} + T_{y} \\end{aligned} \\] The key is understanding that this can only occur when there are exactly two lives , regardless of the actual distribution : \\[ \\begin{array}{|c|c|c|} \\hline \\text{} & \\text{X Dies First} & \\text{Y Dies First} \\\\ \\hline T_{x:y} & \\min(T_{x}, T_{y}) = T_{x} & \\min(T_{x}, T_{y}) = T_{y} \\\\ \\hline T_{\\overline{x:y}} & \\max(T_{x}, T_{y}) = T_{y} & \\max(T_{x}, T_{y}) = T_{x} \\\\ \\hline \\end{array} \\] Thus, any linear function of the random variable will follow the same relationship: \\[ \\begin{aligned} {}_{t}p_{x:y} + {}_{t}p_{\\overline{x:y}} &= {}_{t}p_{x} + {}_{t}p_{y} \\\\ {}_{t}q_{x:y} + {}_{t}q_{\\overline{x:y}} &= {}_{t}q_{x} + {}_{t}q_{y} \\\\ e_{x:y} + e_{\\overline{x:y}} &= e_{x} + e_{y} \\end{aligned} \\] Note The above relationship applies to the second moment, but NOT the variance: \\[ \\begin{aligned} E(T_{x:y}) + E(T_{\\overline{x:y}}) &= E(T_{x}) + E(T_{y}) \\\\ E(T^{2}_{x:y}) + E(T^{2}_{\\overline{x:y}}) &= E(T^{2}_{x}) + E(T^{2}_{y}) \\\\ \\text{Var}(T_{x:y}) + \\text{Var}(T_{\\overline{x:y}}) &\\ne \\text{Var}(T_{x}) + \\text{Var}(T_{y}) \\end{aligned} \\] The second moment is still a linear function (expectation), which is why it is valid. The variance is a non-linear function, which is why the relationship no longer applies. Tip The above proof can be used without the need for independence. Thus, this is the preferred method of calculating values for last survivor statuses, as they are less straightforward. Another interesting relationship is to consider the Covariance between Joint Lives and Last Survivor statuses: \\[ \\begin{aligned} \\text{Cov}(T_{x:y}, T_{\\overline{x:y}}) &= E(T_{x:y} \\cdot T_{\\overline{x:y}}) - E(T_{x:y}) \\cdot E(T_{\\overline{x:y}}) \\\\ &= E(T_{x} \\cdot T_{y}) - E(T_{x:y}) \\cdot [E(T_{x}) + E(T_{y}) - E(T_{x:y})] \\\\ &= E(T_{x}) \\cdot E(T_{y}) - E(T_{x:y}) E(T_{x}) - E(T_{x:y}) E(T_{y}) + [E(T_{x:y})]^2 \\\\ &= E(T_{x}) \\cdot [E(T_{y}) - E(T_{x:y})] - E(T_{x:y}) \\cdot [E(T_{y}) - E(T_{x:y})] \\\\ &= [E(T_{x}) - E(T_{x:y})] \\cdot [E(T_{y}) - E(T_{x:y})] \\\\ \\end{aligned} \\] Warning This measures the Covariance between the Joint Life and Last Survivor variables, NOT the individual lives. Remember that a key assumption was made that the individual lives are Independent : \\[ \\begin{aligned} \\text{Cov}(T_{x}, T_{y}) &= 0 \\\\ E(T_{x}, T_{y}) - E(T_{x}) \\cdot E(T_{y}) &= 0 \\\\ E(T_{x}, T_{y}) &= E(T_{x}) \\cdot E(T_{y}) \\end{aligned} \\] However, the joint life and last survivor statuses are NOT independent because they share the underlying independent lives. Tip The key to the proof uses similar logic to before. Given exactly two lives, the following relationship always holds true: \\[ \\begin{aligned} T_{x:y} \\cdot T_{\\overline{x:y}} &= \\min (T_{x}, T_{y}) \\cdot \\max (T_{x}, T_{y}) \\\\ &= T_{x} \\cdot T_{y} \\end{aligned} \\] Common Shock \u00b6 Consider a variation of the survival model where the two lives are dependent : Let \\(T^{*}_{x}\\) and \\(T^{*}_{y}\\) represent the future lifetime in the absence of a common shock Let \\(T_{x}\\) and \\(T_{y}\\) represent the future lifetime in the presence of a common shock Let \\(Z\\) be the random variable representing time until a Common Shock that can kill both individuals simultaneously (EG. Earthquake) \\[ \\begin{aligned} T_{x} = \\min (T^{*}_{x}, Z) \\\\ T_{y} = \\min (T^{*}_{y}, Z) \\\\ \\\\ Z \\sim \\text{Exponential}(\\lambda) \\end{aligned} \\] Note \\(Z\\) tends to follows an exponential distribution as it is used to model time to an event. However, the question could set it to follow any distribution. Warning The absence of a common shock does NOT imply independence ! As stated in the opening, always check the given probabilities and functions to determine independence. \\(T_{x}\\) and \\(T_{y}\\) are dependent because they share \\(Z\\) . However, it is important to distinguish that \\(T^{*}_{x}\\) , \\(T^{*}_{y}\\) and \\(Z\\) are independent . For simplicity, we consider only the results for the Joint Life status. If the last survivor variations are needed, it is recommended to convert from the joint life versions. \\[ \\begin{aligned} {}_{t}p_{x} &= P(T_{x} \\gt t) \\\\ &= P(\\min (T^{*}_{x}, Z) \\gt t) \\\\ &= P(T^{*}_{x} \\gt t) \\cdot P(Z \\gt t), \\ \\text{Independence} \\\\ &= {}_{t}p^{*}_{x} \\cdot e^{-\\lambda t} \\\\ \\\\ \\therefore {}_{t}p_{y} &= {}_{t}p^{*}_{y} \\cdot e^{-\\lambda t} \\\\ \\\\ {}_{t}p_{x:y} &= P(\\min (T_{x}, T_{y}) \\gt t) \\\\ &= P(\\min (T^{*}_{x},T^{*}_{y}, Z) \\gt t) \\\\ &= P(T^{*}_{x} \\gt t) \\cdot P(T^{*}_{y} \\gt t) \\cdot P(Z \\gt t), \\ \\text{Independence} \\\\ &= {}_{t}p^{*}_{x} \\cdot {}_{t}p^{*}_{y} \\cdot e^{-\\lambda t} \\\\ &= {}_{t}p^{*}_{x:y} \\cdot e^{-\\lambda t} \\\\ \\\\ \\mu_{x+t:y+t} &= \\frac{d}{dt} \\ln {}_{t}p_{x:y} \\\\ &= \\frac{d}{dt} \\ln ({}_{t}p_{x} \\cdot {}_{t}p_{y} \\cdot e^{-\\lambda t}) \\\\ &= \\frac{d}{dt} \\ln {}_{t}p_{x} + \\ln {}_{t}p_{y} - \\lambda t \\\\ &= \\mu_{x+t} + \\mu_{y+t} - \\lambda \\end{aligned} \\] Warning It is a common misconception to think that common shock only affects the multi-life probabilities. In reality, they affect the single life probabilities as well. For instance, assume the common shock was due to driving a car (EG. Risk of both individuals dying in a car crash). Even after one party dies, the other will still continue to drive a car, and thus would be exposed to the same risk as before. Consider the expression for an assurance or annuity with a common shock: \\[ \\begin{aligned} \\bar{a}_{x:y} &= \\int {}_{t}p_{x:y} \\cdot e^{-\\delta t} \\\\ &= \\int {}_{t}p^{*}_{x:y} \\cdot e^{-\\lambda t} \\cdot e^{-\\delta t} \\\\ &= \\int {}_{t}p^{*}_{x:y} \\cdot e^{-(\\lambda + \\delta)t} \\\\ &= \\bar{a}^{*}_{x:y} |_{\\delta^{*} = \\lambda + \\delta} \\end{aligned} \\] Fractional Ages \u00b6 The same fractional age assumptions from the single life model apply in a multi-life setting as well. There is NO need to memorize a special formula - convert them into single life probabilities and apply the usual conversions from there. For instance, if UDD was assumed: For instance, if the Uniform Distribution of Deaths was assumed: \\[ \\begin{aligned} {}_{s}q_{x:y} &= 1 - {}_{s}p_{x:y} \\\\ &= 1 - {}_{s}p_{x} \\cdot {}_{s}p_{y} \\\\ &= 1 - (1-{}_{s}q_{x}) \\cdot (1-{}_{s}q_{y}) \\\\ &= \\dots \\end{aligned} \\] Contingent Events \u00b6 The life statuses introduced do NOT specify the order of deaths - they do not care specifically if \\(x\\) or \\(y\\) dies first. When the order of deaths are specified, they are known as Contingent Events . This is because the probability is contingent on the other life being alive or dead at the time. Dies First Dies Last Probability that \\(x\\) dies first Probability that \\(x\\) dies last \\(y\\) must be alive when \\(x\\) dies \\(y\\) must be dead when \\(x\\) dies \\({}_{t}q^{1}_{x:y}\\) \\({}_{t}q^{2}_{x:y}\\) At least one dies within \\(t\\) years Both die within \\(t\\) years Note The notation used is similar to what was introduced for term assurances , where the number on the accent specifies the order of failure . Probability \u00b6 The probability of \\(x\\) dying first can be understood as the following: Both \\(x\\) and \\(y\\) surviving to some time Then the force of mortality acts on \\(x\\) only , causing only \\(x\\) to die Thus, \\(x\\) dies before \\(y\\) \\[ {}_{t}q^{1}_{x:y} = \\int^{t}_{0} {}_{t}p_{x:y} \\cdot \\mu_{x+t} \\] Tip The same intuition of integrating the scenario we want to achieve can be used to construct any of the death probabilities. Conversely, the probability of \\(x\\) dying second can be understood as the following: \\(y\\) dies while \\(x\\) survives Then the force of mortality acts on \\(x\\) , causing only \\(x\\) to die as well Thus, \\(x\\) dies after \\(y\\) \\[ {}_{t}q^{2}_{x:y} = \\int^{t}_{0} {}_{t}q_{y} \\cdot {}_{t}p_{x} \\cdot \\mu_{x+t} \\] Note that the two are NOT complementary to one another: It might seem intuitive that if \\(x\\) dies first, the opposite case must mean that \\(y\\) dies first However, this is NOT the case as there is a timeframe \\(t\\) attached to the probabilities The opposite of \\(x\\) dying first within \\(t\\) years is not only \\(y\\) dying first; both could survive past \\(t\\) years and die at some future time Thus, only when no specific timeframes are considered, then they are complements: \\[ \\begin{aligned} {}_{\\infty}q^{1}_{x:y} + {}_{\\infty}q^{\\> \\> \\> 1}_{x:y} &= 1 \\\\ {}_{\\infty}q^{2}_{x:y} + {}_{\\infty}q^{\\> \\> \\> 2}_{x:y} &= 1 \\\\ \\\\ {}_{t}q^{1}_{x:y} + {}_{t}q^{\\> \\> \\> 1}_{x:y} & \\ne 1 \\\\ {}_{t}q^{2}_{x:y} + {}_{t}q^{\\> \\> \\> 2}_{x:y} & \\ne 1 \\\\ \\end{aligned} \\] Note Following a similar line of reasoning, given only two lives, \\(x\\) dying first must necessarily mean that \\(y\\) dies second, but only if no specific timeframe is considered: \\[ \\begin{aligned} {}_{\\infty}q^{1}_{x:y} &= {}_{\\infty}q^{\\> \\> \\> 1}_{x:y} \\\\ {}_{t}q^{1}_{x:y} & \\ne {}_{t}q^{\\> \\> \\> 1}_{x:y} \\end{aligned} \\] Thus, the equality will only hold if we account for the other possibility - of \\(y\\) living past \\(t\\) : \\[ {}_{t}q^{1}_{x:y} = {}_{t}q^{\\ \\ \\ 2}_{x:y} + {}_{t}q_{x} \\cdot {}_{t}p_{y} \\] Warning The timeframes for the two are different: \\({}_{t}q^{1}_{x:y}\\) : \\(y\\) dies after \\(x\\) , NOT necessarily WITHIN \\(t\\) years \\({}_{t}q^{2}_{x:y}\\) : \\(x\\) dies after \\(y\\) , WITHIN \\(t\\) years Similar to last survivor probabilities, it assumes that BOTH individuals are alive at the start of the period . Thus, the deferred probability must account for both lives surviving till the deferred point (time \\(u\\) ) and then dying in the specified order within \\(t\\) years: \\[ \\begin{aligned} {}_{u \\mid t}q^1_{x:y} &= {}_{u}p_{x:y} \\cdot {}_{t}q^{1}_{x+u:y+u} \\\\ {}_{u \\mid t}q^2_{x:y} &= {}_{u}p_{x:y} \\cdot {}_{t}q^{2}_{x+u:y+u} \\\\ \\end{aligned} \\] Contingent Relationships \u00b6 The most intuitive relationship is that for a single life to die, it must either die first or die second : \\[ {}_{t}q_{x} = {}_{t}q^{1}_{x:y} + {}_{t}q^{2}_{x:y} \\] A relationship can be formed with the multi-life probabilities as well: \\[ \\begin{aligned} P(\\text{First Death}) &= P(\\text{X dies first}) + P(\\text{Y dies first}) \\\\ {}_{t}q_{x:y} &= {}_{t}q^{1}_{x:y} + {}_{t}q^{\\> \\> \\> 1}_{x:y} \\\\ \\\\ P(\\text{Last Death}) &= P(\\text{X dies Last}) + P(\\text{Y dies Last}) \\\\ {}_{t}q_{\\overline{x:y}} &= {}_{t}q^{2}_{x:y} + {}_{t}q^{\\> \\> \\> 2}_{x:y} \\end{aligned} \\] Note Technically speaking, the true probability is the following: \\[ \\begin{aligned} P(\\text{First Death}) &= P(\\text{X dies first}) + P(\\text{Y dies first}) \\\\ &- P(\\text{X and Y die together}) \\\\ \\\\ P(\\text{Last Death}) &= P(\\text{X dies Last}) + P(\\text{Y dies Last}) \\\\ &- P(\\text{X and Y die together}) \\end{aligned} \\] The above assumes that is it not possible for both \\(x\\) and \\(y\\) to die at the same time - no common shock that could kill both simultaneously. Thus, the last term is zero . \\[ \\therefore P(\\text{X and Y die together}) = 0 \\] Insurance Applications \u00b6 Multi-life assurances and annuities are analagous to their single life counterparts . Similar to probability, in many cases, the single life subscript can simply be replaced with the multi-life subscript. This section highlights key differences when considering a second life. Joint Life \u00b6 Consider the definition of a joint life assurance and annuity: Joint Life Assurance - Pays a death benefit upon the first death between \\(x\\) and \\(y\\) Joint Life Annuity - Pays a benefit till the first death between \\(x\\) and \\(y\\) (while both are alive) Using first principles, the EPV of the Joint Life assurances and annuities can be calculated as: \\[ \\begin{aligned} A_{x:y} &= \\sum v^{k} {}_{k \\mid}q_{x:y} \\\\ \\bar{A}_{x:y} &= \\int v^{t} {}_{t}p_{x:y} \\mu_{x+t:y+t} \\\\ \\\\ \\ddot{a}_{x:y} &= \\sum v^{k} {}_{k}p_{x:y} \\\\ \\bar{a}_{x:y} &= \\int v^{t} {}_{t}p_{x:y} \\\\ \\end{aligned} \\] The usual term conversions apply as well: \\[ \\begin{aligned} A_{x:y:\\enclose{actuarial}{n}} &= A_{x:y} - {}_{n}E_{x:y} \\cdot A_{x+n:y+n} \\\\ \\ddot{a}_{x:y:\\enclose{actuarial}{n}} &= \\ddot{a}_{x:y} - {}_{n}E_{x:y} \\cdot \\ddot{a}_{x+n:y+n} \\end{aligned} \\] Tip Using first principles, the joint life pure endowment function can be calculated as: \\[ \\begin{aligned} {}_{n}E_{x:y} &= v^{n} \\cdot {}_{n}p_{x:y} \\\\ &= v^{n} \\cdot {}_{n}p_{x} \\cdot {}_{n}p_{y} \\\\ \\end{aligned} \\] However, it can also be expressed in terms of the single life PE functions, which is more convenient to calculate from the SULT: \\[ \\begin{aligned} {}_{n}E_{x:y} &= v^{n} {}_{n}p_{x} \\cdot {}_{n}p_{y} \\\\ &= v^{n} {}_{n}p_{x} \\cdot v^{n} {}_{n}p_{y} \\cdot (1+i)^{n} \\\\ &= {}_{n}E_{x} \\cdot {}_{n}E_{y} \\cdot (1+i)^{n} \\end{aligned} \\] Lastly, the usual conversion between Assurances and Annuities hold as well: \\[ \\begin{aligned} \\ddot{a}_{x:y} &= \\frac{1 - A_{x:y}}{d} \\\\ \\bar{a}_{x:y} &= \\frac{1 - \\bar{A}_{x:y}}{\\delta} \\end{aligned} \\] Last Survivor \u00b6 Consider the definition of a last survivor assurance and annuity: Last Survivor Assurance - Pays a death benefit on the second death between \\(x\\) and \\(y\\) Last Survivor Annuity - Pays a benefit till the second death between \\(x\\) and \\(y\\) (while at least one is alive) Given the complexity in the survival probability for last survivors, it is difficult to calculate the EPV using from principles. Thus, we can leverage on the previous relationship in the random variables: \\[ \\begin{aligned} v^{T_{x:y}} + v^{T_{\\overline{x:y}}} &= v^{T_{x}} + v^{T_{y}} \\\\ A_{x:y} + A_{\\overline{x:y}} &= A_{x} + A_{y} \\\\ \\ddot{a}_{x:y} + \\ddot{a}_{\\overline{x:y}} &= \\ddot{a}_{x} + \\ddot{a}_{y} \\\\ {}_{n}E_{x:y} + {}_{n}E_{\\overline{x:y}} &= {}_{n}E_{x} + {}_{n}E_{y} \\\\ \\end{aligned} \\] Another reason why this method is preferred is because the Joint Life and Single Life values are already provided on the SULT ; it is much easier to utilize them rather than to go bottom's up. Warning The usual method for term conversions do NOT apply in a last survivor context: \\[ A_{\\overbrace{\\overline{x:y}}:\\enclose{actuarial}{n}}^{\\> \\> \\> 1} \\ne A_{\\overline{x:y}} - {}_{n}E_{\\overline{x:y}} \\cdot A_{\\overline{x+n:y+n}} \\] Similar to before, this is due to the difference in basis - the second term is on the basis that both lives are alive while the PE term only assumes that at least one will survive (not both). This means that even for last survivor TAs, they MUST be calculated from the joint life version. Note It is possible for questions to ask for the EPV of a TA with a short duration, requiring manual calculation of the EPV. Thus, it is important to take note of the following quirk in last survivor probabilities: In the first year, there is only one way to fail the last survivor status - both individuals die in the year: \\[ P(K_{\\overline{x:y}} = 0) = q_{x} \\cdot q_{y} \\] In every subsequent year , there are three possible cases:: \\(y\\) dies in the first year and \\(x\\) dies in the second year \\(x\\) dies in the first year and \\(y\\) dies in the second year Both \\(x\\) and \\(y\\) die in the second year \\[ \\begin{aligned} P(K_{\\overline{x:y}} = 1) &= q_{y} \\cdot p_{x} q_{x+1} \\\\ &+ q_{x} \\cdot p_{y} q_{y+1} \\\\ &+ p_{x} q_{x+1} \\cdot p_{y} q_{y+1} \\end{aligned} \\] Variance \u00b6 Recall that the Joint Life and Last Survivor functions themselves are not independent of each other. Thus, if asked to determine their Variance, the Covariance must be considered: \\[ \\begin{aligned} \\text{Var} \\left(v^{T_{x:y}} + v^{T_{\\overline{x:y}}} \\right) &= \\text{Var} \\left(v^{T_{x:y}} \\right) + \\text{Var} \\left(v^{T_{\\overline{x:y}}} \\right) + \\text{Cov} \\left(v^{T_{x:y}}, v^{T_{\\overline{x:y}}} \\right) \\end{aligned} \\] Using similar results from FAM-L and Covariance proof from before, the individual components can be computed: \\[ \\begin{aligned} \\text{Var} \\left(v^{T_{x:y}} \\right) &= {}^{2}A_{x:y} - (A_{x:y})^{2} \\\\ \\text{Var} \\left(v^{T_{\\overline{x:y}}} \\right) &= {}^{2}A_{\\overline{x:y}} - (A_{\\overline{x:y}})^{2} \\\\ \\text{Cov} \\left(v^{T_{x:y}}, v^{T_{\\overline{x:y}}} \\right) &= \\left(A_{x} - A_{x:y} \\right) \\left(A_{y} - A_{x:y} \\right) \\end{aligned} \\] Warning The Covariance for annuities are slightly different : \\[ \\begin{aligned} \\text{Cov} \\left(\\bar{a}_{\\enclose{actuarial}{T_{x:y}}}, \\bar{a}_{\\enclose{actuarial}{T_{\\overline{x:y}}}} \\right) &= \\text{Cov} \\left(\\frac{1-v^{T_{x:y}}}{\\delta}, \\frac{1-v^{T_{\\overline{x:y}}}}{\\delta} \\right) \\\\ &= \\frac{1}{\\delta} \\cdot \\frac{1}{\\delta} \\text{Cov} \\left(1-v^{T_{x:y}}, 1-v^{T_{\\overline{x:y}}} \\right) \\\\ &= \\frac{1}{\\delta^2} \\cdot \\text{Cov} \\left(-v^{T_{x:y}}, -v^{T_{\\overline{x:y}}} \\right) \\\\ &= \\frac{1}{\\delta^2} \\cdot (-1)(-1) \\cdot \\text{Cov} \\left(v^{T_{x:y}}, v^{T_{\\overline{x:y}}} \\right) \\\\ &= \\frac{1}{\\delta^2} \\cdot \\text{Cov} \\left(v^{T_{x:y}}, v^{T_{\\overline{x:y}}} \\right) \\\\ &= \\frac{1}{\\delta^2} \\cdot \\left(\\bar{A}_{x} - \\bar{A}_{x:y} \\right) \\left(\\bar{A}_{y} - \\bar{A}_{x:y} \\right) \\end{aligned} \\] The above is consistent with what was covered in FAM-L, where the variance of an annuity is derived from an assurance . However, it can be further simplified to reach an analagous expression to the assurances: \\[ \\begin{aligned} \\text{Cov} \\left(\\ddot{a}_{\\enclose{actuarial}{T_{x:y}}}, \\ddot{a}_{\\enclose{actuarial}{T_{\\overline{x:y}}}} \\right) &= \\frac{1}{\\delta^2} \\cdot \\left(\\bar{A}_{x} - \\bar{A}_{x:y} \\right) \\left(\\bar{A}_{y} - \\bar{A}_{x:y} \\right) \\\\ &= \\frac{1}{\\delta^2} \\cdot \\left(\\bar{A}_{x} - \\bar{A}_{x:y} + 1 - 1 \\right) \\left(\\bar{A}_{y} - \\bar{A}_{x:y} + 1 - 1 \\right) \\\\ &= \\frac{1}{\\delta^2} \\cdot \\left[-1 + \\bar{A}_{x} + 1 - \\bar{A}_{x:y} \\right] \\left[-1 + \\bar{A}_{y} + 1 - \\bar{A}_{x:y} \\right] \\\\ &= \\frac{1}{\\delta^2} \\cdot \\left[- (1 - \\bar{A}_{x}) + (1 - \\bar{A}_{x:y}) \\right] \\left[- (1 - \\bar{A}_{y}) + (1 - \\bar{A}_{x:y}) \\right] \\\\ &= \\frac{1}{\\delta^2} \\cdot (-1)(-1) \\left[(1 - \\bar{A}_{x}) - (1 - \\bar{A}_{x:y}) \\right] \\left[(1 - \\bar{A}_{y}) - (1 - \\bar{A}_{x:y}) \\right] \\\\ &= \\frac{\\left[(1 - \\bar{A}_{x}) - (1 - \\bar{A}_{x:y}) \\right]}{\\delta} \\frac{\\left[(1 - \\bar{A}_{y}) - (1 - \\bar{A}_{x:y}) \\right]}{\\delta} \\\\ &= \\left(\\frac{1 - \\bar{A}_{x}}{\\delta} - \\frac{1 - \\bar{A}_{x:y}}{\\delta} \\right) - \\left(\\frac{1 - \\bar{A}_{y}}{\\delta} - \\frac{1 - \\bar{A}_{x:y}}{\\delta} \\right) \\\\ &= \\left(\\bar{a}_{x} - \\bar{a}_{x:y} \\right) \\left(\\bar{a}_{y} - \\bar{a}_{x:y} \\right) \\end{aligned} \\] Contingent Assurance \u00b6 A contingent assurance pays a death benefit upon the death of \\(x\\) , depending on whether \\(y\\) is alive or dead at that time ; depending on the order of death . Using first principles, we can calculate the EPV as the following: \\[ \\begin{aligned} \\bar{A}^{1}_{x:y} &= \\int {}_{t}p_{x:y} \\cdot \\mu_{x+t} \\\\ \\bar{A}^{2}_{x:y} &= \\int {}_{t}q_{y} {}_{t}p_{x} \\mu_{x+t} \\end{aligned} \\] However, it is much to leverage on the relationships that were previously covered: \\[ \\begin{aligned} A_{x} &= A^{1}_{x:y} + A^{2}_{x:y} \\\\ A_{x:y} &= A^{1}_{x:y} + A^{\\> \\> \\> 1}_{x:y} \\\\ A_{\\overline{x:y}} &= A^{2}_{x:y} + A^{\\> \\> \\> 2}_{x:y} \\\\ \\end{aligned} \\] Contingent Annuity \u00b6 A contingent annuity pays a benefit as long as \\(y\\) alive, depending on whether \\(x\\) is alive or dead at that time ; depending on the order of death . In particular, a Reversionary Annuity is a form of contingent annuity that pays a benefit as long as \\(y\\) is alive, provided that \\(x\\) has died . Note The trigger for the annuity starting is the death of \\(x\\) , since it is contingent on this event. This is somewhat hard to understand, as we dont usually associate death starting an annuity . A reversionary annuity can thus be phrased as paying a stream of benefits upon the death of \\(x\\) , payable as long as \\(y\\) is alive. \\(\\ddot{a}_{x \\mid y}\\) is the notation for a reversionary annuity where: \\(x\\) is the failing life ; life that has to die \\(y\\) is the annuitant ; life that must stay alive A reversionary annuity can be understood as a deferred annuity where the deferment period is the time where both individuals are alive. The key difference from FAM-L is that this period is now variable, rather than fixed: \\[ \\ddot{a}_{x \\mid y} = \\ddot{a}_{y} - \\ddot{a}_{x:y} \\\\ \\] Tip The above result can be understood using the following logic: \\(\\ddot{a}_{y}\\) pays a benefit until \\(y\\) dies \\(\\ddot{a}_{x:y}\\) pays a benefit until the first person dies As long as \\(x\\) dies BEFORE \\(y\\) , the former will always have more payments than the latter . Thus, the difference is equivalent to a reversionary annuity: However, if \\(x\\) dies AFTER \\(y\\) , then the two terms are equal and thus the difference is 0; equivalent to the reversionary annuity NOT making any payments: Another variation of contingent annuities is one that makes payments as long as only one person is alive ( regardless of which person). Intuitively, this is simply the combination of two reversionary annuities : \\[ \\begin{aligned} \\text{EPV Annuity Only One Alive} &= \\ddot{a}_{x \\mid y} + \\ddot{a}_{y \\mid x} \\\\ &= \\ddot{a}_{y} - \\ddot{a}_{x:y} + \\ddot{a}_{x} - \\ddot{a}_{x:y} \\\\ &= (\\ddot{a}_{y} + \\ddot{a}_{x} - \\ddot{a}_{x:y}) - \\ddot{a}_{x:y} \\\\ &= \\ddot{a}_{\\overline{x:y}} - \\ddot{a}_{x:y} \\end{aligned} \\] Tip Notice that the above is essentially the formula for 'XOR' logic - only making a payment when either \\(x\\) or \\(y\\) is alive, but NOT both: Note The same logic can be applied to be probabilities as well. Note that when there are only two individuals, the probability of only person being alive is the same as the probability of only person dying : \\[ \\begin{aligned} \\text{P(Exactly One Lives)} &= \\text{P(Exactly One Dies)} \\\\ &= p_{\\overline{x:y}} - p_{x:y} \\\\ &= q_{x:y} - q_{\\overline{x:y}} \\\\ &= 1 - (1 - q_{x})(1 - q_{y}) - q_{x} \\cdot q_{y} \\\\ &= 1 - (1 - q_{y} - q{x} + q_{x} \\cdot q_{y}) - q_{x} \\cdot q_{y} \\\\ &= q_{y} + q_{x} - 2 \\cdot q_{x} \\cdot q_{y} \\\\ &= q_{y} - q_{x} \\cdot q_{y} + q_{x} - q_{x} \\cdot q_{y} \\\\ &= q_{y} (1 - q_{y}) + q_{x} (1 - q_{y}) \\\\ &= q_{y} p_{x} + q_{x} p_{y} \\\\ &= \\text{P(X lives, Y dies)} + \\text{P(X dies, Y lives)} \\end{aligned} \\] Approximations \u00b6 All the usual approximations from FAM-L to convert an assurance/annuity from payable yearly to more frequently apply in a multi-life setting as well. For instance, consider the woolhouse approximation:d \\[ \\ddot{a}^{(m)}_{x:y} = \\ddot{a}_{x:y} - \\frac{m-1}{2m} \\] Note Whole Life Last Survivor annuities can be converted directly as well: \\[ \\begin{aligned} \\ddot{a}^{(m)}_{\\overline{x:y}} &= \\ddot{a}^{(m)}_{x} + \\ddot{a}^{(m)}_{y} - \\ddot{a}v_{x:y} \\\\ &= \\ddot{a}_{x} - \\frac{m-1}{2m} + \\ddot{a}_{y} - \\frac{m-1}{2m} - (\\ddot{a}_{x:y} - \\frac{m-1}{2m}) \\\\ &= \\ddot{a}_{x} - \\frac{m-1}{2m} + \\ddot{a}_{y} - \\frac{m-1}{2m} - \\ddot{a}_{x:y} + \\frac{m-1}{2m} \\\\ &= \\ddot{a}_{x} + \\ddot{a}_{y} - \\ddot{a}_{x:y} - \\frac{m-1}{2m} \\\\ &= \\ddot{a}_{\\overline{x:y}} - \\frac{m-1}{2m} \\end{aligned} \\] The same rules from FAM-L apply as well - the approximation only works for whole life annuities. For any other type of annuity, decompose it into the a combination of whole life annuities then apply the conversion.","title":"Multi Life Models"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#multiple-life-model","text":"","title":"Multiple Life Model"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#overview","text":"In FAM-L, the survival model focused on the future lifetime of an individual, used to model insurance on a single life. In practice, it is possible to have an insurance on multiple lives , thus the survival model must be extended to model such insurance contracts: Life Insurance which pays a benefit on the first death of the couple Annuities which pays benefits until the last of the beneficiaries have died Tip All the concepts and notation from the base survival model also apply to a multi-life setting and thus will not be repeated. For the purposes of simplicity , unless explicitly stated otherwise, there are two assumptions to be made: Only two lives with life status \\(x\\) and \\(y\\) Lives are independent Note Although the most common situation is two lives, it is possible to extend the principles to a case of 3 or more lives , which some questions may ask.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#multiple-life-status","text":"In order to track when these benefits should be paid, a multiple life status is used. For the purposes of this exam, only the following two are considered: Joint Life Status Last Survivor Status Active as long as ALL are alive Active as long as at least one is alive Fails when the first person dies Fails when the last person dies \\(x:y\\) \\(\\overline{x:y}\\) The names of the statuses are intuitive -- they directly describe the condition which the statuses are active . Tip It is important to have a strong understanding of the concept of Status that was introduced in FAM-L. Please refer to the corresponding section for a quick refresher.","title":"Multiple Life Status"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#joint-life-status","text":"A Joint Life Status fails when the first person dies , thus the future lifetime of the entity is the smaller of the two: \\[ T_{x:y} = \\min (T_{x}, T_{y}) \\]","title":"Joint Life Status"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#probability","text":"There is only one case that fulfils the joint life status for survival - both individuals surviving past the specified time. Thus, it can be shown to be the probability that both \\(x\\) AND \\(y\\) survive : \\[ \\begin{aligned} {}_{t}p_{x:y} &= P(T_{x:y} \\gt t) \\\\ &= P(\\min (T_{x}, T_{y}) \\gt t) \\\\ &= P(T_{x} \\gt t, T_{y} \\gt t) \\\\ &= P(T_{x} \\gt t) \\cdot P(T_{y} \\gt t), \\text{Independence} \\\\ &= {}_{t}p_{x} \\cdot {}_{t}p_{y} \\end{aligned} \\] Tip The multiplication rule still applies in a joint life context, but note the difference in notation : \\[ {}_{t+u}p_{x:y} = {}_{t}p_{x:y} \\cdot {}_{u}p_{x+t:y+t} \\] Conversely, the death probability has multiple cases to consider: \\(x\\) dies before \\(y\\) \\(y\\) dies before \\(x\\) BOTH die before \\(t\\) In other words, it is the probability that at least one dies (either \\(x\\) OR \\(y\\) ): \\[ \\begin{aligned} {}_{t}q_{x:y} &= 1 - {}_{t}p_{x:y} \\\\ &= 1 - {}_{t}p_{x} \\cdot {}_{t}p_{y} \\\\ &= 1 - (1 - {}_{t}q_{x}) \\cdot (1 - {}_{t}q_{y}) \\\\ &= 1 - (1 - {}_{t}q_{y} - {}_{t}q_{x} + {}_{t}q_{x} \\cdot {}_{t}q_{y}) \\\\ &= {}_{t}q_{y} + {}_{t}q_{x} - {}_{t}q_{x} \\cdot {}_{t}q_{y} \\end{aligned} \\] Note Remember that the intersection must be subtracted in order to avoid double counting of the possibility. The probability of deferred death is analagous to the single life model: \\[ \\begin{aligned} {}_{u \\mid t}q_{x:y} &= {}_{u}p_{x:y} - {}_{u+t}p_{x:y} \\\\ &= {}_{u+t}q_{x:y} - {}_{u}q_{x:y} \\\\ &= {}_{u}p_{x:y} \\cdot {}_{t}q_{x+u:y+u} \\end{aligned} \\] Tip Note that the deferred death probability can be split using the same death probability logic: \\[ \\begin{aligned} {}_{u \\mid t}q_{x:y} &= {}_{u \\mid t}q_{y} + {}_{u \\mid t}q_{x} - {}_{u \\mid t}q_{x} \\cdot {}_{u \\mid t}q_{y} \\end{aligned} \\]","title":"Probability"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#force-of-mortality","text":"The force of mortality can be derived from first principles : \\[ \\begin{aligned} \\mu_{x+t:y+t} &= \\frac{f_{x:y}(t)}{S_{x:y}(t)} \\\\ &= \\frac{- \\frac{d}{dt} S_{x:y}(t)}{S_{x:y}(t)} \\\\ &= - \\frac{d}{dt} \\ln S_{x:y}(t) \\\\ &= - \\frac{d}{dt} \\ln S_{x}(t) \\cdot S_{y}(t) \\\\ &= - \\frac{d}{dt} (\\ln S_{x}(t) + \\ln S_{y}(t)) \\\\ &= - \\frac{d}{dt} \\ln S_{x}(t) - \\frac{d}{dt} \\ln S_{y}(t) \\\\ &= \\mu_{x+t} + \\mu_{y+t} \\end{aligned} \\] Thus, the two probabilities can be reformulated as: \\[ \\begin{aligned} {}_{t}p_{x:y} &= e^{- \\int \\mu_{x+t:y+t}} \\\\ {}_{t}q_{x:y} &= \\int {}_{t}p_{x:y} \\cdot \\mu_{x+t:y+t} \\end{aligned} \\]","title":"Force of Mortality"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#moments","text":"The expectation of future life is also analagous to the single life version: \\[ \\begin{aligned} E(T_{x:y}) &= \\mathring{e}_{x:y} \\\\ &= \\int^{\\infty}_{0} t \\cdot f_{x:y}(t) \\\\ &= \\int^{\\infty}_{0} {}_{t}p_{x:y} \\\\ \\end{aligned} \\] Recall that if the individual has a maximum age ( \\(\\omega\\) ), the upper limit of the integration should be set to the maximum age: \\[ \\mathring{e}_{x} = \\int^{\\omega - x}_{0} {}_{t}p_{x} \\] For a joint life model, the limit of the integration should be set to the lower maximum age of the two lives: \\[ \\mathring{e}_{x:y} = \\int^{\\min(w_{x} - x, w_{y} - y)}_{0} {}_{t}p_{x:y} \\] The intuition is as follows - the joint life expectation is the expected time till the first life dies . Since the individuals will inevitably die at their maximum age, the maximum possible time is till the smaller of the two maximum ages . Note Overall, the joint life model is analagous to the that of the single life model. For most items, the single life status ( \\(x\\) ) can simply be replaced with the joint life subscripts ( \\(x:y\\) ).","title":"Moments"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#last-survivor-status","text":"A Last Survivor life status fails when the last person dies, thus the future lifetime of the entity is the larger of the two: \\[ T_{\\overline{x:y}} = \\max (T_{x}, T_{y}) \\]","title":"Last Survivor Status"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#probability_1","text":"Similarly, there is only one case that fulfils the last survivor status for death - both individuals dying by specified time. Thus, it can be shown to be the probability that both \\(x\\) AND \\(y\\) die : \\[ \\begin{aligned} {}_{t}q_{\\overline{x:y}} &= P(T_{\\overline{x:y}} \\lt t) \\\\ &= P(\\max (T_{x}, T_{y}) \\lt t) \\\\ &= P(T_{x} \\lt t, T_{y} \\lt t) \\\\ &= P(T_{x} \\lt t) \\cdot P(T_{y} \\lt t), \\text{Independence} \\\\ &= {}_{t}q_{x} \\cdot {}_{t}q_{y} \\end{aligned} \\] Conversely, the probability of survival has multiple cases to consider: \\(x\\) survives longer than \\(y\\) \\(y\\) survives longer than \\(x\\) Both survive past \\(t\\) Similarly, the survival probability is the probability that at least one survives (either \\(x\\) OR \\(y\\) ): \\[ \\begin{aligned} {}_{t}p_{\\overline{x:y}} &= 1 - {}_{t}q_{\\overline{x:y}} \\\\ &= 1 - {}_{t}q_{x} \\cdot {}_{t}q_{y} \\\\ &= 1 - (1 - {}_{t}p_{x}) \\cdot (1 - {}_{t}p_{y}) \\\\ &= 1 - (1 - {}_{t}p_{y} - {}_{t}p_{x} + {}_{t}p_{x} \\cdot {}_{t}p_{y}) \\\\ &= {}_{t}p_{y} + {}_{t}p_{x} - {}_{t}p_{x} \\cdot {}_{t}p_{y} \\end{aligned} \\] Warning The multiplication rule DOES NOT apply in a last survivor context! \\[ {}_{t+u}p_{\\overline{x:y}} \\ne {}_{t}p_{\\overline{x:y}} \\cdot {}_{u}p_{\\overline{x+t:y+t}} \\] This is because \\({}_{u}p_{\\overline{x+t:y+t}}\\) is the probability that BOTH lives have survived \\(t\\) years. However, only the third case of \\({}_{t}p_{\\overline{x:y}}\\) fulfils this description. The two terms are on a different basis , thus the multiplication rule cannot be applied . Similarly, the probabilitiy of deferred death is analagous to the single life model. However, given that the multiplication rule cannot be applied in a last survivor setting, the last variation cannot be applied : \\[ \\begin{aligned} {}_{u \\mid t}q_{\\overline{x:y}} &= {}_{u}p_{\\overline{x:y}} - {}_{u+t}p_{\\overline{x:y}} \\\\ &= {}_{u+t}q_{\\overline{x:y}} - {}_{u}q_{\\overline{x:y}} \\\\ \\end{aligned} \\] Tip Notice that the Joint Life and Last Survivor probabilities are mirror images of one another: Joint Life Status Last Survivor Status 'AND' Logic (Survival perspective) 'OR' Logic (Survival Perspective) 'OR' Logic (Death perspective) 'AND' Logic (Death Perspective)","title":"Probability"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#force-of-mortality_1","text":"Similarly, the force of mortality can be derived from first principles : \\[ \\begin{aligned} \\mu_{\\overline{x+t:y+t}} &= \\frac{f_{\\overline{x:y}}(t)}{S_{\\overline{x:y}}(t)} \\\\ &= \\frac{- \\frac{d}{dt} S_{\\overline{x:y}}(t)}{S_{\\overline{x:y}}(t)} \\\\ \\\\ \\frac{d}{dt} S_{\\overline{x:y}}(t) &= \\frac{d}{dt} S_{x}(t) + S_{y}(t) - S_{x:y}(t) \\\\ &= S_{x}(t) \\mu_{x+t} + S_{y}(t) \\mu_{y+t} - S_{x:y}(t) \\mu_{x+t:y+t} \\\\ &= S_{x}(t) \\mu_{x+t} + S_{y}(t) \\mu_{y+t} - S_{x}(t) S_{y}(t) (\\mu_{x+t} + \\mu_{y+t}) \\\\ &= S_{x}(t) \\mu_{x+t} (1 - S_{y}(t)) + S_{y}(t) \\mu_{y+t} (1 - S_{x}(t)) \\\\ &= {}_{t}p_{x} \\mu_{x+t} \\cdot {}_{t}q_{y} + {}_{t}p_{y} \\mu_{y+t} \\cdot {}_{t}q_{x} \\\\ \\\\ \\therefore \\mu_{\\overline{x+t:y+t}} &= \\frac {{}_{t}p_{x} \\mu_{x+t} \\cdot {}_{t}q_{y} + {}_{t}p_{y} \\mu_{y+t} \\cdot {}_{t}q_{x}} {{}_{t}p_{x} + {}_{t}p_{y} + {}_{t}p_{x:y}} \\end{aligned} \\] Unlike joint lives, the probabilities cannot be reformulated in terms of the force of mortality. Note Recall the intuition for the integration - Life survives till some \\(t\\) , then the force of mortality acts on the life at age \\(x+t\\) , causing them to die. This is calculated for every possible \\(t\\) . The last survivor survival probability is the probability that at least one survives but the last survivor force is supposed to act on both lives . Thus, similar to the multiplication property, there is a mismatch in basis and thus no formulation exists.","title":"Force of Mortality"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#moments_1","text":"Similarly, the last survivor expectation of life can be calculated as: \\[ \\begin{aligned} E(T_{\\overline{x:y}}) &= \\mathring{e}_{\\overline{x:y}} \\\\ &= \\int^{\\infty}_{0} t \\cdot f_{\\overline{x:y}}(t) \\\\ &= \\int^{\\infty}_{0} {}_{t}p_{\\overline{x:y}} \\\\ \\end{aligned} \\] Similarly, if there is a maximum age to the lives, exceptional logic must be applied: \\[ \\mathring{e}_{\\overline{x:y}} = \\int^{\\min (\\omega_{x} - x, \\omega_{y} - y)}_{0} {}_{t}p_{\\overline{x:y}} + \\begin{cases} \\int^{\\omega_{x} - x}_{\\omega_{y} - y} {}_{t}p_{x} \\\\ \\int^{\\omega_{y} - y}_{\\omega_{x} - x} {}_{t}p_{y} \\end{cases} \\] The intuition is similar to before - the last survivor expectation measures the expected time till both lives die : The first term calculates the probability that both lives die by the smaller maximum age One of the lives WILL die by this age while the other may survive (different maximum age) Thus, there must be an additional term to account for the remaining lifetime of the surviving life The second term will use the expectation of a single life since only one life remains Note To better illustrate this, consider the following example: \\(x\\) is currently aged 65 and has a maximum lifetime of 75 \\(y\\) is currently aged 60 but has a maximum lifetime of 85 \\[ \\mathring{e}_{\\overline{65:60}} = \\int^{10}_{0} {}_{t}p_{\\overline{60:65}} + \\int^{25}_{10} {}_{t}p_{\\overline{60}} \\] There is no need to update the age of the individual in the second term - it is implied via the limits of the integration.","title":"Moments"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#multi-life-relationship","text":"There is an interesting relationship between the two life statuses: \\[ \\begin{aligned} T_{x:y} + T_{\\overline{x:y}} &= \\min (T_{x}, T_{y}) + \\max (T_{x}, T_{y}) \\\\ &= T_{x} + T_{y} \\end{aligned} \\] The key is understanding that this can only occur when there are exactly two lives , regardless of the actual distribution : \\[ \\begin{array}{|c|c|c|} \\hline \\text{} & \\text{X Dies First} & \\text{Y Dies First} \\\\ \\hline T_{x:y} & \\min(T_{x}, T_{y}) = T_{x} & \\min(T_{x}, T_{y}) = T_{y} \\\\ \\hline T_{\\overline{x:y}} & \\max(T_{x}, T_{y}) = T_{y} & \\max(T_{x}, T_{y}) = T_{x} \\\\ \\hline \\end{array} \\] Thus, any linear function of the random variable will follow the same relationship: \\[ \\begin{aligned} {}_{t}p_{x:y} + {}_{t}p_{\\overline{x:y}} &= {}_{t}p_{x} + {}_{t}p_{y} \\\\ {}_{t}q_{x:y} + {}_{t}q_{\\overline{x:y}} &= {}_{t}q_{x} + {}_{t}q_{y} \\\\ e_{x:y} + e_{\\overline{x:y}} &= e_{x} + e_{y} \\end{aligned} \\] Note The above relationship applies to the second moment, but NOT the variance: \\[ \\begin{aligned} E(T_{x:y}) + E(T_{\\overline{x:y}}) &= E(T_{x}) + E(T_{y}) \\\\ E(T^{2}_{x:y}) + E(T^{2}_{\\overline{x:y}}) &= E(T^{2}_{x}) + E(T^{2}_{y}) \\\\ \\text{Var}(T_{x:y}) + \\text{Var}(T_{\\overline{x:y}}) &\\ne \\text{Var}(T_{x}) + \\text{Var}(T_{y}) \\end{aligned} \\] The second moment is still a linear function (expectation), which is why it is valid. The variance is a non-linear function, which is why the relationship no longer applies. Tip The above proof can be used without the need for independence. Thus, this is the preferred method of calculating values for last survivor statuses, as they are less straightforward. Another interesting relationship is to consider the Covariance between Joint Lives and Last Survivor statuses: \\[ \\begin{aligned} \\text{Cov}(T_{x:y}, T_{\\overline{x:y}}) &= E(T_{x:y} \\cdot T_{\\overline{x:y}}) - E(T_{x:y}) \\cdot E(T_{\\overline{x:y}}) \\\\ &= E(T_{x} \\cdot T_{y}) - E(T_{x:y}) \\cdot [E(T_{x}) + E(T_{y}) - E(T_{x:y})] \\\\ &= E(T_{x}) \\cdot E(T_{y}) - E(T_{x:y}) E(T_{x}) - E(T_{x:y}) E(T_{y}) + [E(T_{x:y})]^2 \\\\ &= E(T_{x}) \\cdot [E(T_{y}) - E(T_{x:y})] - E(T_{x:y}) \\cdot [E(T_{y}) - E(T_{x:y})] \\\\ &= [E(T_{x}) - E(T_{x:y})] \\cdot [E(T_{y}) - E(T_{x:y})] \\\\ \\end{aligned} \\] Warning This measures the Covariance between the Joint Life and Last Survivor variables, NOT the individual lives. Remember that a key assumption was made that the individual lives are Independent : \\[ \\begin{aligned} \\text{Cov}(T_{x}, T_{y}) &= 0 \\\\ E(T_{x}, T_{y}) - E(T_{x}) \\cdot E(T_{y}) &= 0 \\\\ E(T_{x}, T_{y}) &= E(T_{x}) \\cdot E(T_{y}) \\end{aligned} \\] However, the joint life and last survivor statuses are NOT independent because they share the underlying independent lives. Tip The key to the proof uses similar logic to before. Given exactly two lives, the following relationship always holds true: \\[ \\begin{aligned} T_{x:y} \\cdot T_{\\overline{x:y}} &= \\min (T_{x}, T_{y}) \\cdot \\max (T_{x}, T_{y}) \\\\ &= T_{x} \\cdot T_{y} \\end{aligned} \\]","title":"Multi Life Relationship"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#common-shock","text":"Consider a variation of the survival model where the two lives are dependent : Let \\(T^{*}_{x}\\) and \\(T^{*}_{y}\\) represent the future lifetime in the absence of a common shock Let \\(T_{x}\\) and \\(T_{y}\\) represent the future lifetime in the presence of a common shock Let \\(Z\\) be the random variable representing time until a Common Shock that can kill both individuals simultaneously (EG. Earthquake) \\[ \\begin{aligned} T_{x} = \\min (T^{*}_{x}, Z) \\\\ T_{y} = \\min (T^{*}_{y}, Z) \\\\ \\\\ Z \\sim \\text{Exponential}(\\lambda) \\end{aligned} \\] Note \\(Z\\) tends to follows an exponential distribution as it is used to model time to an event. However, the question could set it to follow any distribution. Warning The absence of a common shock does NOT imply independence ! As stated in the opening, always check the given probabilities and functions to determine independence. \\(T_{x}\\) and \\(T_{y}\\) are dependent because they share \\(Z\\) . However, it is important to distinguish that \\(T^{*}_{x}\\) , \\(T^{*}_{y}\\) and \\(Z\\) are independent . For simplicity, we consider only the results for the Joint Life status. If the last survivor variations are needed, it is recommended to convert from the joint life versions. \\[ \\begin{aligned} {}_{t}p_{x} &= P(T_{x} \\gt t) \\\\ &= P(\\min (T^{*}_{x}, Z) \\gt t) \\\\ &= P(T^{*}_{x} \\gt t) \\cdot P(Z \\gt t), \\ \\text{Independence} \\\\ &= {}_{t}p^{*}_{x} \\cdot e^{-\\lambda t} \\\\ \\\\ \\therefore {}_{t}p_{y} &= {}_{t}p^{*}_{y} \\cdot e^{-\\lambda t} \\\\ \\\\ {}_{t}p_{x:y} &= P(\\min (T_{x}, T_{y}) \\gt t) \\\\ &= P(\\min (T^{*}_{x},T^{*}_{y}, Z) \\gt t) \\\\ &= P(T^{*}_{x} \\gt t) \\cdot P(T^{*}_{y} \\gt t) \\cdot P(Z \\gt t), \\ \\text{Independence} \\\\ &= {}_{t}p^{*}_{x} \\cdot {}_{t}p^{*}_{y} \\cdot e^{-\\lambda t} \\\\ &= {}_{t}p^{*}_{x:y} \\cdot e^{-\\lambda t} \\\\ \\\\ \\mu_{x+t:y+t} &= \\frac{d}{dt} \\ln {}_{t}p_{x:y} \\\\ &= \\frac{d}{dt} \\ln ({}_{t}p_{x} \\cdot {}_{t}p_{y} \\cdot e^{-\\lambda t}) \\\\ &= \\frac{d}{dt} \\ln {}_{t}p_{x} + \\ln {}_{t}p_{y} - \\lambda t \\\\ &= \\mu_{x+t} + \\mu_{y+t} - \\lambda \\end{aligned} \\] Warning It is a common misconception to think that common shock only affects the multi-life probabilities. In reality, they affect the single life probabilities as well. For instance, assume the common shock was due to driving a car (EG. Risk of both individuals dying in a car crash). Even after one party dies, the other will still continue to drive a car, and thus would be exposed to the same risk as before. Consider the expression for an assurance or annuity with a common shock: \\[ \\begin{aligned} \\bar{a}_{x:y} &= \\int {}_{t}p_{x:y} \\cdot e^{-\\delta t} \\\\ &= \\int {}_{t}p^{*}_{x:y} \\cdot e^{-\\lambda t} \\cdot e^{-\\delta t} \\\\ &= \\int {}_{t}p^{*}_{x:y} \\cdot e^{-(\\lambda + \\delta)t} \\\\ &= \\bar{a}^{*}_{x:y} |_{\\delta^{*} = \\lambda + \\delta} \\end{aligned} \\]","title":"Common Shock"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#fractional-ages","text":"The same fractional age assumptions from the single life model apply in a multi-life setting as well. There is NO need to memorize a special formula - convert them into single life probabilities and apply the usual conversions from there. For instance, if UDD was assumed: For instance, if the Uniform Distribution of Deaths was assumed: \\[ \\begin{aligned} {}_{s}q_{x:y} &= 1 - {}_{s}p_{x:y} \\\\ &= 1 - {}_{s}p_{x} \\cdot {}_{s}p_{y} \\\\ &= 1 - (1-{}_{s}q_{x}) \\cdot (1-{}_{s}q_{y}) \\\\ &= \\dots \\end{aligned} \\]","title":"Fractional Ages"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#contingent-events","text":"The life statuses introduced do NOT specify the order of deaths - they do not care specifically if \\(x\\) or \\(y\\) dies first. When the order of deaths are specified, they are known as Contingent Events . This is because the probability is contingent on the other life being alive or dead at the time. Dies First Dies Last Probability that \\(x\\) dies first Probability that \\(x\\) dies last \\(y\\) must be alive when \\(x\\) dies \\(y\\) must be dead when \\(x\\) dies \\({}_{t}q^{1}_{x:y}\\) \\({}_{t}q^{2}_{x:y}\\) At least one dies within \\(t\\) years Both die within \\(t\\) years Note The notation used is similar to what was introduced for term assurances , where the number on the accent specifies the order of failure .","title":"Contingent Events"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#probability_2","text":"The probability of \\(x\\) dying first can be understood as the following: Both \\(x\\) and \\(y\\) surviving to some time Then the force of mortality acts on \\(x\\) only , causing only \\(x\\) to die Thus, \\(x\\) dies before \\(y\\) \\[ {}_{t}q^{1}_{x:y} = \\int^{t}_{0} {}_{t}p_{x:y} \\cdot \\mu_{x+t} \\] Tip The same intuition of integrating the scenario we want to achieve can be used to construct any of the death probabilities. Conversely, the probability of \\(x\\) dying second can be understood as the following: \\(y\\) dies while \\(x\\) survives Then the force of mortality acts on \\(x\\) , causing only \\(x\\) to die as well Thus, \\(x\\) dies after \\(y\\) \\[ {}_{t}q^{2}_{x:y} = \\int^{t}_{0} {}_{t}q_{y} \\cdot {}_{t}p_{x} \\cdot \\mu_{x+t} \\] Note that the two are NOT complementary to one another: It might seem intuitive that if \\(x\\) dies first, the opposite case must mean that \\(y\\) dies first However, this is NOT the case as there is a timeframe \\(t\\) attached to the probabilities The opposite of \\(x\\) dying first within \\(t\\) years is not only \\(y\\) dying first; both could survive past \\(t\\) years and die at some future time Thus, only when no specific timeframes are considered, then they are complements: \\[ \\begin{aligned} {}_{\\infty}q^{1}_{x:y} + {}_{\\infty}q^{\\> \\> \\> 1}_{x:y} &= 1 \\\\ {}_{\\infty}q^{2}_{x:y} + {}_{\\infty}q^{\\> \\> \\> 2}_{x:y} &= 1 \\\\ \\\\ {}_{t}q^{1}_{x:y} + {}_{t}q^{\\> \\> \\> 1}_{x:y} & \\ne 1 \\\\ {}_{t}q^{2}_{x:y} + {}_{t}q^{\\> \\> \\> 2}_{x:y} & \\ne 1 \\\\ \\end{aligned} \\] Note Following a similar line of reasoning, given only two lives, \\(x\\) dying first must necessarily mean that \\(y\\) dies second, but only if no specific timeframe is considered: \\[ \\begin{aligned} {}_{\\infty}q^{1}_{x:y} &= {}_{\\infty}q^{\\> \\> \\> 1}_{x:y} \\\\ {}_{t}q^{1}_{x:y} & \\ne {}_{t}q^{\\> \\> \\> 1}_{x:y} \\end{aligned} \\] Thus, the equality will only hold if we account for the other possibility - of \\(y\\) living past \\(t\\) : \\[ {}_{t}q^{1}_{x:y} = {}_{t}q^{\\ \\ \\ 2}_{x:y} + {}_{t}q_{x} \\cdot {}_{t}p_{y} \\] Warning The timeframes for the two are different: \\({}_{t}q^{1}_{x:y}\\) : \\(y\\) dies after \\(x\\) , NOT necessarily WITHIN \\(t\\) years \\({}_{t}q^{2}_{x:y}\\) : \\(x\\) dies after \\(y\\) , WITHIN \\(t\\) years Similar to last survivor probabilities, it assumes that BOTH individuals are alive at the start of the period . Thus, the deferred probability must account for both lives surviving till the deferred point (time \\(u\\) ) and then dying in the specified order within \\(t\\) years: \\[ \\begin{aligned} {}_{u \\mid t}q^1_{x:y} &= {}_{u}p_{x:y} \\cdot {}_{t}q^{1}_{x+u:y+u} \\\\ {}_{u \\mid t}q^2_{x:y} &= {}_{u}p_{x:y} \\cdot {}_{t}q^{2}_{x+u:y+u} \\\\ \\end{aligned} \\]","title":"Probability"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#contingent-relationships","text":"The most intuitive relationship is that for a single life to die, it must either die first or die second : \\[ {}_{t}q_{x} = {}_{t}q^{1}_{x:y} + {}_{t}q^{2}_{x:y} \\] A relationship can be formed with the multi-life probabilities as well: \\[ \\begin{aligned} P(\\text{First Death}) &= P(\\text{X dies first}) + P(\\text{Y dies first}) \\\\ {}_{t}q_{x:y} &= {}_{t}q^{1}_{x:y} + {}_{t}q^{\\> \\> \\> 1}_{x:y} \\\\ \\\\ P(\\text{Last Death}) &= P(\\text{X dies Last}) + P(\\text{Y dies Last}) \\\\ {}_{t}q_{\\overline{x:y}} &= {}_{t}q^{2}_{x:y} + {}_{t}q^{\\> \\> \\> 2}_{x:y} \\end{aligned} \\] Note Technically speaking, the true probability is the following: \\[ \\begin{aligned} P(\\text{First Death}) &= P(\\text{X dies first}) + P(\\text{Y dies first}) \\\\ &- P(\\text{X and Y die together}) \\\\ \\\\ P(\\text{Last Death}) &= P(\\text{X dies Last}) + P(\\text{Y dies Last}) \\\\ &- P(\\text{X and Y die together}) \\end{aligned} \\] The above assumes that is it not possible for both \\(x\\) and \\(y\\) to die at the same time - no common shock that could kill both simultaneously. Thus, the last term is zero . \\[ \\therefore P(\\text{X and Y die together}) = 0 \\]","title":"Contingent Relationships"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#insurance-applications","text":"Multi-life assurances and annuities are analagous to their single life counterparts . Similar to probability, in many cases, the single life subscript can simply be replaced with the multi-life subscript. This section highlights key differences when considering a second life.","title":"Insurance Applications"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#joint-life","text":"Consider the definition of a joint life assurance and annuity: Joint Life Assurance - Pays a death benefit upon the first death between \\(x\\) and \\(y\\) Joint Life Annuity - Pays a benefit till the first death between \\(x\\) and \\(y\\) (while both are alive) Using first principles, the EPV of the Joint Life assurances and annuities can be calculated as: \\[ \\begin{aligned} A_{x:y} &= \\sum v^{k} {}_{k \\mid}q_{x:y} \\\\ \\bar{A}_{x:y} &= \\int v^{t} {}_{t}p_{x:y} \\mu_{x+t:y+t} \\\\ \\\\ \\ddot{a}_{x:y} &= \\sum v^{k} {}_{k}p_{x:y} \\\\ \\bar{a}_{x:y} &= \\int v^{t} {}_{t}p_{x:y} \\\\ \\end{aligned} \\] The usual term conversions apply as well: \\[ \\begin{aligned} A_{x:y:\\enclose{actuarial}{n}} &= A_{x:y} - {}_{n}E_{x:y} \\cdot A_{x+n:y+n} \\\\ \\ddot{a}_{x:y:\\enclose{actuarial}{n}} &= \\ddot{a}_{x:y} - {}_{n}E_{x:y} \\cdot \\ddot{a}_{x+n:y+n} \\end{aligned} \\] Tip Using first principles, the joint life pure endowment function can be calculated as: \\[ \\begin{aligned} {}_{n}E_{x:y} &= v^{n} \\cdot {}_{n}p_{x:y} \\\\ &= v^{n} \\cdot {}_{n}p_{x} \\cdot {}_{n}p_{y} \\\\ \\end{aligned} \\] However, it can also be expressed in terms of the single life PE functions, which is more convenient to calculate from the SULT: \\[ \\begin{aligned} {}_{n}E_{x:y} &= v^{n} {}_{n}p_{x} \\cdot {}_{n}p_{y} \\\\ &= v^{n} {}_{n}p_{x} \\cdot v^{n} {}_{n}p_{y} \\cdot (1+i)^{n} \\\\ &= {}_{n}E_{x} \\cdot {}_{n}E_{y} \\cdot (1+i)^{n} \\end{aligned} \\] Lastly, the usual conversion between Assurances and Annuities hold as well: \\[ \\begin{aligned} \\ddot{a}_{x:y} &= \\frac{1 - A_{x:y}}{d} \\\\ \\bar{a}_{x:y} &= \\frac{1 - \\bar{A}_{x:y}}{\\delta} \\end{aligned} \\]","title":"Joint Life"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#last-survivor","text":"Consider the definition of a last survivor assurance and annuity: Last Survivor Assurance - Pays a death benefit on the second death between \\(x\\) and \\(y\\) Last Survivor Annuity - Pays a benefit till the second death between \\(x\\) and \\(y\\) (while at least one is alive) Given the complexity in the survival probability for last survivors, it is difficult to calculate the EPV using from principles. Thus, we can leverage on the previous relationship in the random variables: \\[ \\begin{aligned} v^{T_{x:y}} + v^{T_{\\overline{x:y}}} &= v^{T_{x}} + v^{T_{y}} \\\\ A_{x:y} + A_{\\overline{x:y}} &= A_{x} + A_{y} \\\\ \\ddot{a}_{x:y} + \\ddot{a}_{\\overline{x:y}} &= \\ddot{a}_{x} + \\ddot{a}_{y} \\\\ {}_{n}E_{x:y} + {}_{n}E_{\\overline{x:y}} &= {}_{n}E_{x} + {}_{n}E_{y} \\\\ \\end{aligned} \\] Another reason why this method is preferred is because the Joint Life and Single Life values are already provided on the SULT ; it is much easier to utilize them rather than to go bottom's up. Warning The usual method for term conversions do NOT apply in a last survivor context: \\[ A_{\\overbrace{\\overline{x:y}}:\\enclose{actuarial}{n}}^{\\> \\> \\> 1} \\ne A_{\\overline{x:y}} - {}_{n}E_{\\overline{x:y}} \\cdot A_{\\overline{x+n:y+n}} \\] Similar to before, this is due to the difference in basis - the second term is on the basis that both lives are alive while the PE term only assumes that at least one will survive (not both). This means that even for last survivor TAs, they MUST be calculated from the joint life version. Note It is possible for questions to ask for the EPV of a TA with a short duration, requiring manual calculation of the EPV. Thus, it is important to take note of the following quirk in last survivor probabilities: In the first year, there is only one way to fail the last survivor status - both individuals die in the year: \\[ P(K_{\\overline{x:y}} = 0) = q_{x} \\cdot q_{y} \\] In every subsequent year , there are three possible cases:: \\(y\\) dies in the first year and \\(x\\) dies in the second year \\(x\\) dies in the first year and \\(y\\) dies in the second year Both \\(x\\) and \\(y\\) die in the second year \\[ \\begin{aligned} P(K_{\\overline{x:y}} = 1) &= q_{y} \\cdot p_{x} q_{x+1} \\\\ &+ q_{x} \\cdot p_{y} q_{y+1} \\\\ &+ p_{x} q_{x+1} \\cdot p_{y} q_{y+1} \\end{aligned} \\]","title":"Last Survivor"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#variance","text":"Recall that the Joint Life and Last Survivor functions themselves are not independent of each other. Thus, if asked to determine their Variance, the Covariance must be considered: \\[ \\begin{aligned} \\text{Var} \\left(v^{T_{x:y}} + v^{T_{\\overline{x:y}}} \\right) &= \\text{Var} \\left(v^{T_{x:y}} \\right) + \\text{Var} \\left(v^{T_{\\overline{x:y}}} \\right) + \\text{Cov} \\left(v^{T_{x:y}}, v^{T_{\\overline{x:y}}} \\right) \\end{aligned} \\] Using similar results from FAM-L and Covariance proof from before, the individual components can be computed: \\[ \\begin{aligned} \\text{Var} \\left(v^{T_{x:y}} \\right) &= {}^{2}A_{x:y} - (A_{x:y})^{2} \\\\ \\text{Var} \\left(v^{T_{\\overline{x:y}}} \\right) &= {}^{2}A_{\\overline{x:y}} - (A_{\\overline{x:y}})^{2} \\\\ \\text{Cov} \\left(v^{T_{x:y}}, v^{T_{\\overline{x:y}}} \\right) &= \\left(A_{x} - A_{x:y} \\right) \\left(A_{y} - A_{x:y} \\right) \\end{aligned} \\] Warning The Covariance for annuities are slightly different : \\[ \\begin{aligned} \\text{Cov} \\left(\\bar{a}_{\\enclose{actuarial}{T_{x:y}}}, \\bar{a}_{\\enclose{actuarial}{T_{\\overline{x:y}}}} \\right) &= \\text{Cov} \\left(\\frac{1-v^{T_{x:y}}}{\\delta}, \\frac{1-v^{T_{\\overline{x:y}}}}{\\delta} \\right) \\\\ &= \\frac{1}{\\delta} \\cdot \\frac{1}{\\delta} \\text{Cov} \\left(1-v^{T_{x:y}}, 1-v^{T_{\\overline{x:y}}} \\right) \\\\ &= \\frac{1}{\\delta^2} \\cdot \\text{Cov} \\left(-v^{T_{x:y}}, -v^{T_{\\overline{x:y}}} \\right) \\\\ &= \\frac{1}{\\delta^2} \\cdot (-1)(-1) \\cdot \\text{Cov} \\left(v^{T_{x:y}}, v^{T_{\\overline{x:y}}} \\right) \\\\ &= \\frac{1}{\\delta^2} \\cdot \\text{Cov} \\left(v^{T_{x:y}}, v^{T_{\\overline{x:y}}} \\right) \\\\ &= \\frac{1}{\\delta^2} \\cdot \\left(\\bar{A}_{x} - \\bar{A}_{x:y} \\right) \\left(\\bar{A}_{y} - \\bar{A}_{x:y} \\right) \\end{aligned} \\] The above is consistent with what was covered in FAM-L, where the variance of an annuity is derived from an assurance . However, it can be further simplified to reach an analagous expression to the assurances: \\[ \\begin{aligned} \\text{Cov} \\left(\\ddot{a}_{\\enclose{actuarial}{T_{x:y}}}, \\ddot{a}_{\\enclose{actuarial}{T_{\\overline{x:y}}}} \\right) &= \\frac{1}{\\delta^2} \\cdot \\left(\\bar{A}_{x} - \\bar{A}_{x:y} \\right) \\left(\\bar{A}_{y} - \\bar{A}_{x:y} \\right) \\\\ &= \\frac{1}{\\delta^2} \\cdot \\left(\\bar{A}_{x} - \\bar{A}_{x:y} + 1 - 1 \\right) \\left(\\bar{A}_{y} - \\bar{A}_{x:y} + 1 - 1 \\right) \\\\ &= \\frac{1}{\\delta^2} \\cdot \\left[-1 + \\bar{A}_{x} + 1 - \\bar{A}_{x:y} \\right] \\left[-1 + \\bar{A}_{y} + 1 - \\bar{A}_{x:y} \\right] \\\\ &= \\frac{1}{\\delta^2} \\cdot \\left[- (1 - \\bar{A}_{x}) + (1 - \\bar{A}_{x:y}) \\right] \\left[- (1 - \\bar{A}_{y}) + (1 - \\bar{A}_{x:y}) \\right] \\\\ &= \\frac{1}{\\delta^2} \\cdot (-1)(-1) \\left[(1 - \\bar{A}_{x}) - (1 - \\bar{A}_{x:y}) \\right] \\left[(1 - \\bar{A}_{y}) - (1 - \\bar{A}_{x:y}) \\right] \\\\ &= \\frac{\\left[(1 - \\bar{A}_{x}) - (1 - \\bar{A}_{x:y}) \\right]}{\\delta} \\frac{\\left[(1 - \\bar{A}_{y}) - (1 - \\bar{A}_{x:y}) \\right]}{\\delta} \\\\ &= \\left(\\frac{1 - \\bar{A}_{x}}{\\delta} - \\frac{1 - \\bar{A}_{x:y}}{\\delta} \\right) - \\left(\\frac{1 - \\bar{A}_{y}}{\\delta} - \\frac{1 - \\bar{A}_{x:y}}{\\delta} \\right) \\\\ &= \\left(\\bar{a}_{x} - \\bar{a}_{x:y} \\right) \\left(\\bar{a}_{y} - \\bar{a}_{x:y} \\right) \\end{aligned} \\]","title":"Variance"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#contingent-assurance","text":"A contingent assurance pays a death benefit upon the death of \\(x\\) , depending on whether \\(y\\) is alive or dead at that time ; depending on the order of death . Using first principles, we can calculate the EPV as the following: \\[ \\begin{aligned} \\bar{A}^{1}_{x:y} &= \\int {}_{t}p_{x:y} \\cdot \\mu_{x+t} \\\\ \\bar{A}^{2}_{x:y} &= \\int {}_{t}q_{y} {}_{t}p_{x} \\mu_{x+t} \\end{aligned} \\] However, it is much to leverage on the relationships that were previously covered: \\[ \\begin{aligned} A_{x} &= A^{1}_{x:y} + A^{2}_{x:y} \\\\ A_{x:y} &= A^{1}_{x:y} + A^{\\> \\> \\> 1}_{x:y} \\\\ A_{\\overline{x:y}} &= A^{2}_{x:y} + A^{\\> \\> \\> 2}_{x:y} \\\\ \\end{aligned} \\]","title":"Contingent Assurance"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#contingent-annuity","text":"A contingent annuity pays a benefit as long as \\(y\\) alive, depending on whether \\(x\\) is alive or dead at that time ; depending on the order of death . In particular, a Reversionary Annuity is a form of contingent annuity that pays a benefit as long as \\(y\\) is alive, provided that \\(x\\) has died . Note The trigger for the annuity starting is the death of \\(x\\) , since it is contingent on this event. This is somewhat hard to understand, as we dont usually associate death starting an annuity . A reversionary annuity can thus be phrased as paying a stream of benefits upon the death of \\(x\\) , payable as long as \\(y\\) is alive. \\(\\ddot{a}_{x \\mid y}\\) is the notation for a reversionary annuity where: \\(x\\) is the failing life ; life that has to die \\(y\\) is the annuitant ; life that must stay alive A reversionary annuity can be understood as a deferred annuity where the deferment period is the time where both individuals are alive. The key difference from FAM-L is that this period is now variable, rather than fixed: \\[ \\ddot{a}_{x \\mid y} = \\ddot{a}_{y} - \\ddot{a}_{x:y} \\\\ \\] Tip The above result can be understood using the following logic: \\(\\ddot{a}_{y}\\) pays a benefit until \\(y\\) dies \\(\\ddot{a}_{x:y}\\) pays a benefit until the first person dies As long as \\(x\\) dies BEFORE \\(y\\) , the former will always have more payments than the latter . Thus, the difference is equivalent to a reversionary annuity: However, if \\(x\\) dies AFTER \\(y\\) , then the two terms are equal and thus the difference is 0; equivalent to the reversionary annuity NOT making any payments: Another variation of contingent annuities is one that makes payments as long as only one person is alive ( regardless of which person). Intuitively, this is simply the combination of two reversionary annuities : \\[ \\begin{aligned} \\text{EPV Annuity Only One Alive} &= \\ddot{a}_{x \\mid y} + \\ddot{a}_{y \\mid x} \\\\ &= \\ddot{a}_{y} - \\ddot{a}_{x:y} + \\ddot{a}_{x} - \\ddot{a}_{x:y} \\\\ &= (\\ddot{a}_{y} + \\ddot{a}_{x} - \\ddot{a}_{x:y}) - \\ddot{a}_{x:y} \\\\ &= \\ddot{a}_{\\overline{x:y}} - \\ddot{a}_{x:y} \\end{aligned} \\] Tip Notice that the above is essentially the formula for 'XOR' logic - only making a payment when either \\(x\\) or \\(y\\) is alive, but NOT both: Note The same logic can be applied to be probabilities as well. Note that when there are only two individuals, the probability of only person being alive is the same as the probability of only person dying : \\[ \\begin{aligned} \\text{P(Exactly One Lives)} &= \\text{P(Exactly One Dies)} \\\\ &= p_{\\overline{x:y}} - p_{x:y} \\\\ &= q_{x:y} - q_{\\overline{x:y}} \\\\ &= 1 - (1 - q_{x})(1 - q_{y}) - q_{x} \\cdot q_{y} \\\\ &= 1 - (1 - q_{y} - q{x} + q_{x} \\cdot q_{y}) - q_{x} \\cdot q_{y} \\\\ &= q_{y} + q_{x} - 2 \\cdot q_{x} \\cdot q_{y} \\\\ &= q_{y} - q_{x} \\cdot q_{y} + q_{x} - q_{x} \\cdot q_{y} \\\\ &= q_{y} (1 - q_{y}) + q_{x} (1 - q_{y}) \\\\ &= q_{y} p_{x} + q_{x} p_{y} \\\\ &= \\text{P(X lives, Y dies)} + \\text{P(X dies, Y lives)} \\end{aligned} \\]","title":"Contingent Annuity"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/1.%20Multi%20Life%20Models/#approximations","text":"All the usual approximations from FAM-L to convert an assurance/annuity from payable yearly to more frequently apply in a multi-life setting as well. For instance, consider the woolhouse approximation:d \\[ \\ddot{a}^{(m)}_{x:y} = \\ddot{a}_{x:y} - \\frac{m-1}{2m} \\] Note Whole Life Last Survivor annuities can be converted directly as well: \\[ \\begin{aligned} \\ddot{a}^{(m)}_{\\overline{x:y}} &= \\ddot{a}^{(m)}_{x} + \\ddot{a}^{(m)}_{y} - \\ddot{a}v_{x:y} \\\\ &= \\ddot{a}_{x} - \\frac{m-1}{2m} + \\ddot{a}_{y} - \\frac{m-1}{2m} - (\\ddot{a}_{x:y} - \\frac{m-1}{2m}) \\\\ &= \\ddot{a}_{x} - \\frac{m-1}{2m} + \\ddot{a}_{y} - \\frac{m-1}{2m} - \\ddot{a}_{x:y} + \\frac{m-1}{2m} \\\\ &= \\ddot{a}_{x} + \\ddot{a}_{y} - \\ddot{a}_{x:y} - \\frac{m-1}{2m} \\\\ &= \\ddot{a}_{\\overline{x:y}} - \\frac{m-1}{2m} \\end{aligned} \\] The same rules from FAM-L apply as well - the approximation only works for whole life annuities. For any other type of annuity, decompose it into the a combination of whole life annuities then apply the conversion.","title":"Approximations"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/","text":"Multiple Decrement Model \u00b6 Overview \u00b6 In FAM-L, we only considered simple insurance policies which paid only a death benefit. In reality, insurance contracts often pay one of a few possible benefits: Benefit Trigger Death Benefit Death by natural causes Accidental Death Benefit Death by accident Surrender Benefit Lapsing the policy Dread Disease Benefit Contracting Dread Disease Note Functionally, multi-decrement models can be understood as a one to many multi-state model . For the purposes of this exam, unless otherwise stated, it is assumed that these type of policies only pay ONE of the benefits. Making a claim on either of the benefits will trigger a claim and then terminate the policy . The different benefit triggers are different ways that the policy can terminate , known as a Decrement . Thus, the simple policy considered in FAM-L is an example of a Single Decrement Model while the policy described above is an example of a Multiple Decrement Model . Info The definition of a Decrement is \"to decrease\". In the context of life insurance, it decreases the number of active policies. Probability \u00b6 Previously, since death was the only decrement, we only considered the probability of death. However, in a multiple decrement setting, we more generally consider the probability of termination . The multiple decrement model has two components: Discrete Random Variable \\(J\\) - Denoting the decrements in the model Continuous Random Variable \\(T\\) - Denoting the time to termintaton based on that decrement Combining the two, the probability of termination in \\(t\\) years due to a particular decrement \\(j\\) : \\[ \\begin{aligned} {}_{t}q^{(j)}_{x} &= P(T_{x} \\lt t, J_{x} = j) \\\\ &= \\int^{t}_{0} f_{T_{x}, J_{x}}(t, j) \\end{aligned} \\] The probability of termination due to ANY DECREMENT in \\(t\\) years is the sum of all individual probabilities of all \\(m\\) decrements: \\[ \\begin{aligned} {}_{t}q^{(\\tau)}_{x} &= {}_{t}q^{(1)}_{x} + {}_{t}q^{(2)}_{x} + \\dots + {}_{t}q^{(m)}_{x} \\\\ &= \\sum^{j=m}_{j=1} {}_{t}q^{(j)}_{x} \\end{aligned} \\] Note \\(\\tau\\) is the notation used the denote ALL possible decrements. The above is valid because we assumed that the policyholder can only terminate from ONE of the decrements. Thus, each decrement is mutually exclusive , where the probability of termination from any cause is the sum of each individual probability (OR logic). Thus, the probability of surviving \\(t\\) years is denoted as: \\[ \\begin{aligned} {}_{t}p^{(\\tau)}_{x} &= 1 - {}_{t}q^{(\\tau)}_{x} \\\\ &= e^{- \\int \\mu^{(\\tau)}_{x+t}} \\end{aligned} \\] Warning There is NO such thing as \\({}_{t}p^{(j)}_{x}\\) ! The above would be the probability of surviving \\(t\\) years from decrement \\(j\\) . However, even if they do not terminate from decrement \\(j\\) , they can still terminate from OTHER decrements . Ultimately, we are only concerned whether the individual is still alive at time \\(t\\) , not whether or not they survive a particular decrement. The force of decrement has similar properties as the termination probability: \\[ \\begin{aligned} \\mu^{(j)}_{x}(t) &= \\frac{\\frac{d}{dt} {}_{t}q^{(j)}_{x}}{{}_{t}p^{(\\tau)}_{x}} \\\\ \\\\ \\mu^{(\\tau)}_{x}(t) &= \\frac{\\frac{d}{dt} {}_{t}q^{(\\tau)}_{x}}{{}_{t}p^{(\\tau)}_{x}} \\\\ &= \\frac{1}{{}_{t}p^{(\\tau)}_{x}} \\left (\\frac{d}{dt} {}_{t}q^{(\\tau)}_{x} \\right) \\\\ &= \\frac{1}{{}_{t}p^{(\\tau)}_{x}} \\frac{d}{dt} \\left[{}_{t}q^{(1)}_{x} + \\dots {}_{t}q^{(m)}_{x} \\right] \\\\ &= \\mu^{(1)}_{x+t} + \\dots + \\mu^{(m)}_{x+t} \\end{aligned} \\] Thus, the PDF of the joint distribution can be expressed as: \\[ \\begin{aligned} \\mu^{(j)}_{x}(t) &= \\frac{\\frac{d}{dt} {}_{t}q^{(j)}_{x}}{{}_{t}p^{(\\tau)}_{x}} \\\\ \\frac{d}{dt} {}_{t}q^{(j)}_{x} &= {}_{t}p^{(\\tau)}_{x} \\cdot \\mu^{(j)}_{x+t} \\\\ {}_{t}q^{(j)}_{x} &= \\int^{t}_{0} {}_{t}p^{(\\tau)}_{x} \\cdot \\mu^{(j)}_{x+t} \\end{aligned} \\] Tip Given that it can be expressed as a multi-state model, the second line in the above equation is the Kolmogorov's Forward Equation : \\[ \\text{Change} = \\text{Probability IN} - \\text{Probability OUT} \\] For multi-decrement models, there is only a one directional path to each decremented state, which are always absorbing . Thus, the second component is always 0. Tip Questions may sometimes provide the forces of decrement for each of the individual decrements and asked to calculate the associated termination probability for each. Given the properties of the multiple decrement model, this problem can be simplified to avoid integrating multiple times. It may not always be clear on first sight, but for most euqestions, the forces of decrement can be shown to be a fraction of the total force of mortality : \\[ \\mu^{(j)}_{x}(t) = c \\cdot \\mu^{(\\tau)}_{x} \\] Assuming this relationship exists, we can show that the termintation probability for a particular decrement is the same fraction of the total termination probability: \\[ \\begin{aligned} {}_{t}q^{(j)}_{x} &= \\int {}_{t}p^{(\\tau)}_{x} \\cdot \\mu^{(j)}_{x+t} \\\\ &= \\int {}_{t}p^{(\\tau)}_{x} \\cdot c \\cdot \\mu^{(\\tau)}_{x+t} \\\\ &= c \\cdot \\int {}_{t}p^{(\\tau)}_{x} \\mu^{(\\tau)}_{x+t} \\\\ &= c \\cdot {}_{t}q^{(\\tau)}_{x} \\end{aligned} \\] Similar to the multi life section, all other results not explicitly covered above are analagous to their single decrement counterparts . For most items, simply add the superscript (j) to indicate that the probability is for a particular decrement. Independent Rates \u00b6 In a multi-decrement setting, there are multiple decrements acting on the policyholder, which are said to be competing with one another to terminate the policy. The rates shown so far are known as Dependent Rates , which implicitly assumes that the competition among decrements has already been accounted for. Dependent Rates should be used for projection to reflect the true underlying situation. However, decrements are often studied independently , resulting in Independent Rates : Dependent Rates Independent Rates Presence of other decrements Absence of other decrements Competing Not competing Non-prime notation Prime notation \\({}_{t}q^{(j)}_{x}\\) \\({}_{t}q^{'(j)}_{x}\\) The key is understanding the differences and how to convert from one form to the other. Key Intuition \u00b6 Consider the following scenario: There are 1000 lives in the population The independent death rate is 20% ( \\(q^{(\\text{Death})}_{x}\\) ) The independent lapse rate is 10% ( \\(q^{(\\text{Death})}_{x}\\) ) Consider the following outcomes: If death was the only decrement, we expect 200 people to die by the end of the year If lapse was the only decrement, we expect 100 people to lapse by the end of the year If both death and lapse are present, we would NOT expect 200 deaths and 100 lapses The third point is true because: Someone who might have died in the later part of the year might have already lapsed in the earlier part Someone who might have lapsed in the later part of the year might have already died in the earlier part Thus, we expect fewer deaths and lapses when there are multiple decrements acting simultaneously Note The proper terminology to use when explaining this is that the lives are exposed to the force of death or lapse respectively. The above is known as the Theory of Competing Risk , which leads to the following relationship: \\[ {}_{t}q^{'(j)}_{x} \\ge {}_{t}q^{(j)}_{x} \\] Tip Recall that \\({}_{}p^{(j)}_{x}\\) was not a valid expression as it did not make sense to just survive one of the many decrements. However, under an independent rate basis, there is only ONE decrement being considered, thus \\({}_{}p^{'(j)}_{x}\\) is a valid expression . Relationship \u00b6 Recall that the survival model in FAM-L is essentially a single decrement model. Thus, the key results can be applied here as well: \\[ \\begin{aligned} \\mu^{(j)}_{x+t} &= \\frac{\\frac{d}{dt} {}_{}q^{'(j)}_{x}}{{}_{}p^{'(j)}_{x}} \\\\ \\\\ {}_{}q^{'(j)}_{x} &= \\int {}_{}p^{'(j)}_{x} \\cdot \\mu^{(j)}_{x+t} \\\\ &= 1 - {}_{}p^{'(j)}_{x} \\\\ \\\\ {}_{}p^{'(j)}_{x} &= e^{- \\int \\mu^{(j)}_{x+t}} \\end{aligned} \\] The key is understanding that the force of decrement is the SAME under a dependent and an independent basis. This is because the force acts instantly - even under a dependent basis, there is no competition at an instant. Warning This means that there is NO such thing as \\(\\mu^{'(j)}_{x+t}\\) ! Thus, both dependent and independent rates share the same total force of decrements and total probabilities as well: \\[ \\begin{aligned} \\mu^{(\\tau)}_{x+t} &= \\mu^{(1)}_{x+t} + \\dots + \\mu^{(m)}_{x+t} \\\\ \\\\ \\therefore {}_{}p^{(\\tau)}_{x} &= e^{- \\int \\mu^{(\\tau)}_{x}} \\\\ &= e^{- \\int (\\mu^{(1)}_{x+t} + \\dots + \\mu^{(m)}_{x+t})} \\\\ &= e^{- \\int \\mu^{(1)}_{x+t}} \\cdot \\dots \\cdot e^{- \\int \\mu^{(m)}_{x+t}} \\\\ &= {}_{}p^{'(1)}_{x+t} \\cdot \\dots \\cdot {}_{}p^{'(m)}_{x+t} \\end{aligned} \\] Note This is conceptually identical to the probability of independent events occuring together : \\[ \\text{P(A and B)} = \\text{P(A)} \\cdot \\text{P(B)} \\] Conversion \u00b6 The key principle is to form an expression equating the two basis together , and solving for the missing component based on the given information: \\[ \\begin{array}{|c|c|c|} \\hline \\text{} & \\text{Independent Rates} & \\text{Dependent Rates} \\\\ \\hline \\text{Force} & \\mu^{(j)}_{x} = \\frac{- \\frac{d}{dt} {}_{t}p^{'(j)}}{{}_{t}p^{'(j)}} & \\mu^{(j)}_{x} = \\frac{\\frac{d}{dt} {}_{t}q^{(j)}}{{}_{t}p^{(\\tau)}} \\\\ \\hline \\text{Survival} & {}_{t}p^{(\\tau)}_{x} = \\prod {}_{t}p^{'(j)} & {}_{t}p^{(\\tau)}_{x} = 1 - {}_{t}q^{(\\tau)} \\\\ \\hline \\text{Termination} & {}_{t}q^{(\\tau)}_{x} = 1 - {}_{t}p^{(\\tau)} & {}_{t}q^{(\\tau)}_{x} = \\sum {}_{t}q^{(j)} \\\\ \\hline \\end{array} \\] Unless otherwise stated, the rates provided should be assumed to be independent rates . This is important as this concept is applicable to every other chapter in the exam. Questions of this nature tend to involve fractional times (though not always). Similar to before, additional assumptions are required in order to determine the probabilities. These add another layer of complexity to the conversion. Constant Force \u00b6 Recall that under a constant force of mortality, the survival probabilities can be simplified to the following: \\[ \\begin{aligned} {}_{t}p^{'(j)}_{x} &= e^{- \\mu^{(j)}_{x} \\cdot t} \\\\ \\\\ {}_{t}p^{(\\tau)}_{x} &= e^{- \\mu^{(\\tau)}_{x} \\cdot t} \\end{aligned} \\] Since the force is a constant , we can directly manipulate the powers to convert from the total probability to the individual decrements: \\[ \\begin{aligned} {}_{t}p^{(\\tau)}_{x} &= e^{- \\mu^{(\\tau)}_{x} \\cdot t} \\\\ \\left({}_{t}p^{(\\tau)}_{x} \\right)^{\\frac{\\mu^{(j)}_{x}}{\\mu^{(\\tau)}_{x}}} &= \\left(e^{- \\mu^{(\\tau)}_{x} \\cdot t} \\right)^{\\frac{\\mu^{(j)}_{x}}{\\mu^{(\\tau)}_{x}}} \\\\ \\left({}_{t}p^{(\\tau)}_{x} \\right)^{\\frac{\\mu^{(j)}_{x}}{\\mu^{(\\tau)}_{x}}} &= e^{- \\mu^{(j)}_{x} \\cdot t} \\\\ {}_{t}p^{'(j)}_{x} &= \\left({}_{t}p^{(\\tau)}_{x} \\right)^{\\frac{\\mu^{(j)}_{x}}{\\mu^{(\\tau)}_{x}}} \\end{aligned} \\] A more intuitive way to remember the formula would be to remember the ratio : \\[ \\begin{aligned} {}_{t}p^{'(j)}_{x} &= \\left({}_{t}p^{(\\tau)}_{x} \\right)^{\\frac{\\mu^{(j)}_{x}}{\\mu^{(\\tau)}_{x}}} \\\\ \\ln {}_{t}p^{'(j)}_{x} &= {\\frac{\\mu^{(j)}_{x}}{\\mu^{(\\tau)}_{x}}} \\cdot \\ln {}_{t}p^{'(\\tau)}_{x} \\\\ \\frac{\\ln {}_{t}p^{'(j)}_{x}}{\\ln {}_{t}p^{(\\tau)}_{x}} &= {\\frac{\\mu^{(j)}_{x}}{\\mu^{(\\tau)}_{x}}} \\end{aligned} \\] If the ratio of the force is not available, it can be alternatively calculated via the termination probability : \\[ \\begin{aligned} {}_{t}q^{(j)}_{x} &= \\int {}_{t}p^{(\\tau)}_{x} \\cdot \\mu^{(j)}_{x+t} \\\\ &= \\frac{\\mu^{(\\tau)}_{x+t}}{\\mu^{(\\tau)}_{x+t}} \\cdot \\int {}_{t}p^{(\\tau)}_{x} \\cdot \\mu^{(j)}_{x+t} \\\\ &= \\frac{\\mu^{(j)}_{x+t}}{\\mu^{(\\tau)}_{x+t}} \\cdot \\int {}_{t}p^{(\\tau)}_{x} \\cdot \\mu^{(\\tau)}_{x+t} \\\\ &= \\frac{\\mu^{(j)}_{x+t}}{\\mu^{(\\tau)}_{x+t}} \\cdot {}_{t}q^{(\\tau)}_{x+t} \\\\ \\frac{\\mu^{(j)}_{x+t}}{\\mu^{(\\tau)}_{x+t}} &= \\frac{{}_{t}q^{(j)}_{x}}{{}_{t}q^{(\\tau)}_{x}} \\\\ \\\\ \\therefore \\frac{\\ln {}_{t}p^{'(j)}_{x}}{\\ln {}_{t}p^{(\\tau)}_{x}} &= {\\frac{\\mu^{(j)}_{x}}{\\mu^{(\\tau)}_{x}}} = \\frac{{}_{t}q^{(j)}_{x}}{{}_{t}q^{(\\tau)}_{x}} \\end{aligned} \\] Dependent UDD \u00b6 In a multi-decrement context, UDD stands for Uniform Distribution of Decrements (NOT death). However, because there are two sets of rates (Dependent vs Independent), there are two different UDD assumptions - based on which set is UDD. This section assumes that the Dependent Rates are distributed UDD. It follows the same principles covered in FAM-L: \\[ {}_{t}q^{(j)}_{x} = t \\cdot q^{(j)}_{x} \\] Using the above, the force of decrement can be expressed as the following: \\[ \\begin{aligned} {}_{t}q^{(j)}_{x} &= t \\cdot q^{(j)}_{x} \\\\ t \\cdot q^{(j)}_{x} &= \\int {}_{t}p^{(\\tau)}_{x} \\cdot \\mu^{(j)}_{x+t} \\\\ q^{(j)}_{x} &= {}_{t}p^{(\\tau)}_{x} \\cdot \\mu^{(j)}_{x+t}, \\ \\text{Differentiation} \\\\ \\mu^{(j)}_{x+t} &= \\frac{q^{(j)}_{x}}{{}_{t}p^{(\\tau)}_{x}} \\\\ \\end{aligned} \\] Thus, the independent survival probability can be expressed as: \\[ \\begin{aligned} {}_{t}p^{'(j)}_{x} &= e^{- \\int \\mu^{(j)}_{x+t}} \\\\ &= e^{- \\int \\frac{q^{(j)}_{x}}{{}_{t}p^{(\\tau)}_{x+t}}} \\\\ &= e^{-q^{(j)}_{x}} \\cdot e^{\\int \\frac{1}{1 - t \\cdot q^{(\\tau)}_{x}}} \\\\ &= e^{-q^{(j)}_{x}} \\cdot e^{\\frac{\\ln 1 - t \\cdot q^{(\\tau)}_{x}}{- q^{(\\tau)}_{x}}} \\\\ &= e^{-q^{(j)}_{x}} \\cdot e^{\\frac{\\ln {}_{t}p^{(\\tau)}_{x}}{- q^{(\\tau)}_{x}}} \\\\ &= e^{\\ln {}_{t}p^{(\\tau)} \\cdot \\frac{-q^{(j)}_{x}}{-q^{(\\tau)}_{x}}} \\\\ &= \\left(e^{\\ln {}_{t}p^{(\\tau)}} \\right)^{\\frac{q^{(j)}_{x}}{q^{(\\tau)}_{x}}} \\\\ &= \\left({}_{t}p^{(\\tau)} \\right)^{\\frac{q^{(j)}_{x}}{q^{(\\tau)}_{x}}} \\\\ \\\\ \\therefore \\frac{\\ln {}_{t}p^{'(j)}_{x}}{\\ln {}_{t}p^{(\\tau)}_{x}} &= \\frac{{}_{t}q^{(j)}_{x}}{{}_{t}q^{(\\tau)}_{x}} = \\frac{q^{(j)}_{x}}{q^{(\\tau)}_{x}} \\end{aligned} \\] Notice that is the exact SAME RESULT as the constant force scenario! Thus, it is sufficient to remember the ratio . Tip The key to the proof is understanding that the survival probability MUST be converted into into the termination probability (under UDD) in order to simplify the integration: \\[ \\begin{aligned} {}_{t}p^{(\\tau)}_{x} &= 1 - {}_{t}q^{(\\tau)}_{x} \\\\ &= 1 - t \\cdot q^{(\\tau)}_{x} \\end{aligned} \\] Independent UDD \u00b6 Conversely, this section assumes that the independent rates are distributed UDD: \\[ {}_{t}q^{'(j)}_{x} = t \\cdot q^{'(j)}_{x} \\] Going through a similar process as dependent UDD, we can express the independent termination probability as: \\[ \\begin{aligned} {}_{t}q^{'(j)}_{x} &= t \\cdot q^{'(j)}_{x} \\\\ t \\cdot q^{'(j)}_{x} &= \\int {}_{t}p^{'(j)}_{x} \\cdot \\mu^{(j)}_{x+t} \\\\ q^{'(j)}_{x} &= {}_{t}p^{'(j)}_{x} \\cdot \\mu^{(j)}_{x+t}, \\ \\text{Differentiation} \\\\ \\end{aligned} \\] Thus, the dependent termination probability can be expressed as: \\[ \\begin{aligned} {}_{t}q^{(j)}_{x} &= \\int {}_{t}p^{(\\tau)}_{x} \\cdot \\mu^{(j)}_{x} \\\\ &= \\int {}_{t}p^{(\\tau)}_{x} \\prod_{i \\ne j} {}_{t}p^{'(i)}_{x} \\cdot \\mu^{(j)}_{x} \\\\ &= \\int {}_{t}p^{'(j)}_{x} \\cdot \\mu^{(j)}_{x} \\cdot \\prod_{i \\ne j} {}_{t}p^{'(i)}_{x} \\\\ &= \\int q^{'(j)}_{x} \\cdot \\prod_{i \\ne j} {}_{t}p^{'(i)}_{x} \\\\ &= q^{'(j)}_{x} \\int \\prod_{i \\ne j} {}_{t}p^{'(i)}_{x} \\end{aligned} \\] Tip The key is understanding that the termination probability is a constant within the integration, thus can be factorized out. The final expression depends on the number of decrements in the model. The most commonly tested variation is the two decrement model : \\[ \\begin{aligned} {}_{t}q^{(1)}_{x} &= q^{'(1)}_{x} \\cdot \\int {}_{t}p^{'(1)}_{x} \\\\ &= q^{'(1)}_{x} \\cdot \\int \\left(1 - t \\cdot {}_{}q^{'(2)}_{x} \\right) \\\\ &= q^{'(1)}_{x} \\cdot \\left(t - \\frac{t^2}{2} {}_{}q^{'(2)}_{x} \\right) \\\\ &= t \\cdot q^{'(1)}_{x} \\cdot \\left(1 - \\frac{t}{2} {}_{}q^{'(2)}_{x} \\right) \\\\ \\\\ \\therefore {}_{t}q^{(2)}_{x} &= t \\cdot q^{'(2)}_{x} \\cdot \\left(1 - \\frac{t}{2} {}_{}q^{'(1)}_{x} \\right) \\\\ \\end{aligned} \\] Tip Assume that \\(t=1\\) , \\[ \\begin{aligned} {}_{t}q^{(1)}_{x} &= q^{'(1)}_{x} \\cdot \\left(1 - \\frac{1}{2} {}_{}q^{'(2)}_{x} \\right) \\\\ {}_{t}q^{(2)}_{x} &= q^{'(2)}_{x} \\cdot \\left(1 - \\frac{1}{2} {}_{}q^{'(1)}_{x} \\right) \\\\ \\end{aligned} \\] Decrement 1 and 2 are happening simultaneously and uniformly throughout the year. The intuition is that, on average , HALF the population would have been decremented by lapse by before death can act on it; vice-versa. Less commonly tested is the Three Decrement Model . The proof is similar to above, but much more tedious due to the additional product. Thus, it is recommended to memorize the result instead: \\[ {}_{t}q^{(1)}_{x} = t \\cdot q^{'(1)}_{x} \\cdot \\left(1 - \\frac{t}{2} {}_{}q^{'(2)}_{x} - \\frac{t}{2} {}_{}q^{'(3)}_{x} + \\frac{t}{3} {}_{}q^{'(2)}_{x} {}_{}q^{'(3)}_{x} \\right) \\] Exact Age Decrements \u00b6 In the above UDD section, we have assumed that the decrements are acting throughout the entire year . However, decrements could instead occur at exact times , in which case no competition actually takes place, affecting the conversion from independent to dependent rates. Generally speaking, there are three main scenarios to consider: Occurs only at the beginning of the year Occurs only at the end of the year Occurs only at some point during the year Tip It is NOT recommended to memorize any of the results in this section as the the situation will change from question to question. It is much better to understand the principles behind the calculation so that no matter what combination is given, the probabilities can be easily calculated. Warning The timing of the decrements only affects the split between the dependent decrements. Since the underlying independent rates did not change, the total probabilities did not change as well. Beginning of Year \u00b6 Consider a simple model with two decrements: Decrement 1 : Acts throughout the year Decrement 2 : Acts only at the beginning of the year The key is to understand that although decrement 1 acts throughout the year, it does not precisely act at the beginning , thus decrement 2 can act without competition . Since decrement 2 is the first decrement without competition , the independent and dependent rates are the same: \\[ q^{(2)}_{x} = q^{'(2)}_{x} \\] Tip The key is to understand the two components that allow the relationship to be true: First Decrement : Acts on the starting population No Competition : No other decrements to reduce the population Thus, the dependent rates also act on the starting population (same as independent). Even though decrement 1 acts without competition, the population has already been decremented by decrement 2. Thus, it acts on the remaining population : \\[ q^{(1)}_{x} = q^{'(1)}_{x} \\cdot (1 - q^{'(2)}_{x}) \\] End of Year \u00b6 Consider a simple model with two decrements: Decrement 1 : Acts throughout the year Decrement 2 : Acts only at the end of the year Using the same logic as before, decrement 1 is the first decrement to act without competition: \\[ q^{(1)}_{x} = q^{'(1)}_{x} \\] Warning Do not mistakenly assume that the above relationship only holds for exact age decrements or non-exact decrements - it can be either of the two . Similarly, decrement 2 acts on the remaining population : \\[ q^{(2)}_{x} = q^{'(2)}_{x} \\cdot (1 - q^{'(1)}_{x}) \\] An alternative method (also applicable for BOY scenario) is to take find the other component as the balancing item (assuming it is given): \\[ q^{(2)}_{x} = q^{(\\tau)}_{x} - q^{(1)}_{x} \\] Tip There is no need to memorize any of the above formulas - they are rather intuitive and can be derived on the fly. Middle of Year \u00b6 Consider a simple model with two decrements: Decrement 1 : Acts throughout the year Decrement 2 : Acts only at the some point during the year Unlike the other two scenarios, although the decrements are acting without competition, they are not able to act on the entire starting population, thus there is no equality between the dependent and independent rate. Decrement 2 occurs at some point \\(s\\) during the year. At this point, decrement 1 has already acted for \\(s\\) months, thus decrement 2 acts on the remaining population : \\[ \\begin{aligned} q^{(2)}_{x} &= q^{'(2)}_{x} \\cdot (1 - {}_{s}q^{'(1)}_{x}) \\\\ &= q^{'(2)}_{x} \\cdot (1 - s \\cdot q^{'(1)}_{x}) \\end{aligned} \\] The easiest method to solve for decrement 1 is to use the complement method: \\[ q^{(1)}_{x} = q^{(\\tau)}_{x} - q^{(2)}_{x} \\] However, it is possible to solve for the probability directly. There are only two scenarios that the policyholder can fail to decrement 1: Policyholder fails to decrement 1 between time \\(0\\) and \\(s\\) Policyholder survives decrement 1 from till time \\(s\\) , survives all of decrement 2 but fails to decrement 1 between time \\(s\\) to time \\(1\\) \\[ \\begin{aligned} q^{(1)}_{x} &= \\text{P(Scenario 1)} + \\text{P(Scenario 2)} \\\\ &= {}_{s}q^{'(1)}_{x} + {}_{s}p^{'(1)}_{x} \\cdot p^{'(2)}_{x} \\cdot {}_{1-s}q^{'(1)}_{x+s} \\\\ &= s \\cdot q^{'(1)}_{x} + {}_{s}p^{'(1)}_{x} \\cdot p^{'(2)}_{x} \\cdot \\frac{(1-s) \\cdot q^{'(1)}_{x}}{{}_{s}p^{'(1)}_{x}} \\\\ &= s \\cdot q^{'(1)}_{x} + \\cdot p^{'(2)}_{x} \\cdot (1-s) q^{'(1)}_{x} \\end{aligned} \\] Warning It is a common mistake to apply the UDD assumption directly to the above equation: \\[ {}_{1-s}q^{'(1)}_{x+s} \\ne (1-s) q^{'(1)}_{x} \\] This is because the UDD assumption is only applicable to an individual who is alive at the start of the year . Thus, only when survival probability is accounted for can it be used: \\[ {}_{s}p^{'(1)}_{x} \\cdot {}_{1-s}q^{'(1)}_{x+s} = (1-s) q^{'(1)}_{x} \\] This can be proven via conditional probability: \\[ \\begin{aligned} {}_{s}p^{'(1)}_{x} \\cdot {}_{1-s}q^{'(1)}_{x+s} &= \\text{P(Fail from s to 1 | Survive to s)} \\\\ &= \\frac{\\text{P(Fail from s to 1 and survive to s)}}{\\text{P(Survive to s)}} \\\\ &= \\frac{q^{'(1)}_{x} - {}_{s}q^{'(1)}_{x}}{{}_{s}p^{'(1)}_{x}} \\\\ &= \\frac{q^{'(1)}_{x} - s \\cdot q^{'(1)}_{x}}{{}_{s}p^{'(1)}_{x}} \\\\ &= \\frac{(1-s) \\cdot q^{'(1)}_{x}}{{}_{s}p^{'(1)}_{x}} \\end{aligned} \\] BOY Combination \u00b6 Consider the following scenario: Decrement 1 and 2 occur throughout the year Decrement 3 occurs at the beginning of the year Using the principles established before: \\[ \\begin{aligned} q^{(3)}_{x} &= q^{'(3)}_{x} \\\\ q^{(1)}_{x} &= p^{'(3)}_{x} \\cdot q^{'(1)}_{x} \\cdot (1 - \\frac{1}{2} \\cdot q^{'(2)}_{x}) \\\\ q^{(2)}_{x} &= p^{'(3)}_{x} \\cdot q^{'(2)}_{x} \\cdot (1 - \\frac{1}{2} \\cdot q^{'(1)}_{x}) \\\\ \\end{aligned} \\] EOY Combination \u00b6 Consider the following scenario: Decrement 1 and 2 occur throughout the year Decrement 3 occurs at the end of the year \\[ \\begin{aligned} q^{(1)}_{x} &= q^{'(1)}_{x} \\cdot (1 - \\frac{1}{2} \\cdot q^{'(2)_{x}}) \\\\ q^{(2)}_{x} &= q^{'(2)}_{x} \\cdot (1 - \\frac{1}{2} \\cdot q^{'(1)_{x}}) \\\\ q^{(3)}_{x} &= (1 - q^{'(1)}_{x}) \\cdot (1 - q^{'(2)}_{x}) \\cdot q^{'(3)}_{x} \\end{aligned} \\] MOY Combination \u00b6 Consider the following scenario: Decrement 1 occurs throughout the year Decrement 2 occurs in the middle of the year Decrement 3 occurs at the end of the year \\[ \\begin{aligned} q^{(2)}_{x} &= (1 - {}_{s}q^{'(1)}_{x}) \\cdot q^{'(2)}_{x} \\\\ q^{(1)}_{x} &= {}_{s}q^{'(1)}_{x} + {}_{s}p^{'(1)}_{x} \\cdot {}_{s}p^{'(2)}_{x} \\cdot {}_{1-s}q^{'(2)}_{x} \\\\ q^{(3)}_{x} &= (1 - q^{'(1)}_{x}) \\cdot (1 - q^{'(2)}_{x}) \\cdot q^{'(3)}_{x} \\end{aligned} \\] Tip The most important thing to remember is that all the above equations are attempting to calculate dependent rates from independent ones . Insurance Applications \u00b6 Multi-decrement assurances and annuities are concpetually similar to their single decrement counterparts - Equivalence Principle, Approximation etc. The key difference is that the benefit is the combination of multiple benefits , typically for each decrement in the model: \\[ \\text{EPV Benefit} = \\text{EPV Benefit 1} + \\text{EPV Benefit 2} + \\dots \\] Warning Not all benefits may be a lump sum payment. It is possible to receive an annuity as a benefit (EG. Receiving 5 years of payments upon diagnosis of a disability). \\[ \\text{\"Annuity\" Benefit} = \\text{PV (Payments)} \\] Random Variable \u00b6 Recall that in FAM-L, we derived the EPV of the contract using a bottom\u2019s up approach with the Random Variable denoting the PV of the benefits . For a multi-decrement policy, there are different benefits for each decrement. Thus, the random variable is expressed as a Piecewise Function for each decrement: \\[ \\begin{aligned} Z &= \\begin{cases} B_{1} \\cdot v^{k+1}, & {}_{k \\mid}q^{(1)}_{x} \\\\ B_{2} \\cdot v^{k+1}, & {}_{k \\mid}q^{(2)}_{x} \\end{cases} \\end{aligned} \\] Using this, we can derive an expression for the EPV of the policy: \\[ \\begin{aligned} \\text{EPV} &= E(Z) \\\\ &= \\sum B_{1} v^{k+1} \\cdot {}_{k \\mid}q^{(1)}_{x} + \\sum B_{2} v^{k+1} \\cdot {}_{k \\mid}q^{(2)}_{x} \\\\ &= \\sum B_{1} v^{k+1} \\cdot {}_{k}p^{(\\tau)}_{x} q^{(1)}_{x+k} + \\sum B_{2} v^{k+1} \\cdot {}_{k}p^{(\\tau)}_{x} q^{(2)}_{x+k} \\end{aligned} \\] The key difference is how the Variance is calculated. Since there are now two different benefits, there is a correlation term that must be handled: \\[ \\begin{aligned} E(Z^{2}) &= \\sum \\left(B_{1} \\cdot v^{k+1} \\right)^{2} \\cdot {}_{k \\mid}q^{(1)}_{x} + \\sum \\left(B_{2} \\cdot v^{k+1} \\right)^{2} \\cdot {}_{k \\mid}q^{(2)}_{x} \\\\ [E(Z)]^{2} &= \\left (\\sum B_{1} v^{k+1} \\cdot {}_{k}p^{(\\tau)}_{x} q^{(1)}_{x+k} \\right)^{2} \\\\ &+ \\left (\\sum B_{2} v^{k+1} \\cdot {}_{k}p^{(\\tau)}_{x} q^{(2)}_{x+k} \\right)^{2} \\\\ &+ 2 \\cdot \\left (\\sum B_{2} v^{k+1} \\cdot {}_{k}p^{(\\tau)}_{x} q^{(2)}_{x+k} \\right) \\cdot \\left(\\sum B_{2} v^{k+1} \\cdot {}_{k}p^{(\\tau)}_{x} q^{(2)}_{x+k} \\right) \\end{aligned} \\] Warning The notation for these type of questions are extremely heavy, there is a high tendency to make mistakes as a result. Surrender Benefit \u00b6 The most common additional benefit is the Surrender Benefit . In practice, virtually all whole life insurance policies offer a surrender benefit of the policy. It is also known as the cash value of the policy. However, surrender benefits are not typically offered for annuities. This is to avoid Adverse Selection : Goal of risk pooling annuities is to use the excess funds from those who die early to fund those who live longer This is why annuities are often offerred without underwriting ; it is in the insurer's favour if someone of poor health joins the pool However, if a surrender benefit is offered, those in poor health will be incentivized to surrender, leaving the pool with only healthy individuals who tend to live long lives Tip Some questions may not directly call it a surrender benefit. It could be phrased as a conversion from this policy type to another. Conversions usually occur in the following mechanism - the existing policy is surrendered for its cash value , and used to purchase a new single premium policy of the new type. Note This is the opposite of insurance , where the goal of the risk pool is to use the excess funds from those live longer fund those who die younger. Thus, the insurer only wants those in relatively good health to join the pool, which is why there is a rigorous underwriting process . Adverse Selection will result in actual experience being far worse than what was assumed during pricing, which would lead to the policies being under-reserved, which could threaten the solvency of the insurer. Thus, there are two approaches to deal with this: Preventive measures to avoid adverse selection (Usual approach) Account for adverse selection during pricing stage Note Adverse Selection is a situation where the policyholder has more information about their risk compared to the insurer (Asymmetric Information). The policyholder will then choose the option that is most beneficial to them - choosing high insurance coverage or in the above case, surrendering an annuity for cash value (if possible).","title":"Multi Decrement Models"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#multiple-decrement-model","text":"","title":"Multiple Decrement Model"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#overview","text":"In FAM-L, we only considered simple insurance policies which paid only a death benefit. In reality, insurance contracts often pay one of a few possible benefits: Benefit Trigger Death Benefit Death by natural causes Accidental Death Benefit Death by accident Surrender Benefit Lapsing the policy Dread Disease Benefit Contracting Dread Disease Note Functionally, multi-decrement models can be understood as a one to many multi-state model . For the purposes of this exam, unless otherwise stated, it is assumed that these type of policies only pay ONE of the benefits. Making a claim on either of the benefits will trigger a claim and then terminate the policy . The different benefit triggers are different ways that the policy can terminate , known as a Decrement . Thus, the simple policy considered in FAM-L is an example of a Single Decrement Model while the policy described above is an example of a Multiple Decrement Model . Info The definition of a Decrement is \"to decrease\". In the context of life insurance, it decreases the number of active policies.","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#probability","text":"Previously, since death was the only decrement, we only considered the probability of death. However, in a multiple decrement setting, we more generally consider the probability of termination . The multiple decrement model has two components: Discrete Random Variable \\(J\\) - Denoting the decrements in the model Continuous Random Variable \\(T\\) - Denoting the time to termintaton based on that decrement Combining the two, the probability of termination in \\(t\\) years due to a particular decrement \\(j\\) : \\[ \\begin{aligned} {}_{t}q^{(j)}_{x} &= P(T_{x} \\lt t, J_{x} = j) \\\\ &= \\int^{t}_{0} f_{T_{x}, J_{x}}(t, j) \\end{aligned} \\] The probability of termination due to ANY DECREMENT in \\(t\\) years is the sum of all individual probabilities of all \\(m\\) decrements: \\[ \\begin{aligned} {}_{t}q^{(\\tau)}_{x} &= {}_{t}q^{(1)}_{x} + {}_{t}q^{(2)}_{x} + \\dots + {}_{t}q^{(m)}_{x} \\\\ &= \\sum^{j=m}_{j=1} {}_{t}q^{(j)}_{x} \\end{aligned} \\] Note \\(\\tau\\) is the notation used the denote ALL possible decrements. The above is valid because we assumed that the policyholder can only terminate from ONE of the decrements. Thus, each decrement is mutually exclusive , where the probability of termination from any cause is the sum of each individual probability (OR logic). Thus, the probability of surviving \\(t\\) years is denoted as: \\[ \\begin{aligned} {}_{t}p^{(\\tau)}_{x} &= 1 - {}_{t}q^{(\\tau)}_{x} \\\\ &= e^{- \\int \\mu^{(\\tau)}_{x+t}} \\end{aligned} \\] Warning There is NO such thing as \\({}_{t}p^{(j)}_{x}\\) ! The above would be the probability of surviving \\(t\\) years from decrement \\(j\\) . However, even if they do not terminate from decrement \\(j\\) , they can still terminate from OTHER decrements . Ultimately, we are only concerned whether the individual is still alive at time \\(t\\) , not whether or not they survive a particular decrement. The force of decrement has similar properties as the termination probability: \\[ \\begin{aligned} \\mu^{(j)}_{x}(t) &= \\frac{\\frac{d}{dt} {}_{t}q^{(j)}_{x}}{{}_{t}p^{(\\tau)}_{x}} \\\\ \\\\ \\mu^{(\\tau)}_{x}(t) &= \\frac{\\frac{d}{dt} {}_{t}q^{(\\tau)}_{x}}{{}_{t}p^{(\\tau)}_{x}} \\\\ &= \\frac{1}{{}_{t}p^{(\\tau)}_{x}} \\left (\\frac{d}{dt} {}_{t}q^{(\\tau)}_{x} \\right) \\\\ &= \\frac{1}{{}_{t}p^{(\\tau)}_{x}} \\frac{d}{dt} \\left[{}_{t}q^{(1)}_{x} + \\dots {}_{t}q^{(m)}_{x} \\right] \\\\ &= \\mu^{(1)}_{x+t} + \\dots + \\mu^{(m)}_{x+t} \\end{aligned} \\] Thus, the PDF of the joint distribution can be expressed as: \\[ \\begin{aligned} \\mu^{(j)}_{x}(t) &= \\frac{\\frac{d}{dt} {}_{t}q^{(j)}_{x}}{{}_{t}p^{(\\tau)}_{x}} \\\\ \\frac{d}{dt} {}_{t}q^{(j)}_{x} &= {}_{t}p^{(\\tau)}_{x} \\cdot \\mu^{(j)}_{x+t} \\\\ {}_{t}q^{(j)}_{x} &= \\int^{t}_{0} {}_{t}p^{(\\tau)}_{x} \\cdot \\mu^{(j)}_{x+t} \\end{aligned} \\] Tip Given that it can be expressed as a multi-state model, the second line in the above equation is the Kolmogorov's Forward Equation : \\[ \\text{Change} = \\text{Probability IN} - \\text{Probability OUT} \\] For multi-decrement models, there is only a one directional path to each decremented state, which are always absorbing . Thus, the second component is always 0. Tip Questions may sometimes provide the forces of decrement for each of the individual decrements and asked to calculate the associated termination probability for each. Given the properties of the multiple decrement model, this problem can be simplified to avoid integrating multiple times. It may not always be clear on first sight, but for most euqestions, the forces of decrement can be shown to be a fraction of the total force of mortality : \\[ \\mu^{(j)}_{x}(t) = c \\cdot \\mu^{(\\tau)}_{x} \\] Assuming this relationship exists, we can show that the termintation probability for a particular decrement is the same fraction of the total termination probability: \\[ \\begin{aligned} {}_{t}q^{(j)}_{x} &= \\int {}_{t}p^{(\\tau)}_{x} \\cdot \\mu^{(j)}_{x+t} \\\\ &= \\int {}_{t}p^{(\\tau)}_{x} \\cdot c \\cdot \\mu^{(\\tau)}_{x+t} \\\\ &= c \\cdot \\int {}_{t}p^{(\\tau)}_{x} \\mu^{(\\tau)}_{x+t} \\\\ &= c \\cdot {}_{t}q^{(\\tau)}_{x} \\end{aligned} \\] Similar to the multi life section, all other results not explicitly covered above are analagous to their single decrement counterparts . For most items, simply add the superscript (j) to indicate that the probability is for a particular decrement.","title":"Probability"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#independent-rates","text":"In a multi-decrement setting, there are multiple decrements acting on the policyholder, which are said to be competing with one another to terminate the policy. The rates shown so far are known as Dependent Rates , which implicitly assumes that the competition among decrements has already been accounted for. Dependent Rates should be used for projection to reflect the true underlying situation. However, decrements are often studied independently , resulting in Independent Rates : Dependent Rates Independent Rates Presence of other decrements Absence of other decrements Competing Not competing Non-prime notation Prime notation \\({}_{t}q^{(j)}_{x}\\) \\({}_{t}q^{'(j)}_{x}\\) The key is understanding the differences and how to convert from one form to the other.","title":"Independent Rates"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#key-intuition","text":"Consider the following scenario: There are 1000 lives in the population The independent death rate is 20% ( \\(q^{(\\text{Death})}_{x}\\) ) The independent lapse rate is 10% ( \\(q^{(\\text{Death})}_{x}\\) ) Consider the following outcomes: If death was the only decrement, we expect 200 people to die by the end of the year If lapse was the only decrement, we expect 100 people to lapse by the end of the year If both death and lapse are present, we would NOT expect 200 deaths and 100 lapses The third point is true because: Someone who might have died in the later part of the year might have already lapsed in the earlier part Someone who might have lapsed in the later part of the year might have already died in the earlier part Thus, we expect fewer deaths and lapses when there are multiple decrements acting simultaneously Note The proper terminology to use when explaining this is that the lives are exposed to the force of death or lapse respectively. The above is known as the Theory of Competing Risk , which leads to the following relationship: \\[ {}_{t}q^{'(j)}_{x} \\ge {}_{t}q^{(j)}_{x} \\] Tip Recall that \\({}_{}p^{(j)}_{x}\\) was not a valid expression as it did not make sense to just survive one of the many decrements. However, under an independent rate basis, there is only ONE decrement being considered, thus \\({}_{}p^{'(j)}_{x}\\) is a valid expression .","title":"Key Intuition"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#relationship","text":"Recall that the survival model in FAM-L is essentially a single decrement model. Thus, the key results can be applied here as well: \\[ \\begin{aligned} \\mu^{(j)}_{x+t} &= \\frac{\\frac{d}{dt} {}_{}q^{'(j)}_{x}}{{}_{}p^{'(j)}_{x}} \\\\ \\\\ {}_{}q^{'(j)}_{x} &= \\int {}_{}p^{'(j)}_{x} \\cdot \\mu^{(j)}_{x+t} \\\\ &= 1 - {}_{}p^{'(j)}_{x} \\\\ \\\\ {}_{}p^{'(j)}_{x} &= e^{- \\int \\mu^{(j)}_{x+t}} \\end{aligned} \\] The key is understanding that the force of decrement is the SAME under a dependent and an independent basis. This is because the force acts instantly - even under a dependent basis, there is no competition at an instant. Warning This means that there is NO such thing as \\(\\mu^{'(j)}_{x+t}\\) ! Thus, both dependent and independent rates share the same total force of decrements and total probabilities as well: \\[ \\begin{aligned} \\mu^{(\\tau)}_{x+t} &= \\mu^{(1)}_{x+t} + \\dots + \\mu^{(m)}_{x+t} \\\\ \\\\ \\therefore {}_{}p^{(\\tau)}_{x} &= e^{- \\int \\mu^{(\\tau)}_{x}} \\\\ &= e^{- \\int (\\mu^{(1)}_{x+t} + \\dots + \\mu^{(m)}_{x+t})} \\\\ &= e^{- \\int \\mu^{(1)}_{x+t}} \\cdot \\dots \\cdot e^{- \\int \\mu^{(m)}_{x+t}} \\\\ &= {}_{}p^{'(1)}_{x+t} \\cdot \\dots \\cdot {}_{}p^{'(m)}_{x+t} \\end{aligned} \\] Note This is conceptually identical to the probability of independent events occuring together : \\[ \\text{P(A and B)} = \\text{P(A)} \\cdot \\text{P(B)} \\]","title":"Relationship"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#conversion","text":"The key principle is to form an expression equating the two basis together , and solving for the missing component based on the given information: \\[ \\begin{array}{|c|c|c|} \\hline \\text{} & \\text{Independent Rates} & \\text{Dependent Rates} \\\\ \\hline \\text{Force} & \\mu^{(j)}_{x} = \\frac{- \\frac{d}{dt} {}_{t}p^{'(j)}}{{}_{t}p^{'(j)}} & \\mu^{(j)}_{x} = \\frac{\\frac{d}{dt} {}_{t}q^{(j)}}{{}_{t}p^{(\\tau)}} \\\\ \\hline \\text{Survival} & {}_{t}p^{(\\tau)}_{x} = \\prod {}_{t}p^{'(j)} & {}_{t}p^{(\\tau)}_{x} = 1 - {}_{t}q^{(\\tau)} \\\\ \\hline \\text{Termination} & {}_{t}q^{(\\tau)}_{x} = 1 - {}_{t}p^{(\\tau)} & {}_{t}q^{(\\tau)}_{x} = \\sum {}_{t}q^{(j)} \\\\ \\hline \\end{array} \\] Unless otherwise stated, the rates provided should be assumed to be independent rates . This is important as this concept is applicable to every other chapter in the exam. Questions of this nature tend to involve fractional times (though not always). Similar to before, additional assumptions are required in order to determine the probabilities. These add another layer of complexity to the conversion.","title":"Conversion"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#constant-force","text":"Recall that under a constant force of mortality, the survival probabilities can be simplified to the following: \\[ \\begin{aligned} {}_{t}p^{'(j)}_{x} &= e^{- \\mu^{(j)}_{x} \\cdot t} \\\\ \\\\ {}_{t}p^{(\\tau)}_{x} &= e^{- \\mu^{(\\tau)}_{x} \\cdot t} \\end{aligned} \\] Since the force is a constant , we can directly manipulate the powers to convert from the total probability to the individual decrements: \\[ \\begin{aligned} {}_{t}p^{(\\tau)}_{x} &= e^{- \\mu^{(\\tau)}_{x} \\cdot t} \\\\ \\left({}_{t}p^{(\\tau)}_{x} \\right)^{\\frac{\\mu^{(j)}_{x}}{\\mu^{(\\tau)}_{x}}} &= \\left(e^{- \\mu^{(\\tau)}_{x} \\cdot t} \\right)^{\\frac{\\mu^{(j)}_{x}}{\\mu^{(\\tau)}_{x}}} \\\\ \\left({}_{t}p^{(\\tau)}_{x} \\right)^{\\frac{\\mu^{(j)}_{x}}{\\mu^{(\\tau)}_{x}}} &= e^{- \\mu^{(j)}_{x} \\cdot t} \\\\ {}_{t}p^{'(j)}_{x} &= \\left({}_{t}p^{(\\tau)}_{x} \\right)^{\\frac{\\mu^{(j)}_{x}}{\\mu^{(\\tau)}_{x}}} \\end{aligned} \\] A more intuitive way to remember the formula would be to remember the ratio : \\[ \\begin{aligned} {}_{t}p^{'(j)}_{x} &= \\left({}_{t}p^{(\\tau)}_{x} \\right)^{\\frac{\\mu^{(j)}_{x}}{\\mu^{(\\tau)}_{x}}} \\\\ \\ln {}_{t}p^{'(j)}_{x} &= {\\frac{\\mu^{(j)}_{x}}{\\mu^{(\\tau)}_{x}}} \\cdot \\ln {}_{t}p^{'(\\tau)}_{x} \\\\ \\frac{\\ln {}_{t}p^{'(j)}_{x}}{\\ln {}_{t}p^{(\\tau)}_{x}} &= {\\frac{\\mu^{(j)}_{x}}{\\mu^{(\\tau)}_{x}}} \\end{aligned} \\] If the ratio of the force is not available, it can be alternatively calculated via the termination probability : \\[ \\begin{aligned} {}_{t}q^{(j)}_{x} &= \\int {}_{t}p^{(\\tau)}_{x} \\cdot \\mu^{(j)}_{x+t} \\\\ &= \\frac{\\mu^{(\\tau)}_{x+t}}{\\mu^{(\\tau)}_{x+t}} \\cdot \\int {}_{t}p^{(\\tau)}_{x} \\cdot \\mu^{(j)}_{x+t} \\\\ &= \\frac{\\mu^{(j)}_{x+t}}{\\mu^{(\\tau)}_{x+t}} \\cdot \\int {}_{t}p^{(\\tau)}_{x} \\cdot \\mu^{(\\tau)}_{x+t} \\\\ &= \\frac{\\mu^{(j)}_{x+t}}{\\mu^{(\\tau)}_{x+t}} \\cdot {}_{t}q^{(\\tau)}_{x+t} \\\\ \\frac{\\mu^{(j)}_{x+t}}{\\mu^{(\\tau)}_{x+t}} &= \\frac{{}_{t}q^{(j)}_{x}}{{}_{t}q^{(\\tau)}_{x}} \\\\ \\\\ \\therefore \\frac{\\ln {}_{t}p^{'(j)}_{x}}{\\ln {}_{t}p^{(\\tau)}_{x}} &= {\\frac{\\mu^{(j)}_{x}}{\\mu^{(\\tau)}_{x}}} = \\frac{{}_{t}q^{(j)}_{x}}{{}_{t}q^{(\\tau)}_{x}} \\end{aligned} \\]","title":"Constant Force"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#dependent-udd","text":"In a multi-decrement context, UDD stands for Uniform Distribution of Decrements (NOT death). However, because there are two sets of rates (Dependent vs Independent), there are two different UDD assumptions - based on which set is UDD. This section assumes that the Dependent Rates are distributed UDD. It follows the same principles covered in FAM-L: \\[ {}_{t}q^{(j)}_{x} = t \\cdot q^{(j)}_{x} \\] Using the above, the force of decrement can be expressed as the following: \\[ \\begin{aligned} {}_{t}q^{(j)}_{x} &= t \\cdot q^{(j)}_{x} \\\\ t \\cdot q^{(j)}_{x} &= \\int {}_{t}p^{(\\tau)}_{x} \\cdot \\mu^{(j)}_{x+t} \\\\ q^{(j)}_{x} &= {}_{t}p^{(\\tau)}_{x} \\cdot \\mu^{(j)}_{x+t}, \\ \\text{Differentiation} \\\\ \\mu^{(j)}_{x+t} &= \\frac{q^{(j)}_{x}}{{}_{t}p^{(\\tau)}_{x}} \\\\ \\end{aligned} \\] Thus, the independent survival probability can be expressed as: \\[ \\begin{aligned} {}_{t}p^{'(j)}_{x} &= e^{- \\int \\mu^{(j)}_{x+t}} \\\\ &= e^{- \\int \\frac{q^{(j)}_{x}}{{}_{t}p^{(\\tau)}_{x+t}}} \\\\ &= e^{-q^{(j)}_{x}} \\cdot e^{\\int \\frac{1}{1 - t \\cdot q^{(\\tau)}_{x}}} \\\\ &= e^{-q^{(j)}_{x}} \\cdot e^{\\frac{\\ln 1 - t \\cdot q^{(\\tau)}_{x}}{- q^{(\\tau)}_{x}}} \\\\ &= e^{-q^{(j)}_{x}} \\cdot e^{\\frac{\\ln {}_{t}p^{(\\tau)}_{x}}{- q^{(\\tau)}_{x}}} \\\\ &= e^{\\ln {}_{t}p^{(\\tau)} \\cdot \\frac{-q^{(j)}_{x}}{-q^{(\\tau)}_{x}}} \\\\ &= \\left(e^{\\ln {}_{t}p^{(\\tau)}} \\right)^{\\frac{q^{(j)}_{x}}{q^{(\\tau)}_{x}}} \\\\ &= \\left({}_{t}p^{(\\tau)} \\right)^{\\frac{q^{(j)}_{x}}{q^{(\\tau)}_{x}}} \\\\ \\\\ \\therefore \\frac{\\ln {}_{t}p^{'(j)}_{x}}{\\ln {}_{t}p^{(\\tau)}_{x}} &= \\frac{{}_{t}q^{(j)}_{x}}{{}_{t}q^{(\\tau)}_{x}} = \\frac{q^{(j)}_{x}}{q^{(\\tau)}_{x}} \\end{aligned} \\] Notice that is the exact SAME RESULT as the constant force scenario! Thus, it is sufficient to remember the ratio . Tip The key to the proof is understanding that the survival probability MUST be converted into into the termination probability (under UDD) in order to simplify the integration: \\[ \\begin{aligned} {}_{t}p^{(\\tau)}_{x} &= 1 - {}_{t}q^{(\\tau)}_{x} \\\\ &= 1 - t \\cdot q^{(\\tau)}_{x} \\end{aligned} \\]","title":"Dependent UDD"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#independent-udd","text":"Conversely, this section assumes that the independent rates are distributed UDD: \\[ {}_{t}q^{'(j)}_{x} = t \\cdot q^{'(j)}_{x} \\] Going through a similar process as dependent UDD, we can express the independent termination probability as: \\[ \\begin{aligned} {}_{t}q^{'(j)}_{x} &= t \\cdot q^{'(j)}_{x} \\\\ t \\cdot q^{'(j)}_{x} &= \\int {}_{t}p^{'(j)}_{x} \\cdot \\mu^{(j)}_{x+t} \\\\ q^{'(j)}_{x} &= {}_{t}p^{'(j)}_{x} \\cdot \\mu^{(j)}_{x+t}, \\ \\text{Differentiation} \\\\ \\end{aligned} \\] Thus, the dependent termination probability can be expressed as: \\[ \\begin{aligned} {}_{t}q^{(j)}_{x} &= \\int {}_{t}p^{(\\tau)}_{x} \\cdot \\mu^{(j)}_{x} \\\\ &= \\int {}_{t}p^{(\\tau)}_{x} \\prod_{i \\ne j} {}_{t}p^{'(i)}_{x} \\cdot \\mu^{(j)}_{x} \\\\ &= \\int {}_{t}p^{'(j)}_{x} \\cdot \\mu^{(j)}_{x} \\cdot \\prod_{i \\ne j} {}_{t}p^{'(i)}_{x} \\\\ &= \\int q^{'(j)}_{x} \\cdot \\prod_{i \\ne j} {}_{t}p^{'(i)}_{x} \\\\ &= q^{'(j)}_{x} \\int \\prod_{i \\ne j} {}_{t}p^{'(i)}_{x} \\end{aligned} \\] Tip The key is understanding that the termination probability is a constant within the integration, thus can be factorized out. The final expression depends on the number of decrements in the model. The most commonly tested variation is the two decrement model : \\[ \\begin{aligned} {}_{t}q^{(1)}_{x} &= q^{'(1)}_{x} \\cdot \\int {}_{t}p^{'(1)}_{x} \\\\ &= q^{'(1)}_{x} \\cdot \\int \\left(1 - t \\cdot {}_{}q^{'(2)}_{x} \\right) \\\\ &= q^{'(1)}_{x} \\cdot \\left(t - \\frac{t^2}{2} {}_{}q^{'(2)}_{x} \\right) \\\\ &= t \\cdot q^{'(1)}_{x} \\cdot \\left(1 - \\frac{t}{2} {}_{}q^{'(2)}_{x} \\right) \\\\ \\\\ \\therefore {}_{t}q^{(2)}_{x} &= t \\cdot q^{'(2)}_{x} \\cdot \\left(1 - \\frac{t}{2} {}_{}q^{'(1)}_{x} \\right) \\\\ \\end{aligned} \\] Tip Assume that \\(t=1\\) , \\[ \\begin{aligned} {}_{t}q^{(1)}_{x} &= q^{'(1)}_{x} \\cdot \\left(1 - \\frac{1}{2} {}_{}q^{'(2)}_{x} \\right) \\\\ {}_{t}q^{(2)}_{x} &= q^{'(2)}_{x} \\cdot \\left(1 - \\frac{1}{2} {}_{}q^{'(1)}_{x} \\right) \\\\ \\end{aligned} \\] Decrement 1 and 2 are happening simultaneously and uniformly throughout the year. The intuition is that, on average , HALF the population would have been decremented by lapse by before death can act on it; vice-versa. Less commonly tested is the Three Decrement Model . The proof is similar to above, but much more tedious due to the additional product. Thus, it is recommended to memorize the result instead: \\[ {}_{t}q^{(1)}_{x} = t \\cdot q^{'(1)}_{x} \\cdot \\left(1 - \\frac{t}{2} {}_{}q^{'(2)}_{x} - \\frac{t}{2} {}_{}q^{'(3)}_{x} + \\frac{t}{3} {}_{}q^{'(2)}_{x} {}_{}q^{'(3)}_{x} \\right) \\]","title":"Independent UDD"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#exact-age-decrements","text":"In the above UDD section, we have assumed that the decrements are acting throughout the entire year . However, decrements could instead occur at exact times , in which case no competition actually takes place, affecting the conversion from independent to dependent rates. Generally speaking, there are three main scenarios to consider: Occurs only at the beginning of the year Occurs only at the end of the year Occurs only at some point during the year Tip It is NOT recommended to memorize any of the results in this section as the the situation will change from question to question. It is much better to understand the principles behind the calculation so that no matter what combination is given, the probabilities can be easily calculated. Warning The timing of the decrements only affects the split between the dependent decrements. Since the underlying independent rates did not change, the total probabilities did not change as well.","title":"Exact Age Decrements"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#beginning-of-year","text":"Consider a simple model with two decrements: Decrement 1 : Acts throughout the year Decrement 2 : Acts only at the beginning of the year The key is to understand that although decrement 1 acts throughout the year, it does not precisely act at the beginning , thus decrement 2 can act without competition . Since decrement 2 is the first decrement without competition , the independent and dependent rates are the same: \\[ q^{(2)}_{x} = q^{'(2)}_{x} \\] Tip The key is to understand the two components that allow the relationship to be true: First Decrement : Acts on the starting population No Competition : No other decrements to reduce the population Thus, the dependent rates also act on the starting population (same as independent). Even though decrement 1 acts without competition, the population has already been decremented by decrement 2. Thus, it acts on the remaining population : \\[ q^{(1)}_{x} = q^{'(1)}_{x} \\cdot (1 - q^{'(2)}_{x}) \\]","title":"Beginning of Year"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#end-of-year","text":"Consider a simple model with two decrements: Decrement 1 : Acts throughout the year Decrement 2 : Acts only at the end of the year Using the same logic as before, decrement 1 is the first decrement to act without competition: \\[ q^{(1)}_{x} = q^{'(1)}_{x} \\] Warning Do not mistakenly assume that the above relationship only holds for exact age decrements or non-exact decrements - it can be either of the two . Similarly, decrement 2 acts on the remaining population : \\[ q^{(2)}_{x} = q^{'(2)}_{x} \\cdot (1 - q^{'(1)}_{x}) \\] An alternative method (also applicable for BOY scenario) is to take find the other component as the balancing item (assuming it is given): \\[ q^{(2)}_{x} = q^{(\\tau)}_{x} - q^{(1)}_{x} \\] Tip There is no need to memorize any of the above formulas - they are rather intuitive and can be derived on the fly.","title":"End of Year"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#middle-of-year","text":"Consider a simple model with two decrements: Decrement 1 : Acts throughout the year Decrement 2 : Acts only at the some point during the year Unlike the other two scenarios, although the decrements are acting without competition, they are not able to act on the entire starting population, thus there is no equality between the dependent and independent rate. Decrement 2 occurs at some point \\(s\\) during the year. At this point, decrement 1 has already acted for \\(s\\) months, thus decrement 2 acts on the remaining population : \\[ \\begin{aligned} q^{(2)}_{x} &= q^{'(2)}_{x} \\cdot (1 - {}_{s}q^{'(1)}_{x}) \\\\ &= q^{'(2)}_{x} \\cdot (1 - s \\cdot q^{'(1)}_{x}) \\end{aligned} \\] The easiest method to solve for decrement 1 is to use the complement method: \\[ q^{(1)}_{x} = q^{(\\tau)}_{x} - q^{(2)}_{x} \\] However, it is possible to solve for the probability directly. There are only two scenarios that the policyholder can fail to decrement 1: Policyholder fails to decrement 1 between time \\(0\\) and \\(s\\) Policyholder survives decrement 1 from till time \\(s\\) , survives all of decrement 2 but fails to decrement 1 between time \\(s\\) to time \\(1\\) \\[ \\begin{aligned} q^{(1)}_{x} &= \\text{P(Scenario 1)} + \\text{P(Scenario 2)} \\\\ &= {}_{s}q^{'(1)}_{x} + {}_{s}p^{'(1)}_{x} \\cdot p^{'(2)}_{x} \\cdot {}_{1-s}q^{'(1)}_{x+s} \\\\ &= s \\cdot q^{'(1)}_{x} + {}_{s}p^{'(1)}_{x} \\cdot p^{'(2)}_{x} \\cdot \\frac{(1-s) \\cdot q^{'(1)}_{x}}{{}_{s}p^{'(1)}_{x}} \\\\ &= s \\cdot q^{'(1)}_{x} + \\cdot p^{'(2)}_{x} \\cdot (1-s) q^{'(1)}_{x} \\end{aligned} \\] Warning It is a common mistake to apply the UDD assumption directly to the above equation: \\[ {}_{1-s}q^{'(1)}_{x+s} \\ne (1-s) q^{'(1)}_{x} \\] This is because the UDD assumption is only applicable to an individual who is alive at the start of the year . Thus, only when survival probability is accounted for can it be used: \\[ {}_{s}p^{'(1)}_{x} \\cdot {}_{1-s}q^{'(1)}_{x+s} = (1-s) q^{'(1)}_{x} \\] This can be proven via conditional probability: \\[ \\begin{aligned} {}_{s}p^{'(1)}_{x} \\cdot {}_{1-s}q^{'(1)}_{x+s} &= \\text{P(Fail from s to 1 | Survive to s)} \\\\ &= \\frac{\\text{P(Fail from s to 1 and survive to s)}}{\\text{P(Survive to s)}} \\\\ &= \\frac{q^{'(1)}_{x} - {}_{s}q^{'(1)}_{x}}{{}_{s}p^{'(1)}_{x}} \\\\ &= \\frac{q^{'(1)}_{x} - s \\cdot q^{'(1)}_{x}}{{}_{s}p^{'(1)}_{x}} \\\\ &= \\frac{(1-s) \\cdot q^{'(1)}_{x}}{{}_{s}p^{'(1)}_{x}} \\end{aligned} \\]","title":"Middle of Year"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#boy-combination","text":"Consider the following scenario: Decrement 1 and 2 occur throughout the year Decrement 3 occurs at the beginning of the year Using the principles established before: \\[ \\begin{aligned} q^{(3)}_{x} &= q^{'(3)}_{x} \\\\ q^{(1)}_{x} &= p^{'(3)}_{x} \\cdot q^{'(1)}_{x} \\cdot (1 - \\frac{1}{2} \\cdot q^{'(2)}_{x}) \\\\ q^{(2)}_{x} &= p^{'(3)}_{x} \\cdot q^{'(2)}_{x} \\cdot (1 - \\frac{1}{2} \\cdot q^{'(1)}_{x}) \\\\ \\end{aligned} \\]","title":"BOY Combination"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#eoy-combination","text":"Consider the following scenario: Decrement 1 and 2 occur throughout the year Decrement 3 occurs at the end of the year \\[ \\begin{aligned} q^{(1)}_{x} &= q^{'(1)}_{x} \\cdot (1 - \\frac{1}{2} \\cdot q^{'(2)_{x}}) \\\\ q^{(2)}_{x} &= q^{'(2)}_{x} \\cdot (1 - \\frac{1}{2} \\cdot q^{'(1)_{x}}) \\\\ q^{(3)}_{x} &= (1 - q^{'(1)}_{x}) \\cdot (1 - q^{'(2)}_{x}) \\cdot q^{'(3)}_{x} \\end{aligned} \\]","title":"EOY Combination"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#moy-combination","text":"Consider the following scenario: Decrement 1 occurs throughout the year Decrement 2 occurs in the middle of the year Decrement 3 occurs at the end of the year \\[ \\begin{aligned} q^{(2)}_{x} &= (1 - {}_{s}q^{'(1)}_{x}) \\cdot q^{'(2)}_{x} \\\\ q^{(1)}_{x} &= {}_{s}q^{'(1)}_{x} + {}_{s}p^{'(1)}_{x} \\cdot {}_{s}p^{'(2)}_{x} \\cdot {}_{1-s}q^{'(2)}_{x} \\\\ q^{(3)}_{x} &= (1 - q^{'(1)}_{x}) \\cdot (1 - q^{'(2)}_{x}) \\cdot q^{'(3)}_{x} \\end{aligned} \\] Tip The most important thing to remember is that all the above equations are attempting to calculate dependent rates from independent ones .","title":"MOY Combination"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#insurance-applications","text":"Multi-decrement assurances and annuities are concpetually similar to their single decrement counterparts - Equivalence Principle, Approximation etc. The key difference is that the benefit is the combination of multiple benefits , typically for each decrement in the model: \\[ \\text{EPV Benefit} = \\text{EPV Benefit 1} + \\text{EPV Benefit 2} + \\dots \\] Warning Not all benefits may be a lump sum payment. It is possible to receive an annuity as a benefit (EG. Receiving 5 years of payments upon diagnosis of a disability). \\[ \\text{\"Annuity\" Benefit} = \\text{PV (Payments)} \\]","title":"Insurance Applications"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#random-variable","text":"Recall that in FAM-L, we derived the EPV of the contract using a bottom\u2019s up approach with the Random Variable denoting the PV of the benefits . For a multi-decrement policy, there are different benefits for each decrement. Thus, the random variable is expressed as a Piecewise Function for each decrement: \\[ \\begin{aligned} Z &= \\begin{cases} B_{1} \\cdot v^{k+1}, & {}_{k \\mid}q^{(1)}_{x} \\\\ B_{2} \\cdot v^{k+1}, & {}_{k \\mid}q^{(2)}_{x} \\end{cases} \\end{aligned} \\] Using this, we can derive an expression for the EPV of the policy: \\[ \\begin{aligned} \\text{EPV} &= E(Z) \\\\ &= \\sum B_{1} v^{k+1} \\cdot {}_{k \\mid}q^{(1)}_{x} + \\sum B_{2} v^{k+1} \\cdot {}_{k \\mid}q^{(2)}_{x} \\\\ &= \\sum B_{1} v^{k+1} \\cdot {}_{k}p^{(\\tau)}_{x} q^{(1)}_{x+k} + \\sum B_{2} v^{k+1} \\cdot {}_{k}p^{(\\tau)}_{x} q^{(2)}_{x+k} \\end{aligned} \\] The key difference is how the Variance is calculated. Since there are now two different benefits, there is a correlation term that must be handled: \\[ \\begin{aligned} E(Z^{2}) &= \\sum \\left(B_{1} \\cdot v^{k+1} \\right)^{2} \\cdot {}_{k \\mid}q^{(1)}_{x} + \\sum \\left(B_{2} \\cdot v^{k+1} \\right)^{2} \\cdot {}_{k \\mid}q^{(2)}_{x} \\\\ [E(Z)]^{2} &= \\left (\\sum B_{1} v^{k+1} \\cdot {}_{k}p^{(\\tau)}_{x} q^{(1)}_{x+k} \\right)^{2} \\\\ &+ \\left (\\sum B_{2} v^{k+1} \\cdot {}_{k}p^{(\\tau)}_{x} q^{(2)}_{x+k} \\right)^{2} \\\\ &+ 2 \\cdot \\left (\\sum B_{2} v^{k+1} \\cdot {}_{k}p^{(\\tau)}_{x} q^{(2)}_{x+k} \\right) \\cdot \\left(\\sum B_{2} v^{k+1} \\cdot {}_{k}p^{(\\tau)}_{x} q^{(2)}_{x+k} \\right) \\end{aligned} \\] Warning The notation for these type of questions are extremely heavy, there is a high tendency to make mistakes as a result.","title":"Random Variable"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/2.%20Multi%20Decrement%20Models/#surrender-benefit","text":"The most common additional benefit is the Surrender Benefit . In practice, virtually all whole life insurance policies offer a surrender benefit of the policy. It is also known as the cash value of the policy. However, surrender benefits are not typically offered for annuities. This is to avoid Adverse Selection : Goal of risk pooling annuities is to use the excess funds from those who die early to fund those who live longer This is why annuities are often offerred without underwriting ; it is in the insurer's favour if someone of poor health joins the pool However, if a surrender benefit is offered, those in poor health will be incentivized to surrender, leaving the pool with only healthy individuals who tend to live long lives Tip Some questions may not directly call it a surrender benefit. It could be phrased as a conversion from this policy type to another. Conversions usually occur in the following mechanism - the existing policy is surrendered for its cash value , and used to purchase a new single premium policy of the new type. Note This is the opposite of insurance , where the goal of the risk pool is to use the excess funds from those live longer fund those who die younger. Thus, the insurer only wants those in relatively good health to join the pool, which is why there is a rigorous underwriting process . Adverse Selection will result in actual experience being far worse than what was assumed during pricing, which would lead to the policies being under-reserved, which could threaten the solvency of the insurer. Thus, there are two approaches to deal with this: Preventive measures to avoid adverse selection (Usual approach) Account for adverse selection during pricing stage Note Adverse Selection is a situation where the policyholder has more information about their risk compared to the insurer (Asymmetric Information). The policyholder will then choose the option that is most beneficial to them - choosing high insurance coverage or in the above case, surrendering an annuity for cash value (if possible).","title":"Surrender Benefit"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/","text":"Multi State Models \u00b6 Overview \u00b6 In FAM-L, we only considered simple contracts that paid a benefit upon the death or survival of the policyholder. In reality, more complicated contracts exist: Paying a stream of benefits while the policyholder is stricken with disease Paying a benefit upon death of the policyholder Info The above example is known as Disability Income insurance. To effectively analyze these more complicated contracts, the survival model must be reformulated into a Multi-State Model . States \u00b6 States are a set of distinct values that the system can be in. In the context of a simple assurance or annuity, the policyholder is either Alive or Dead , resulting in a two-state model : In the context of the disability income insurance described earlier, a policyholder could either be Healthy , Sick or Dead : Note Notice that this is an extension of the Alive-Dead model, where the Alive state is further broken down into Healthy or Sick. The state of the system is measured at different points in time . The goal is to model how the system transitions across various states over time via probability . Discrete Process : Countable number of time points Continuous Process : Uncountable number of time points When moving from one time to another, there are two possible events that can occur: System transitions into another state System remains in the same state Let \\(Y_{t}\\) be the random variable denoting the state of the system at time \\(t\\) . Thus, the transition probabilities can be denoted as follows: Transition Remain Probability of transition from \\(i\\) to \\(j\\) Probability of remaining in state \\(i\\) \\(P \\left(Y_{x+t} = j \\mid Y_{x} = i \\right)\\) \\(P \\left(Y_{x+t} = i, \\text{for all t} \\mid Y_{x} = i \\right)\\) \\({}_{t}p^{ij}_{x}\\) \\({}_{t}p^{\\bar{ii}}_{x}\\) Note Within actuarial notation, \\(i\\) is commonly referred to as the Departing State while \\(j\\) is referred to as the Arriving State . Tip The two have the following relationship: \\[ \\begin{aligned} {}_{t}p^{\\bar{ii}}_{x} \\le {}_{t}p^{ii}_{x} \\end{aligned} \\] This is due to the definition of the two probabilities, making the LHS a subset of the RHS: LHS : Must stay in state \\(i\\) RHS : Stay in state \\(i\\) OR transition out and back to state \\(i\\) by time t The key is to understand that any number of transitions could have actually occurred between the \\(x\\) and \\(x+t\\) - the probability is only concerned with the departing and arriving state at the specified times . It is typically assumed that the process also exhibits the Markov Property - that the probability of transition at any point is independent of the history of the process. The only factor affecting the probability is the current state of the system. \\[ P(Y_{t+1} = i \\mid Y_{t} = j, Y_{t-1} = k, \\dots) = P(Y_{t+1} = i \\mid Y_{t} = j) \\] Note The history of the process refers to which states the entity has been in and for how long . An extreme example is that an entity who JUST transitioned into the state will have the SAME probability as another who has been in the state for a long while . Consider an individual who is temporarily disabled - the longer they are temporarily disabled, the more likely they are of becoming permanently disabled; not following the markov property. This is also inconsistent with an observed trend where mortality tends to be higher in the period immediately following a diagnosis of a critical illness. Discrete Multi State Models \u00b6 A discrete multi state model that follows the markov property is known as a Markov Chain . One Period Transitions \u00b6 Consider a model with \\(n\\) distinct states. The ONE-period transition probabilities for each state to every other state can be summarized into a Transition Matrix : \\[ \\begin{aligned} \\boldsymbol{P}_{x} &= \\begin{pmatrix} p_{x}^{00} & p_{x}^{01} & \\dots & p_{x}^{0n} \\\\ p_{x}^{10} & p_{x}^{11} & \\dots & p_{x}^{1n} \\\\ \\vdots & & \\ddots \\\\ p_{x}^{n0} & p_{x}^{n1} & \\dots & p_{x}^{nn} \\end{pmatrix} \\end{aligned} \\] There are a few key properties about the matrix: Since there are \\(n\\) distinct states, the dimensions of the matrix are \\(n\\) by \\(n\\) Each row represents a unique departing state ( \\(i\\) ) Each column represents a unique arriving state ( \\(j\\) ) The diagonal represents the probability of being in the same state Tip Notice that the probabilities along the SAME ROW represent all possible transitions from that particular departing state. Thus, the probabilities in each row MUST add up to 1: \\[ p_{x}^{00} + p_{x}^{01} + \\dots + p_{x}^{0n} = 1 \\] Some questions might not provide the probabilities for all transitions, thus the above can be used to derive the missing probabilities. Warning The transition matrix captures ALL combinations of state transitions, not just what is possible . Thus, it is not uncommon for these matrices to have a large chunk of 0s , as some state transitions are impossible. In particular, if transitioning out of a state is impossible ( \\(p_{x}^{nn} = 1\\) ), then the state is known as an Absorbing State . If not, it is known as a Transient State . Based on the description, some states should automatically be assumed to be absorbing (EG. Death). Questions may not specify the transition probabilities for these states as it should be understood that the probabilities of transitioning out of the state must be 0. Multiple Period Transitions \u00b6 Similarly, we can consider the transition matrix over multiple periods: \\[ \\begin{aligned} {}_{n}\\boldsymbol{P}_{x} &= \\begin{pmatrix} {}_{n}p_{x}^{00} & {}_{n}p_{x}^{01} & \\dots & {}_{n}p_{x}^{0n} \\\\ {}_{n}p_{x}^{10} & {}_{k}p_{x}^{11} & \\dots & {}_{n}p_{x}^{1n} \\\\ \\vdots & & \\ddots \\\\ {}_{n}p_{x}^{n0} & {}_{n}p_{x}^{n1} & \\dots & {}_{n}p_{x}^{nn} \\end{pmatrix} \\end{aligned} \\] It can be shown that this matrix is the product of all the one-period transition matrices that came before it: \\[ {}_{n}\\boldsymbol{P}_{x} = {}_{n}\\boldsymbol{P}_{x} {}_{n}\\boldsymbol{P}_{x+1} \\dots {}_{n}\\boldsymbol{P}_{x+k-1} \\] Tip The above uses Matrix Multiplication . The reverse \"L\" method can be used to quickly multiply matrices: Warning A common mistake is thinking that since the matrices can be multiplied together, the individual probabilities can be directly multiplied as well. \\[ {}_{n}p^{00}_{x} \\ne p^{00}_{x} \\cdot p^{00}_{x+1} \\dots p^{00}_{x+n-1} \\] This logic does NOT work due to the nature of matrix multiplication. It does not account for the possibility of transitioning to another state but eventually transitioning back to state 0 by time \\(x+n\\) . Note If the transition matrix is the same for each period, then it is known as a Homogenous Transition Matrix . Although this simplifies the calculations needed, it may not be appropriate as transition probabilities may be dependent on the age of the policyholder or other time related factors. Chapman Kolmogorov Equation \u00b6 Matrix multiplication is a convenient way of summarizing ALL possible transitions. However, if only one transition is needed, it is faster to directly compute it instead. Consider the following three time points: Time \\(x\\) : System is in state \\(i\\) Time \\(x+n-1\\) : System can be in ANY state (denoted as state \\(k\\) ) Time \\(x+n\\) : System is in state \\(j\\) By the law of total probability , the probability of transitioning from state \\(i\\) to \\(j\\) in \\(k\\) periods is the sumproduct of: The probability of transitioning from state \\(i\\) to any state \\(k\\) in \\(m-1\\) periods The probability of transitioning from any state \\(k\\) to state \\(j\\) in one period \\[ \\begin{aligned} {}_{n}p^{ij}_{x} &= P(Y_{t+n} = j \\mid Y_{t} = i) \\\\ &= \\sum P(Y_{t+n-1} = k \\mid Y_{t} = i) \\cdot P(Y_{t+n} = j \\mid Y_{t+n-1} = k, Y_{t} = i) \\\\ &= \\sum P(Y_{t+n-1} = k \\mid Y_{t} = i) \\cdot P(Y_{t+n} = j \\mid Y_{t+n-1} = k), \\ \\text{Memoryless} \\\\ &= \\sum {}_{n-1}p^{ik}_{x} \\cdot p^{kj}_{x+n-1} \\end{aligned} \\] This expression is known as the Chapman-Kolmogorov Equation . Notice that it is recursive - the \\(m-1\\) component can be decomposed the same way, resulting in all components being one-period transition probabilities . Note The example can be made the other way round as well: State \\(i\\) to state \\(k\\) in one period State \\(k\\) to state \\(j\\) in \\(n-1\\) periods Both will lead to the same outcome via recursion. To demonstrate this, consider the following two state system: Consider all possible ways to go from state 1 to state 2 in two periods: State 1 > State 1 > State 1 State 1 > State 2 > State 2 \\[ {}_{2}p^{12}_{x} = p^{11}_{x} \\cdot p^{12}_{x+1} + p^{12}_{x} \\cdot p^{22}_{x+1} \\] Warning It is a common mistake to forget that the system can stay in the current state. Similar to FAM-L, questions might provide a starting population and ask for Expectation, Variance or the probability of a specific number of survivors. In these cases, use the above multi-state calculations to determine the probability of an individual and apply it in the Binomial Distribution (assuming lives are independent) to determine the requested quantities. Continuous Multi State Model \u00b6 As its name suggests, a continuous multi-state model allows for transitions at any time . It is commonly referred to as a Continuous Markov Chain . Similar to before, rather than probabilities, we instead consider a Transition Intensities : \\[ \\mu^{ij}_{x}(t) = \\frac{d}{dt} {}_{t}p^{ij}_{x} \\] It is analagous to the force of mortality in the base survival model. It represents the \"probability\" of transitioning from state \\(i\\) to state \\(j\\) instantly at time \\(x+t\\) . Note This implicitly assumes that only ONE transition can occur in a small period of time. Direct Transitions \u00b6 The probability of staying in the same state can thus be calculated as follows: \\[ {}_{t}p^{\\bar{ii}}_{x} = e^{- \\int \\mu^{i \\tau}_{x}(t)} \\] The force represents the combination of forces that would cause a transition out of state \\(i\\) : \\[ \\begin{aligned} \\mu^{i \\tau}_{x}(t) &= \\sum_{j \\ne i} \\mu^{ij}_{x}(t) \\\\ &= \\mu^{12}_{x}(t) + \\mu^{13}_{x}(t) + \\dots + \\mu^{1n}_{x}(t) \\end{aligned} \\] Tip Similar to FAM-L, if the force of transition is constant, then the result will simplify to: \\[ {}_{t}p^{\\bar{ii}}_{x} = e^{- \\mu^{i \\tau}_{x}(t) \\cdot t} \\] In a continuous setting, it is not possible to know the exactly when a transition will occur. Thus, the probability involves three components: Staying in the same state till some time \\(s\\) Instantly transitioning to the target state at time \\(s\\) Staying in the target state for the remainder of time \\(t-s\\) \\[ {}_{t}p^{ij}_{x} = \\int^{t}_{0} {}_{s}p^{\\bar{ii}}_{x} \\cdot \\mu^{ij}_{x}(s) \\cdot {}_{t-s}p^{\\bar{jj}}_{x+s} \\] If the target state is absorbing, then the probability of staying in that state is always 1. Thus, the last term can be dropped in that scenario. Tip Using the same principles, it is possible to determine the probability of the system transitioning into state \\(j\\) and then transitioning out before time \\(t\\) : \\[ \\text{Transition In then Out} = \\int^{t}_{0} {}_{s}p^{\\bar{ii}}_{x} \\cdot \\mu^{ij}_{x}(s) \\cdot (1 - {}_{t-s}p^{\\bar{jj}}_{x+s}) \\] Similarly, the probability of just transitioning, regardless of the state at time \\(t\\) is simply the first two terms: \\[ \\text{Transition In Only} = \\int^{t}_{0} {}_{s}p^{\\bar{ii}}_{x} \\cdot \\mu^{ij}_{x}(s) \\] Indirect Transitions \u00b6 The above assumes that there is only ONE WAY of transitioning - directly from state \\(i\\) to state \\(j\\) . Consider a model where there multiple ways to transition : Direct transition from state 0 to 2 (State 0 > State 2) Indirect transition via state 1 (State 0 > State 1 > State 2) \\[ \\text{Indirect Transition 02} = \\int^{t}_{0} {}_{s}p^{01}_{x} \\mu^{12}_{x+s} {}_{t-s}p^{\\bar{22}}_{x+t} \\] Tip The above is based on a simplified example where the target state is absorbing. Generally, it is much easier to just use the complement for such cases: \\[ \\begin{aligned} {}_{t}p^{00}_{x} + {}_{t}p^{01}_{x} + {}_{t}p^{02}_{x} &= 1 \\\\ {}_{t}p^{02}_{x} &= 1 - {}_{t}p^{00}_{x} - {}_{t}p^{01}_{x} \\end{aligned} \\] Kolmogorov Equation \u00b6 Notice that in the previous section, we have implicitly assumed that the system cannot transition back into a previous state . For such cases, a more complex method known as the Kolmogorov's Forward Equation is required. It is a differential equation that can solve for the probability of transition. Note Very loosely speaking, a differential equation is a method of using the derivatives of a function to solve for the original function itself. Loosely speaking, the first derivative of the probability is the change in probability of moving from state \\(i\\) to state \\(j\\) : \\[ \\begin{aligned} \\frac{d}{dt} {}_{t}p^{ij}_{x} &= \\text{P(Start in i, move INTO j)} - \\text{P(Start in i, move OUT of j)} \\\\ &= \\sum_{k \\ne j} {}_{t}p^{ik}_{x} \\cdot \\mu^{kj}_{x}(t) - \\sum_{k \\ne j} {}_{t}p^{ij}_{x} \\cdot \\mu^{jk}_{x}(t) \\end{aligned} \\] Note If the target state is absorbing, then it is impossible to move out from that state. Hence, the second component would be 0 . If solved using Linear Algebra or if the value of the derivative is provided , the kolmogorov equations provide an exact solution . However, that is out of scope for the purposes of the exam. Instead, the equation can be approximately solved using Euler's Method : \\[ \\begin{aligned} \\frac{d}{dt} {}_{t}p^{ij}_{x} &= \\sum_{k \\ne j} [{}_{t}p^{ik}_{x} \\cdot \\mu^{kj}_{x}(t) - {}_{t}p^{ij}_{x} \\cdot \\mu^{jk}_{x}(t)] \\\\ \\frac{{}_{t+h}p^{ij}_{x} - {}_{t}p^{ij}_{x}}{h} &= \\sum_{k \\ne j} [{}_{t}p^{ik}_{x} \\cdot \\mu^{kj}_{x}(t) - {}_{t}p^{ij}_{x} \\cdot \\mu^{jk}_{x}(t)] \\\\ {}_{t+h}p^{ij}_{x} &= {}_{t}p^{ij}_{x} + h \\cdot \\sum_{k \\ne j} [{}_{t}p^{ik}_{x} \\cdot \\mu^{kj}_{x}(t) - {}_{t}p^{ij}_{x} \\cdot \\mu^{jk}_{x}(t)] \\end{aligned} \\] Note Very loosely speaking, Euler's method approximates the first derivative by setting it equal to the tangent of the graph at that point. The key intuition is that it will repeat this process iteratively , until it converges on a solution. Note that if a step size is not provided, it means that the question likely does not expect Euler's method to be used; there should be another method to sovle the equation based on the information given. Tip There are two variations of the method: \\[ \\begin{aligned} \\text{Forward Method} &= \\frac{{}_{t+h}p - {}_{t}p}{h} &= \\frac{d}{dt} {}_{t}p_{x} \\\\ \\text{Backward Method} &= \\frac{{}_{t}p - {}_{t-h}p}{h} &= \\frac{d}{dt} {}_{t}p_{x} \\end{aligned} \\] The key is to remember the following: The change term always follow \\(t\\) The target term is always \\(t+h\\) or \\(t-h\\) ; the positioning is different for the Forward and Backward variation Use the method that best suits the situation - to recurse forwards or backwards - based on the given information. The step size \\(h\\) is usually defined by the question . It affects the number of iterations needed to solve the question. Consider the following model: \\[ \\begin{aligned} \\text{Objective} &= {}_{1}p^{10}_{x} \\\\ \\text{Step Size} &= \\frac{1}{2} \\\\ \\\\ {}_{0.5}p^{10}_{x} &= {}_{0}p^{10}_{x} + 0.5 [{}_{0}p^{11}_{x} \\mu^{10}_{x}(t) - {}_{0}p^{10}_{x} \\mu^{01}_{x}(t)] \\\\ {}_{0.5}p^{11}_{x} &= {}_{0}p^{11}_{x} + 0.5 \\left({}_{0}p^{10}_{x} \\mu^{01}_{x}(t) - {}_{0}p^{10}_{x} [\\mu^{10}_{x}(t) + \\mu^{12}_{x}(t)] \\right) \\\\ \\therefore {}_{1}p^{10}_{x} &= {}_{0.5}p^{10} + 0.5 [{}_{0.5}p^{11}_{x} \\mu^{10}_{x}(t) - {}_{0.5}p^{10}_{x} \\mu^{01}_{x}(t)] \\\\ \\end{aligned} \\] The equation can be solved because the following two items can always be simplified: \\({}_{0}p^{10}_{x} = 0\\) : System cant be in two states at the same time \\({}_{0}p^{11}_{x} = 1\\) : System must be in that state at the current time The above are known as the boundary conditions of the DE. Loosely speaking, they are constraints that must exist in order for the DE to be solved. If the equation is solving for \\({}_{t}p^{ij}_{x}\\) , then the associated boundary condition is \\({}_{0}p^{ij}_{x}\\) . Note The intuition is that we know the probability at the boundary . Using the kolmogorov equations, we can estimate how the probability will change over a given interval \\(h\\) at different time points. Thus, by stacking the estimated changed onto the boundary conditions, we can estimate the probability at any desired duration. Due to the recursive nature of the problem, it is much easier to perform the calculations in Excel: Notice that the two kolmogorov equations (Discrete & Continuous) follow the same intuition - transitioning to some intermediate state before reaching the final state. Continuous to Discrete \u00b6 Using Euler's Method, it can be shown that a discrete markov chain with intervals of \\(h\\) can be used to approximate the kolmogorov equation as well: \\[ \\begin{aligned} {}_{t+h}p^{11}_{x} &= {}_{t}p^{11}_{x} {}_{h}p^{11}_{x+t} + {}_{t}p^{10}_{x} {}_{h}p^{01}_{x+t} \\\\ {}_{t+h}p^{11}_{x} &= {}_{t}p^{11}_{x} (1 - {}_{h}p^{10}_{x+t} + {}_{h}p^{12}_{x+t}) + {}_{t}p^{10}_{x} {}_{h}p^{01}_{x+t} \\\\ {}_{t+h}p^{11}_{x} &= {}_{t}p^{11}_{x} - {}_{t}p^{11}_{x} ({}_{h}p^{10}_{x+t} + {}_{h}p^{12}_{x+t}) + {}_{t}p^{10}_{x} {}_{h}p^{01}_{x+t} \\\\ {}_{t+h}p^{11}_{x} - {}_{t}p^{11}_{x} &= {}_{t}p^{10}_{x} {}_{h}p^{01}_{x+t} - {}_{t}p^{11}_{x} ({}_{h}p^{10}_{x+t} + {}_{h}p^{12}_{x+t}) \\\\ \\frac{{}_{t+h}p^{11}_{x} - {}_{t}p^{11}_{x}}{h} &= {}_{t}p^{10}_{x} \\frac{{}_{h}p^{01}_{x+t}}{h} - {}_{t}p^{11}_{x} \\left(\\frac{{}_{h}p^{10}_{x+t}}{h} + \\frac{{}_{h}p^{12}_{x+t}}{h} \\right) \\\\ \\lim \\left[\\frac{{}_{t+h}p^{11}_{x} - {}_{t}p^{11}_{x}}{h} \\right] &= \\lim \\left[{}_{t}p^{10}_{x} \\frac{{}_{h}p^{01}_{x+t}}{h} - {}_{t}p^{11}_{x} \\left(\\frac{{}_{h}p^{10}_{x+t}}{h} + \\frac{{}_{h}p^{12}_{x+t}}{h} \\right) \\right] \\\\ \\frac{d}{dt} {}_{t}p^{11}_{x} &= {}_{t}p^{10}_{x} \\mu^{11}_{x}(t) - {}_{t}p^{11}_{x} [\\mu^{10}_{x}(t) + \\mu^{12}_{x}(t)] \\end{aligned} \\] Note The key portion of the proof is the derivation of the force of transition. The intuition can be found in this section of FAM-L. The probabilities for the markov chain can be approximated using the following: \\[ \\begin{aligned} {}_{h}p^{ij}_{x} &= h \\cdot \\mu^{ij}_{x}(t) \\\\ {}_{h}p^{ii}_{x} &= 1 - \\sum h \\cdot \\mu^{ik}_{x}(t) \\\\ &= 1 - h \\sum \\mu^{ik}_{x}(t) \\\\ &= 1 - h \\cdot \\mu^{i \\tau}_{x}(t) \\end{aligned} \\] A smaller step size will lead to a more accurate approximation. Tip The key to remember this is that the force of transition represents a transition - thus the approximation is for the transition probability. The probability of staying is thus the complement of the total transition probability. Using the same example as before: \\[ \\begin{aligned} \\text{Pathway 1} &: 1 - 1 - 0 \\\\ \\text{Pathway 2} &: 1 - 0 - 0 \\\\ \\\\ \\therefore {}_{1}p^{10}_{x} &= {}_{0.5}p^{11}_{x} \\cdot {}_{0.5}p^{10}_{x+0.5} + {}_{0.5}p^{10}_{x} \\cdot {}_{0.5}p^{00}_{x+0.5} \\end{aligned} \\] Tip One constraint with this method is whether or not the force of transition at the intemediate ages (EG. \\(x+0.5\\) ) are provided. However, if the force of transition is constant, then the same force can be used for all ages. Insurance Applications \u00b6 Multi-state assurances and annuities are concpetually similar to their single decrement counterparts - Equivalence Principle, Continuous Discrete Approximation etc. This section highlights the key differences in a multi-state context. Multi State Assurances \u00b6 A Multi-State Assurance pays a benefit whenever the insured transitions into a particular state. The actuarial value of such an assurance is denoted by \\(A^{ij}_{x}\\) , where \\(i\\) is the current state of the insured and \\(j\\) is the state which triggers a payment . The key is understanding that any transition into the state will trigger the payout. Thus, all possible transitions into state must be considered at each time period. Warning It is a common mistake to assume that because there is no direct transition to the insured state, the value of the assurance must be 0. This is incorrect, as all possible transition pathways, regardless of the number of steps required , should be considered. Consider the following example: 2-year discrete term assurance Benefit payment at the end of the year upon transition into state 2 Insured is currently in state 0 \\[ \\begin{array}{|c|c|c|c|} \\hline \\text{Policy Year End } & \\text{State} & \\text{Probability} & \\text{PV Benefit} \\\\ \\hline 0 & \\text{0} & - & - \\\\ \\hline 1 & \\text{0 - 0} & p^{\\bar{00}}_{x} & 0 \\\\ \\hline 1 & \\text{0 - 1} & p^{01}_{x} & 0 \\\\ \\hline 1 & \\text{0 - 2} & p^{02}_{x} & Bv \\\\ \\hline 2 & \\text{0 - 0 - 0} & p^{\\bar{00}}_{x} \\cdot p^{\\bar{00}}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 0 - 1} & p^{\\bar{00}}_{x} \\cdot p^{01}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 0 - 2} & p^{\\bar{00}}_{x} \\cdot p^{02}_{x+1} & Bv^2 \\\\ \\hline 2 & \\text{0 - 1 - 1} & p^{01}_{x} \\cdot p^{\\bar{11}}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 1 - 2} & p^{01}_{x} \\cdot p^{12}_{x+1} & Bv^2 \\\\ \\hline 2 & \\text{0 - 2 - 2} & p^{02}_{x} \\cdot p^{\\bar{22}}_{x+1} & 0 \\\\ \\hline \\end{array} \\] Warning It is a common mistake to assume that the last case of 0 - 2 - 2 should have a benefit listed there as the insured is in state 2 at the end of the year. However, the benefit is only payable if the insured transitions INTO state 2 from some OTHER state . \\[ \\begin{aligned} A^{02}_{x\\enclose{actuarial}{2}} &= p^{02}_{x} \\cdot Bv + p^{00}_{x} p^{02}_{x+1} \\cdot Bv^2 + p^{01}_{x} \\cdot p^{12}_{x+1} \\cdot Bv^2 \\\\ &= p^{02}_{x} \\cdot Bv + (p^{00}_{x} p^{02}_{x+1} + p^{01}_{x} \\cdot p^{12}_{x+1}) \\cdot Bv^2 \\\\ &= p^{02}_{x} \\cdot Bv + {}_{2}p^{02}_{x} \\cdot Bv^2 \\\\ \\end{aligned} \\] Warning The notation for a TA is slightly different from what is normally used. It does not make sense to include the \"1\" in the superscript as it is already being used by the state notation. The key difference from FAM-L is that there are multiple transitions that could lead to a payout, not just death. Thus, it can be more generally expressed as: \\[ \\begin{aligned} A^{ij}_{x} &= \\sum^{\\infty}_{t=0} \\sum_{i \\ne k} v^{t} {}_{t}p^{ik}_{x} \\\\ \\bar{A}^{ij}_{x} &= \\int^{\\infty}_{0} e^{-\\delta t} \\sum_{k \\ne j} {}_{t}p^{ik}_{x} \\mu^{kj}_{x+t} \\end{aligned} \\] Similarly, the relationship between Term and WL assurances remains true, with a slight modification to consider ALL possible state changes : \\[ A^{ij}_{x\\enclose{actuarial}{n}} = A^{ij}_{x} - v^{n} \\sum_{k \\ne j} {}_{n}p^{ik}_{x} A^{kj}_{x+n} \\] The key idea is that at the valuation time, the life is known to be in state \\(i\\) . However, in \\(n\\) years time, the insured could be in any state , INCLUDING the target state. ALL the possibilities must be accounted for. Tip The same relationship holds for multi-state temporary and deferred annuities as well. Multi State Annuities \u00b6 Conversely, a Multi-State Annuity pays a benefit whenever the insured is in a particular state . The actuarial value is denoted by \\(\\ddot{a}^{ij}_{x}\\) , where \\(i\\) is the current state of the insured and \\(j\\) is the state which triggers a payment . Note The annuity will pay even for re-entries into state \\(j\\) . Consider the following scenario: At time 2, the life transition into state \\(j\\) At time 4, the life transitions into state \\(k\\) At time 6, the life transitions back into state \\(j\\) and remains there The above annuity will make a payment for all times that the life is state \\(j\\) ; from time 2 to 4 as well as after time 6. Warning The general notation assumes that the insured is NOT already in the state that would trigger payment. For instance, an annuity that pays only while the insured is disabled. This is is contrary to the typical life annuity that was covered, where the insured is already alive, and will receive payments for as long as they are alive. Consider the following example: 3-year Discrete Temporary Annuity Benefit payable at the beginning of the year while in state 1 Insured is currently in state 0 \\[ \\begin{array}{|c|c|c|c|} \\hline \\text{Policy Year End} & \\text{State} & \\text{Probability} & \\text{PV Benefit} \\\\ \\hline 0 & \\text{0} & - & - \\\\ \\hline 1 & \\text{0 - 0} & p^{\\bar{00}}_{x} & 0 \\\\ \\hline 1 & \\text{0 - 1} & p^{01}_{x} & Bv \\\\ \\hline 1 & \\text{0 - 2} & p^{02}_{x} & 0 \\\\ \\hline 2 & \\text{0 - 0 - 0} & p^{\\bar{00}}_{x} \\cdot p^{\\bar{00}}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 0 - 1} & p^{\\bar{00}}_{x} \\cdot p^{01}_{x+1} & Bv^{2} \\\\ \\hline 2 & \\text{0 - 0 - 2} & p^{\\bar{00}}_{x} \\cdot p^{02}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 1 - 1} & p^{01}_{x} \\cdot p^{\\bar{11}}_{x+1} & Bv^2 \\\\ \\hline 2 & \\text{0 - 1 - 2} & p^{01}_{x} \\cdot p^{12}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 2 - 2} & p^{02}_{x} \\cdot p^{\\bar{22}}_{x+1} & 0 \\\\ \\hline \\end{array} \\] Tip Even though the policy term is 3 years, there is no need to consider what happens at the end of the third policy year as the benefits are payable at the start of the year - the last benefit is paid at the beginning of the third year (end of the second year). \\[ \\begin{aligned} \\ddot{a}^{01}_{x\\enclose{actuarial}{3}} &= p^{01}_{x} \\cdot Bv + p^{\\bar{00}}_{x} \\cdot p^{01}_{x+1} \\cdot Bv^{2} + p^{\\bar{00}}_{x} \\cdot p^{01}_{x+1} \\cdot Bv^{2} \\\\ &= p^{01}_{x} \\cdot Bv + (p^{\\bar{00}}_{x} \\cdot p^{01}_{x+1} + p^{\\bar{00}}_{x} \\cdot p^{01}_{x+1}) \\cdot Bv^{2} \\\\ &= p^{01}_{x} \\cdot Bv + {}_{2}p^{01}_{x} \\cdot Bv^{2} \\\\ \\end{aligned} \\] Similarly, annuities can be generally expressed as: \\[ \\begin{aligned} \\ddot{a}^{ij}_{x} &= \\sum^{\\infty}_{t=0} v^{t} {}_{t}p^{ij}_{x} \\\\ \\bar{a}^{ij}_{x} &= \\int^{\\infty}_{0} e^{-\\delta t} {}_{t}p^{ij}_{x} \\end{aligned} \\] Relationships \u00b6 There is an interesting relationship among multi-state annuities. Consider the same 3-state model from before: \\[ \\begin{aligned} \\bar{a}^{00}_{x} + \\bar{a}^{01}_{x} + \\bar{a}^{02}_{x} &= \\int^{n}_{0} {}_{t}p^{00}_{x} v^{n} + {}_{t}p^{01}_{x} v^{n} + {}_{t}p^{02}_{x} v^{n} \\\\ &= \\int^{n}_{0} v^{n} \\cdot ({}_{t}p^{00}_{x} + {}_{t}p^{01}_{x} + {}_{t}p^{02}_{x}) \\\\ &= \\int^{n}_{0} v^{n} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{n}} \\\\ \\therefore \\ddot{a}^{00}_{x} + \\ddot{a}^{01}_{x} + \\ddot{a}^{02}_{x} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{\\infty}} \\end{aligned} \\] Warning It is a common mistake to forget the probability of staying in the same state, as most state diagrams do not typically include it. Note The formulas for the annuities certain are from FM and can be found below: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= \\frac{1 - v^{n}}{d} \\\\ \\bar{a}_{\\enclose{actuarial}{n}} &= \\frac{1 - v^{n}}{\\delta} \\\\ \\\\ \\ddot{a}_{\\enclose{actuarial}{\\infty}} &= \\frac{1}{d} \\bar{a}_{\\enclose{actuarial}{\\infty}} &= \\frac{1}{\\delta} \\end{aligned} \\] Recall from FAM-L that annuities and assurances are related to one another. In a multi-state setting, it only holds true if the target assurance is transitioning into an ABSORBING STATE : \\[ A^{ik} = 1 - d \\cdot \\sum_{j \\ne k} \\ddot{a}^{ij}_{x} \\\\ \\] Note The above formula is a generalization of the conversion formula in FAM-L. Under a two-state model, the target state is always absorbing while the negated states are always non-absorbing. Sorjourn Annuity \u00b6 The definition of \"Sorjourn\" is to stay temporarily . Thus, a Sojourn annuity is an annuity that pays a benefit while the insured stays in a specific state , given that they are already in that state (hence stay). It's acturial value is denoted by \\(\\ddot{a}^{\\bar{ii}}_{x}\\) . They are usually used to calculate the EPV of premiums as premiums are usually only payable while the insured is healthy. Consider the following example: 3-year Discrete Temporary Annuity Premium payable at the beginning of the year while in state 0 Insured is currently in state 0 \\[ \\begin{array}{|c|c|c|c|} \\hline \\text{Policy Year End} & \\text{State} & \\text{Probability} & \\text{PV Benefit} \\\\ \\hline 0 & \\text{0} & - & B \\\\ \\hline 1 & \\text{0 - 0} & p^{\\bar{00}}_{x} & Bv \\\\ \\hline 1 & \\text{0 - 1} & p^{01}_{x} & 0 \\\\ \\hline 1 & \\text{0 - 2} & p^{02}_{x} & 0 \\\\ \\hline 2 & \\text{0 - 0 - 0} & p^{\\bar{00}}_{x} \\cdot p^{\\bar{00}}_{x+1} & Bv^{2} \\\\ \\hline 2 & \\text{0 - 0 - 1} & p^{\\bar{00}}_{x} \\cdot p^{01}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 0 - 2} & p^{\\bar{00}}_{x} \\cdot p^{02}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 1 - 1} & p^{01}_{x} \\cdot p^{\\bar{11}}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 1 - 2} & p^{01}_{x} \\cdot p^{12}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 2 - 2} & p^{02}_{x} \\cdot p^{\\bar{22}}_{x+1} & 0 \\\\ \\hline \\end{array} \\] Tip Since the insured already starts in the premium paying state , there is a cashflow at time 0 which was not seen in the other examples. \\[ \\begin{aligned} \\ddot{a}^{\\bar{00}}_{x\\enclose{actuarial}{3}} &= B + p^{\\bar{00}}_{x} \\cdot Bv + p^{\\bar{00}}_{x} \\cdot p^{\\bar{00}}_{x+1} \\cdot Bv^{2} \\\\ &= B + p^{\\bar{00}}_{x} \\cdot Bv + {}_{2}p^{\\bar{00}}_{x} \\cdot Bv^{2} \\end{aligned} \\] More generally, they can be expressed as: \\[ \\begin{aligned} \\ddot{a}^{\\bar{00}}_{x} &= \\sum^{\\infty}_{0} v^{t} {}_{t}p^{\\bar{ii}}_{x} \\\\ \\\\ \\bar{a}^{\\bar{00}}_{x} &= \\int^{\\infty}_{0} e^{-\\delta t} {}_{t}p^{\\bar{ii}}_{x} \\end{aligned} \\] Tip The general multi-state annuity can be expressed in the form of a Sojourn annuity: \\[ \\bar{a}^{ij}_{x} = \\int^{\\infty}_{0} {}_{t}p^{ik}_{x} \\mu^{kj}_{x+t} \\cdot e^{-\\delta t} \\bar{a}^{\\bar{jj}}_{x+t} \\] It represents the probability that the policyholder transitions into state \\(j\\) and receives an annuity at that moment that pays for as long as they remain in that state. This expression is more flexible than the original expression because it allows us to specify the duration of payments. In the above example, a whole life annuity is used, which implies that the benefit will be paid for as long as they are in the state. However, a temporary annuity can be specified instead, which implies that the benefit will be paid a limited period of time EACH TIME the policyholder enters that state: \\[ \\int^{\\infty}_{0} \\sum {}_{t}p^{ik}_{x} \\mu^{kj}_{x+t} \\cdot e^{-\\delta t} \\bar{a}^{\\bar{jj}}_{x+t:\\enclose{actuarial}{n}} \\] Note that the expression is typically what we see of an Assurance, rather than an annuity, which is why it might be confusing to grasp. Warning Note that the final expression in the above example is NOT equivalent to a temporary multi-state annuity: \\[ \\bar{a}^{ij}_{x:\\enclose{actuarial}{n}} \\ne \\int^{\\infty}_{0} {}_{t}p^{ik}_{x} \\mu^{kj}_{x+t} \\cdot e^{-\\delta t} \\bar{a}^{\\bar{jj}}_{x+t:\\enclose{actuarial}{n}} \\] The LHS corresponds to a policy that lasts for \\(n\\) years while the RHS corresponds to a policy that pays for only the first \\(n\\) years on each entry into the state. Woolhouse Approximation \u00b6 Recall that the Woolhouse Approximation was used to approximate Continuous or Monthly annuities from their yearly discrete variants. Skipping the formal proof, the three term woolhouse approximation in a multi-state setting is as follows: \\[ \\begin{aligned} \\ddot{a}^{(m)ij}_{x} &= \\ddot{a}^{ij}_{x} + \\frac{m^{2} - 1}{12m^{2}} \\mu^{ij}_{x}(t) \\\\ \\\\ \\ddot{a}^{(m)\\bar{ii}}_{x} &= \\ddot{a}^{\\bar{ii}}_{x} - \\frac{m-1}{2m} - \\frac{m^{2} - 1}{12m^{2}} [\\mu^{i \\tau}_{x}(t) + \\delta] \\end{aligned} \\] The above is not a typo; there are different formulas for the Soujourn and Non-Sourjourn annuities. Both are provided on the formula sheet. Tip Despite the non-sojourn annuity having only two terms , it is still known as the three term woolhouse approximation. If asked for the two term approximation, drop the last term . The approximation simply equal to the existing value. One interesting use case is to thus approximate the payable m-thly case from the continuous case using the two term approximation: \\[ \\ddot{a}^{(m)ij}_{x} = \\ddot{a}^{ij}_{x} \\] This especially relevant for the Standard Sickness Death Model, where the continuous annuity functions are provided. By assuming m=1, the discrete annuity functions can be obtained. Sojourn approximations are closest to the typical life annuity covered. Thus, it is expected that the sojourn approximation is almost identical to the original approximation. Note The force term is meant to be of the OPPOSITE of the annuity event . In FAM-L, the force was the force of dying, opposite of the annuity benefit which pays while living. For a sojourn annuity, the opposite of staying in the same state is a transition to any other state . Non-sojourn approximations are seemingly similar to the usual approximations, but there are multiple differences : There are only two terms instead of three (Usual second term is missing) The sign is flipped on the 'third' term ('+' instead of '-') The force term is of the SAME annuity event Reserves \u00b6 Recall that reserves are calculated as the EPV of the net outflows each period. In a multi-state setting, the outflows are dependent on the state of the insured at the time of valuation. Thus, the reserve must reflect two dimensions - both time and state. \\[ {}_{t}V^{(i)} = \\text{EPV(Benefits)}_{t}^{i} + \\text{EPV(Expenses)}_{t}^{i} - \\text{EPV(Premiums)}_{t}^{i} \\] Tip Note that some terms may naturally evaluate to 0 . For instance, Premiums may not be payable while in that state Certain benefits may not payable in that state Thiele's DE \u00b6 Reserves for a continuous assurance or annuity can be instead calculated using Thiele's Differential Equation : \\[ \\begin{aligned} \\frac{d}{dt} {}_{t}V^{i} &= \\delta_{t} \\cdot {}_{t}V^{i} \\underbrace{+ P_{t}^{i} - B_{t}^{i} - e_{t}^{i}}_{\\text{Net In-State Outgo}} - \\sum_{j \\ne i} \\mu^{ij}_{x}(t) \\cdot (b^{ij} + {}_{t}V^{j} - {}_{t}V^{i}) \\end{aligned} \\] Tip If one of the target states is absorbing (EG. Death), then the reserve for that state is 0 as there are no more future payments. Warning One implicit assumption made is that the reserve is computed before receiving premiums. If the question states that the basis is after premiums, add the premiums to the opening reserve INSTEAD . The difference is material as it affects whether or not the premiums earn interest. Similar to before, the DE is trying to approximate the change in the reserve : Reserve earns interest Released to pay off expected net benefits and expenses (while in state & upon transition) Form a new reserve for every possible resulting state Warning Do NOT mix up the two benefit variables: Big B : Rate of benefit payment while in the current state Small B : Lump Sum benefit payment upon transition into other state The reserve item is the expected CHANGE in reserve - not just the expected future reserve. Do NOT confuse this with profit testing basis. Similarly, the DE can be solved using Euler's Method . Thiele's DE usually uses the backward variation instead: \\[ \\begin{aligned} \\frac{d}{dt} {}_{t}V^{i} &= \\delta_{t} \\cdot {}_{t}V^{i} + P - B - E - \\sum_{j \\ne i} \\mu^{ij}_{x}(t) \\cdot (b^{ij} + {}_{t}V^{j} - {}_{t}V^{i}) \\\\ \\frac{{}_{t}V^{i} - {}_{t-h}V^{i}}{h} &= \\delta_{t} \\cdot {}_{t}V^{i} + P - B - E - \\sum_{j \\ne i} \\mu^{ij}_{x}(t) \\cdot (b^{ij} + {}_{t}V^{j} - {}_{t}V^{i}) \\\\ {}_{t}V^{i} &= {}_{t-h}V^{i} + h \\cdot \\left(\\delta_{t} \\cdot {}_{t}V^{i} + P - B - E - \\sum_{j \\ne i} \\mu^{ij}_{x}(t) \\cdot (b^{ij} + {}_{t}V^{j} - {}_{t}V^{i}) \\right) \\end{aligned} \\] Tip Similar to Kolmogorov forward equation, these type of questions are best done on Excel to easily perform the recursions: Recursion \u00b6 One method of determining the reserve for the euler's method is to use Recursion . It follows the same intuition as before, with the key difference having to account for the multiple possibilities for the expected benefits and ending reserve. \\[ \\begin{aligned} ({}_{t}V^{i} + P - e - B^{\\text{In State}})(1+i) &= \\sum p^{ik}_{x+t} \\cdot B^{\\text{Upon Transition}} + \\sum p^{ik}_{x+t} \\cdot {}_{t}V^{k} \\end{aligned} \\] Impact Assessment \u00b6 For most multi-state models, the policy does not terminate upon transitioning to another state. Thus, when assessing the impact of a change in probability on EPVs, it is necessary to consider two levels of impact: First Order Impact : Increased probability of transition to the target state; policyholder will transition sooner , resulting in an increase to the EPV Second Order Impact : From the affected state , the probability of transitioning into other states maybe higher or lower, thus causing the EPV of those states to change as well","title":"Multi State Models"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#multi-state-models","text":"","title":"Multi State Models"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#overview","text":"In FAM-L, we only considered simple contracts that paid a benefit upon the death or survival of the policyholder. In reality, more complicated contracts exist: Paying a stream of benefits while the policyholder is stricken with disease Paying a benefit upon death of the policyholder Info The above example is known as Disability Income insurance. To effectively analyze these more complicated contracts, the survival model must be reformulated into a Multi-State Model .","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#states","text":"States are a set of distinct values that the system can be in. In the context of a simple assurance or annuity, the policyholder is either Alive or Dead , resulting in a two-state model : In the context of the disability income insurance described earlier, a policyholder could either be Healthy , Sick or Dead : Note Notice that this is an extension of the Alive-Dead model, where the Alive state is further broken down into Healthy or Sick. The state of the system is measured at different points in time . The goal is to model how the system transitions across various states over time via probability . Discrete Process : Countable number of time points Continuous Process : Uncountable number of time points When moving from one time to another, there are two possible events that can occur: System transitions into another state System remains in the same state Let \\(Y_{t}\\) be the random variable denoting the state of the system at time \\(t\\) . Thus, the transition probabilities can be denoted as follows: Transition Remain Probability of transition from \\(i\\) to \\(j\\) Probability of remaining in state \\(i\\) \\(P \\left(Y_{x+t} = j \\mid Y_{x} = i \\right)\\) \\(P \\left(Y_{x+t} = i, \\text{for all t} \\mid Y_{x} = i \\right)\\) \\({}_{t}p^{ij}_{x}\\) \\({}_{t}p^{\\bar{ii}}_{x}\\) Note Within actuarial notation, \\(i\\) is commonly referred to as the Departing State while \\(j\\) is referred to as the Arriving State . Tip The two have the following relationship: \\[ \\begin{aligned} {}_{t}p^{\\bar{ii}}_{x} \\le {}_{t}p^{ii}_{x} \\end{aligned} \\] This is due to the definition of the two probabilities, making the LHS a subset of the RHS: LHS : Must stay in state \\(i\\) RHS : Stay in state \\(i\\) OR transition out and back to state \\(i\\) by time t The key is to understand that any number of transitions could have actually occurred between the \\(x\\) and \\(x+t\\) - the probability is only concerned with the departing and arriving state at the specified times . It is typically assumed that the process also exhibits the Markov Property - that the probability of transition at any point is independent of the history of the process. The only factor affecting the probability is the current state of the system. \\[ P(Y_{t+1} = i \\mid Y_{t} = j, Y_{t-1} = k, \\dots) = P(Y_{t+1} = i \\mid Y_{t} = j) \\] Note The history of the process refers to which states the entity has been in and for how long . An extreme example is that an entity who JUST transitioned into the state will have the SAME probability as another who has been in the state for a long while . Consider an individual who is temporarily disabled - the longer they are temporarily disabled, the more likely they are of becoming permanently disabled; not following the markov property. This is also inconsistent with an observed trend where mortality tends to be higher in the period immediately following a diagnosis of a critical illness.","title":"States"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#discrete-multi-state-models","text":"A discrete multi state model that follows the markov property is known as a Markov Chain .","title":"Discrete Multi State Models"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#one-period-transitions","text":"Consider a model with \\(n\\) distinct states. The ONE-period transition probabilities for each state to every other state can be summarized into a Transition Matrix : \\[ \\begin{aligned} \\boldsymbol{P}_{x} &= \\begin{pmatrix} p_{x}^{00} & p_{x}^{01} & \\dots & p_{x}^{0n} \\\\ p_{x}^{10} & p_{x}^{11} & \\dots & p_{x}^{1n} \\\\ \\vdots & & \\ddots \\\\ p_{x}^{n0} & p_{x}^{n1} & \\dots & p_{x}^{nn} \\end{pmatrix} \\end{aligned} \\] There are a few key properties about the matrix: Since there are \\(n\\) distinct states, the dimensions of the matrix are \\(n\\) by \\(n\\) Each row represents a unique departing state ( \\(i\\) ) Each column represents a unique arriving state ( \\(j\\) ) The diagonal represents the probability of being in the same state Tip Notice that the probabilities along the SAME ROW represent all possible transitions from that particular departing state. Thus, the probabilities in each row MUST add up to 1: \\[ p_{x}^{00} + p_{x}^{01} + \\dots + p_{x}^{0n} = 1 \\] Some questions might not provide the probabilities for all transitions, thus the above can be used to derive the missing probabilities. Warning The transition matrix captures ALL combinations of state transitions, not just what is possible . Thus, it is not uncommon for these matrices to have a large chunk of 0s , as some state transitions are impossible. In particular, if transitioning out of a state is impossible ( \\(p_{x}^{nn} = 1\\) ), then the state is known as an Absorbing State . If not, it is known as a Transient State . Based on the description, some states should automatically be assumed to be absorbing (EG. Death). Questions may not specify the transition probabilities for these states as it should be understood that the probabilities of transitioning out of the state must be 0.","title":"One Period Transitions"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#multiple-period-transitions","text":"Similarly, we can consider the transition matrix over multiple periods: \\[ \\begin{aligned} {}_{n}\\boldsymbol{P}_{x} &= \\begin{pmatrix} {}_{n}p_{x}^{00} & {}_{n}p_{x}^{01} & \\dots & {}_{n}p_{x}^{0n} \\\\ {}_{n}p_{x}^{10} & {}_{k}p_{x}^{11} & \\dots & {}_{n}p_{x}^{1n} \\\\ \\vdots & & \\ddots \\\\ {}_{n}p_{x}^{n0} & {}_{n}p_{x}^{n1} & \\dots & {}_{n}p_{x}^{nn} \\end{pmatrix} \\end{aligned} \\] It can be shown that this matrix is the product of all the one-period transition matrices that came before it: \\[ {}_{n}\\boldsymbol{P}_{x} = {}_{n}\\boldsymbol{P}_{x} {}_{n}\\boldsymbol{P}_{x+1} \\dots {}_{n}\\boldsymbol{P}_{x+k-1} \\] Tip The above uses Matrix Multiplication . The reverse \"L\" method can be used to quickly multiply matrices: Warning A common mistake is thinking that since the matrices can be multiplied together, the individual probabilities can be directly multiplied as well. \\[ {}_{n}p^{00}_{x} \\ne p^{00}_{x} \\cdot p^{00}_{x+1} \\dots p^{00}_{x+n-1} \\] This logic does NOT work due to the nature of matrix multiplication. It does not account for the possibility of transitioning to another state but eventually transitioning back to state 0 by time \\(x+n\\) . Note If the transition matrix is the same for each period, then it is known as a Homogenous Transition Matrix . Although this simplifies the calculations needed, it may not be appropriate as transition probabilities may be dependent on the age of the policyholder or other time related factors.","title":"Multiple Period Transitions"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#chapman-kolmogorov-equation","text":"Matrix multiplication is a convenient way of summarizing ALL possible transitions. However, if only one transition is needed, it is faster to directly compute it instead. Consider the following three time points: Time \\(x\\) : System is in state \\(i\\) Time \\(x+n-1\\) : System can be in ANY state (denoted as state \\(k\\) ) Time \\(x+n\\) : System is in state \\(j\\) By the law of total probability , the probability of transitioning from state \\(i\\) to \\(j\\) in \\(k\\) periods is the sumproduct of: The probability of transitioning from state \\(i\\) to any state \\(k\\) in \\(m-1\\) periods The probability of transitioning from any state \\(k\\) to state \\(j\\) in one period \\[ \\begin{aligned} {}_{n}p^{ij}_{x} &= P(Y_{t+n} = j \\mid Y_{t} = i) \\\\ &= \\sum P(Y_{t+n-1} = k \\mid Y_{t} = i) \\cdot P(Y_{t+n} = j \\mid Y_{t+n-1} = k, Y_{t} = i) \\\\ &= \\sum P(Y_{t+n-1} = k \\mid Y_{t} = i) \\cdot P(Y_{t+n} = j \\mid Y_{t+n-1} = k), \\ \\text{Memoryless} \\\\ &= \\sum {}_{n-1}p^{ik}_{x} \\cdot p^{kj}_{x+n-1} \\end{aligned} \\] This expression is known as the Chapman-Kolmogorov Equation . Notice that it is recursive - the \\(m-1\\) component can be decomposed the same way, resulting in all components being one-period transition probabilities . Note The example can be made the other way round as well: State \\(i\\) to state \\(k\\) in one period State \\(k\\) to state \\(j\\) in \\(n-1\\) periods Both will lead to the same outcome via recursion. To demonstrate this, consider the following two state system: Consider all possible ways to go from state 1 to state 2 in two periods: State 1 > State 1 > State 1 State 1 > State 2 > State 2 \\[ {}_{2}p^{12}_{x} = p^{11}_{x} \\cdot p^{12}_{x+1} + p^{12}_{x} \\cdot p^{22}_{x+1} \\] Warning It is a common mistake to forget that the system can stay in the current state. Similar to FAM-L, questions might provide a starting population and ask for Expectation, Variance or the probability of a specific number of survivors. In these cases, use the above multi-state calculations to determine the probability of an individual and apply it in the Binomial Distribution (assuming lives are independent) to determine the requested quantities.","title":"Chapman Kolmogorov Equation"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#continuous-multi-state-model","text":"As its name suggests, a continuous multi-state model allows for transitions at any time . It is commonly referred to as a Continuous Markov Chain . Similar to before, rather than probabilities, we instead consider a Transition Intensities : \\[ \\mu^{ij}_{x}(t) = \\frac{d}{dt} {}_{t}p^{ij}_{x} \\] It is analagous to the force of mortality in the base survival model. It represents the \"probability\" of transitioning from state \\(i\\) to state \\(j\\) instantly at time \\(x+t\\) . Note This implicitly assumes that only ONE transition can occur in a small period of time.","title":"Continuous Multi State Model"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#direct-transitions","text":"The probability of staying in the same state can thus be calculated as follows: \\[ {}_{t}p^{\\bar{ii}}_{x} = e^{- \\int \\mu^{i \\tau}_{x}(t)} \\] The force represents the combination of forces that would cause a transition out of state \\(i\\) : \\[ \\begin{aligned} \\mu^{i \\tau}_{x}(t) &= \\sum_{j \\ne i} \\mu^{ij}_{x}(t) \\\\ &= \\mu^{12}_{x}(t) + \\mu^{13}_{x}(t) + \\dots + \\mu^{1n}_{x}(t) \\end{aligned} \\] Tip Similar to FAM-L, if the force of transition is constant, then the result will simplify to: \\[ {}_{t}p^{\\bar{ii}}_{x} = e^{- \\mu^{i \\tau}_{x}(t) \\cdot t} \\] In a continuous setting, it is not possible to know the exactly when a transition will occur. Thus, the probability involves three components: Staying in the same state till some time \\(s\\) Instantly transitioning to the target state at time \\(s\\) Staying in the target state for the remainder of time \\(t-s\\) \\[ {}_{t}p^{ij}_{x} = \\int^{t}_{0} {}_{s}p^{\\bar{ii}}_{x} \\cdot \\mu^{ij}_{x}(s) \\cdot {}_{t-s}p^{\\bar{jj}}_{x+s} \\] If the target state is absorbing, then the probability of staying in that state is always 1. Thus, the last term can be dropped in that scenario. Tip Using the same principles, it is possible to determine the probability of the system transitioning into state \\(j\\) and then transitioning out before time \\(t\\) : \\[ \\text{Transition In then Out} = \\int^{t}_{0} {}_{s}p^{\\bar{ii}}_{x} \\cdot \\mu^{ij}_{x}(s) \\cdot (1 - {}_{t-s}p^{\\bar{jj}}_{x+s}) \\] Similarly, the probability of just transitioning, regardless of the state at time \\(t\\) is simply the first two terms: \\[ \\text{Transition In Only} = \\int^{t}_{0} {}_{s}p^{\\bar{ii}}_{x} \\cdot \\mu^{ij}_{x}(s) \\]","title":"Direct Transitions"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#indirect-transitions","text":"The above assumes that there is only ONE WAY of transitioning - directly from state \\(i\\) to state \\(j\\) . Consider a model where there multiple ways to transition : Direct transition from state 0 to 2 (State 0 > State 2) Indirect transition via state 1 (State 0 > State 1 > State 2) \\[ \\text{Indirect Transition 02} = \\int^{t}_{0} {}_{s}p^{01}_{x} \\mu^{12}_{x+s} {}_{t-s}p^{\\bar{22}}_{x+t} \\] Tip The above is based on a simplified example where the target state is absorbing. Generally, it is much easier to just use the complement for such cases: \\[ \\begin{aligned} {}_{t}p^{00}_{x} + {}_{t}p^{01}_{x} + {}_{t}p^{02}_{x} &= 1 \\\\ {}_{t}p^{02}_{x} &= 1 - {}_{t}p^{00}_{x} - {}_{t}p^{01}_{x} \\end{aligned} \\]","title":"Indirect Transitions"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#kolmogorov-equation","text":"Notice that in the previous section, we have implicitly assumed that the system cannot transition back into a previous state . For such cases, a more complex method known as the Kolmogorov's Forward Equation is required. It is a differential equation that can solve for the probability of transition. Note Very loosely speaking, a differential equation is a method of using the derivatives of a function to solve for the original function itself. Loosely speaking, the first derivative of the probability is the change in probability of moving from state \\(i\\) to state \\(j\\) : \\[ \\begin{aligned} \\frac{d}{dt} {}_{t}p^{ij}_{x} &= \\text{P(Start in i, move INTO j)} - \\text{P(Start in i, move OUT of j)} \\\\ &= \\sum_{k \\ne j} {}_{t}p^{ik}_{x} \\cdot \\mu^{kj}_{x}(t) - \\sum_{k \\ne j} {}_{t}p^{ij}_{x} \\cdot \\mu^{jk}_{x}(t) \\end{aligned} \\] Note If the target state is absorbing, then it is impossible to move out from that state. Hence, the second component would be 0 . If solved using Linear Algebra or if the value of the derivative is provided , the kolmogorov equations provide an exact solution . However, that is out of scope for the purposes of the exam. Instead, the equation can be approximately solved using Euler's Method : \\[ \\begin{aligned} \\frac{d}{dt} {}_{t}p^{ij}_{x} &= \\sum_{k \\ne j} [{}_{t}p^{ik}_{x} \\cdot \\mu^{kj}_{x}(t) - {}_{t}p^{ij}_{x} \\cdot \\mu^{jk}_{x}(t)] \\\\ \\frac{{}_{t+h}p^{ij}_{x} - {}_{t}p^{ij}_{x}}{h} &= \\sum_{k \\ne j} [{}_{t}p^{ik}_{x} \\cdot \\mu^{kj}_{x}(t) - {}_{t}p^{ij}_{x} \\cdot \\mu^{jk}_{x}(t)] \\\\ {}_{t+h}p^{ij}_{x} &= {}_{t}p^{ij}_{x} + h \\cdot \\sum_{k \\ne j} [{}_{t}p^{ik}_{x} \\cdot \\mu^{kj}_{x}(t) - {}_{t}p^{ij}_{x} \\cdot \\mu^{jk}_{x}(t)] \\end{aligned} \\] Note Very loosely speaking, Euler's method approximates the first derivative by setting it equal to the tangent of the graph at that point. The key intuition is that it will repeat this process iteratively , until it converges on a solution. Note that if a step size is not provided, it means that the question likely does not expect Euler's method to be used; there should be another method to sovle the equation based on the information given. Tip There are two variations of the method: \\[ \\begin{aligned} \\text{Forward Method} &= \\frac{{}_{t+h}p - {}_{t}p}{h} &= \\frac{d}{dt} {}_{t}p_{x} \\\\ \\text{Backward Method} &= \\frac{{}_{t}p - {}_{t-h}p}{h} &= \\frac{d}{dt} {}_{t}p_{x} \\end{aligned} \\] The key is to remember the following: The change term always follow \\(t\\) The target term is always \\(t+h\\) or \\(t-h\\) ; the positioning is different for the Forward and Backward variation Use the method that best suits the situation - to recurse forwards or backwards - based on the given information. The step size \\(h\\) is usually defined by the question . It affects the number of iterations needed to solve the question. Consider the following model: \\[ \\begin{aligned} \\text{Objective} &= {}_{1}p^{10}_{x} \\\\ \\text{Step Size} &= \\frac{1}{2} \\\\ \\\\ {}_{0.5}p^{10}_{x} &= {}_{0}p^{10}_{x} + 0.5 [{}_{0}p^{11}_{x} \\mu^{10}_{x}(t) - {}_{0}p^{10}_{x} \\mu^{01}_{x}(t)] \\\\ {}_{0.5}p^{11}_{x} &= {}_{0}p^{11}_{x} + 0.5 \\left({}_{0}p^{10}_{x} \\mu^{01}_{x}(t) - {}_{0}p^{10}_{x} [\\mu^{10}_{x}(t) + \\mu^{12}_{x}(t)] \\right) \\\\ \\therefore {}_{1}p^{10}_{x} &= {}_{0.5}p^{10} + 0.5 [{}_{0.5}p^{11}_{x} \\mu^{10}_{x}(t) - {}_{0.5}p^{10}_{x} \\mu^{01}_{x}(t)] \\\\ \\end{aligned} \\] The equation can be solved because the following two items can always be simplified: \\({}_{0}p^{10}_{x} = 0\\) : System cant be in two states at the same time \\({}_{0}p^{11}_{x} = 1\\) : System must be in that state at the current time The above are known as the boundary conditions of the DE. Loosely speaking, they are constraints that must exist in order for the DE to be solved. If the equation is solving for \\({}_{t}p^{ij}_{x}\\) , then the associated boundary condition is \\({}_{0}p^{ij}_{x}\\) . Note The intuition is that we know the probability at the boundary . Using the kolmogorov equations, we can estimate how the probability will change over a given interval \\(h\\) at different time points. Thus, by stacking the estimated changed onto the boundary conditions, we can estimate the probability at any desired duration. Due to the recursive nature of the problem, it is much easier to perform the calculations in Excel: Notice that the two kolmogorov equations (Discrete & Continuous) follow the same intuition - transitioning to some intermediate state before reaching the final state.","title":"Kolmogorov Equation"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#continuous-to-discrete","text":"Using Euler's Method, it can be shown that a discrete markov chain with intervals of \\(h\\) can be used to approximate the kolmogorov equation as well: \\[ \\begin{aligned} {}_{t+h}p^{11}_{x} &= {}_{t}p^{11}_{x} {}_{h}p^{11}_{x+t} + {}_{t}p^{10}_{x} {}_{h}p^{01}_{x+t} \\\\ {}_{t+h}p^{11}_{x} &= {}_{t}p^{11}_{x} (1 - {}_{h}p^{10}_{x+t} + {}_{h}p^{12}_{x+t}) + {}_{t}p^{10}_{x} {}_{h}p^{01}_{x+t} \\\\ {}_{t+h}p^{11}_{x} &= {}_{t}p^{11}_{x} - {}_{t}p^{11}_{x} ({}_{h}p^{10}_{x+t} + {}_{h}p^{12}_{x+t}) + {}_{t}p^{10}_{x} {}_{h}p^{01}_{x+t} \\\\ {}_{t+h}p^{11}_{x} - {}_{t}p^{11}_{x} &= {}_{t}p^{10}_{x} {}_{h}p^{01}_{x+t} - {}_{t}p^{11}_{x} ({}_{h}p^{10}_{x+t} + {}_{h}p^{12}_{x+t}) \\\\ \\frac{{}_{t+h}p^{11}_{x} - {}_{t}p^{11}_{x}}{h} &= {}_{t}p^{10}_{x} \\frac{{}_{h}p^{01}_{x+t}}{h} - {}_{t}p^{11}_{x} \\left(\\frac{{}_{h}p^{10}_{x+t}}{h} + \\frac{{}_{h}p^{12}_{x+t}}{h} \\right) \\\\ \\lim \\left[\\frac{{}_{t+h}p^{11}_{x} - {}_{t}p^{11}_{x}}{h} \\right] &= \\lim \\left[{}_{t}p^{10}_{x} \\frac{{}_{h}p^{01}_{x+t}}{h} - {}_{t}p^{11}_{x} \\left(\\frac{{}_{h}p^{10}_{x+t}}{h} + \\frac{{}_{h}p^{12}_{x+t}}{h} \\right) \\right] \\\\ \\frac{d}{dt} {}_{t}p^{11}_{x} &= {}_{t}p^{10}_{x} \\mu^{11}_{x}(t) - {}_{t}p^{11}_{x} [\\mu^{10}_{x}(t) + \\mu^{12}_{x}(t)] \\end{aligned} \\] Note The key portion of the proof is the derivation of the force of transition. The intuition can be found in this section of FAM-L. The probabilities for the markov chain can be approximated using the following: \\[ \\begin{aligned} {}_{h}p^{ij}_{x} &= h \\cdot \\mu^{ij}_{x}(t) \\\\ {}_{h}p^{ii}_{x} &= 1 - \\sum h \\cdot \\mu^{ik}_{x}(t) \\\\ &= 1 - h \\sum \\mu^{ik}_{x}(t) \\\\ &= 1 - h \\cdot \\mu^{i \\tau}_{x}(t) \\end{aligned} \\] A smaller step size will lead to a more accurate approximation. Tip The key to remember this is that the force of transition represents a transition - thus the approximation is for the transition probability. The probability of staying is thus the complement of the total transition probability. Using the same example as before: \\[ \\begin{aligned} \\text{Pathway 1} &: 1 - 1 - 0 \\\\ \\text{Pathway 2} &: 1 - 0 - 0 \\\\ \\\\ \\therefore {}_{1}p^{10}_{x} &= {}_{0.5}p^{11}_{x} \\cdot {}_{0.5}p^{10}_{x+0.5} + {}_{0.5}p^{10}_{x} \\cdot {}_{0.5}p^{00}_{x+0.5} \\end{aligned} \\] Tip One constraint with this method is whether or not the force of transition at the intemediate ages (EG. \\(x+0.5\\) ) are provided. However, if the force of transition is constant, then the same force can be used for all ages.","title":"Continuous to Discrete"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#insurance-applications","text":"Multi-state assurances and annuities are concpetually similar to their single decrement counterparts - Equivalence Principle, Continuous Discrete Approximation etc. This section highlights the key differences in a multi-state context.","title":"Insurance Applications"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#multi-state-assurances","text":"A Multi-State Assurance pays a benefit whenever the insured transitions into a particular state. The actuarial value of such an assurance is denoted by \\(A^{ij}_{x}\\) , where \\(i\\) is the current state of the insured and \\(j\\) is the state which triggers a payment . The key is understanding that any transition into the state will trigger the payout. Thus, all possible transitions into state must be considered at each time period. Warning It is a common mistake to assume that because there is no direct transition to the insured state, the value of the assurance must be 0. This is incorrect, as all possible transition pathways, regardless of the number of steps required , should be considered. Consider the following example: 2-year discrete term assurance Benefit payment at the end of the year upon transition into state 2 Insured is currently in state 0 \\[ \\begin{array}{|c|c|c|c|} \\hline \\text{Policy Year End } & \\text{State} & \\text{Probability} & \\text{PV Benefit} \\\\ \\hline 0 & \\text{0} & - & - \\\\ \\hline 1 & \\text{0 - 0} & p^{\\bar{00}}_{x} & 0 \\\\ \\hline 1 & \\text{0 - 1} & p^{01}_{x} & 0 \\\\ \\hline 1 & \\text{0 - 2} & p^{02}_{x} & Bv \\\\ \\hline 2 & \\text{0 - 0 - 0} & p^{\\bar{00}}_{x} \\cdot p^{\\bar{00}}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 0 - 1} & p^{\\bar{00}}_{x} \\cdot p^{01}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 0 - 2} & p^{\\bar{00}}_{x} \\cdot p^{02}_{x+1} & Bv^2 \\\\ \\hline 2 & \\text{0 - 1 - 1} & p^{01}_{x} \\cdot p^{\\bar{11}}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 1 - 2} & p^{01}_{x} \\cdot p^{12}_{x+1} & Bv^2 \\\\ \\hline 2 & \\text{0 - 2 - 2} & p^{02}_{x} \\cdot p^{\\bar{22}}_{x+1} & 0 \\\\ \\hline \\end{array} \\] Warning It is a common mistake to assume that the last case of 0 - 2 - 2 should have a benefit listed there as the insured is in state 2 at the end of the year. However, the benefit is only payable if the insured transitions INTO state 2 from some OTHER state . \\[ \\begin{aligned} A^{02}_{x\\enclose{actuarial}{2}} &= p^{02}_{x} \\cdot Bv + p^{00}_{x} p^{02}_{x+1} \\cdot Bv^2 + p^{01}_{x} \\cdot p^{12}_{x+1} \\cdot Bv^2 \\\\ &= p^{02}_{x} \\cdot Bv + (p^{00}_{x} p^{02}_{x+1} + p^{01}_{x} \\cdot p^{12}_{x+1}) \\cdot Bv^2 \\\\ &= p^{02}_{x} \\cdot Bv + {}_{2}p^{02}_{x} \\cdot Bv^2 \\\\ \\end{aligned} \\] Warning The notation for a TA is slightly different from what is normally used. It does not make sense to include the \"1\" in the superscript as it is already being used by the state notation. The key difference from FAM-L is that there are multiple transitions that could lead to a payout, not just death. Thus, it can be more generally expressed as: \\[ \\begin{aligned} A^{ij}_{x} &= \\sum^{\\infty}_{t=0} \\sum_{i \\ne k} v^{t} {}_{t}p^{ik}_{x} \\\\ \\bar{A}^{ij}_{x} &= \\int^{\\infty}_{0} e^{-\\delta t} \\sum_{k \\ne j} {}_{t}p^{ik}_{x} \\mu^{kj}_{x+t} \\end{aligned} \\] Similarly, the relationship between Term and WL assurances remains true, with a slight modification to consider ALL possible state changes : \\[ A^{ij}_{x\\enclose{actuarial}{n}} = A^{ij}_{x} - v^{n} \\sum_{k \\ne j} {}_{n}p^{ik}_{x} A^{kj}_{x+n} \\] The key idea is that at the valuation time, the life is known to be in state \\(i\\) . However, in \\(n\\) years time, the insured could be in any state , INCLUDING the target state. ALL the possibilities must be accounted for. Tip The same relationship holds for multi-state temporary and deferred annuities as well.","title":"Multi State Assurances"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#multi-state-annuities","text":"Conversely, a Multi-State Annuity pays a benefit whenever the insured is in a particular state . The actuarial value is denoted by \\(\\ddot{a}^{ij}_{x}\\) , where \\(i\\) is the current state of the insured and \\(j\\) is the state which triggers a payment . Note The annuity will pay even for re-entries into state \\(j\\) . Consider the following scenario: At time 2, the life transition into state \\(j\\) At time 4, the life transitions into state \\(k\\) At time 6, the life transitions back into state \\(j\\) and remains there The above annuity will make a payment for all times that the life is state \\(j\\) ; from time 2 to 4 as well as after time 6. Warning The general notation assumes that the insured is NOT already in the state that would trigger payment. For instance, an annuity that pays only while the insured is disabled. This is is contrary to the typical life annuity that was covered, where the insured is already alive, and will receive payments for as long as they are alive. Consider the following example: 3-year Discrete Temporary Annuity Benefit payable at the beginning of the year while in state 1 Insured is currently in state 0 \\[ \\begin{array}{|c|c|c|c|} \\hline \\text{Policy Year End} & \\text{State} & \\text{Probability} & \\text{PV Benefit} \\\\ \\hline 0 & \\text{0} & - & - \\\\ \\hline 1 & \\text{0 - 0} & p^{\\bar{00}}_{x} & 0 \\\\ \\hline 1 & \\text{0 - 1} & p^{01}_{x} & Bv \\\\ \\hline 1 & \\text{0 - 2} & p^{02}_{x} & 0 \\\\ \\hline 2 & \\text{0 - 0 - 0} & p^{\\bar{00}}_{x} \\cdot p^{\\bar{00}}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 0 - 1} & p^{\\bar{00}}_{x} \\cdot p^{01}_{x+1} & Bv^{2} \\\\ \\hline 2 & \\text{0 - 0 - 2} & p^{\\bar{00}}_{x} \\cdot p^{02}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 1 - 1} & p^{01}_{x} \\cdot p^{\\bar{11}}_{x+1} & Bv^2 \\\\ \\hline 2 & \\text{0 - 1 - 2} & p^{01}_{x} \\cdot p^{12}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 2 - 2} & p^{02}_{x} \\cdot p^{\\bar{22}}_{x+1} & 0 \\\\ \\hline \\end{array} \\] Tip Even though the policy term is 3 years, there is no need to consider what happens at the end of the third policy year as the benefits are payable at the start of the year - the last benefit is paid at the beginning of the third year (end of the second year). \\[ \\begin{aligned} \\ddot{a}^{01}_{x\\enclose{actuarial}{3}} &= p^{01}_{x} \\cdot Bv + p^{\\bar{00}}_{x} \\cdot p^{01}_{x+1} \\cdot Bv^{2} + p^{\\bar{00}}_{x} \\cdot p^{01}_{x+1} \\cdot Bv^{2} \\\\ &= p^{01}_{x} \\cdot Bv + (p^{\\bar{00}}_{x} \\cdot p^{01}_{x+1} + p^{\\bar{00}}_{x} \\cdot p^{01}_{x+1}) \\cdot Bv^{2} \\\\ &= p^{01}_{x} \\cdot Bv + {}_{2}p^{01}_{x} \\cdot Bv^{2} \\\\ \\end{aligned} \\] Similarly, annuities can be generally expressed as: \\[ \\begin{aligned} \\ddot{a}^{ij}_{x} &= \\sum^{\\infty}_{t=0} v^{t} {}_{t}p^{ij}_{x} \\\\ \\bar{a}^{ij}_{x} &= \\int^{\\infty}_{0} e^{-\\delta t} {}_{t}p^{ij}_{x} \\end{aligned} \\]","title":"Multi State Annuities"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#relationships","text":"There is an interesting relationship among multi-state annuities. Consider the same 3-state model from before: \\[ \\begin{aligned} \\bar{a}^{00}_{x} + \\bar{a}^{01}_{x} + \\bar{a}^{02}_{x} &= \\int^{n}_{0} {}_{t}p^{00}_{x} v^{n} + {}_{t}p^{01}_{x} v^{n} + {}_{t}p^{02}_{x} v^{n} \\\\ &= \\int^{n}_{0} v^{n} \\cdot ({}_{t}p^{00}_{x} + {}_{t}p^{01}_{x} + {}_{t}p^{02}_{x}) \\\\ &= \\int^{n}_{0} v^{n} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{n}} \\\\ \\therefore \\ddot{a}^{00}_{x} + \\ddot{a}^{01}_{x} + \\ddot{a}^{02}_{x} \\\\ &= \\ddot{a}_{\\enclose{actuarial}{\\infty}} \\end{aligned} \\] Warning It is a common mistake to forget the probability of staying in the same state, as most state diagrams do not typically include it. Note The formulas for the annuities certain are from FM and can be found below: \\[ \\begin{aligned} \\ddot{a}_{\\enclose{actuarial}{n}} &= \\frac{1 - v^{n}}{d} \\\\ \\bar{a}_{\\enclose{actuarial}{n}} &= \\frac{1 - v^{n}}{\\delta} \\\\ \\\\ \\ddot{a}_{\\enclose{actuarial}{\\infty}} &= \\frac{1}{d} \\bar{a}_{\\enclose{actuarial}{\\infty}} &= \\frac{1}{\\delta} \\end{aligned} \\] Recall from FAM-L that annuities and assurances are related to one another. In a multi-state setting, it only holds true if the target assurance is transitioning into an ABSORBING STATE : \\[ A^{ik} = 1 - d \\cdot \\sum_{j \\ne k} \\ddot{a}^{ij}_{x} \\\\ \\] Note The above formula is a generalization of the conversion formula in FAM-L. Under a two-state model, the target state is always absorbing while the negated states are always non-absorbing.","title":"Relationships"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#sorjourn-annuity","text":"The definition of \"Sorjourn\" is to stay temporarily . Thus, a Sojourn annuity is an annuity that pays a benefit while the insured stays in a specific state , given that they are already in that state (hence stay). It's acturial value is denoted by \\(\\ddot{a}^{\\bar{ii}}_{x}\\) . They are usually used to calculate the EPV of premiums as premiums are usually only payable while the insured is healthy. Consider the following example: 3-year Discrete Temporary Annuity Premium payable at the beginning of the year while in state 0 Insured is currently in state 0 \\[ \\begin{array}{|c|c|c|c|} \\hline \\text{Policy Year End} & \\text{State} & \\text{Probability} & \\text{PV Benefit} \\\\ \\hline 0 & \\text{0} & - & B \\\\ \\hline 1 & \\text{0 - 0} & p^{\\bar{00}}_{x} & Bv \\\\ \\hline 1 & \\text{0 - 1} & p^{01}_{x} & 0 \\\\ \\hline 1 & \\text{0 - 2} & p^{02}_{x} & 0 \\\\ \\hline 2 & \\text{0 - 0 - 0} & p^{\\bar{00}}_{x} \\cdot p^{\\bar{00}}_{x+1} & Bv^{2} \\\\ \\hline 2 & \\text{0 - 0 - 1} & p^{\\bar{00}}_{x} \\cdot p^{01}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 0 - 2} & p^{\\bar{00}}_{x} \\cdot p^{02}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 1 - 1} & p^{01}_{x} \\cdot p^{\\bar{11}}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 1 - 2} & p^{01}_{x} \\cdot p^{12}_{x+1} & 0 \\\\ \\hline 2 & \\text{0 - 2 - 2} & p^{02}_{x} \\cdot p^{\\bar{22}}_{x+1} & 0 \\\\ \\hline \\end{array} \\] Tip Since the insured already starts in the premium paying state , there is a cashflow at time 0 which was not seen in the other examples. \\[ \\begin{aligned} \\ddot{a}^{\\bar{00}}_{x\\enclose{actuarial}{3}} &= B + p^{\\bar{00}}_{x} \\cdot Bv + p^{\\bar{00}}_{x} \\cdot p^{\\bar{00}}_{x+1} \\cdot Bv^{2} \\\\ &= B + p^{\\bar{00}}_{x} \\cdot Bv + {}_{2}p^{\\bar{00}}_{x} \\cdot Bv^{2} \\end{aligned} \\] More generally, they can be expressed as: \\[ \\begin{aligned} \\ddot{a}^{\\bar{00}}_{x} &= \\sum^{\\infty}_{0} v^{t} {}_{t}p^{\\bar{ii}}_{x} \\\\ \\\\ \\bar{a}^{\\bar{00}}_{x} &= \\int^{\\infty}_{0} e^{-\\delta t} {}_{t}p^{\\bar{ii}}_{x} \\end{aligned} \\] Tip The general multi-state annuity can be expressed in the form of a Sojourn annuity: \\[ \\bar{a}^{ij}_{x} = \\int^{\\infty}_{0} {}_{t}p^{ik}_{x} \\mu^{kj}_{x+t} \\cdot e^{-\\delta t} \\bar{a}^{\\bar{jj}}_{x+t} \\] It represents the probability that the policyholder transitions into state \\(j\\) and receives an annuity at that moment that pays for as long as they remain in that state. This expression is more flexible than the original expression because it allows us to specify the duration of payments. In the above example, a whole life annuity is used, which implies that the benefit will be paid for as long as they are in the state. However, a temporary annuity can be specified instead, which implies that the benefit will be paid a limited period of time EACH TIME the policyholder enters that state: \\[ \\int^{\\infty}_{0} \\sum {}_{t}p^{ik}_{x} \\mu^{kj}_{x+t} \\cdot e^{-\\delta t} \\bar{a}^{\\bar{jj}}_{x+t:\\enclose{actuarial}{n}} \\] Note that the expression is typically what we see of an Assurance, rather than an annuity, which is why it might be confusing to grasp. Warning Note that the final expression in the above example is NOT equivalent to a temporary multi-state annuity: \\[ \\bar{a}^{ij}_{x:\\enclose{actuarial}{n}} \\ne \\int^{\\infty}_{0} {}_{t}p^{ik}_{x} \\mu^{kj}_{x+t} \\cdot e^{-\\delta t} \\bar{a}^{\\bar{jj}}_{x+t:\\enclose{actuarial}{n}} \\] The LHS corresponds to a policy that lasts for \\(n\\) years while the RHS corresponds to a policy that pays for only the first \\(n\\) years on each entry into the state.","title":"Sorjourn Annuity"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#woolhouse-approximation","text":"Recall that the Woolhouse Approximation was used to approximate Continuous or Monthly annuities from their yearly discrete variants. Skipping the formal proof, the three term woolhouse approximation in a multi-state setting is as follows: \\[ \\begin{aligned} \\ddot{a}^{(m)ij}_{x} &= \\ddot{a}^{ij}_{x} + \\frac{m^{2} - 1}{12m^{2}} \\mu^{ij}_{x}(t) \\\\ \\\\ \\ddot{a}^{(m)\\bar{ii}}_{x} &= \\ddot{a}^{\\bar{ii}}_{x} - \\frac{m-1}{2m} - \\frac{m^{2} - 1}{12m^{2}} [\\mu^{i \\tau}_{x}(t) + \\delta] \\end{aligned} \\] The above is not a typo; there are different formulas for the Soujourn and Non-Sourjourn annuities. Both are provided on the formula sheet. Tip Despite the non-sojourn annuity having only two terms , it is still known as the three term woolhouse approximation. If asked for the two term approximation, drop the last term . The approximation simply equal to the existing value. One interesting use case is to thus approximate the payable m-thly case from the continuous case using the two term approximation: \\[ \\ddot{a}^{(m)ij}_{x} = \\ddot{a}^{ij}_{x} \\] This especially relevant for the Standard Sickness Death Model, where the continuous annuity functions are provided. By assuming m=1, the discrete annuity functions can be obtained. Sojourn approximations are closest to the typical life annuity covered. Thus, it is expected that the sojourn approximation is almost identical to the original approximation. Note The force term is meant to be of the OPPOSITE of the annuity event . In FAM-L, the force was the force of dying, opposite of the annuity benefit which pays while living. For a sojourn annuity, the opposite of staying in the same state is a transition to any other state . Non-sojourn approximations are seemingly similar to the usual approximations, but there are multiple differences : There are only two terms instead of three (Usual second term is missing) The sign is flipped on the 'third' term ('+' instead of '-') The force term is of the SAME annuity event","title":"Woolhouse Approximation"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#reserves","text":"Recall that reserves are calculated as the EPV of the net outflows each period. In a multi-state setting, the outflows are dependent on the state of the insured at the time of valuation. Thus, the reserve must reflect two dimensions - both time and state. \\[ {}_{t}V^{(i)} = \\text{EPV(Benefits)}_{t}^{i} + \\text{EPV(Expenses)}_{t}^{i} - \\text{EPV(Premiums)}_{t}^{i} \\] Tip Note that some terms may naturally evaluate to 0 . For instance, Premiums may not be payable while in that state Certain benefits may not payable in that state","title":"Reserves"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#thieles-de","text":"Reserves for a continuous assurance or annuity can be instead calculated using Thiele's Differential Equation : \\[ \\begin{aligned} \\frac{d}{dt} {}_{t}V^{i} &= \\delta_{t} \\cdot {}_{t}V^{i} \\underbrace{+ P_{t}^{i} - B_{t}^{i} - e_{t}^{i}}_{\\text{Net In-State Outgo}} - \\sum_{j \\ne i} \\mu^{ij}_{x}(t) \\cdot (b^{ij} + {}_{t}V^{j} - {}_{t}V^{i}) \\end{aligned} \\] Tip If one of the target states is absorbing (EG. Death), then the reserve for that state is 0 as there are no more future payments. Warning One implicit assumption made is that the reserve is computed before receiving premiums. If the question states that the basis is after premiums, add the premiums to the opening reserve INSTEAD . The difference is material as it affects whether or not the premiums earn interest. Similar to before, the DE is trying to approximate the change in the reserve : Reserve earns interest Released to pay off expected net benefits and expenses (while in state & upon transition) Form a new reserve for every possible resulting state Warning Do NOT mix up the two benefit variables: Big B : Rate of benefit payment while in the current state Small B : Lump Sum benefit payment upon transition into other state The reserve item is the expected CHANGE in reserve - not just the expected future reserve. Do NOT confuse this with profit testing basis. Similarly, the DE can be solved using Euler's Method . Thiele's DE usually uses the backward variation instead: \\[ \\begin{aligned} \\frac{d}{dt} {}_{t}V^{i} &= \\delta_{t} \\cdot {}_{t}V^{i} + P - B - E - \\sum_{j \\ne i} \\mu^{ij}_{x}(t) \\cdot (b^{ij} + {}_{t}V^{j} - {}_{t}V^{i}) \\\\ \\frac{{}_{t}V^{i} - {}_{t-h}V^{i}}{h} &= \\delta_{t} \\cdot {}_{t}V^{i} + P - B - E - \\sum_{j \\ne i} \\mu^{ij}_{x}(t) \\cdot (b^{ij} + {}_{t}V^{j} - {}_{t}V^{i}) \\\\ {}_{t}V^{i} &= {}_{t-h}V^{i} + h \\cdot \\left(\\delta_{t} \\cdot {}_{t}V^{i} + P - B - E - \\sum_{j \\ne i} \\mu^{ij}_{x}(t) \\cdot (b^{ij} + {}_{t}V^{j} - {}_{t}V^{i}) \\right) \\end{aligned} \\] Tip Similar to Kolmogorov forward equation, these type of questions are best done on Excel to easily perform the recursions:","title":"Thiele's DE"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#recursion","text":"One method of determining the reserve for the euler's method is to use Recursion . It follows the same intuition as before, with the key difference having to account for the multiple possibilities for the expected benefits and ending reserve. \\[ \\begin{aligned} ({}_{t}V^{i} + P - e - B^{\\text{In State}})(1+i) &= \\sum p^{ik}_{x+t} \\cdot B^{\\text{Upon Transition}} + \\sum p^{ik}_{x+t} \\cdot {}_{t}V^{k} \\end{aligned} \\]","title":"Recursion"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/3.%20Multi%20State%20Models/#impact-assessment","text":"For most multi-state models, the policy does not terminate upon transitioning to another state. Thus, when assessing the impact of a change in probability on EPVs, it is necessary to consider two levels of impact: First Order Impact : Increased probability of transition to the target state; policyholder will transition sooner , resulting in an increase to the EPV Second Order Impact : From the affected state , the probability of transitioning into other states maybe higher or lower, thus causing the EPV of those states to change as well","title":"Impact Assessment"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/","text":"Multi State Applications \u00b6 Disability Income Insurance \u00b6 Disability Income Insurance (DII) is a contract that pays out routine benefits to the insured for as long as they are unable to work due to a disability (\"Unhealthy\"). The primary purpose of DII is to replace a portion of the income of the insured while they are unable to work. Note Benefits are paid out periodically (rather than a lump sum) is usually due to the following reasons: The intention is to replace the salary that the individual would have otherwise earned; which is on a periodic basis Avoid investment risk on the insured's end; if they receive a lump sum, they might not have the ability to manage the funds in a manner to tide them through the period of their disability Product Features \u00b6 Premiums are usually non-guaranteed and level throughout the policy term, payable regularly as long as the insured is healthy . Benefits are payable regularly while the insured is unhealthy , till the end of the benefit term or till the insured recovers ( whichever comes first ). There is typically a waiting period which is the minimum amount of time that must pass since becoming disabled before a benefit is paid out. Note There are several reasons for implementing waiting periods: Reduces anti-selection : Individuals are less likely to abuse the coverage if they have to wait Expense Control : Many illnesses are short-term in nature; reduces claims expenses incurred Affordability : Most individuals have coverage in the short-term (Savings/Employer). Need coverage for longer term illnesses once those sources are depleted; having elimintation period helps to make the coverage more affordable The Off Period is the duration of time that must elapsed between claims for each claim to be treated as seperate instances of disability. It is is counted from the END of the first disability period (when they stop receiving benefits due to recovering or the benefit term ending). Note This is important as the benefit term is the maximum amount of time that benefits are payable for each instance of disability. If the claims are treated as the same instance, they would count towards the same benefit term and potentially receive fewer benefit payments as the maximum has been reached. However, being treated as part of the same instance could potentially result in more payments as the insured does not need to incur another waiting period. The magnitude of benefit is dependent on the type of coverage purchased. The more likely the coverage is to be claimed, the lower the benefit: Total Disability : Unable to perform any job; unable to earn any income Partial Disability : Unable to perform own job; unable to earn original income Benefits for partial disability (own job) are smaller than total disability (any job) Note The magnitude of benefit received is typically smaller than the original salary of the insured due to the following reasons: To incentivize the insured to make a full recovery and return to work Annuity payments have no or lower tax rates; lower gross amount to match the same net income Modelling \u00b6 DII is modelled using the Sickness Death Model : State 0 : Healthy (No disability) State 1 : Sick (Disability present) State 2 : Dead The key feature that differentiates DII from a regular annuity is the waiting period: The true term of the policy is \\(n-w\\) ; the policy would expire before the waiting period is over if they become disabled after that time Insured becomes disabled at age \\(x+t\\) but benefits only start at age \\(x+t+w\\) ; functionally a deferred annuity Note Recalled that a deferred annuity can be expressed as the difference between a life annuity and an annuity certain equal to the deferral period: \\[ {}_{w \\mid}\\bar{a}_{x:\\enclose{actuarial}{n}} = \\bar{a}_{x:\\enclose{actuarial}{n}} - \\bar{a}_{\\enclose{actuarial}{w}} \\] The deferred period is an annuity certain, as the waiting period is known and fixed. The typical expression for multi-state annuities cannot be used due to the waiting period. This is because the expression implicitly assumes that payments will start immediately upon transitioning into the state. However, for DII, the payments must be delayed. Thus, the alternative, more flexible expression can be used instead. The annuity to specify is \\(w\\) deferred annuity, which implies that the policy will only pay AFTER \\(w\\) periods upon transition into the target state: \\[ \\begin{aligned} \\text{EPV(Benefits)} &= \\int^{n-w}_{0} {}_{t}p^{ik}_{x} \\mu^{kj}_{x+t} \\cdot e^{-\\delta t} \\cdot \\left(\\bar{a}^{\\bar{jj}}_{x+t:\\enclose{actuarial}{n-t}} - \\bar{a}^{\\bar{jj}}_{x+t:\\enclose{actuarial}{w}} \\right) \\end{aligned} \\] The key is understanding the limits of the integration: Upper Limit : Worst Case scenario; time where policyholder receives NO benefits because the policy will end exactly as the waiting period is finished Lower Limit : Best Case scenario; time where policyholder receives the MAXIMUM number of payments because they become disabled as soon as the policy incepts Special Case \u00b6 The above implicitly assumes that there is no benefit term . If there was a benefit term of \\(m\\) , the following expression would be used instead: \\[ \\begin{aligned} \\text{EPV(Benefits)} &= \\int^{n-m-w}_{0} {}_{t}p^{ik}_{x} \\mu^{kj}_{x+t} \\cdot e^{-\\delta t} \\cdot \\left(\\bar{a}^{\\bar{jj}}_{x+t:\\enclose{actuarial}{m+w}} - \\bar{a}^{\\bar{jj}}_{x+t:\\enclose{actuarial}{w}} \\right) \\\\ &+ \\int^{n-w}_{n-m-w} {}_{t}p^{ik}_{x} \\mu^{kj}_{x+t} \\cdot e^{-\\delta t} \\cdot \\left(\\bar{a}^{\\bar{jj}}_{x+t:\\enclose{actuarial}{n}} - \\bar{a}^{\\bar{jj}}_{x+t:\\enclose{actuarial}{w}} \\right) \\end{aligned} \\] With a specified benefit term, the previous EPV needs to be split into two components to account for differences in benefits payout across the two periods: The first case is that policyholder transitions sufficiently early , such that they can receive the full benefits before the end of the policy: Note The premise is that the policyholder will be able to receive the full benefits. Thus, the worst and best case is about the timing of when they receive it: Best Case : Earliest possible time to receive the full benefits Worst Case : Latest possble time to receive the full benefits The other case is that the Policyholder transitions succifiently late , such that they only receive a portion of the benefits because the policy would end before the full payout is made: Note The premise is that the policyholder will NOT be able to receive the full benefits. Thus, the worst and best case is about how much benefits that they can receive: Best Case : Time where the policyholder will still receive some benefits (but not all) Worst Case : Time where the policyholder will receive NO benefits Long Term Care Insurance \u00b6 Long Term Care Insurance (LTCI) is a contract that pays out routine benefits to the insured for as long as they cannot live independently (\"Unhealthy\"), requiring long term care. LTCI is a form of health insurance whose primary purpose is to cover the costs of administering LTC, such as nursing homes. Being unable to live independently is defined as being unable to independently perform one or more Activities of Daily Living (ADL): Transferring (getting in and out of a bed or chair) Continence (ability to control bladder and bowel functions) Toileting (ability to use the toilet and manage personal hygiene) Bathing Dressing Eating Tip It is best to remember this in the form of a story when waking up in the morning: Transferring (Wake up and get out of the bed ) Continence (Go to the toilet to pee) Toileting (Brushing teeth) Bathing (Washing up) Dressing (Getting changed for the day) Eating (Breakfast) Product Features \u00b6 Premiums are usually non-guaranteed and level throughout the policy term, payable regularly as long as the insured is healthy . Benefits are also payable regularly while the insured is unhealthy, till the earliest of the benefit term or the death of the insured. However, they typically come in two forms: Reimbursements : Paid to caregiving organization ; subject to a certain limit Fixed Payments : Paid to policyholder ; flexibility to use the cash Note LTCI is also subject to a waiting and off period to avoid adverse selection. Hybrid Features \u00b6 In practice, LTCI may be sold as a Rider to a main life insurance plan rather than being a standalone product. In these cases, there are some additional features that unique to the rider. One common feature would be a Return of Premiums (ROP) benefit. If the amount of benefit paid out under the LTCI portion is less than the total premiums paid , the shortfall will be returned by being added to the death benefit of the policy. Another feature would be Benefit Acceleration . LTCI benefits would be paid out from the sum assured of the policy, reducing it accordingly. The leftover amount would be paid out as the death benefit of the policy. If there is an acceleration feature, then the policyholder could also opt in for an Extension of Benefits feature. It ensures that the benefit payments will continue for an extended term even after the sum assured has been depleted. Modelling \u00b6 For the purposes of this exam, LTCI can be modelled using the following 5 state model: State 0 : Able to perform 5 or more ADLs independently ( Healthy ) State 1 : Only able to perform exactly 4 ADLs independently ( Disabled ) State 2 : Only able to perform 3 or fewer ADLs independently ( Severely Disabled ) State 3 : Cognitive Impairment State 4 : Dead There are several points to take note of: The states are defined by the number of ADLs they CAN perform rather than cannot (Slightly counter-intuitive) The insured can recover from physical impairment but cannot recover from cognitive impairment Given the complexity of the model, it is unlikely that a question would ask for the calculation of premiums or reserves directly. They would most likely require the use of the Kolmogorov or Thiele's approximation methods . Continuing Care Retirement Communities \u00b6 One example of LTC is a Continuing Care Retirement Community (CCRC). They are residential facilities dedicated to providing care to seniors of varying needs : Independent Living Units (ILU): Minimal assistance provided Assisted Living Units (ALU): Only non-medical assistance provided (helping with ADL) Skilled Nursing Facility (SNF): Medical assistance provided Product Features \u00b6 There are three types of arrangements that people can opt for: Type A Type B Type C Full Life Care Modified Life Care Fee For Service High Entry Fee Medium Entry Fee Low Entry Fee Level payments for all units Payments step up for new units Pay per use at market rate Youngest Middle Age Oldest Note Type A and B contracts are considered insurance contracts as they transfer the risk of increasing healthcare cost to the CCRC provider. Similar to life insurance, the monthly payments are higher than the actual cost of care in the early ages (Pre-funding) and smaller than what the actual cost is later on. As such, type A and B customers must go through medical underwriting to prove that they are healthy enough to live independently at the time of entry. Since health tends to deteriorate with age, this is why type A and B individuals tend to be younger. Some contracts also offer an Entry Fee Refund feature where a portion of the initial entry fee is paid to their beneficiaries when they leave the CCRC. A variation of the above is that the entry fee is used to buy an ownership stake in the unit they are living in, which will be sold to the next occupier when they move out. The proceeds will then be split between the individual and CCRC , functionally returning part of the entry fee. Modelling \u00b6 CCRC can be modelled using the following 5 state model: State 0 : Independent Living Unit ( Healthy ) State 1 : Assisted Living Unit ( Disabled ) State 2 : Skilled Nursing Facility ( Severely Disabled ) State 3 : Temporary Skilled Nursing Facility ( Medical Emergency ) State 4 : Dead One key difference from the LTCI model is that this model does NOT allow for recoveries . This greatly simplifies the model, which opens up the type of questions that can be asked. Info Intuitively, the reason is because CCRC is open to Seniors only , whose health will undoubtedly deteriorate with time. Critical Illness Insurance \u00b6 Critical Illness Insurance (CII) is a contract that pays out a lump sum benefit to the insured upon being diagnosed with a critical illness . Similar to life insurance, the primary purpose of CII is to replace the income of the insured as they are unable to work while they are recovering from CI. The definition of CI varies from insurer to insurer, but most insurers cover a basic set of illnesses such as Heart Attack, Stroke, Major Organ Failure and most common forms of Cancer. Product Features \u00b6 Premiums are usually guaranteed and level throughout the policy term, payable regularly as long as the insured is healthy . An additional lump sum benefit is payable when the insured becomes diagosed with a CI. The benefit ceases once the benefit has been paid - but the policy does not necessarily lapse as a death benefit may be payable as well. Unlike LTCI, CII main plans themselve could offer a partial Return of Premium feature if the policy expires or lapses with no benefits paid. Warning Although using the name, the ROP feature for CII and LTCI function very differently. CII ROP can only be used if NO claim was made at all. Hybrid Features \u00b6 Similarly, CII could be instead offered as a rider to a life insurance policy. In this case, the CI benefit will accelerate the death benefit and thus reduce the sum assured accordingly: Balance left : CI benefit only partially accelerated the DB; remaining amount payable on death No balance left : CI benefit fully accelerated the DB; nothing payable left thus policy lapses Modelling \u00b6 CII can be modelled using any of the following models: Model 1 : CI Benefits are fully accelerated Model 2 : CI Benefits are partially accelerated Model 3 : CI Benefits as an additional lump sum The key is remembering that policy does NOT terminate upon being diagnosed with CI. Thus, the policy will still hold a reserve for the remaining death benefit. Warning Technically, any of the models can be used for any of the type of CI with the correct set up. The proposed models above are the models that best suit the structure. Questions that contain a Return of Premium feature usually also provide the Increasing Assurance factor \\(\\text{(IA)}_{x}\\) . There are two main variations: Case 1 : The ROP feature starts immediately - only the IA factor is needed Case 2 : The ROP feature starts later - combination of A and IA factor must be used \\[ \\begin{aligned} \\text{EPV}_{\\text{Case 1}} &= P \\cdot \\text{(IA)}_{x} \\\\ \\text{EPV}_{\\text{Case 2}} &= {}_{n}E_{x} \\cdot [n \\cdot P \\cdot A_{x+n} + P \\cdot \\text{(IA)}_{x+n}] \\end{aligned} \\] Structured Settlements \u00b6 Structured Settlements are an arrangement in a liability case , where the Responsible Party (RP) must compensate the Injured Party (IP). Compensation typically comes in the form of an annuity with monthly benefits, with similar reasoning to DII. The compensation schedule is usually determined by the RP\u2019s Liability Insurance policy. The RP could either be another person (EG. Motor Insurance) or could be a corporation (EG. Worker\u2019s Compensation). Product Features \u00b6 The magnitude of annuity payment is typically smaller than the the original income for reasons similar to DII. However, one unique reason for structured settlements is that the IP could be partially at fault for the accident as well, thus should bear part of the cost. If the damage is severe , the long-term care needed will only be known after the IP goes through an initial period of treatment, till they reach the point of Maximum Medical Improvement (MMI). Before MMI, the RP will pay an interim benefit till the settlment is finalized. Modelling \u00b6 For the purposes of this exam, only structured settlements involving Workers Compensation will be covered. The key difference is that they are reviewable . This means that the annuity benefits can be conditional and stop once the individual is recovered (rather than being for life under a non-reviewable settlement). Note One problem with reviewable settlements is Anti-Selection . Since the payments will stop upon recovery, the IP has an incentive NOT to return to work even if they can. There are also additional expenses involved to prove monitor the health of the IP over time and prove that they are recovered. Reviewable Workers Compensation settlements can be modelled using the following 4 state model: State 0 : Uncertain Diagnosis State 1 : Recovered State 2 : Permanently Disabled State 3 : Dead Multi Life Model \u00b6 The multi-life model discussed previously can be expressed in the form of the following multi-state model: For multi-life questions, it is recommended to default to the above model to aid how we think about the problem. It is a common mistake to forget to consider future possibilities of only one of the lives being alive; using the model will reduce the chance of this error. Tip To determine the assurance factor for death, the conversion relationship can be used, where \\(k\\) is the absorbing state: \\[ \\begin{aligned} A^{ik}_{x} &= 1 - d \\cdot \\sum_{j \\ne k} \\ddot{a}^{ij}_{x} \\\\ \\therefore A^{03}_{x} &= 1 - d \\cdot (\\ddot{a}^{00}_{x} + \\ddot{a}^{01}_{x} + \\ddot{a}^{02}_{x}) \\end{aligned} \\] Warning There is one key difference when comparing the above model to the usual multi-life probabilities. Consider the following two probabilities: \\({}_{t}p^{01}_{x:y}\\) : Probability that ONLY \\(x\\) dies within \\(t\\) years; \\(y\\) dies AFTER \\(t\\) \\({}_{t}q^{1}_{x:y}\\) : Probability that \\(x\\) dies within \\(t\\) years; \\(y\\) can die before OR after \\(t\\) The confusion comes about because both are a situation where \\(X\\) dies before \\(Y\\) but the key difference is whether or not \\(Y\\) dies within \\(t\\) . The difference of the two is the probability that \\(y\\) dies second within \\(t\\) years : \\[ \\begin{aligned} {}_{t}q^{1}_{x:y} - {}_{t}p^{01}_{x:y} &= {}_{t}q^{\\ \\ \\ 2}_{x:y} \\\\ {}_{t}q^{1}_{x:y} &= {}_{t}q^{\\ \\ \\ 2}_{x:y} + {}_{t}q_{x} \\cdot {}_{t}p_{y} \\end{aligned} \\] The above is one of the key relationships covered in the multi-life section. Dependence \u00b6 One common question involving this model is to determine whether the future lifetimes are dependent. Even if the model does NOT have a common shock, it is still possible to be dependent in other ways : Broken Heart Syndrome : Increase in mortality of surviving partner following partner's death Common Lifestyle : Couples tend to have similar lifestyles which results in similar mortalities (EG. Smokers) Common Shock : Couples tend to be exposed to the same risks , making it likely that they die together The ONLY definitive way to determine independence is to compare the forces of transition, to determine if the forces are dependent on the other life being alive. The following pairs of forces are measuring the same event of either life dying: Death of \\(x\\) : \\(\\mu^{02}\\) and \\(\\mu^{13}\\) Death of \\(y\\) : \\(\\mu^{01}\\) and \\(\\mu^{23}\\) The difference within each pair is whether or not the other life is alive. If the lives are independent, then the presence of the other life should not matter; the two should be equal . Note If the force of the surviving lives are higher than when both are alive, it is an example of Broken Heart Syndrome : \\[ \\begin{aligned} \\mu^{13} \\gt \\mu^{02} \\\\ \\mu^{23} \\gt \\mu^{01} \\end{aligned} \\] Common Shock Variation \u00b6 If a Common Shock is allowed for, there will be an additional transition from state 0 to state 3 that could kill both lives simultaneously : Recall from the multi-life section that the common shock affects all probabilities, even those affecting just a single life . The above model already has the common shock parameter baked into the transition intensities . Thus, a more intuitive version of the model that matches the previous analysis would be the following: The above multi-state approach will produce the same results as the original random variable approach: \\[ \\begin{aligned} {}_{t}p_{x:y} &= {}_{t}p^{00}_{x:y} \\\\ &= e^{- (\\int \\mu^{01} + \\lambda + \\mu^{02} + \\lambda - \\lambda)} \\\\ &= e^{- (\\int \\mu^{01} + \\mu^{02} + \\lambda)} \\\\ &= e^{- (\\int \\mu^{01} + \\mu^{02})} e^{-(\\lambda)} \\\\ &= {}_{t}p_{y} \\cdot {}_{t}p_{x} \\cdot e^{-\\lambda} \\end{aligned} \\] Warning If asked to calculate the reserves for this model, note that in state 1 and 2, only one life remains. Thus, the corresponding single life functions should be used, NOT multi-life! If the two lives are of the same age, the above model can be instead simplified into a three state model : Multi-Decrement Model \u00b6 A multi-decrement model can be expressed as the following multi-state model: All multi-state concepts apply to this model as well. Model Estimation \u00b6 Similar to FAM-L, the goal of this subsection is to estimate the Force of Transition using Maximum Likelihood Estimation . Recall that the goal of MLE is to determine the population parameter (Force of Transition) that most likely resulted in the observed sample. Recall that the likelihood function is the joint probability density of all observtions (assuming sample is iid). Thus, each observation contributes its probability density to the likelihood: \\[ \\begin{aligned} L(\\theta \\mid x) &= \\text{General PDF} \\cdot \\text{Probability of Observation} \\\\ &= {}_{t}p^{\\bar{ii}}_{x} \\sum \\mu^{i(\\tau)}_{x+t} \\cdot \\frac{\\mu^{ij}_{x+t}}{\\mu^{i(\\tau)}_{x+t}} \\\\ &= {}_{t}p^{\\bar{ii}}_{x} \\cdot \\mu^{ij}_{x+t} \\end{aligned} \\] Most questions assume that the Force of Transition is constant between integer ages. Thus, the fractional age of the policyholder does not matter , allowing the above to be expanded to: \\[ \\begin{aligned} L(\\theta \\mid x) &= {}_{t}p^{\\bar{ii}}_{x} \\cdot \\mu^{ij}_{x} \\\\ &= e^{-t \\cdot \\sum \\mu^{i(\\tau)}_{x}} \\cdot \\mu^{ij}_{x} \\end{aligned} \\] Thus, the likelihood function for the entire sample is the product of all individual likelihood functions: \\[ \\begin{aligned} L(\\theta) &= \\prod L(\\theta \\mid x) \\\\ &= e^{-T^{i}_{x} \\cdot \\sum \\mu^{i(\\tau)}_{x}} \\cdot (\\mu^{ij}_{x})^{D^{ij}_{x}} \\cdot (\\mu^{ik}_{x})^{D^{ik}_{x}} \\cdot e^{-T^{j}_{x} \\cdot \\sum \\mu^{j(\\tau)}_{x}} \\cdot (\\mu^{jk}_{x})^{D^{jk}_{x}} \\cdot (\\mu^{ji}_{x})^{D^{ji}_{x}} \\\\ &= e^{-T^{i}_{x} \\cdot \\sum \\mu^{i(\\tau)}_{x} - T^{j}_{x} \\cdot \\sum \\mu^{j(\\tau)}_{x}} \\cdot (\\mu^{ij}_{x})^{D^{ij}_{x}} \\cdot (\\mu^{ik}_{x})^{D^{ik}_{x}} \\cdot (\\mu^{jk}_{x})^{D^{jk}_{x}} \\cdot (\\mu^{ji}_{x})^{D^{ji}_{x}} \\end{aligned} \\] The likelihood function can always be simplified into the above, where: \\(T^{i}_{x}\\) : Total observed time spent in state \\(i\\) at age \\(x\\) \\(D^{ij}_{x}\\) : Total observed transitions from state \\(i\\) to \\(j\\) at age \\(x\\) Note There are two things to be aware of: Total : Implies for ALL individuals in the study Observed : Implies starting from when the study starts Note Some questions might use lower case variables to represent the time spent and observations for a single observation and use upper case for the entire sample . The log-likehood function can thus be shown to be: \\[ \\begin{aligned} \\ell(\\theta) &= \\ln L(\\theta) \\\\ &= -T^{i}_{x} \\cdot \\sum \\mu^{i(\\tau)}_{x} - T^{j}_{x} \\cdot \\sum \\mu^{j(\\tau)}_{x} \\\\ &+ D^{ij}_{x} \\ln \\mu^{ij}_{x} + D^{ik}_{x} \\ln \\mu^{ik}_{x} + D^{jk}_{x} \\ln \\mu^{jk}_{x} + D^{ji}_{x} \\ln \\mu^{ji}_{x} \\end{aligned} \\] Note There may be odd situations where there are multiple years being studied or when the individuals do not start at an intger age. In these cases, it is best to visualize the transitions using a table: The MLE estimate for the forces of transition can be found by maximizing the log-likelihood function . Since there are multiple forces involved, the maximum can be found by taking the partial derivative with respect to each of the forces: \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\mu^{ij}_{x}} &= 0 \\\\ -T^{i}_{x} + \\frac{D^{ij}_{x}}{\\hat{\\mu}^{ij}_{x}} &= 0 \\\\ \\\\ \\therefore \\hat{\\mu}^{ij}_{x} &= \\frac{D^{ij}_{x}}{T^{i}_{x}} \\\\ &= \\frac{\\text{Number of Transitions To Target State}}{\\text{Time Spent In Current State}} \\end{aligned} \\] Generally speaking, estimates are more reliable given a larger number of data points . Since most policyholders incept at state 0, estimates for transitions out of state 0 are generally more reliable given the larger pool of experience. Tip The above formula can be used WITHOUT proof - there is no need to go through the process of deriving the likelihood function. Note The MLE estimates for each of the transition intensities are independent . This is because the cross-derivatives for each pair of transition intensities are zero: \\[ \\frac{\\partial^{2}}{\\partial \\mu^{ij}_{x} \\partial \\mu^{jk}_{x}} = 0 \\] Variance \u00b6 The Variance of the above MLE estimate can be approximated using Fisher's Information . The proof is beyond the scope of this exam. It is sufficient to know the following: The Asymptotic Variance is the Inverse of the Fisher's Information Function Fisher's Information is the Expected Value of the Second Partial Derivative \\[ \\begin{aligned} \\text{Var} (\\hat{\\mu}^{ij}_{x}) &= [I(\\mu^{ij}_{x})]^{-1} \\\\ &= \\left(- E \\left[\\frac{\\partial^{2}}{\\partial (\\mu^{ij}_{x})^{2}} \\right] \\right)^{-1} \\\\ &= \\left(- E \\left[- \\frac{D^{ij}_{x}}{(\\mu^{ij}_{x})^{2}} \\right] \\right)^{-1} \\\\ &= \\left(\\frac{E[D^{ij}_{x}]}{(\\hat{\\mu}^{ij}_{x})^{2}} \\right)^{-1} \\\\ &= \\frac{(\\mu^{ij}_{x})^{2}}{E[D^{ij}_{x}]} \\\\ &= \\frac{(\\mu^{ij}_{x})^{2}}{D^{ij}_{x}} \\end{aligned} \\] Note that the force of transition in the above is the ACTUAL force of transition. It can be approximated using the MLE estimate of the force: \\[ \\begin{aligned} \\text{Var} (\\hat{\\mu}^{ij}_{x}) &= \\frac{(\\mu^{ij}_{x})^{2}}{D^{ij}_{x}} \\\\ &= \\frac{\\left(\\frac{D^{ij}_{x}}{T^{i}_{x}} \\right)^{2}}{D^{ij}_{x}} \\\\ &= \\frac{D^{ij}_{x}}{(T^{i}_{x})^{2}} \\end{aligned} \\] Confidence Interval \u00b6 Using the variance of the estimator, the Linear Confidence Interval can be derived: \\[ \\begin{aligned} \\hat{\\mu}^{ij}_{x} \\pm Z_{\\frac{1+\\alpha}{2}} \\cdot \\sqrt{\\text{Var} (\\hat{\\mu}^{ij}_{x})} \\\\ \\left(\\hat{\\mu}^{ij}_{x} + Z_{\\frac{1+\\alpha}{2}} \\cdot \\sqrt{\\text{Var} (\\hat{\\mu}^{ij}_{x})}, \\hat{\\mu}^{ij}_{x} - Z_{\\frac{1+\\alpha}{2}} \\cdot \\sqrt{\\text{Var} (\\hat{\\mu}^{ij}_{x})} \\right) \\end{aligned} \\] Probabilities \u00b6 Recall that the probability of staying in a particular state can be given by: \\[ {}_{t}p^{\\bar{ii}}_{x} = e^{- \\mu^{i(\\tau)}_{x}} \\] Thus, the MLE estimate of the probability uses the MLE estimate of the forces exiting the state : \\[ \\begin{aligned} \\hat{\\mu}^{i(\\tau)}_{x} &= \\sum \\hat{\\mu}^{ik}_{x} \\\\ \\therefore {}_{t}\\hat{p}^{\\bar{ii}}_{x} &= e^{- \\hat{\\mu}^{i(\\tau)}_{x}} \\end{aligned} \\] Since the individual MLE estimates are independent of another, the variance of the above is simply the sum of the individual variances : \\[ \\text{Var} \\left(\\hat{\\mu}^{i(\\tau)}_{x} \\right) = \\sum \\text{Var} \\left(\\hat{\\mu}^{ik}_{x} \\right) \\] The confidence interval of the probability is the probabilities computed using the resulting confidence interval of the forces . This can be applied for any function involving MLE estimates: \\[ (e^{- \\hat{\\mu}^{i(\\tau)}_{x, \\text{Lower Bound}}}, e^{- \\hat{\\mu}^{i(\\tau)}_{x, \\text{Upper Bound}}}) \\]","title":"Multi State Applications"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#multi-state-applications","text":"","title":"Multi State Applications"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#disability-income-insurance","text":"Disability Income Insurance (DII) is a contract that pays out routine benefits to the insured for as long as they are unable to work due to a disability (\"Unhealthy\"). The primary purpose of DII is to replace a portion of the income of the insured while they are unable to work. Note Benefits are paid out periodically (rather than a lump sum) is usually due to the following reasons: The intention is to replace the salary that the individual would have otherwise earned; which is on a periodic basis Avoid investment risk on the insured's end; if they receive a lump sum, they might not have the ability to manage the funds in a manner to tide them through the period of their disability","title":"Disability Income Insurance"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#product-features","text":"Premiums are usually non-guaranteed and level throughout the policy term, payable regularly as long as the insured is healthy . Benefits are payable regularly while the insured is unhealthy , till the end of the benefit term or till the insured recovers ( whichever comes first ). There is typically a waiting period which is the minimum amount of time that must pass since becoming disabled before a benefit is paid out. Note There are several reasons for implementing waiting periods: Reduces anti-selection : Individuals are less likely to abuse the coverage if they have to wait Expense Control : Many illnesses are short-term in nature; reduces claims expenses incurred Affordability : Most individuals have coverage in the short-term (Savings/Employer). Need coverage for longer term illnesses once those sources are depleted; having elimintation period helps to make the coverage more affordable The Off Period is the duration of time that must elapsed between claims for each claim to be treated as seperate instances of disability. It is is counted from the END of the first disability period (when they stop receiving benefits due to recovering or the benefit term ending). Note This is important as the benefit term is the maximum amount of time that benefits are payable for each instance of disability. If the claims are treated as the same instance, they would count towards the same benefit term and potentially receive fewer benefit payments as the maximum has been reached. However, being treated as part of the same instance could potentially result in more payments as the insured does not need to incur another waiting period. The magnitude of benefit is dependent on the type of coverage purchased. The more likely the coverage is to be claimed, the lower the benefit: Total Disability : Unable to perform any job; unable to earn any income Partial Disability : Unable to perform own job; unable to earn original income Benefits for partial disability (own job) are smaller than total disability (any job) Note The magnitude of benefit received is typically smaller than the original salary of the insured due to the following reasons: To incentivize the insured to make a full recovery and return to work Annuity payments have no or lower tax rates; lower gross amount to match the same net income","title":"Product Features"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#modelling","text":"DII is modelled using the Sickness Death Model : State 0 : Healthy (No disability) State 1 : Sick (Disability present) State 2 : Dead The key feature that differentiates DII from a regular annuity is the waiting period: The true term of the policy is \\(n-w\\) ; the policy would expire before the waiting period is over if they become disabled after that time Insured becomes disabled at age \\(x+t\\) but benefits only start at age \\(x+t+w\\) ; functionally a deferred annuity Note Recalled that a deferred annuity can be expressed as the difference between a life annuity and an annuity certain equal to the deferral period: \\[ {}_{w \\mid}\\bar{a}_{x:\\enclose{actuarial}{n}} = \\bar{a}_{x:\\enclose{actuarial}{n}} - \\bar{a}_{\\enclose{actuarial}{w}} \\] The deferred period is an annuity certain, as the waiting period is known and fixed. The typical expression for multi-state annuities cannot be used due to the waiting period. This is because the expression implicitly assumes that payments will start immediately upon transitioning into the state. However, for DII, the payments must be delayed. Thus, the alternative, more flexible expression can be used instead. The annuity to specify is \\(w\\) deferred annuity, which implies that the policy will only pay AFTER \\(w\\) periods upon transition into the target state: \\[ \\begin{aligned} \\text{EPV(Benefits)} &= \\int^{n-w}_{0} {}_{t}p^{ik}_{x} \\mu^{kj}_{x+t} \\cdot e^{-\\delta t} \\cdot \\left(\\bar{a}^{\\bar{jj}}_{x+t:\\enclose{actuarial}{n-t}} - \\bar{a}^{\\bar{jj}}_{x+t:\\enclose{actuarial}{w}} \\right) \\end{aligned} \\] The key is understanding the limits of the integration: Upper Limit : Worst Case scenario; time where policyholder receives NO benefits because the policy will end exactly as the waiting period is finished Lower Limit : Best Case scenario; time where policyholder receives the MAXIMUM number of payments because they become disabled as soon as the policy incepts","title":"Modelling"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#special-case","text":"The above implicitly assumes that there is no benefit term . If there was a benefit term of \\(m\\) , the following expression would be used instead: \\[ \\begin{aligned} \\text{EPV(Benefits)} &= \\int^{n-m-w}_{0} {}_{t}p^{ik}_{x} \\mu^{kj}_{x+t} \\cdot e^{-\\delta t} \\cdot \\left(\\bar{a}^{\\bar{jj}}_{x+t:\\enclose{actuarial}{m+w}} - \\bar{a}^{\\bar{jj}}_{x+t:\\enclose{actuarial}{w}} \\right) \\\\ &+ \\int^{n-w}_{n-m-w} {}_{t}p^{ik}_{x} \\mu^{kj}_{x+t} \\cdot e^{-\\delta t} \\cdot \\left(\\bar{a}^{\\bar{jj}}_{x+t:\\enclose{actuarial}{n}} - \\bar{a}^{\\bar{jj}}_{x+t:\\enclose{actuarial}{w}} \\right) \\end{aligned} \\] With a specified benefit term, the previous EPV needs to be split into two components to account for differences in benefits payout across the two periods: The first case is that policyholder transitions sufficiently early , such that they can receive the full benefits before the end of the policy: Note The premise is that the policyholder will be able to receive the full benefits. Thus, the worst and best case is about the timing of when they receive it: Best Case : Earliest possible time to receive the full benefits Worst Case : Latest possble time to receive the full benefits The other case is that the Policyholder transitions succifiently late , such that they only receive a portion of the benefits because the policy would end before the full payout is made: Note The premise is that the policyholder will NOT be able to receive the full benefits. Thus, the worst and best case is about how much benefits that they can receive: Best Case : Time where the policyholder will still receive some benefits (but not all) Worst Case : Time where the policyholder will receive NO benefits","title":"Special Case"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#long-term-care-insurance","text":"Long Term Care Insurance (LTCI) is a contract that pays out routine benefits to the insured for as long as they cannot live independently (\"Unhealthy\"), requiring long term care. LTCI is a form of health insurance whose primary purpose is to cover the costs of administering LTC, such as nursing homes. Being unable to live independently is defined as being unable to independently perform one or more Activities of Daily Living (ADL): Transferring (getting in and out of a bed or chair) Continence (ability to control bladder and bowel functions) Toileting (ability to use the toilet and manage personal hygiene) Bathing Dressing Eating Tip It is best to remember this in the form of a story when waking up in the morning: Transferring (Wake up and get out of the bed ) Continence (Go to the toilet to pee) Toileting (Brushing teeth) Bathing (Washing up) Dressing (Getting changed for the day) Eating (Breakfast)","title":"Long Term Care Insurance"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#product-features_1","text":"Premiums are usually non-guaranteed and level throughout the policy term, payable regularly as long as the insured is healthy . Benefits are also payable regularly while the insured is unhealthy, till the earliest of the benefit term or the death of the insured. However, they typically come in two forms: Reimbursements : Paid to caregiving organization ; subject to a certain limit Fixed Payments : Paid to policyholder ; flexibility to use the cash Note LTCI is also subject to a waiting and off period to avoid adverse selection.","title":"Product Features"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#hybrid-features","text":"In practice, LTCI may be sold as a Rider to a main life insurance plan rather than being a standalone product. In these cases, there are some additional features that unique to the rider. One common feature would be a Return of Premiums (ROP) benefit. If the amount of benefit paid out under the LTCI portion is less than the total premiums paid , the shortfall will be returned by being added to the death benefit of the policy. Another feature would be Benefit Acceleration . LTCI benefits would be paid out from the sum assured of the policy, reducing it accordingly. The leftover amount would be paid out as the death benefit of the policy. If there is an acceleration feature, then the policyholder could also opt in for an Extension of Benefits feature. It ensures that the benefit payments will continue for an extended term even after the sum assured has been depleted.","title":"Hybrid Features"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#modelling_1","text":"For the purposes of this exam, LTCI can be modelled using the following 5 state model: State 0 : Able to perform 5 or more ADLs independently ( Healthy ) State 1 : Only able to perform exactly 4 ADLs independently ( Disabled ) State 2 : Only able to perform 3 or fewer ADLs independently ( Severely Disabled ) State 3 : Cognitive Impairment State 4 : Dead There are several points to take note of: The states are defined by the number of ADLs they CAN perform rather than cannot (Slightly counter-intuitive) The insured can recover from physical impairment but cannot recover from cognitive impairment Given the complexity of the model, it is unlikely that a question would ask for the calculation of premiums or reserves directly. They would most likely require the use of the Kolmogorov or Thiele's approximation methods .","title":"Modelling"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#continuing-care-retirement-communities","text":"One example of LTC is a Continuing Care Retirement Community (CCRC). They are residential facilities dedicated to providing care to seniors of varying needs : Independent Living Units (ILU): Minimal assistance provided Assisted Living Units (ALU): Only non-medical assistance provided (helping with ADL) Skilled Nursing Facility (SNF): Medical assistance provided","title":"Continuing Care Retirement Communities"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#product-features_2","text":"There are three types of arrangements that people can opt for: Type A Type B Type C Full Life Care Modified Life Care Fee For Service High Entry Fee Medium Entry Fee Low Entry Fee Level payments for all units Payments step up for new units Pay per use at market rate Youngest Middle Age Oldest Note Type A and B contracts are considered insurance contracts as they transfer the risk of increasing healthcare cost to the CCRC provider. Similar to life insurance, the monthly payments are higher than the actual cost of care in the early ages (Pre-funding) and smaller than what the actual cost is later on. As such, type A and B customers must go through medical underwriting to prove that they are healthy enough to live independently at the time of entry. Since health tends to deteriorate with age, this is why type A and B individuals tend to be younger. Some contracts also offer an Entry Fee Refund feature where a portion of the initial entry fee is paid to their beneficiaries when they leave the CCRC. A variation of the above is that the entry fee is used to buy an ownership stake in the unit they are living in, which will be sold to the next occupier when they move out. The proceeds will then be split between the individual and CCRC , functionally returning part of the entry fee.","title":"Product Features"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#modelling_2","text":"CCRC can be modelled using the following 5 state model: State 0 : Independent Living Unit ( Healthy ) State 1 : Assisted Living Unit ( Disabled ) State 2 : Skilled Nursing Facility ( Severely Disabled ) State 3 : Temporary Skilled Nursing Facility ( Medical Emergency ) State 4 : Dead One key difference from the LTCI model is that this model does NOT allow for recoveries . This greatly simplifies the model, which opens up the type of questions that can be asked. Info Intuitively, the reason is because CCRC is open to Seniors only , whose health will undoubtedly deteriorate with time.","title":"Modelling"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#critical-illness-insurance","text":"Critical Illness Insurance (CII) is a contract that pays out a lump sum benefit to the insured upon being diagnosed with a critical illness . Similar to life insurance, the primary purpose of CII is to replace the income of the insured as they are unable to work while they are recovering from CI. The definition of CI varies from insurer to insurer, but most insurers cover a basic set of illnesses such as Heart Attack, Stroke, Major Organ Failure and most common forms of Cancer.","title":"Critical Illness Insurance"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#product-features_3","text":"Premiums are usually guaranteed and level throughout the policy term, payable regularly as long as the insured is healthy . An additional lump sum benefit is payable when the insured becomes diagosed with a CI. The benefit ceases once the benefit has been paid - but the policy does not necessarily lapse as a death benefit may be payable as well. Unlike LTCI, CII main plans themselve could offer a partial Return of Premium feature if the policy expires or lapses with no benefits paid. Warning Although using the name, the ROP feature for CII and LTCI function very differently. CII ROP can only be used if NO claim was made at all.","title":"Product Features"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#hybrid-features_1","text":"Similarly, CII could be instead offered as a rider to a life insurance policy. In this case, the CI benefit will accelerate the death benefit and thus reduce the sum assured accordingly: Balance left : CI benefit only partially accelerated the DB; remaining amount payable on death No balance left : CI benefit fully accelerated the DB; nothing payable left thus policy lapses","title":"Hybrid Features"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#modelling_3","text":"CII can be modelled using any of the following models: Model 1 : CI Benefits are fully accelerated Model 2 : CI Benefits are partially accelerated Model 3 : CI Benefits as an additional lump sum The key is remembering that policy does NOT terminate upon being diagnosed with CI. Thus, the policy will still hold a reserve for the remaining death benefit. Warning Technically, any of the models can be used for any of the type of CI with the correct set up. The proposed models above are the models that best suit the structure. Questions that contain a Return of Premium feature usually also provide the Increasing Assurance factor \\(\\text{(IA)}_{x}\\) . There are two main variations: Case 1 : The ROP feature starts immediately - only the IA factor is needed Case 2 : The ROP feature starts later - combination of A and IA factor must be used \\[ \\begin{aligned} \\text{EPV}_{\\text{Case 1}} &= P \\cdot \\text{(IA)}_{x} \\\\ \\text{EPV}_{\\text{Case 2}} &= {}_{n}E_{x} \\cdot [n \\cdot P \\cdot A_{x+n} + P \\cdot \\text{(IA)}_{x+n}] \\end{aligned} \\]","title":"Modelling"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#structured-settlements","text":"Structured Settlements are an arrangement in a liability case , where the Responsible Party (RP) must compensate the Injured Party (IP). Compensation typically comes in the form of an annuity with monthly benefits, with similar reasoning to DII. The compensation schedule is usually determined by the RP\u2019s Liability Insurance policy. The RP could either be another person (EG. Motor Insurance) or could be a corporation (EG. Worker\u2019s Compensation).","title":"Structured Settlements"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#product-features_4","text":"The magnitude of annuity payment is typically smaller than the the original income for reasons similar to DII. However, one unique reason for structured settlements is that the IP could be partially at fault for the accident as well, thus should bear part of the cost. If the damage is severe , the long-term care needed will only be known after the IP goes through an initial period of treatment, till they reach the point of Maximum Medical Improvement (MMI). Before MMI, the RP will pay an interim benefit till the settlment is finalized.","title":"Product Features"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#modelling_4","text":"For the purposes of this exam, only structured settlements involving Workers Compensation will be covered. The key difference is that they are reviewable . This means that the annuity benefits can be conditional and stop once the individual is recovered (rather than being for life under a non-reviewable settlement). Note One problem with reviewable settlements is Anti-Selection . Since the payments will stop upon recovery, the IP has an incentive NOT to return to work even if they can. There are also additional expenses involved to prove monitor the health of the IP over time and prove that they are recovered. Reviewable Workers Compensation settlements can be modelled using the following 4 state model: State 0 : Uncertain Diagnosis State 1 : Recovered State 2 : Permanently Disabled State 3 : Dead","title":"Modelling"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#multi-life-model","text":"The multi-life model discussed previously can be expressed in the form of the following multi-state model: For multi-life questions, it is recommended to default to the above model to aid how we think about the problem. It is a common mistake to forget to consider future possibilities of only one of the lives being alive; using the model will reduce the chance of this error. Tip To determine the assurance factor for death, the conversion relationship can be used, where \\(k\\) is the absorbing state: \\[ \\begin{aligned} A^{ik}_{x} &= 1 - d \\cdot \\sum_{j \\ne k} \\ddot{a}^{ij}_{x} \\\\ \\therefore A^{03}_{x} &= 1 - d \\cdot (\\ddot{a}^{00}_{x} + \\ddot{a}^{01}_{x} + \\ddot{a}^{02}_{x}) \\end{aligned} \\] Warning There is one key difference when comparing the above model to the usual multi-life probabilities. Consider the following two probabilities: \\({}_{t}p^{01}_{x:y}\\) : Probability that ONLY \\(x\\) dies within \\(t\\) years; \\(y\\) dies AFTER \\(t\\) \\({}_{t}q^{1}_{x:y}\\) : Probability that \\(x\\) dies within \\(t\\) years; \\(y\\) can die before OR after \\(t\\) The confusion comes about because both are a situation where \\(X\\) dies before \\(Y\\) but the key difference is whether or not \\(Y\\) dies within \\(t\\) . The difference of the two is the probability that \\(y\\) dies second within \\(t\\) years : \\[ \\begin{aligned} {}_{t}q^{1}_{x:y} - {}_{t}p^{01}_{x:y} &= {}_{t}q^{\\ \\ \\ 2}_{x:y} \\\\ {}_{t}q^{1}_{x:y} &= {}_{t}q^{\\ \\ \\ 2}_{x:y} + {}_{t}q_{x} \\cdot {}_{t}p_{y} \\end{aligned} \\] The above is one of the key relationships covered in the multi-life section.","title":"Multi Life Model"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#dependence","text":"One common question involving this model is to determine whether the future lifetimes are dependent. Even if the model does NOT have a common shock, it is still possible to be dependent in other ways : Broken Heart Syndrome : Increase in mortality of surviving partner following partner's death Common Lifestyle : Couples tend to have similar lifestyles which results in similar mortalities (EG. Smokers) Common Shock : Couples tend to be exposed to the same risks , making it likely that they die together The ONLY definitive way to determine independence is to compare the forces of transition, to determine if the forces are dependent on the other life being alive. The following pairs of forces are measuring the same event of either life dying: Death of \\(x\\) : \\(\\mu^{02}\\) and \\(\\mu^{13}\\) Death of \\(y\\) : \\(\\mu^{01}\\) and \\(\\mu^{23}\\) The difference within each pair is whether or not the other life is alive. If the lives are independent, then the presence of the other life should not matter; the two should be equal . Note If the force of the surviving lives are higher than when both are alive, it is an example of Broken Heart Syndrome : \\[ \\begin{aligned} \\mu^{13} \\gt \\mu^{02} \\\\ \\mu^{23} \\gt \\mu^{01} \\end{aligned} \\]","title":"Dependence"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#common-shock-variation","text":"If a Common Shock is allowed for, there will be an additional transition from state 0 to state 3 that could kill both lives simultaneously : Recall from the multi-life section that the common shock affects all probabilities, even those affecting just a single life . The above model already has the common shock parameter baked into the transition intensities . Thus, a more intuitive version of the model that matches the previous analysis would be the following: The above multi-state approach will produce the same results as the original random variable approach: \\[ \\begin{aligned} {}_{t}p_{x:y} &= {}_{t}p^{00}_{x:y} \\\\ &= e^{- (\\int \\mu^{01} + \\lambda + \\mu^{02} + \\lambda - \\lambda)} \\\\ &= e^{- (\\int \\mu^{01} + \\mu^{02} + \\lambda)} \\\\ &= e^{- (\\int \\mu^{01} + \\mu^{02})} e^{-(\\lambda)} \\\\ &= {}_{t}p_{y} \\cdot {}_{t}p_{x} \\cdot e^{-\\lambda} \\end{aligned} \\] Warning If asked to calculate the reserves for this model, note that in state 1 and 2, only one life remains. Thus, the corresponding single life functions should be used, NOT multi-life! If the two lives are of the same age, the above model can be instead simplified into a three state model :","title":"Common Shock Variation"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#multi-decrement-model","text":"A multi-decrement model can be expressed as the following multi-state model: All multi-state concepts apply to this model as well.","title":"Multi-Decrement Model"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#model-estimation","text":"Similar to FAM-L, the goal of this subsection is to estimate the Force of Transition using Maximum Likelihood Estimation . Recall that the goal of MLE is to determine the population parameter (Force of Transition) that most likely resulted in the observed sample. Recall that the likelihood function is the joint probability density of all observtions (assuming sample is iid). Thus, each observation contributes its probability density to the likelihood: \\[ \\begin{aligned} L(\\theta \\mid x) &= \\text{General PDF} \\cdot \\text{Probability of Observation} \\\\ &= {}_{t}p^{\\bar{ii}}_{x} \\sum \\mu^{i(\\tau)}_{x+t} \\cdot \\frac{\\mu^{ij}_{x+t}}{\\mu^{i(\\tau)}_{x+t}} \\\\ &= {}_{t}p^{\\bar{ii}}_{x} \\cdot \\mu^{ij}_{x+t} \\end{aligned} \\] Most questions assume that the Force of Transition is constant between integer ages. Thus, the fractional age of the policyholder does not matter , allowing the above to be expanded to: \\[ \\begin{aligned} L(\\theta \\mid x) &= {}_{t}p^{\\bar{ii}}_{x} \\cdot \\mu^{ij}_{x} \\\\ &= e^{-t \\cdot \\sum \\mu^{i(\\tau)}_{x}} \\cdot \\mu^{ij}_{x} \\end{aligned} \\] Thus, the likelihood function for the entire sample is the product of all individual likelihood functions: \\[ \\begin{aligned} L(\\theta) &= \\prod L(\\theta \\mid x) \\\\ &= e^{-T^{i}_{x} \\cdot \\sum \\mu^{i(\\tau)}_{x}} \\cdot (\\mu^{ij}_{x})^{D^{ij}_{x}} \\cdot (\\mu^{ik}_{x})^{D^{ik}_{x}} \\cdot e^{-T^{j}_{x} \\cdot \\sum \\mu^{j(\\tau)}_{x}} \\cdot (\\mu^{jk}_{x})^{D^{jk}_{x}} \\cdot (\\mu^{ji}_{x})^{D^{ji}_{x}} \\\\ &= e^{-T^{i}_{x} \\cdot \\sum \\mu^{i(\\tau)}_{x} - T^{j}_{x} \\cdot \\sum \\mu^{j(\\tau)}_{x}} \\cdot (\\mu^{ij}_{x})^{D^{ij}_{x}} \\cdot (\\mu^{ik}_{x})^{D^{ik}_{x}} \\cdot (\\mu^{jk}_{x})^{D^{jk}_{x}} \\cdot (\\mu^{ji}_{x})^{D^{ji}_{x}} \\end{aligned} \\] The likelihood function can always be simplified into the above, where: \\(T^{i}_{x}\\) : Total observed time spent in state \\(i\\) at age \\(x\\) \\(D^{ij}_{x}\\) : Total observed transitions from state \\(i\\) to \\(j\\) at age \\(x\\) Note There are two things to be aware of: Total : Implies for ALL individuals in the study Observed : Implies starting from when the study starts Note Some questions might use lower case variables to represent the time spent and observations for a single observation and use upper case for the entire sample . The log-likehood function can thus be shown to be: \\[ \\begin{aligned} \\ell(\\theta) &= \\ln L(\\theta) \\\\ &= -T^{i}_{x} \\cdot \\sum \\mu^{i(\\tau)}_{x} - T^{j}_{x} \\cdot \\sum \\mu^{j(\\tau)}_{x} \\\\ &+ D^{ij}_{x} \\ln \\mu^{ij}_{x} + D^{ik}_{x} \\ln \\mu^{ik}_{x} + D^{jk}_{x} \\ln \\mu^{jk}_{x} + D^{ji}_{x} \\ln \\mu^{ji}_{x} \\end{aligned} \\] Note There may be odd situations where there are multiple years being studied or when the individuals do not start at an intger age. In these cases, it is best to visualize the transitions using a table: The MLE estimate for the forces of transition can be found by maximizing the log-likelihood function . Since there are multiple forces involved, the maximum can be found by taking the partial derivative with respect to each of the forces: \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\mu^{ij}_{x}} &= 0 \\\\ -T^{i}_{x} + \\frac{D^{ij}_{x}}{\\hat{\\mu}^{ij}_{x}} &= 0 \\\\ \\\\ \\therefore \\hat{\\mu}^{ij}_{x} &= \\frac{D^{ij}_{x}}{T^{i}_{x}} \\\\ &= \\frac{\\text{Number of Transitions To Target State}}{\\text{Time Spent In Current State}} \\end{aligned} \\] Generally speaking, estimates are more reliable given a larger number of data points . Since most policyholders incept at state 0, estimates for transitions out of state 0 are generally more reliable given the larger pool of experience. Tip The above formula can be used WITHOUT proof - there is no need to go through the process of deriving the likelihood function. Note The MLE estimates for each of the transition intensities are independent . This is because the cross-derivatives for each pair of transition intensities are zero: \\[ \\frac{\\partial^{2}}{\\partial \\mu^{ij}_{x} \\partial \\mu^{jk}_{x}} = 0 \\]","title":"Model Estimation"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#variance","text":"The Variance of the above MLE estimate can be approximated using Fisher's Information . The proof is beyond the scope of this exam. It is sufficient to know the following: The Asymptotic Variance is the Inverse of the Fisher's Information Function Fisher's Information is the Expected Value of the Second Partial Derivative \\[ \\begin{aligned} \\text{Var} (\\hat{\\mu}^{ij}_{x}) &= [I(\\mu^{ij}_{x})]^{-1} \\\\ &= \\left(- E \\left[\\frac{\\partial^{2}}{\\partial (\\mu^{ij}_{x})^{2}} \\right] \\right)^{-1} \\\\ &= \\left(- E \\left[- \\frac{D^{ij}_{x}}{(\\mu^{ij}_{x})^{2}} \\right] \\right)^{-1} \\\\ &= \\left(\\frac{E[D^{ij}_{x}]}{(\\hat{\\mu}^{ij}_{x})^{2}} \\right)^{-1} \\\\ &= \\frac{(\\mu^{ij}_{x})^{2}}{E[D^{ij}_{x}]} \\\\ &= \\frac{(\\mu^{ij}_{x})^{2}}{D^{ij}_{x}} \\end{aligned} \\] Note that the force of transition in the above is the ACTUAL force of transition. It can be approximated using the MLE estimate of the force: \\[ \\begin{aligned} \\text{Var} (\\hat{\\mu}^{ij}_{x}) &= \\frac{(\\mu^{ij}_{x})^{2}}{D^{ij}_{x}} \\\\ &= \\frac{\\left(\\frac{D^{ij}_{x}}{T^{i}_{x}} \\right)^{2}}{D^{ij}_{x}} \\\\ &= \\frac{D^{ij}_{x}}{(T^{i}_{x})^{2}} \\end{aligned} \\]","title":"Variance"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#confidence-interval","text":"Using the variance of the estimator, the Linear Confidence Interval can be derived: \\[ \\begin{aligned} \\hat{\\mu}^{ij}_{x} \\pm Z_{\\frac{1+\\alpha}{2}} \\cdot \\sqrt{\\text{Var} (\\hat{\\mu}^{ij}_{x})} \\\\ \\left(\\hat{\\mu}^{ij}_{x} + Z_{\\frac{1+\\alpha}{2}} \\cdot \\sqrt{\\text{Var} (\\hat{\\mu}^{ij}_{x})}, \\hat{\\mu}^{ij}_{x} - Z_{\\frac{1+\\alpha}{2}} \\cdot \\sqrt{\\text{Var} (\\hat{\\mu}^{ij}_{x})} \\right) \\end{aligned} \\]","title":"Confidence Interval"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/4.%20Multi%20State%20Applications/#probabilities","text":"Recall that the probability of staying in a particular state can be given by: \\[ {}_{t}p^{\\bar{ii}}_{x} = e^{- \\mu^{i(\\tau)}_{x}} \\] Thus, the MLE estimate of the probability uses the MLE estimate of the forces exiting the state : \\[ \\begin{aligned} \\hat{\\mu}^{i(\\tau)}_{x} &= \\sum \\hat{\\mu}^{ik}_{x} \\\\ \\therefore {}_{t}\\hat{p}^{\\bar{ii}}_{x} &= e^{- \\hat{\\mu}^{i(\\tau)}_{x}} \\end{aligned} \\] Since the individual MLE estimates are independent of another, the variance of the above is simply the sum of the individual variances : \\[ \\text{Var} \\left(\\hat{\\mu}^{i(\\tau)}_{x} \\right) = \\sum \\text{Var} \\left(\\hat{\\mu}^{ik}_{x} \\right) \\] The confidence interval of the probability is the probabilities computed using the resulting confidence interval of the forces . This can be applied for any function involving MLE estimates: \\[ (e^{- \\hat{\\mu}^{i(\\tau)}_{x, \\text{Lower Bound}}}, e^{- \\hat{\\mu}^{i(\\tau)}_{x, \\text{Upper Bound}}}) \\]","title":"Probabilities"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/","text":"Profit Testing \u00b6 Overview \u00b6 For regular consumer goods, the cost of the product is known beforehand, thus its profitability can be determined beforehand. For insurance, the cost is contingent on the mortality of the policyholder, thus it is impossible to determine if the premium charged was adequate or not. Thus, by making certain assumptions about the future experience of the policyholder, the insurer can project forward and determine the expected profit from the policy. The insurer can then make adjustments to assumptions or features to achieve their desired profitability - the process is known as Profit Testing . Profit testing can be used to: Set Premiums Set Reserves Measure Profitability Determine Distributable Surplus (For participating policies) Key Intuition \u00b6 For a traditional level regular premium life insurance product, the insurer collects more premium in the earlier years than the cost of providing insurance (expected benefit). In the later years , this reverses and the insurer collects less premiums than the cost of the expected benefit. From a profit perspective, this means that the insurer will recognize positive profits in the earlier years but negative profits in the later years. Note This is why virtually all life insurance policies are Lapse Supported - the insurer is materially better off if the policyholder lapses the policy. The insurer will only recognize the positive profits in the early years, without needing to experience negative profits in the later years. From a layman's perspective, the insurer receives the revenue without needing to pay the cost . Even if the policy has a cash value, the cash value will be less than the benefit of the policy that the insurer had priced for, still leaving them better off. Lapse Supportability is somewhat 'dangerous' as insurer's might price products assuming an assumed lapse rate. However, since Lapse Rates are very unpredictable , actual experience could turn out to be much lower , resulting in negative profits for the insurer. Thus, given the unpredictability, it is NOT advised to price products assuming a high lapse rate as it artificially inflates the financials of the product. Note The alternative to making level payments is to pay for the expected cost at that age - this kind of arrangement is known as a Yearly Renewable Term . Thus, making level payments is functionally pre-paying for future cost increases. The policyholder will pay more in the early years when their actual cost is low while pay less in the later years when their actual cost is higher. To prevent insolvency, the insurer MUST set up a reserve (funded from the profits) in the earlier years which can be used to cover the shortfall in premium income in the later years to pay for benefits. Thus, by holding a reserve, the policy effectively becomes self-funded . Note Without a reserve, the insurer might not always have the free cash available to support the years of negative profit, potentially leading to insolvency. This is because the profit from the early years might be distributed to shareholders or reinvested into the company; no longer available for use . In the earlier years, the reserve is being built up (increase in reserve) which reduces early profits . Eventually, the reserve will be released (decrease in reserve), offsetting the negative profits in the later years. Tip There are two forces driving the reserve pattern: Unwinding : Reserve increases as future cashflows are discounted less as policy moves forward in time Expected Payments : Reserve decreases as part of the reserves are used for expected payments in the current period In the early years, there are many future cashflows (large increase) while the expected payment is low (for a young policyholder) - leading to an increase in reserves in the earlier periods. In later years, this effect reverses as there are not many future cashflows left while expected payments increase, leading to the reserve decreasing in later periods. If both are on the same basis , then the excess in the early period will be exactly used to build the reserve while the shortfall in the later periods will be exactly covered. However, since the basis for reserves are more conservative, more than what is available is set aside in the earlier periods (resulting in negative profits early on ) and more than what is needed is released later on (resulting in positive profits in later periods). The key is understanding that the reserve is functionally a savings account that the insurer sets aside to ensure future obligations can be met. The reserve does NOT change the dollar amount of profit; it only affects the timing of the profit . Due to the time value of money, recognizing profits earlier is better as they are discounted less, resulting in greater profits on an NPV basis . Note Consider decrease in reserve across all periods (EG. Due to an Increase of Interest Rates): Negative Profits in the earlier years will decrease as there is less reserve to build up ( Increase in earlier profits ) Positive Profits in the later years will decrease as there is less reserve to release ( Decrease in later profits ) Profits are recognized earlier rather than later Due to the time value of money , the uplift from the earlier periods is stronger, resulting in an increase to NPV. Expected Profit \u00b6 Net Cashflow \u00b6 Consider the cashflows that occur in a single period from the insurer's perspective : Period Start : Collect Premiums (P) and incur Maintenance Expenses (e) Period End : Payout Claims (B) occurring in the period, along with any associated Claim Expenses (CE) It is typically assumed that the insurer will invest the net cashflows at the beginning of the period in order to earn interest . Thus, the net cashflow for the period is the combination of the above: \\[ \\text{Net Cashflow}_{t} = (P_{t} - E_{t})(1 + i) - q_{x} \\cdot (B_{t} + \\text{CE}_{t}) \\] Note Even if the net cashflow in the beginning of the year is negative \\((P - e < 0)\\) , the insurer will still invest the amount. In this case, it can be understood as the insurer has to borrow money , which they must then pay interest on at the end of the period. Similarly, if there is cashflow that occurs in the middle of the year, then the insurer will incur interest on that amount for the remaining time till the end of the year. Tip The above expression can be enhanced to add in other benefits by applying the appropriate risk rates: Surrender Benefit : Lapse Rate Maturity Benefit : Survival Rate (last period only) Annuity Benefit : Survival Rate Notice that the expenses above were listed as maintenance expenses. This is because Acquisition Expenses are assumed to be pre-contract expenses . They are the ONLY cashflow in year 0 and will NOT earn any interest : Info In practice, acquisition expenses are usually grouped with the other cashflows that occur in the first policy year. However, the above is assumed for the purposes of this exam. Emerging Profit \u00b6 For policyholders who die during the period, a claim must be paid out to them. However, the insurer does not need to fund the claim entirely from cashflows during the period; they have a reserve that was previously built up. Conversely, for policyholders who do not die during the period, they must set up a reserve for them. This is because a claim will be inevitably made in some future period. Thus, the reserve can be thought of like a savings account used to fulfil the specific purpose of meeting claims: Period Start : Insurer can access the reserve set up as at the previous period Period End : Insurer must set up a new reserve for the future claims that will be made Similarly, the insurer is assumed to invest the reserve at the beginning of the period to maximize returns. The insurer has both the interest earned and net cashflow arising from that period available to set aside the new reserve. Any remaining amount can be recognised as profit as the insurer has fulfilled their liabilities : \\[ \\begin{aligned} \\text{Profit} &= {}_{t-1}V \\cdot (1+i) + \\text{Net Cashflow} - p_{x} \\cdot {}_{t}V \\\\ &= \\text{Net Cashflow} - \\underbrace{p_{x} \\cdot {}_{t}V - {}_{t-1}V \\cdot (1+i)}_\\text{Change in Reserves} \\end{aligned} \\] Warning Do NOT attempt to calculate the change in reserve as a seperate component as the opening and ending reserves are on a different basis: Opening Reserve : Per Policy Reserve, since profit assumes all policies in-force at start of year Ending Reserve : In-force reserve Since the probabilities will already be accounted for the in the above equation, the chances of making an error are lower if the full equation is used. The above expression is often referred to as the profit that emerges or arises from that period, as it occurs at the end of the period as a combination of all other cashflows that occurred in that period. Tip Some questions may provide the reserves while other questions may require us to compute it ourselves. Regardless, there are always two time points where the reserves are known: Policy Start : Starting reserve is 0 (Reserve has not been set up yet) Policy End : Ending reserve is 0 (No more future liabilities) It is also possible that the question uses Net Premium Reserves even though the profit testing is done a Gross Premium basis . Always read the question properly. The above can be calculated in Excel and presented as the following: Profit Basis \u00b6 Note that the projection basis (the assumptions used) for reserving and profit testing might be different: Reserving Basis : Conservative; Best-estimate with margin for adverse deviation Profit Basis : Realistic; best-estimate rates only Warning This includes ALL assumptions , not just risk rates. In practice, this usually comes from two sources : Margin for Adverse Deviation : Affects risk rates plus some fixed assumptsion (EG. Expense) Interest Rates : Reserves discounted as Risk Free Rate while profit projected as Best Estimate Investment Returns If the reserving and profit testing basis are the same , then there will be NO profit emerging from the policy. This is because the change in reserve (due to cashflows and unwinding) will completely offset the net cashflow emerging from the policy. Thus, if actual experience exactly follows the best estimate assumptions, the insurer should earn a profit from the margin for adverse deviation that was held. Profit Vector \u00b6 The Profit Vector of a policy is a vector containing the emerging profits from each policy year: \\[ \\text{PR} = (\\text{PR}_{0}, \\text{PR}_ {1}, \\dots ,\\text{PR}_{t}) \\] Warning The profit vector includes policy year 0 , which is mainly the pre-contract expenses. HOWEVER, the question could provide a reserve at the end of time 0 - which would count towards the profit from policy year 0 as well. Note One typical problem for questions on this topic is the age of the policyholder to use . Using the above diagram, a relationship can be easily seen: Age at BOP: \\(x+t-1\\) , used for Decrements Age at EOP: \\(x+t\\) , used for Reserves For a typical life insurance contract, there is a typically a negative profit in the first year (new busines strain) due to the high cost of setting up the policy (expenses and reserves). Following that, small magnitudes of positive profits will emerge over its lifetime. Note Most questions will only require computing the profit vector for a short period of time given the intensity needed to calculate the reserves at various points in time. If required to compute the profit vector for a long duration, the question will usually specify to ignore the change in reserves or simply provide them , which greatly simplifies the required calculations. Profit Signature \u00b6 Consider the expression for profit - an implicit assumption it makes is that the policyholder is alive at the start of the period . However, this means that each term in the profit vector is on a different basis and thus cannot be analyzed together. To account for this, each element can be adjusted by accounting for the probabilities of survival to the start of each period . This converts each element into the profit at issue that arises from the period, which can be analyzed together. The resulting vector is known as the Profit Signature of the policy: Profit Vector : Profit at the end of period, assuming the policy is in-force at the start of the period Profit Signature : Profit at the end of period, assuming that the policy is in-force at issue \\[ \\begin{aligned} \\boldsymbol{\\pi} &= (\\pi_{0}, \\pi_{1}, \\dots, \\pi_{t}) \\\\ &= ({}_{0}p_{x} \\cdot \\text{PR}_{0}, {}_{0}p_{x} \\cdot \\text{PR}_{1}, {}_{t-1}p_{x} \\cdot \\text{PR}_{t}) \\end{aligned} \\] Tip Recall that \\({}_{0}p_x = 1\\) , as it represents the probability of surviving till this instant ! Multi State Models \u00b6 Multi State Models add another layer of complexity to profit projection which poses the following issues: Multiple benefits to consider Cashflows are dependent on the state that the policyholder is in Not possible to know which state the insured will be in at issue Thus, the following changes need to be made: Consider the outgo from ALL possible benefits and decrements Dependent Decrements should be used; survival should include ALL decrements Consider ALL possible states the policyholder could be in at the start of the year \\[ \\begin{aligned} \\text{NCF}^{(i)}_{t} &= (P - e)(1+i) - \\sum q^{ij}_{x} \\cdot (B^{ij} + E^{ij}) \\\\ \\text{PR}^{(i)}_{t} &= \\text{NCF}_{t} - \\sum [p^{(ij)}_{x} \\cdot {}_{t}V^{(j)} - {}_{t-1}V^{(i)} \\cdot (1+i)] \\\\ \\pi &= \\sum_{t, i} {}_{t-1}p^{0i} \\cdot \\text{PR}^{i}_{t} \\end{aligned} \\] Warning For multi-state questions, note that the expression for profit may be different for each state - do not autopilot into the formulas. For instance, some states might not receive premiums or might not pay benefits. Warning When dealing with multiple decrements, it is important to remember that only the dependent decrements should be used. Thus, pay attention to the following details: \"Mortality follows the Standard Ultimate Life Table\": Death decrement this way is dependent \"X% of the surviving policyholders surrender\": Survival occurs independently at the end of the year \"Of the 1000 policyholders, X died and Y surrendered\": Death and Survival calculated this way are dependent Profitability Metrics \u00b6 Using the profit cashflows above, insurers can gauge the profitability of the policy based on different metrics. Net Present Value \u00b6 Firstly, the Net Present Value (NPV). It is the EPV of the profits , discounted using a specified Risk Discount Rate : \\[ \\begin{aligned} \\text{NPV} &= \\text{PR}_{0} + v\\text{PR}_{1} + v^{2} p_{x} \\text{PR}_{2} + v^{3} {2}p_{x} \\text{PR}_{3} + \\dots \\\\ &= \\text{PR}_{0} + v\\text{PR}_{1} + v^{2} p_{x} \\text{PR}_{2} + v^{3} p_{x} p_{x+1} \\text{PR}_{3} + \\dots \\\\ \\end{aligned} \\] Note The Risk Discount Rate is also sometimes referred to as the Hurdle Rate . They represent the return that an investor expects to earn based on the risk profile of the business. It is typically different from the interest rates used for reserving or profit testing. Tip Recall the intuition regarding EPV: The complexity of life insurance is that there are two dimensions - Incidence Probability (\u201cE\u201d) & Time Value of Money (\u201cPV\u201d). The key understanding is that the benefit of the policy is fixed. A policy with $100,000 sum assured will always at most pay out $100,000. The incidence probability affects the TIMING of the payment . Higher incidence probability increases expected benefit in the earlier years and decreases expected payments in the later years. On whole, the expected payment for the policy is still $100,000 - but the benefits are expected to pay earlier rather than later . The time value of money then affects how these benefits are valued . Benefits that emerge earlier are discounted less , thus have a higher EPV. For an annuity, the key focus is on expected NUMBER of payments . The incidence probability decreases the probability of staying in the state, thus lowering the expected number of payments. This is useful for determining the impact on the EPV of Premiums and Expenses. Recursion \u00b6 Apart from calculating NPV, one common type of question involves estimating the NPV as at a future valuation date , given the NPV as at the current date. Consider the cashflows for the NPV on issue versus the NPV one period later: \\[ \\begin{aligned} \\text{NPV}_{0} &= \\text{PR}_{0} + v \\text{PR}_{1} + v^{2} p_{x} \\text{PR}_{2} + v^{3} p_{x} p_{x+1} \\text{PR}_{3} + \\dots \\\\ \\text{NPV}_{1} &= \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ v\\text{PR}_{2} + v^{2} p_{x+1} \\text{PR}_{3} + \\dots \\end{aligned} \\] There are three key differences: The expected profits in policy year 0 and 1 have been realized thus are no longer part of NPV The valuation date has moved back one year thus there is an unwinding of interest by one year Similarly, the policyholder has survived one year thus there is 'unwinding' of mortality by one year as well By applying the above changes in a stepwise order , the future NPV can be obtained. Decrement Change \u00b6 Similarly, another type of question involves estimating the impact to NPV given a change in decrements . There are only a few adjustments that need to be made. Consider a change in the first year decrements: \\[ \\begin{aligned} \\text{NPV} &= \\text{PR}_{0} + v \\text{PR}_{1} + v^{2} p_{x} \\text{PR}_{2} + v^{3} p_{x} p_{x+1} \\text{PR}_{3} + \\dots \\\\ \\text{NPV}' &= \\text{PR}_{0} + v \\text{PR}'_{1} + v^{2} p'_{x} \\text{PR}_{2} + v^{3} p'_{x} p_{x+1} \\text{PR}_{3} + \\dots \\\\ \\end{aligned} \\] There are two key differences: The emerging profit from the AFFECTED YEAR must be changed to reflect the new basis The profit signature for all FUTURE YEARS must be changed to reflect the new in-force probability Profit signatures from all prior years are unaffected By applying the above changes in a stepwise order , the adjusted NPV can be obtained. Warning Even if the decrement has no impact on the benefit (EG. Change in Lapse but no surrender benefit), it will still have an impact on the profit as it affects the expected reserve for the year. Actual Experience \u00b6 Another type of question could be to ask for the NPV assuming a certain event occurred - EG. The policyholder actually dies at the end of the second year. In these cases, the decrements must be adjusted. If the policyholder dies in the second year: Death and Lapse decrement in the first year is 0 (WILL NOT die or lapse) Death decrement in the second year is 1 (WILL die) The weighted average of the NPV of all scenarios in the policy will give the total NPV of the policy. This allows the insurer to better understand which scenarios are better for them. Internal Rate of Return \u00b6 Next, the Internal Rate of Return (IRR). It is the Risk Discount Rate that will cause the NPV to be 0: \\[ \\text{NPV}_\\text{IRR} = 0 \\] Warning IRR CANNOT be calcualted via an equation and must be solved using numerical methods or a financial calculator. Note IRR measures the highest possible discount rate such that the project breaks even. If the actual discount rate is lower than the IRR , the resulting NPV will always be positive. Thus, higher IRRs are preferred. One common type of question involves determining how a change in cashflows affects the IRR. Generally speaking, any change that would result in a higher NPV would also result in a higher IRR , as the discount rate would need to be higher to force the new NPV to 0. Discounted Payback Period \u00b6 Next, is the Discounted Payback Period (DPP). It is the earliest period where the partial NPV becomes positive : \\[ \\text{DPP} = \\min [t: \\text{Partial NPV}(t) > 0] \\] Partial NPV at time \\(t\\) is the NPV that is calculated using a subset of cashflows from time 0 up to and including time \\(t\\) . It is a measure of the accumulated profit (on a EPV basis) up till that time. Thus, the discounted payback period is a measure of the earliest time that the accumulated profits become positive . \\[ \\text{Partial NPV}(t) = \\pi_{0} + v^{1} \\pi_{1} + v^{2} \\pi_{2} + \\dots + v^{t} \\pi_{t} \\] Note Most policies have a large pre-contract expense while the profits in the initial years are small (due to building up the reserves). Thus, it takes a few years to recoup the costs - negative partial NPV for the first few years of the policy. Profit Margin \u00b6 Lastly, the Profit Margin . It is the ratio of the EPV of the profits (NPV) to that of the premiums, assuming the same discount rate : \\[ \\begin{aligned} \\text{Profit Margin} &= \\frac{\\text{NPV}}{\\text{EPV(Premiums)}} \\\\ &= \\frac{\\text{NPV (Profit At Issue)}}{\\text{EPV(Premiums)}} \\end{aligned} \\] Warning The timing of the premium and profit cashflows are different - do NOT mistakenly apply the same discounting pattern to them: Zeroization \u00b6 Apart from hitting the specified profitability metrics, another consideration during profit testing is the pattern of profits . As mentioned in the beginning of the section, without a reserve, the typical profit pattern for a life insurance policy is positive in the earlier years while negative in the later years , which motivates the reason for holding a reserve. Thus, the reserve held should achieve the following outcomes: Ensure that there are no negative profits after positive profits (no negative profits in later years) Ensure that positive profits emerge as soon as possible The above objectives can be achieved by holding the minimum reserves at each period. This can be achieved by setting the reserves through a process known as Zeroization : Start from the last policy year Solve for the opening reserve assuming no profit emerges in the year using the recursion formula If positive, set the opening reserve equal to it If negative, set it at zero Repeat for all remaining policy years \\[ \\begin{aligned} \\text{PR}_{t} &= 0 \\\\ \\text{NCF}_{t} + p_{x} \\cdot {}_{t}V - {}_{t-1}V \\cdot (1+i) &= 0 \\\\ {}_{t-1}V &= \\frac{\\text{NCF}_{t} + p_{x} \\cdot {}_{t}V}{1+i} \\\\ \\\\ \\therefore {}_{t-1}V^{z} &= \\max ({}_{t-1}V^{z}, 0) \\end{aligned} \\] There are a few points to take note of: The expected reserve in the final year is 0 - so this process can always be used given all the underlying net cashflows Opening reserve that was solved for will be used as the ending reserve in the next policy year of the process However, ending reserve used for the next process is the EXPECTED ending reserve , thus remember to multiply by the survival probabilities before continuing Note The key intuition is that if the profit arising from that period is positive , then there is no need for a reserve (recall that the motivation for the reserve is to cover negative profit). If the profit arising is negative , then a reserve exactly equal to the negative profit is held to ensure it is non-negative. What this achieves is that the absolute minimum reserve is held ; no \"extra\" reserve that will show up as a profit later on (no negative profits later on achieved). As a result, the maximum amount of positive profits from the early years are recognized. In practice, insurers often do not hold the zeroized reserves due to the following reasons: Minimum reserves required via Statutory Reporting Additional reserves to cover for Adverse Experience Additional reserves to cover for Guarantees Actual Profits \u00b6 If actual experience follows the assumptions exactly, the expected profit will materialize into Actual Profits . In reality, this is unlikely to occur, leading to deviation between the two. The difference is known as the Gain or Loss of the policy: \\[ \\text{Gain/Loss} = \\text{Actual Profit} - \\text{Expected Profit} \\] Note For the purposes of this exam, if the question asks for gains by source, the Expected Profit is assumed to be 0 . \\[ \\begin{aligned} \\text{Total Gain/Loss} &= \\text{Actual Profit} - \\text{Expected Profit} \\\\ &= \\text{Actual Profit} - 0 \\\\ &= \\text{Actual Profit} \\end{aligned} \\] The implicit assumption here is that the profit testing basis is the same as the reserving basis . Note that this only holds true for questions that ask for Gains by Source - thus read the entire question to know if this applies. Since gains occur as a result of assumption deviation, it can decomposed into the gain from specific assumptions . This is useful in the following areas: Assumption Review Resource Allocation Performance Assessment \\[ \\begin{aligned} \\text{Total Gain} &= \\text{Expense Gain} + \\text{Interest Gain} + \\text{Mortality Gain} \\end{aligned} \\] Tip This can be used as a check to ensure that the attribution was done correctly - by checking to ensure that they reconcile to the total. For this section, let the actual experience be denoted with a prime superscript \\('\\) . Stepwise Attribution \u00b6 The method of attribution is known as Stepwise Attribution . It involves changing one assumption at a time (expected to actual), on top of previous changes , till the final set of assumptions are obtained (all actual). The difference between steps is the gain from the assumption that was changed in the step: The order of attribution is important. Although the total gain will remain the same, the amount allocated to each component will be different if a different order is used. This is due to the allocation of the correlation impact : \\[ \\text{Gain from Assumption} = \\text{Absolute Gain} + \\text{Correlation Impact} \\] Thus, different orders of attributions will result in similar but ultimately different values . Questions will usually specify the order of attribution to use. Note An interesting relationship is that not all assumptions are correlated with one another. In other words, the correlation impact between them is 0. Thus, different orders of attribution COULD lead to the same impact : Assuming that there are claims expenses, the following three combinations would result in the same Investment Gain : Expense, Interest & Mortality Expense, Mortality, Interest Mortality, Expense Interest The key is understanding that since Mortality and Interest are not correlated, the only requirement to produce the same attribution is to have Expense be attributed before Intetest . Portfolio Gain \u00b6 Most questions involve calculating the gain from a portfolio of identical policies : \\[ \\text{Portfolio Gain} = \\text{Gain Per Policy} \\cdot \\text{Number of Policies at the Start} \\] Warning It is a common mistake to use the ending number of policies (after actual decrements) because the \"intuition\" is that the gain/loss is made on the remaining policies. Expense Gain \u00b6 Since expense is a cash outflow , gains are recognised when the actual outflow is smaller than the actual outflow: \\[ \\begin{aligned} \\text{Expense Gain Per Policy} &= \\text{Expected Expense Profit} - \\text{Actual Expense Profit} \\\\ &= E(1+i) - q_{x} \\cdot \\text{CE} - [E'(1+i) - q_{x} \\cdot \\text{CE}'] \\\\ &= (E-E')(1+i) - q_{x} (\\text{CE}-\\text{CE}') \\end{aligned} \\] Additionally, since this is the first attribution step , ONLY the difference in expense needs to be accounted for. This is because the other assumptions are still assumed to be at their expected value, thus it will offset one another. Interest Gain \u00b6 On the flipside, since interest is a cash inflow , gain is recognised when the actual inflow is larger than the actual outflow: \\[ \\begin{aligned} \\text{Interest Gain Per Policy} &= \\text{Actual Interest Profit} - \\text{Expected Interest Profit} \\\\ &= ({}_{t-1}V + P - E') \\cdot i' - ({}_{t-1}V + P - E') \\cdot i \\\\ &= (i' - i)({}_{t-1}V + P - E') \\end{aligned} \\] Similarly, since this is NOT the first attribution step, all assumptions that have been attributed before must be included - ALL instances of the assumption should be using its actual value . This is to account for the correlation impact between the two. Warning Even though is says \"Expected Profit\", it is not truly the expected profit. This is because of the stepwise attribution , the expense assumption should be using the actual value instead. Mortality Gain \u00b6 Similarly, mortality is a cash outflow , gain is recognised when the actual outflow is smaller than the actual outflow. \\[ \\begin{aligned} \\text{Mortality Gain Per Policy} &= \\text{Expected Mortality Profit} - \\text{Actual Mortality Profit} \\\\ &= q_{x} \\cdot (B+\\text{CE}') + (1-q_{x}) \\cdot {}_{t}V - [q'_{x} \\cdot (B + \\text{CE}') + (1-q'_{x}) {}_{t}V] \\\\ &= q_{x} \\cdot (B+\\text{CE}') + {}_{t}V - q_{x} \\cdot {}_{t}V - [q'_{x} \\cdot (B+\\text{CE}') + {}_{t}V - q'_{x} {}_{t}V] \\\\ &= q_{x} \\cdot (B+\\text{CE}') + {}_{t}V - q_{x} \\cdot {}_{t}V - q'_{x} \\cdot (B+\\text{CE}') - {}_{t}V + q'_{x} {}_{t}V \\\\ &= (q_{x} - q'_{x})(B+\\text{CE}') - (q_{x} - q'_{x}){}_{t}V \\\\ &= (q_{x} - q'_{x})(B+\\text{CE}'- {}_{t}V) \\\\ \\end{aligned} \\] Questions may alternatively provide the actual number of policyholders (NOP) instead of the probabilities. Thus, we need to solve for the actual probabilities using: \\[ q'_{x} = \\frac{\\text{NOP}_\\text{Beginning} - \\text{NOP}_\\text{Ending}}{\\text{NOP}_\\text{Beginning}} \\] Other Gain \u00b6 Gains for other cashflows (EG. Surrender or Maturity) can be calculated by applying the same principles as above: \\[ \\begin{aligned} \\text{Lapse Gain Per Policy} &= (w_{x} - w'_{x})(B^{w} + \\text{CE}^{w'} - {}_{t}V) \\\\ \\end{aligned} \\] The expression is analagous to the mortality gain. Note that even though the expected reserve appears in both the mortality and lapse gain, there is NO double counting - they each quantify the impact of its respective decrement on the expected reserve. Note If the feature was not expected but eventually paid (EG. Expected profit does contain Surrender but actual profit do), then the expected profit from that component is 0 - the gain or loss is the entire outflow for that benefit. Source Cashflow Formula Expense Outflow Expected - Actual Mortality Outflow Expected - Actual Surrender Outflow Expected - Actual Interest Inflow Actual - Expected Other Applications \u00b6 This method essentially attributes the impact of each assumption difference (Actual vs Profit Testing Basis) on Actual Profit. However, it can be used to determine Expected Profit as well. Recall that expected profit arises due to the difference in basis between Reserving and Profit Testing. Thus, by using the methods above, the expected profit from Investment, Mortality or Expense can be quantified. Generally speaking, this is not very useful as we are usually interested in the whole expected profit, not the expected profit from a specific assumption . However, if there are only 1-2 differences in basis, this method can be used to quickly determine the overall profit. For instance, if the only difference between profit testing and reserving is only the investment return assumption: \\[ \\begin{aligned} \\text{Expected Profit} &= \\text{Expected Profit from Investment} \\\\ &= ({}_{t-1}V + P - E)(i\u2019 - i) \\end{aligned} \\] This is useful as the entire profit calculation does not need to be performed , as well as for situations where insufficient information is given.","title":"Profit Testing"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#profit-testing","text":"","title":"Profit Testing"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#overview","text":"For regular consumer goods, the cost of the product is known beforehand, thus its profitability can be determined beforehand. For insurance, the cost is contingent on the mortality of the policyholder, thus it is impossible to determine if the premium charged was adequate or not. Thus, by making certain assumptions about the future experience of the policyholder, the insurer can project forward and determine the expected profit from the policy. The insurer can then make adjustments to assumptions or features to achieve their desired profitability - the process is known as Profit Testing . Profit testing can be used to: Set Premiums Set Reserves Measure Profitability Determine Distributable Surplus (For participating policies)","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#key-intuition","text":"For a traditional level regular premium life insurance product, the insurer collects more premium in the earlier years than the cost of providing insurance (expected benefit). In the later years , this reverses and the insurer collects less premiums than the cost of the expected benefit. From a profit perspective, this means that the insurer will recognize positive profits in the earlier years but negative profits in the later years. Note This is why virtually all life insurance policies are Lapse Supported - the insurer is materially better off if the policyholder lapses the policy. The insurer will only recognize the positive profits in the early years, without needing to experience negative profits in the later years. From a layman's perspective, the insurer receives the revenue without needing to pay the cost . Even if the policy has a cash value, the cash value will be less than the benefit of the policy that the insurer had priced for, still leaving them better off. Lapse Supportability is somewhat 'dangerous' as insurer's might price products assuming an assumed lapse rate. However, since Lapse Rates are very unpredictable , actual experience could turn out to be much lower , resulting in negative profits for the insurer. Thus, given the unpredictability, it is NOT advised to price products assuming a high lapse rate as it artificially inflates the financials of the product. Note The alternative to making level payments is to pay for the expected cost at that age - this kind of arrangement is known as a Yearly Renewable Term . Thus, making level payments is functionally pre-paying for future cost increases. The policyholder will pay more in the early years when their actual cost is low while pay less in the later years when their actual cost is higher. To prevent insolvency, the insurer MUST set up a reserve (funded from the profits) in the earlier years which can be used to cover the shortfall in premium income in the later years to pay for benefits. Thus, by holding a reserve, the policy effectively becomes self-funded . Note Without a reserve, the insurer might not always have the free cash available to support the years of negative profit, potentially leading to insolvency. This is because the profit from the early years might be distributed to shareholders or reinvested into the company; no longer available for use . In the earlier years, the reserve is being built up (increase in reserve) which reduces early profits . Eventually, the reserve will be released (decrease in reserve), offsetting the negative profits in the later years. Tip There are two forces driving the reserve pattern: Unwinding : Reserve increases as future cashflows are discounted less as policy moves forward in time Expected Payments : Reserve decreases as part of the reserves are used for expected payments in the current period In the early years, there are many future cashflows (large increase) while the expected payment is low (for a young policyholder) - leading to an increase in reserves in the earlier periods. In later years, this effect reverses as there are not many future cashflows left while expected payments increase, leading to the reserve decreasing in later periods. If both are on the same basis , then the excess in the early period will be exactly used to build the reserve while the shortfall in the later periods will be exactly covered. However, since the basis for reserves are more conservative, more than what is available is set aside in the earlier periods (resulting in negative profits early on ) and more than what is needed is released later on (resulting in positive profits in later periods). The key is understanding that the reserve is functionally a savings account that the insurer sets aside to ensure future obligations can be met. The reserve does NOT change the dollar amount of profit; it only affects the timing of the profit . Due to the time value of money, recognizing profits earlier is better as they are discounted less, resulting in greater profits on an NPV basis . Note Consider decrease in reserve across all periods (EG. Due to an Increase of Interest Rates): Negative Profits in the earlier years will decrease as there is less reserve to build up ( Increase in earlier profits ) Positive Profits in the later years will decrease as there is less reserve to release ( Decrease in later profits ) Profits are recognized earlier rather than later Due to the time value of money , the uplift from the earlier periods is stronger, resulting in an increase to NPV.","title":"Key Intuition"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#expected-profit","text":"","title":"Expected Profit"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#net-cashflow","text":"Consider the cashflows that occur in a single period from the insurer's perspective : Period Start : Collect Premiums (P) and incur Maintenance Expenses (e) Period End : Payout Claims (B) occurring in the period, along with any associated Claim Expenses (CE) It is typically assumed that the insurer will invest the net cashflows at the beginning of the period in order to earn interest . Thus, the net cashflow for the period is the combination of the above: \\[ \\text{Net Cashflow}_{t} = (P_{t} - E_{t})(1 + i) - q_{x} \\cdot (B_{t} + \\text{CE}_{t}) \\] Note Even if the net cashflow in the beginning of the year is negative \\((P - e < 0)\\) , the insurer will still invest the amount. In this case, it can be understood as the insurer has to borrow money , which they must then pay interest on at the end of the period. Similarly, if there is cashflow that occurs in the middle of the year, then the insurer will incur interest on that amount for the remaining time till the end of the year. Tip The above expression can be enhanced to add in other benefits by applying the appropriate risk rates: Surrender Benefit : Lapse Rate Maturity Benefit : Survival Rate (last period only) Annuity Benefit : Survival Rate Notice that the expenses above were listed as maintenance expenses. This is because Acquisition Expenses are assumed to be pre-contract expenses . They are the ONLY cashflow in year 0 and will NOT earn any interest : Info In practice, acquisition expenses are usually grouped with the other cashflows that occur in the first policy year. However, the above is assumed for the purposes of this exam.","title":"Net Cashflow"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#emerging-profit","text":"For policyholders who die during the period, a claim must be paid out to them. However, the insurer does not need to fund the claim entirely from cashflows during the period; they have a reserve that was previously built up. Conversely, for policyholders who do not die during the period, they must set up a reserve for them. This is because a claim will be inevitably made in some future period. Thus, the reserve can be thought of like a savings account used to fulfil the specific purpose of meeting claims: Period Start : Insurer can access the reserve set up as at the previous period Period End : Insurer must set up a new reserve for the future claims that will be made Similarly, the insurer is assumed to invest the reserve at the beginning of the period to maximize returns. The insurer has both the interest earned and net cashflow arising from that period available to set aside the new reserve. Any remaining amount can be recognised as profit as the insurer has fulfilled their liabilities : \\[ \\begin{aligned} \\text{Profit} &= {}_{t-1}V \\cdot (1+i) + \\text{Net Cashflow} - p_{x} \\cdot {}_{t}V \\\\ &= \\text{Net Cashflow} - \\underbrace{p_{x} \\cdot {}_{t}V - {}_{t-1}V \\cdot (1+i)}_\\text{Change in Reserves} \\end{aligned} \\] Warning Do NOT attempt to calculate the change in reserve as a seperate component as the opening and ending reserves are on a different basis: Opening Reserve : Per Policy Reserve, since profit assumes all policies in-force at start of year Ending Reserve : In-force reserve Since the probabilities will already be accounted for the in the above equation, the chances of making an error are lower if the full equation is used. The above expression is often referred to as the profit that emerges or arises from that period, as it occurs at the end of the period as a combination of all other cashflows that occurred in that period. Tip Some questions may provide the reserves while other questions may require us to compute it ourselves. Regardless, there are always two time points where the reserves are known: Policy Start : Starting reserve is 0 (Reserve has not been set up yet) Policy End : Ending reserve is 0 (No more future liabilities) It is also possible that the question uses Net Premium Reserves even though the profit testing is done a Gross Premium basis . Always read the question properly. The above can be calculated in Excel and presented as the following:","title":"Emerging Profit"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#profit-basis","text":"Note that the projection basis (the assumptions used) for reserving and profit testing might be different: Reserving Basis : Conservative; Best-estimate with margin for adverse deviation Profit Basis : Realistic; best-estimate rates only Warning This includes ALL assumptions , not just risk rates. In practice, this usually comes from two sources : Margin for Adverse Deviation : Affects risk rates plus some fixed assumptsion (EG. Expense) Interest Rates : Reserves discounted as Risk Free Rate while profit projected as Best Estimate Investment Returns If the reserving and profit testing basis are the same , then there will be NO profit emerging from the policy. This is because the change in reserve (due to cashflows and unwinding) will completely offset the net cashflow emerging from the policy. Thus, if actual experience exactly follows the best estimate assumptions, the insurer should earn a profit from the margin for adverse deviation that was held.","title":"Profit Basis"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#profit-vector","text":"The Profit Vector of a policy is a vector containing the emerging profits from each policy year: \\[ \\text{PR} = (\\text{PR}_{0}, \\text{PR}_ {1}, \\dots ,\\text{PR}_{t}) \\] Warning The profit vector includes policy year 0 , which is mainly the pre-contract expenses. HOWEVER, the question could provide a reserve at the end of time 0 - which would count towards the profit from policy year 0 as well. Note One typical problem for questions on this topic is the age of the policyholder to use . Using the above diagram, a relationship can be easily seen: Age at BOP: \\(x+t-1\\) , used for Decrements Age at EOP: \\(x+t\\) , used for Reserves For a typical life insurance contract, there is a typically a negative profit in the first year (new busines strain) due to the high cost of setting up the policy (expenses and reserves). Following that, small magnitudes of positive profits will emerge over its lifetime. Note Most questions will only require computing the profit vector for a short period of time given the intensity needed to calculate the reserves at various points in time. If required to compute the profit vector for a long duration, the question will usually specify to ignore the change in reserves or simply provide them , which greatly simplifies the required calculations.","title":"Profit Vector"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#profit-signature","text":"Consider the expression for profit - an implicit assumption it makes is that the policyholder is alive at the start of the period . However, this means that each term in the profit vector is on a different basis and thus cannot be analyzed together. To account for this, each element can be adjusted by accounting for the probabilities of survival to the start of each period . This converts each element into the profit at issue that arises from the period, which can be analyzed together. The resulting vector is known as the Profit Signature of the policy: Profit Vector : Profit at the end of period, assuming the policy is in-force at the start of the period Profit Signature : Profit at the end of period, assuming that the policy is in-force at issue \\[ \\begin{aligned} \\boldsymbol{\\pi} &= (\\pi_{0}, \\pi_{1}, \\dots, \\pi_{t}) \\\\ &= ({}_{0}p_{x} \\cdot \\text{PR}_{0}, {}_{0}p_{x} \\cdot \\text{PR}_{1}, {}_{t-1}p_{x} \\cdot \\text{PR}_{t}) \\end{aligned} \\] Tip Recall that \\({}_{0}p_x = 1\\) , as it represents the probability of surviving till this instant !","title":"Profit Signature"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#multi-state-models","text":"Multi State Models add another layer of complexity to profit projection which poses the following issues: Multiple benefits to consider Cashflows are dependent on the state that the policyholder is in Not possible to know which state the insured will be in at issue Thus, the following changes need to be made: Consider the outgo from ALL possible benefits and decrements Dependent Decrements should be used; survival should include ALL decrements Consider ALL possible states the policyholder could be in at the start of the year \\[ \\begin{aligned} \\text{NCF}^{(i)}_{t} &= (P - e)(1+i) - \\sum q^{ij}_{x} \\cdot (B^{ij} + E^{ij}) \\\\ \\text{PR}^{(i)}_{t} &= \\text{NCF}_{t} - \\sum [p^{(ij)}_{x} \\cdot {}_{t}V^{(j)} - {}_{t-1}V^{(i)} \\cdot (1+i)] \\\\ \\pi &= \\sum_{t, i} {}_{t-1}p^{0i} \\cdot \\text{PR}^{i}_{t} \\end{aligned} \\] Warning For multi-state questions, note that the expression for profit may be different for each state - do not autopilot into the formulas. For instance, some states might not receive premiums or might not pay benefits. Warning When dealing with multiple decrements, it is important to remember that only the dependent decrements should be used. Thus, pay attention to the following details: \"Mortality follows the Standard Ultimate Life Table\": Death decrement this way is dependent \"X% of the surviving policyholders surrender\": Survival occurs independently at the end of the year \"Of the 1000 policyholders, X died and Y surrendered\": Death and Survival calculated this way are dependent","title":"Multi State Models"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#profitability-metrics","text":"Using the profit cashflows above, insurers can gauge the profitability of the policy based on different metrics.","title":"Profitability Metrics"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#net-present-value","text":"Firstly, the Net Present Value (NPV). It is the EPV of the profits , discounted using a specified Risk Discount Rate : \\[ \\begin{aligned} \\text{NPV} &= \\text{PR}_{0} + v\\text{PR}_{1} + v^{2} p_{x} \\text{PR}_{2} + v^{3} {2}p_{x} \\text{PR}_{3} + \\dots \\\\ &= \\text{PR}_{0} + v\\text{PR}_{1} + v^{2} p_{x} \\text{PR}_{2} + v^{3} p_{x} p_{x+1} \\text{PR}_{3} + \\dots \\\\ \\end{aligned} \\] Note The Risk Discount Rate is also sometimes referred to as the Hurdle Rate . They represent the return that an investor expects to earn based on the risk profile of the business. It is typically different from the interest rates used for reserving or profit testing. Tip Recall the intuition regarding EPV: The complexity of life insurance is that there are two dimensions - Incidence Probability (\u201cE\u201d) & Time Value of Money (\u201cPV\u201d). The key understanding is that the benefit of the policy is fixed. A policy with $100,000 sum assured will always at most pay out $100,000. The incidence probability affects the TIMING of the payment . Higher incidence probability increases expected benefit in the earlier years and decreases expected payments in the later years. On whole, the expected payment for the policy is still $100,000 - but the benefits are expected to pay earlier rather than later . The time value of money then affects how these benefits are valued . Benefits that emerge earlier are discounted less , thus have a higher EPV. For an annuity, the key focus is on expected NUMBER of payments . The incidence probability decreases the probability of staying in the state, thus lowering the expected number of payments. This is useful for determining the impact on the EPV of Premiums and Expenses.","title":"Net Present Value"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#recursion","text":"Apart from calculating NPV, one common type of question involves estimating the NPV as at a future valuation date , given the NPV as at the current date. Consider the cashflows for the NPV on issue versus the NPV one period later: \\[ \\begin{aligned} \\text{NPV}_{0} &= \\text{PR}_{0} + v \\text{PR}_{1} + v^{2} p_{x} \\text{PR}_{2} + v^{3} p_{x} p_{x+1} \\text{PR}_{3} + \\dots \\\\ \\text{NPV}_{1} &= \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ v\\text{PR}_{2} + v^{2} p_{x+1} \\text{PR}_{3} + \\dots \\end{aligned} \\] There are three key differences: The expected profits in policy year 0 and 1 have been realized thus are no longer part of NPV The valuation date has moved back one year thus there is an unwinding of interest by one year Similarly, the policyholder has survived one year thus there is 'unwinding' of mortality by one year as well By applying the above changes in a stepwise order , the future NPV can be obtained.","title":"Recursion"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#decrement-change","text":"Similarly, another type of question involves estimating the impact to NPV given a change in decrements . There are only a few adjustments that need to be made. Consider a change in the first year decrements: \\[ \\begin{aligned} \\text{NPV} &= \\text{PR}_{0} + v \\text{PR}_{1} + v^{2} p_{x} \\text{PR}_{2} + v^{3} p_{x} p_{x+1} \\text{PR}_{3} + \\dots \\\\ \\text{NPV}' &= \\text{PR}_{0} + v \\text{PR}'_{1} + v^{2} p'_{x} \\text{PR}_{2} + v^{3} p'_{x} p_{x+1} \\text{PR}_{3} + \\dots \\\\ \\end{aligned} \\] There are two key differences: The emerging profit from the AFFECTED YEAR must be changed to reflect the new basis The profit signature for all FUTURE YEARS must be changed to reflect the new in-force probability Profit signatures from all prior years are unaffected By applying the above changes in a stepwise order , the adjusted NPV can be obtained. Warning Even if the decrement has no impact on the benefit (EG. Change in Lapse but no surrender benefit), it will still have an impact on the profit as it affects the expected reserve for the year.","title":"Decrement Change"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#actual-experience","text":"Another type of question could be to ask for the NPV assuming a certain event occurred - EG. The policyholder actually dies at the end of the second year. In these cases, the decrements must be adjusted. If the policyholder dies in the second year: Death and Lapse decrement in the first year is 0 (WILL NOT die or lapse) Death decrement in the second year is 1 (WILL die) The weighted average of the NPV of all scenarios in the policy will give the total NPV of the policy. This allows the insurer to better understand which scenarios are better for them.","title":"Actual Experience"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#internal-rate-of-return","text":"Next, the Internal Rate of Return (IRR). It is the Risk Discount Rate that will cause the NPV to be 0: \\[ \\text{NPV}_\\text{IRR} = 0 \\] Warning IRR CANNOT be calcualted via an equation and must be solved using numerical methods or a financial calculator. Note IRR measures the highest possible discount rate such that the project breaks even. If the actual discount rate is lower than the IRR , the resulting NPV will always be positive. Thus, higher IRRs are preferred. One common type of question involves determining how a change in cashflows affects the IRR. Generally speaking, any change that would result in a higher NPV would also result in a higher IRR , as the discount rate would need to be higher to force the new NPV to 0.","title":"Internal Rate of Return"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#discounted-payback-period","text":"Next, is the Discounted Payback Period (DPP). It is the earliest period where the partial NPV becomes positive : \\[ \\text{DPP} = \\min [t: \\text{Partial NPV}(t) > 0] \\] Partial NPV at time \\(t\\) is the NPV that is calculated using a subset of cashflows from time 0 up to and including time \\(t\\) . It is a measure of the accumulated profit (on a EPV basis) up till that time. Thus, the discounted payback period is a measure of the earliest time that the accumulated profits become positive . \\[ \\text{Partial NPV}(t) = \\pi_{0} + v^{1} \\pi_{1} + v^{2} \\pi_{2} + \\dots + v^{t} \\pi_{t} \\] Note Most policies have a large pre-contract expense while the profits in the initial years are small (due to building up the reserves). Thus, it takes a few years to recoup the costs - negative partial NPV for the first few years of the policy.","title":"Discounted Payback Period"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#profit-margin","text":"Lastly, the Profit Margin . It is the ratio of the EPV of the profits (NPV) to that of the premiums, assuming the same discount rate : \\[ \\begin{aligned} \\text{Profit Margin} &= \\frac{\\text{NPV}}{\\text{EPV(Premiums)}} \\\\ &= \\frac{\\text{NPV (Profit At Issue)}}{\\text{EPV(Premiums)}} \\end{aligned} \\] Warning The timing of the premium and profit cashflows are different - do NOT mistakenly apply the same discounting pattern to them:","title":"Profit Margin"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#zeroization","text":"Apart from hitting the specified profitability metrics, another consideration during profit testing is the pattern of profits . As mentioned in the beginning of the section, without a reserve, the typical profit pattern for a life insurance policy is positive in the earlier years while negative in the later years , which motivates the reason for holding a reserve. Thus, the reserve held should achieve the following outcomes: Ensure that there are no negative profits after positive profits (no negative profits in later years) Ensure that positive profits emerge as soon as possible The above objectives can be achieved by holding the minimum reserves at each period. This can be achieved by setting the reserves through a process known as Zeroization : Start from the last policy year Solve for the opening reserve assuming no profit emerges in the year using the recursion formula If positive, set the opening reserve equal to it If negative, set it at zero Repeat for all remaining policy years \\[ \\begin{aligned} \\text{PR}_{t} &= 0 \\\\ \\text{NCF}_{t} + p_{x} \\cdot {}_{t}V - {}_{t-1}V \\cdot (1+i) &= 0 \\\\ {}_{t-1}V &= \\frac{\\text{NCF}_{t} + p_{x} \\cdot {}_{t}V}{1+i} \\\\ \\\\ \\therefore {}_{t-1}V^{z} &= \\max ({}_{t-1}V^{z}, 0) \\end{aligned} \\] There are a few points to take note of: The expected reserve in the final year is 0 - so this process can always be used given all the underlying net cashflows Opening reserve that was solved for will be used as the ending reserve in the next policy year of the process However, ending reserve used for the next process is the EXPECTED ending reserve , thus remember to multiply by the survival probabilities before continuing Note The key intuition is that if the profit arising from that period is positive , then there is no need for a reserve (recall that the motivation for the reserve is to cover negative profit). If the profit arising is negative , then a reserve exactly equal to the negative profit is held to ensure it is non-negative. What this achieves is that the absolute minimum reserve is held ; no \"extra\" reserve that will show up as a profit later on (no negative profits later on achieved). As a result, the maximum amount of positive profits from the early years are recognized. In practice, insurers often do not hold the zeroized reserves due to the following reasons: Minimum reserves required via Statutory Reporting Additional reserves to cover for Adverse Experience Additional reserves to cover for Guarantees","title":"Zeroization"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#actual-profits","text":"If actual experience follows the assumptions exactly, the expected profit will materialize into Actual Profits . In reality, this is unlikely to occur, leading to deviation between the two. The difference is known as the Gain or Loss of the policy: \\[ \\text{Gain/Loss} = \\text{Actual Profit} - \\text{Expected Profit} \\] Note For the purposes of this exam, if the question asks for gains by source, the Expected Profit is assumed to be 0 . \\[ \\begin{aligned} \\text{Total Gain/Loss} &= \\text{Actual Profit} - \\text{Expected Profit} \\\\ &= \\text{Actual Profit} - 0 \\\\ &= \\text{Actual Profit} \\end{aligned} \\] The implicit assumption here is that the profit testing basis is the same as the reserving basis . Note that this only holds true for questions that ask for Gains by Source - thus read the entire question to know if this applies. Since gains occur as a result of assumption deviation, it can decomposed into the gain from specific assumptions . This is useful in the following areas: Assumption Review Resource Allocation Performance Assessment \\[ \\begin{aligned} \\text{Total Gain} &= \\text{Expense Gain} + \\text{Interest Gain} + \\text{Mortality Gain} \\end{aligned} \\] Tip This can be used as a check to ensure that the attribution was done correctly - by checking to ensure that they reconcile to the total. For this section, let the actual experience be denoted with a prime superscript \\('\\) .","title":"Actual Profits"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#stepwise-attribution","text":"The method of attribution is known as Stepwise Attribution . It involves changing one assumption at a time (expected to actual), on top of previous changes , till the final set of assumptions are obtained (all actual). The difference between steps is the gain from the assumption that was changed in the step: The order of attribution is important. Although the total gain will remain the same, the amount allocated to each component will be different if a different order is used. This is due to the allocation of the correlation impact : \\[ \\text{Gain from Assumption} = \\text{Absolute Gain} + \\text{Correlation Impact} \\] Thus, different orders of attributions will result in similar but ultimately different values . Questions will usually specify the order of attribution to use. Note An interesting relationship is that not all assumptions are correlated with one another. In other words, the correlation impact between them is 0. Thus, different orders of attribution COULD lead to the same impact : Assuming that there are claims expenses, the following three combinations would result in the same Investment Gain : Expense, Interest & Mortality Expense, Mortality, Interest Mortality, Expense Interest The key is understanding that since Mortality and Interest are not correlated, the only requirement to produce the same attribution is to have Expense be attributed before Intetest .","title":"Stepwise Attribution"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#portfolio-gain","text":"Most questions involve calculating the gain from a portfolio of identical policies : \\[ \\text{Portfolio Gain} = \\text{Gain Per Policy} \\cdot \\text{Number of Policies at the Start} \\] Warning It is a common mistake to use the ending number of policies (after actual decrements) because the \"intuition\" is that the gain/loss is made on the remaining policies.","title":"Portfolio Gain"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#expense-gain","text":"Since expense is a cash outflow , gains are recognised when the actual outflow is smaller than the actual outflow: \\[ \\begin{aligned} \\text{Expense Gain Per Policy} &= \\text{Expected Expense Profit} - \\text{Actual Expense Profit} \\\\ &= E(1+i) - q_{x} \\cdot \\text{CE} - [E'(1+i) - q_{x} \\cdot \\text{CE}'] \\\\ &= (E-E')(1+i) - q_{x} (\\text{CE}-\\text{CE}') \\end{aligned} \\] Additionally, since this is the first attribution step , ONLY the difference in expense needs to be accounted for. This is because the other assumptions are still assumed to be at their expected value, thus it will offset one another.","title":"Expense Gain"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#interest-gain","text":"On the flipside, since interest is a cash inflow , gain is recognised when the actual inflow is larger than the actual outflow: \\[ \\begin{aligned} \\text{Interest Gain Per Policy} &= \\text{Actual Interest Profit} - \\text{Expected Interest Profit} \\\\ &= ({}_{t-1}V + P - E') \\cdot i' - ({}_{t-1}V + P - E') \\cdot i \\\\ &= (i' - i)({}_{t-1}V + P - E') \\end{aligned} \\] Similarly, since this is NOT the first attribution step, all assumptions that have been attributed before must be included - ALL instances of the assumption should be using its actual value . This is to account for the correlation impact between the two. Warning Even though is says \"Expected Profit\", it is not truly the expected profit. This is because of the stepwise attribution , the expense assumption should be using the actual value instead.","title":"Interest Gain"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#mortality-gain","text":"Similarly, mortality is a cash outflow , gain is recognised when the actual outflow is smaller than the actual outflow. \\[ \\begin{aligned} \\text{Mortality Gain Per Policy} &= \\text{Expected Mortality Profit} - \\text{Actual Mortality Profit} \\\\ &= q_{x} \\cdot (B+\\text{CE}') + (1-q_{x}) \\cdot {}_{t}V - [q'_{x} \\cdot (B + \\text{CE}') + (1-q'_{x}) {}_{t}V] \\\\ &= q_{x} \\cdot (B+\\text{CE}') + {}_{t}V - q_{x} \\cdot {}_{t}V - [q'_{x} \\cdot (B+\\text{CE}') + {}_{t}V - q'_{x} {}_{t}V] \\\\ &= q_{x} \\cdot (B+\\text{CE}') + {}_{t}V - q_{x} \\cdot {}_{t}V - q'_{x} \\cdot (B+\\text{CE}') - {}_{t}V + q'_{x} {}_{t}V \\\\ &= (q_{x} - q'_{x})(B+\\text{CE}') - (q_{x} - q'_{x}){}_{t}V \\\\ &= (q_{x} - q'_{x})(B+\\text{CE}'- {}_{t}V) \\\\ \\end{aligned} \\] Questions may alternatively provide the actual number of policyholders (NOP) instead of the probabilities. Thus, we need to solve for the actual probabilities using: \\[ q'_{x} = \\frac{\\text{NOP}_\\text{Beginning} - \\text{NOP}_\\text{Ending}}{\\text{NOP}_\\text{Beginning}} \\]","title":"Mortality Gain"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#other-gain","text":"Gains for other cashflows (EG. Surrender or Maturity) can be calculated by applying the same principles as above: \\[ \\begin{aligned} \\text{Lapse Gain Per Policy} &= (w_{x} - w'_{x})(B^{w} + \\text{CE}^{w'} - {}_{t}V) \\\\ \\end{aligned} \\] The expression is analagous to the mortality gain. Note that even though the expected reserve appears in both the mortality and lapse gain, there is NO double counting - they each quantify the impact of its respective decrement on the expected reserve. Note If the feature was not expected but eventually paid (EG. Expected profit does contain Surrender but actual profit do), then the expected profit from that component is 0 - the gain or loss is the entire outflow for that benefit. Source Cashflow Formula Expense Outflow Expected - Actual Mortality Outflow Expected - Actual Surrender Outflow Expected - Actual Interest Inflow Actual - Expected","title":"Other Gain"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/5.%20Profit%20Testing/#other-applications","text":"This method essentially attributes the impact of each assumption difference (Actual vs Profit Testing Basis) on Actual Profit. However, it can be used to determine Expected Profit as well. Recall that expected profit arises due to the difference in basis between Reserving and Profit Testing. Thus, by using the methods above, the expected profit from Investment, Mortality or Expense can be quantified. Generally speaking, this is not very useful as we are usually interested in the whole expected profit, not the expected profit from a specific assumption . However, if there are only 1-2 differences in basis, this method can be used to quickly determine the overall profit. For instance, if the only difference between profit testing and reserving is only the investment return assumption: \\[ \\begin{aligned} \\text{Expected Profit} &= \\text{Expected Profit from Investment} \\\\ &= ({}_{t-1}V + P - E)(i\u2019 - i) \\end{aligned} \\] This is useful as the entire profit calculation does not need to be performed , as well as for situations where insufficient information is given.","title":"Other Applications"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/6.%20Universal%20Life/","text":"Universal Life \u00b6 Universal Life policies are a special type of life insurance policies known for their flexibility . They provide policyholders with the ability to choose how much premiums they want to pay and how much benefits to receive, allowing them to effectively create a policy that is customized to their specific needs. Account Value \u00b6 The defining feature of a universal life policy is that every policy has an account associated with it: Policyholders choose how much premium to deposit into the account (subject to some minimum requirements ) Insurer posts Expenses charges to the account to cover the costs to maintain the policy Insurer posts Mortality charges to the account to cover the cost of providing insurance coverage Insurer credits interest into the account ( proportion of actual return, subject to some minimum rate ) \\[ \\begin{aligned} \\text{AV}_{t} &= (\\text{AV}_{t-1} + P_{t} - E_{t} - \\text{COI}_{t})(1 + i^{c}) \\\\ \\end{aligned} \\] Warning It looks similar to the profit calculation but one key difference is that the interest is earned on the entire balance , not just the beginning of period cashflows. Info The minimum credited rate is one of the key unique selling points of a UL policy - which positions itself as a \"low risk\" savings/investment vehicle. Info The account is purely notional - the insurer does not hold seperate assets for each account; they are kept entirely within the general account of the company. The insurer reviews their investment return to decide how much interest to credit the UL account. Policyholders will receive account statements detailing account transactions. This provides transparency to the policyholders, allowing them to easily distinguish between the insurance and savings component of the policy. The above can be calculated in Excel and presented as the following: Cost of Insurance \u00b6 The policy provides two benefits: Death Benefit Surrender Benefit Upon Death Upon lapse Specified Amount Account Value It is important to understand that these two benefits are mutually exclusive; a policyholder can only receive one of them. Thus, if a death claim is made, the account value can be used to offset the amount since it is no longer needed to pay a potential survival benefit. Thus, the amount of insurance coverage provided (amount that the insurer pays out of pocket ) is the difference between the death benefit and account value, known as the Sum at Risk (also known as the Additional Death Benefit ): \\[ \\text{SAR} = \\text{Death Benefit} - \\text{Account Value} \\] The Cost of Insurance (COI) is the cost of providing life insurance coverage equal to the SAR for that one year. It is equal to the expected outgo of the SAR: \\[ \\text{COI}_{t} = \\text{SAR} \\cdot q^{*}_{x+t-1} \\cdot \\frac{1}{1 + i^{q}} \\] Intuitively, it is essentially the pure premium charged for the insurance, which must be paid at the start of the period . However, the actual death cashflow will only be known at the end of the period. Thus, the the amount must be calculated as at the end of the period and discounted back to obtain the pure cost. Warning The discount rate used for CAN be different from the credited interest rate. This is why the two have different notations ( \\(i^{q}\\) and \\(i^{c}\\) ). If not seperately specified, assume that the it follows the credited interest rate. Tip Notice that the COI is dependent on the AV, which is in turn dependent on the COI; there is a circular dependency : Thus, it is NOT possible to calculate the COI seperately and plugged into the AV calculation. They must be combined into a one large equation where the AV is solved for. If using excel, setting up the equation will throw a circular reference error . There are two ways around this: Enable Iterative Calculations (Changes Excel behaviour to accept the loop ) Create a seperate AV variable and use Goalseek to set them equal ( Manually close the loop) It is safer to use the second approach as turning on iterative calculations might have unintended side effects. However, note that the goalseek must be done for each policy year and must be repeated for any change in the setup. Corridoor Factor \u00b6 There are two types of universal life policies in terms of their Death Benefit (DB) structure: Type A Type B Pre-Defined Death Benefit Variable Death Benefit Decreasing SAR Fixed SAR Info The above diagram assumes that the account value will always increase over time. While this is a reasonable expectation, the account value could decrease over time as well if insufficient premiums are paid. UL policies are still fundamentally an insurance product, not an investment product . Thus, the policy must ensure that significant isurance benefit is provided. This is achieved via a Corridoor ( \\(\\gamma\\) ) - which is the minimum ratio of the death benefit to the account value: \\[ \\begin{aligned} \\gamma &= \\frac{\\text{DB}_{t}}{\\text{AV}_{t}} \\\\ &= \\frac{\\text{AV}_{t} + \\text{SAR}_{t}}{\\text{AV}_{t}} \\\\ &= 1 + \\frac{\\text{SAR}_{t}}{\\text{AV}_{t}} \\end{aligned} \\] The corridoor is applicable to BOTH type A and type B policies: Type A policies tend to be more impacted as the SAR decreases over time, causing it to fall below the minimum threshold in the later years Type B policies have a fixed SAR, thus are unlikely to be impacted unless the account value grows to extremely large size \\[ \\begin{aligned} \\text{DB}_{t, c} &= (1 + \\gamma) \\cdot \\text{AV}_{t} \\\\ \\therefore \\text{DB}_{t} &= \\max (\\text{DB}_{t, c}, \\text{DB}_{t, nc}) \\end{aligned} \\] The presence of a corridoor poses a challenge in terms of how to calculate the account value. This because the target variable (account value) is part of a maximum function. Thus, the following steps must be taken to determine the correct account value: Solve for the account value assuming there is NO corridoor Check the ratio of DB to AV If larger than the corridoor , use the previously calculated AV as the final AV for the period If smaller than the corridoor , recalculate the AV assuming the corridoor applies; use the resulting AV as the final AV for the period Note The same approach can be used from before. Simply update the death benefit logic to account for the Corridoor: Surrender Benefit \u00b6 There are high acquisition costs associated with setting up an insurance policy, which is recovered over the lifetime of the policy. However, if the policyholder surrenders the policy early on, the insurer loses the opportunity to recover these costs. Thus, insurers usually impose a Surrender Charge on the policy if the insured surrenders the policy in the first few years from inception. It can either be pre-determined amount or proportional to the account value. In either case, the charge decreases over time to 0 as the insurer has recovered the costs. \\[ \\text{Surrender Benefit} = \\max (\\text{Account Value} - \\text{Surrender Charge}, 0) \\] Warning It is a common mistake to forget to floor the surrender value - especially for questions where there are multiple scenarios and the weighted surrender benefit is calculated. This is because if calculating the surrender value alone, a negative surrender value would flag out. However, when combined, there would be some offsetting impact from the other scenarios. No Lapse Guarantee \u00b6 Given that policyholders have the liberty to choose their own premiums, they could theoretically underfund the policy, causing the account value to decrease to 0 over time, causing the policy to automatically lapse . In order to hedge against such a situation, some UL policies offer a No Lapse Guarantee (NLG) feature, where the policy would remain in-force even if the account value reaches 0. Info There are usually some conditions tagged to the NLG feature, such as requiring the insured to have paid some minimum level of premium. The NLG feature will provide coverage equal to what the underlying policy would have, typically up to some maximum age . Thus, the NLG feature can be valued the same way as a term insurance policy. If the value of NLG feature is larger than the account value of the policy at the time, the insurer must hold an additional reserve equal to the difference: \\[ \\begin{aligned} \\text{EPV(NLG)} &= \\text{Death Benefit} \\cdot A^{1}_{x:\\enclose{actuarial}{n}} \\\\ \\therefore {}_{t}V^{\\text{NLG}} &= \\max(0, \\text{EPV(NLG)} - \\text{Account Value}) \\end{aligned} \\] Profit Testing \u00b6 Unlike Equity Linked Insurance, the account is PURELY notional . Thus, the usual profit testing approach can be taken - with only the following exceptions: The insurer holds reserves equal to the account value The benefits paid out are a function of the account value (Considering the ADB and Surrender Charge) Some questions might instead hold reserves equal to the Surrender Value (net of the surrender charge) instead. If there are surrender charges, this lowers the overall reserves held, allowing profit to emerge earlier and hence increasing its profitability. Note However, the profit emerging from the second year of the policy might be lower than before. This is due to the following effects: The reserve brought forward is lower than before (due to the surrender charge in the first year) The required reserve at the end of the period is also lower (due to the surrender charge in the second year) However, since surrender charges decrease over time , the decrease in opening reserve is larger than the decrease in the ending reserve, causing profit emerging in that year specifically to be lower than regular reserves. Scenario Testing \u00b6 One common type of questions asks for the change in the account value given a change one of the assumptions . To effectively tackle these questions, the following relationship can be used: \\[ \\begin{aligned} \\text{Account Value} &= \\text{Accumulated Premiums} \\\\ &- \\text{Accumulated Expenses} \\\\ &- \\text{Accumulated COI} \\end{aligned} \\] The key intuition is that the components of Account value are mostly independent of one another (excluding COI). Thus, a change in one of the assumptions should only influence that particular component - there is no need to re-calculate ALL components. For instance, if there is a change in the Y3 COI rate, then only the Y3 COI cashflow needs to be recalculated. Then, determine the impac to the accumulated COI and apply that to the existing account value. Tip If the cashflow and credited interest rate is constant , then the account value at any time can be found using accumulation functions: \\[ \\begin{aligned} \\text{Accumulated Cashflow} &= \\text{Cashflow} \\cdot s_{\\enclose{actuarial}{n}} \\\\ &= \\text{Cashflow} \\cdot \\frac{(1+i^{c})^{n}-1}{i^{c}} \\end{aligned} \\]","title":"Universal Life"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/6.%20Universal%20Life/#universal-life","text":"Universal Life policies are a special type of life insurance policies known for their flexibility . They provide policyholders with the ability to choose how much premiums they want to pay and how much benefits to receive, allowing them to effectively create a policy that is customized to their specific needs.","title":"Universal Life"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/6.%20Universal%20Life/#account-value","text":"The defining feature of a universal life policy is that every policy has an account associated with it: Policyholders choose how much premium to deposit into the account (subject to some minimum requirements ) Insurer posts Expenses charges to the account to cover the costs to maintain the policy Insurer posts Mortality charges to the account to cover the cost of providing insurance coverage Insurer credits interest into the account ( proportion of actual return, subject to some minimum rate ) \\[ \\begin{aligned} \\text{AV}_{t} &= (\\text{AV}_{t-1} + P_{t} - E_{t} - \\text{COI}_{t})(1 + i^{c}) \\\\ \\end{aligned} \\] Warning It looks similar to the profit calculation but one key difference is that the interest is earned on the entire balance , not just the beginning of period cashflows. Info The minimum credited rate is one of the key unique selling points of a UL policy - which positions itself as a \"low risk\" savings/investment vehicle. Info The account is purely notional - the insurer does not hold seperate assets for each account; they are kept entirely within the general account of the company. The insurer reviews their investment return to decide how much interest to credit the UL account. Policyholders will receive account statements detailing account transactions. This provides transparency to the policyholders, allowing them to easily distinguish between the insurance and savings component of the policy. The above can be calculated in Excel and presented as the following:","title":"Account Value"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/6.%20Universal%20Life/#cost-of-insurance","text":"The policy provides two benefits: Death Benefit Surrender Benefit Upon Death Upon lapse Specified Amount Account Value It is important to understand that these two benefits are mutually exclusive; a policyholder can only receive one of them. Thus, if a death claim is made, the account value can be used to offset the amount since it is no longer needed to pay a potential survival benefit. Thus, the amount of insurance coverage provided (amount that the insurer pays out of pocket ) is the difference between the death benefit and account value, known as the Sum at Risk (also known as the Additional Death Benefit ): \\[ \\text{SAR} = \\text{Death Benefit} - \\text{Account Value} \\] The Cost of Insurance (COI) is the cost of providing life insurance coverage equal to the SAR for that one year. It is equal to the expected outgo of the SAR: \\[ \\text{COI}_{t} = \\text{SAR} \\cdot q^{*}_{x+t-1} \\cdot \\frac{1}{1 + i^{q}} \\] Intuitively, it is essentially the pure premium charged for the insurance, which must be paid at the start of the period . However, the actual death cashflow will only be known at the end of the period. Thus, the the amount must be calculated as at the end of the period and discounted back to obtain the pure cost. Warning The discount rate used for CAN be different from the credited interest rate. This is why the two have different notations ( \\(i^{q}\\) and \\(i^{c}\\) ). If not seperately specified, assume that the it follows the credited interest rate. Tip Notice that the COI is dependent on the AV, which is in turn dependent on the COI; there is a circular dependency : Thus, it is NOT possible to calculate the COI seperately and plugged into the AV calculation. They must be combined into a one large equation where the AV is solved for. If using excel, setting up the equation will throw a circular reference error . There are two ways around this: Enable Iterative Calculations (Changes Excel behaviour to accept the loop ) Create a seperate AV variable and use Goalseek to set them equal ( Manually close the loop) It is safer to use the second approach as turning on iterative calculations might have unintended side effects. However, note that the goalseek must be done for each policy year and must be repeated for any change in the setup.","title":"Cost of Insurance"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/6.%20Universal%20Life/#corridoor-factor","text":"There are two types of universal life policies in terms of their Death Benefit (DB) structure: Type A Type B Pre-Defined Death Benefit Variable Death Benefit Decreasing SAR Fixed SAR Info The above diagram assumes that the account value will always increase over time. While this is a reasonable expectation, the account value could decrease over time as well if insufficient premiums are paid. UL policies are still fundamentally an insurance product, not an investment product . Thus, the policy must ensure that significant isurance benefit is provided. This is achieved via a Corridoor ( \\(\\gamma\\) ) - which is the minimum ratio of the death benefit to the account value: \\[ \\begin{aligned} \\gamma &= \\frac{\\text{DB}_{t}}{\\text{AV}_{t}} \\\\ &= \\frac{\\text{AV}_{t} + \\text{SAR}_{t}}{\\text{AV}_{t}} \\\\ &= 1 + \\frac{\\text{SAR}_{t}}{\\text{AV}_{t}} \\end{aligned} \\] The corridoor is applicable to BOTH type A and type B policies: Type A policies tend to be more impacted as the SAR decreases over time, causing it to fall below the minimum threshold in the later years Type B policies have a fixed SAR, thus are unlikely to be impacted unless the account value grows to extremely large size \\[ \\begin{aligned} \\text{DB}_{t, c} &= (1 + \\gamma) \\cdot \\text{AV}_{t} \\\\ \\therefore \\text{DB}_{t} &= \\max (\\text{DB}_{t, c}, \\text{DB}_{t, nc}) \\end{aligned} \\] The presence of a corridoor poses a challenge in terms of how to calculate the account value. This because the target variable (account value) is part of a maximum function. Thus, the following steps must be taken to determine the correct account value: Solve for the account value assuming there is NO corridoor Check the ratio of DB to AV If larger than the corridoor , use the previously calculated AV as the final AV for the period If smaller than the corridoor , recalculate the AV assuming the corridoor applies; use the resulting AV as the final AV for the period Note The same approach can be used from before. Simply update the death benefit logic to account for the Corridoor:","title":"Corridoor Factor"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/6.%20Universal%20Life/#surrender-benefit","text":"There are high acquisition costs associated with setting up an insurance policy, which is recovered over the lifetime of the policy. However, if the policyholder surrenders the policy early on, the insurer loses the opportunity to recover these costs. Thus, insurers usually impose a Surrender Charge on the policy if the insured surrenders the policy in the first few years from inception. It can either be pre-determined amount or proportional to the account value. In either case, the charge decreases over time to 0 as the insurer has recovered the costs. \\[ \\text{Surrender Benefit} = \\max (\\text{Account Value} - \\text{Surrender Charge}, 0) \\] Warning It is a common mistake to forget to floor the surrender value - especially for questions where there are multiple scenarios and the weighted surrender benefit is calculated. This is because if calculating the surrender value alone, a negative surrender value would flag out. However, when combined, there would be some offsetting impact from the other scenarios.","title":"Surrender Benefit"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/6.%20Universal%20Life/#no-lapse-guarantee","text":"Given that policyholders have the liberty to choose their own premiums, they could theoretically underfund the policy, causing the account value to decrease to 0 over time, causing the policy to automatically lapse . In order to hedge against such a situation, some UL policies offer a No Lapse Guarantee (NLG) feature, where the policy would remain in-force even if the account value reaches 0. Info There are usually some conditions tagged to the NLG feature, such as requiring the insured to have paid some minimum level of premium. The NLG feature will provide coverage equal to what the underlying policy would have, typically up to some maximum age . Thus, the NLG feature can be valued the same way as a term insurance policy. If the value of NLG feature is larger than the account value of the policy at the time, the insurer must hold an additional reserve equal to the difference: \\[ \\begin{aligned} \\text{EPV(NLG)} &= \\text{Death Benefit} \\cdot A^{1}_{x:\\enclose{actuarial}{n}} \\\\ \\therefore {}_{t}V^{\\text{NLG}} &= \\max(0, \\text{EPV(NLG)} - \\text{Account Value}) \\end{aligned} \\]","title":"No Lapse Guarantee"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/6.%20Universal%20Life/#profit-testing","text":"Unlike Equity Linked Insurance, the account is PURELY notional . Thus, the usual profit testing approach can be taken - with only the following exceptions: The insurer holds reserves equal to the account value The benefits paid out are a function of the account value (Considering the ADB and Surrender Charge) Some questions might instead hold reserves equal to the Surrender Value (net of the surrender charge) instead. If there are surrender charges, this lowers the overall reserves held, allowing profit to emerge earlier and hence increasing its profitability. Note However, the profit emerging from the second year of the policy might be lower than before. This is due to the following effects: The reserve brought forward is lower than before (due to the surrender charge in the first year) The required reserve at the end of the period is also lower (due to the surrender charge in the second year) However, since surrender charges decrease over time , the decrease in opening reserve is larger than the decrease in the ending reserve, causing profit emerging in that year specifically to be lower than regular reserves.","title":"Profit Testing"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/6.%20Universal%20Life/#scenario-testing","text":"One common type of questions asks for the change in the account value given a change one of the assumptions . To effectively tackle these questions, the following relationship can be used: \\[ \\begin{aligned} \\text{Account Value} &= \\text{Accumulated Premiums} \\\\ &- \\text{Accumulated Expenses} \\\\ &- \\text{Accumulated COI} \\end{aligned} \\] The key intuition is that the components of Account value are mostly independent of one another (excluding COI). Thus, a change in one of the assumptions should only influence that particular component - there is no need to re-calculate ALL components. For instance, if there is a change in the Y3 COI rate, then only the Y3 COI cashflow needs to be recalculated. Then, determine the impac to the accumulated COI and apply that to the existing account value. Tip If the cashflow and credited interest rate is constant , then the account value at any time can be found using accumulation functions: \\[ \\begin{aligned} \\text{Accumulated Cashflow} &= \\text{Cashflow} \\cdot s_{\\enclose{actuarial}{n}} \\\\ &= \\text{Cashflow} \\cdot \\frac{(1+i^{c})^{n}-1}{i^{c}} \\end{aligned} \\]","title":"Scenario Testing"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/","text":"Embedded Options \u00b6 Overview \u00b6 Due to developments in the financial market, insurance products had to evolve to remain relevant. This led to development of Equity Linked Insurance , where policy benefits were linked to the performance of an underlying mutual fund . In order to attract investors, these products usually comes with guarantees on their benefits, providing unlimited upside and limited downsides . However, this results in insurance companies bearing huge financial risks . Info These guarantees may not be part of the product features, but offered as a rider for additional cost. Traditional actuarial methods were not well suited to effectively deal with these risks, which led to the insolvency of many insurance companies in the 1990s when interest rates fell. This led to the creation of Market Consistent valuation basis to ensure that these risks are appropriately accounted for. As its name suggests, market consistent valuation involves valuing an asset at its observed market value . However, insurance contracts are almost never traded, thus does not have an observable market value. Instead, the insurance cashflows are translated into tradeable assets , where the value of the contract is equivalent to the price of the assets. The guarantees offered Equity Linked insurance can be translated into an Option , which is why it can be valued as an Embedded Option . Equity Linked Insurance \u00b6 The defining feature of Equity Linked insurance is that the policyholder's premiums are invested into units of one or more mutual funds of the policyholder's choice. The policy benefits are directly linked to the total value of the units in the policyholder's account. Warning It is a common mistake to confuse universal life (UL) and Equity Linked (EL) policies as both involve an account for the policyholder: Universal Life : Account is purely notional; known as Account Value Equity Linked : Account is actually invested in funds of choice; known as Fund Value Unit VS Non-Unit Account \u00b6 The key to understanding Equity Linked insurance is to understand the difference between: Unit Account : Owned by Policyholders; where premiums are invested Non-Unit Account : Owned by Company (Shareholders); where benefits and expenses are paid out from Every period, charges are posted to the unit account to cover the cost of the policy (Mortality & Expense charges, similar to UL). These charges then flow into the non-unit account as fee income , which is then used to cover the actual cost of the policy. Tip There are two distinct cashflows: Charges : Posted to Unit Account, flows into Non Unit Account as Income Cost : Posted to Non Unit Account, actual cost of the policy The amount charged to the policyholder and the actual are often different - where the charges are higher than the cost, allowing the insurer to earn the spread . For instance, the fund management charge might be $100 - but the insurer only incurs $70 of investment expenses (due to economies of scale), allowing them to earn the difference. Another intuitive way to think about it is that for all other types of policies , there exists only a non-unit account . The premiums paid might be more/less than the cost of the policy, the excess/shortfall is managed by the insurer's investments . The key difference is that the premiums are invested by the policyholder ; only what is needed is deducted each period. The investment risk is borne by the policyholder instead. Premium Allocation \u00b6 Only the Allocated Premium is used to purchase units ; the remaining un allocated premium flows directly into the unit account. The proportion of premium that is allocated is controlled by the Allocation Rate : \\[ \\text{Allocated Premium} = \\text{Premium} \\cdot \\text{Allocation Rate} \\] Note It is possible for the allocation rate to be above 100%. The insurer will fund the additional amount as an outflow. When purchasing units, part of the value will be lost due to Transactional Costs such as the Bid Offer Spread : Bid Price : How much the fund manager is bidding for the unit (Sell price) Offer Price : How much the fund manager is asking for the unit (Buy price) The units are purchased based on the Offer Price but values at the Bid Price. However, Offer prices are always higher than bid prices, this leads to an immediate loss in value compared to the premium paid: \\[ \\text{Unit Value Purchased} = \\text{Allocated Premium} \\cdot (1 - \\text{Bid Offer Spread}) \\] Note Some questions might not use the term BO spread but rather a Premium Charge . In either case, the key concept from this section is to understand that not the entire premium is invested in the fund. Fund Value \u00b6 For the purposes of this exam, the fund value in each period can be calculated as: \\[ \\text{FV} = (\\text{Allocated Premium} - \\text{Management Charge}) \\cdot (1 + \\text{Fund Growth}) \\] For the purposes of this exam, the methods and timing of the above cashflows will be specified by the question . The key component is the management charge timing: BOP : Based on fund value BEFORE or AFTER premium? EOP : Based on fund value BEFORE or AFTER growth? The above can be calculated in Excel and presented as the following: Policy Benefits \u00b6 As mentioned previously, the benefits of the policy are tied to the value of the unit account: \\[ \\text{Death Benefit} = \\text{Surrender Benefit} = \\text{Fund Value} \\] However, due to the volatile nature of investing, some policyholders could end up with low fund values and hence low coverage. Thus, most policies offer a guaranteed minimum benefit that will kick in if the fund value is below a certain threshold . The most common variation is to offer a guaranteed minimum benefit based on some multiple \\(\\lambda\\) of the premiums paid to date : \\[ \\text{Minimum Benefit} = \\lambda \\cdot \\text{Premium Paid to Date} \\] The Guaranteed Benefit (GB) will be paid if the Fund Value (FV) is smaller than the guarantee. Thus, the benefit of the policy can be expressed as the following: \\[ \\begin{aligned} \\text{Policy Benefit} &= \\begin{cases} \\text{FV}, & \\text{FV} > \\text{GB} \\\\ \\text{GB}, & \\text{FV} <= \\text{GB} \\end{cases} \\\\ &= \\max (\\text{FV}, \\text{GB}, 0) \\end{aligned} \\] Thus, the insurer does not pay the full benefit amount - they pay the shortfall of the fund value compared to the benefit. Profit Testing \u00b6 Profit testing is very different for Equity Linked Insurance compared to other products. There are several key differences: Only NON-UNIT account cashflows should be considered Reserves are typically not held for Equity Linked Insurance Warning There are a few points to take note of: The insurer receives only the unallocated portion of the premium The insurer receives the management charges as fee income The insurer only pays the shortfall in the benefit amounts The cost to the insurer INCLUDES any guarantees Note that even though both UL and EL policies pay the shortfall in the benefit amount, they are treated differently during profit testing due to the account set up. Warning Even though only the unallocated premiums flow into the profit calculation, the profit margin is calculated based on the Paid Premium . As mentioned previously, the expected benefit above is the expected payment from the insurer (the shortfall): \\[ \\begin{aligned} \\text{EDB} &= \\max (\\text{DB} - \\text{FV}, 0) \\cdot q_{x} \\\\ \\text{ESB} &= (\\text{SB} - \\text{FV}, 0) \\cdot w_{x} \\\\ \\text{EMB} &= \\max (\\text{SB} - \\text{FV}, 0) \\cdot p_{x} \\end{aligned} \\] Tip Notice that for surrender benefit, the shortfall is NOT floored at 0. This is because the insurer may impose surrender charges which will show up as a \"negative cost\" (the doube negative will cause it to become cash inflow ). Note Another quantity of interest is the Cost of Guarantee . It is similar to the cost of benefit, where it measures the contribution of the guarantee to the benefit: \\[ \\text{COG}_{t} = \\max (\\text{GB}_{t} - \\text{DB}_{t}, 0) \\cdot q_{x+t} \\] For most policies where the death benefit is simply the account value, the cost of guarantee and cost of benefit are the same . However, if there is a multiplier on top of the benefit (EG. 105% times the account value), then the two would deviate: \\[ \\begin{aligned} \\text{COG}_{t} &= \\max (\\text{GB}_{t} - 1.1 \\cdot \\text{FV}_{t}, 0) \\cdot q_{x+t-1} \\\\ \\text{COB}_{t} &= \\max (\\max (\\text{GB}_{t}, 1.1 \\cdot \\text{FV}_{t}) - \\text{FV}, 0) \\cdot q_{x+t-1} \\end{aligned} \\] The COG is typically calculated for all periods on issue: \\[ \\begin{aligned} \\text{COG}_{0} &= \\sum \\max (\\text{GB}_{t} - \\text{DB}_{t}, 0) \\cdot {}_{k \\mid}q_{x} \\\\ \\text{COG}_{0} &= \\sum {}_{k-1}p_{x} \\cdot \\text{COG}_{t} \\end{aligned} \\] Comparison with UL \u00b6 There are several key features that make UL or EL more competitive than the other: Premiums : UL \"invests\" lesser premiums into the account due to the COI Death Benefit : UL DB's tend to be higher due to the Corridoor & ADBs Downside Risk : UL's returns have a minimum guarantee; EL can be negative Upside Risk : UL's returns are only a portion of the actual return; EL will get the full return Embedded Options \u00b6 With some manipulation, the above expression for policy benefit can be expressed in the form a European Put Option Payoff : Guaranteed Benefit is a fixed value , similar to the Strike Price fund value follows a stock price process (with charges applied) \\[ \\begin{aligned} \\text{Put Option Payoff}_{t} &= \\max (K - S_{t}, 0) \\\\ \\\\ \\text{Policy Benefit}_{t} &= \\max (\\text{FV}_{t}, \\text{GB}) \\\\ &= \\max (\\text{GB} - \\text{FV}_{t}, \\text{FV}_{t} - \\text{FV}_{t}) \\\\ &= \\max (\\text{GB} - \\text{FV}_{t}, 0) \\end{aligned} \\] Thus, the same framework used to evaluate european put options - Black Scholes Merton model - can be used to evaluate these guaranteed benefits as well. However, there is one key difference between the two that must be accounted for: Regular Option : Time to expiration (when the option is exercised) is fixed Embedded Option : Time is contingent on the survival/death of the policyholder Thus, the framework must be adjusted slightly to account for the probabilities of death/survival within the given timeframe. Note Please refer to the Derivatives Section of the IFM notes for more information on European Options and the Black Scholes model. Unless otherwise stated, all policies referenced in this section are assumed to be Single Premium policies with no additional top ups . Guaranteed Maturity Benefit \u00b6 The maturity benefit is paid at a known time (at the end of the policy term ), given that the policyholder is alive at that time . For simplicity, assume that the policy has a Guaranteed Minimum Maturity Benefit (GMMB) of some multiple of premiums paid : \\[ \\text{GMMB} = \\lambda P \\] Note The expression is simplified in this case since we assume a single premium policy. The fund value projection earlier assumes that the fund grows at a specified rate . An alternative method is to assume that it grows at the same rate as an underlying stock process . Thus, the fund value at maturity of time \\(n\\) can be expressed as: \\[ \\begin{aligned} \\text{FV}_{n} &= \\text{Opening Value} \\cdot \\text{Charges} \\cdot \\text{Growth} \\\\ &= P \\cdot (1 - \\text{IC}) \\cdot (1 - \\text{RC})^{n-1} \\cdot \\frac{S_{n}}{S_{0}} \\\\ &= P \\cdot \\xi \\cdot \\frac{S_{n}}{S_{0}} \\\\ \\\\ \\xi &= (1 - \\text{IC}) \\cdot (1 - \\text{RC})^{n-1} \\end{aligned} \\] Note The charges in this scenario have have been simplified to Initial Charges (IC) and Renewal Charges (RC). Both are charged as a percentage of the fund value at the time. They are then further combined into \\(\\xi\\) , since they have a fixed value (dependent only on policy term, which is fixed for each policy). Warning Some questions might provide the monthly rates instead of annual rates - do not blindly apply the formulas. The payoff of the option on maturity is denoted as \\(h(n)\\) and can be expressed as the following: \\[ \\begin{aligned} h(n) &= \\max (\\text{GB} - \\text{FV}_{t}, 0) \\\\ &= \\max \\left(\\lambda P - P \\cdot \\xi \\cdot \\frac{S_{n}}{S_{0}}, 0 \\right) \\\\ &= P \\xi \\cdot \\max \\left(\\frac{\\lambda}{\\xi} - \\frac{S_{n}}{S_{0}}, 0 \\right) \\\\ &= P \\xi \\cdot \\max \\left(\\frac{\\lambda}{\\xi} - S_{n}, 0 \\right) \\\\ &= P \\xi \\cdot \\max \\left(K^{*} - S_{n}, 0 \\right) \\\\ \\end{aligned} \\] Tip The above can be intepreted as requiring \\(P \\xi\\) european put options with strike price \\(\\frac{\\lambda}{\\xi}\\) . Under the Black Scholes Merton framework, the value of ONE of the above european put options on inception is the risk neutral expected present value of the payoff: \\[ \\begin{aligned} p(0) &= e^{-rn} \\cdot E^{Q}_{t} \\left[\\max (K^{*} - S_{n}, 0) \\right] \\\\ &= e^{-rn} \\cdot \\left(K^{*} \\cdot \\Phi [- d_{2}(0)] - S_{n} \\cdot \\Phi [-d_{1}(0)] \\right) \\\\ &= K^{*} e^{-rn} \\Phi [- d_{2}(0)] - S_{0} \\cdot \\Phi [-d_{1}(0)] \\\\ \\\\ d_{1}(0) &= \\frac{\\ln \\frac{S_{0}}{K} + (r + \\frac{1}{2} \\sigma^{2}) n}{\\sigma \\sqrt{n}} \\\\ &= d_{2}(0) + \\sigma \\sqrt{n} \\\\ \\\\ d_{2}(0) &= \\frac{\\ln \\frac{S_{0}}{K} + (r - \\frac{1}{2} \\sigma^{2}) n}{\\sigma \\sqrt{n}} \\\\ &= d_{1}(0) - \\sigma \\sqrt{n} \\end{aligned} \\] Tip \\(\\Phi(x)\\) represents the CDF of the standard normal distribution at \\(x\\) . It can be determined via the NORM.S.DIST Excel Function . Note In practice, investors take into account risk when looking for looking for returns (which affects asset prices). For instance, if the risk-free rate is \\(r\\) , then investors demand a risk premium for that them to take on an investment with risk . Under a risk neutral basis, it assumes that investors do NOT take into account risk. Thus, assets will be priced based on the risk free rate , rather than on the actual return (with a risk premium). \\[ \\begin{aligned} S_{n} &= S_{0} \\cdot e^{\\alpha n} = S_{0} \\cdot e^{rn} \\\\ \\therefore e^{-rn} \\cdot S_{n} &= S_{0} \\end{aligned} \\] The value of the entire embedded option is the expected number of european put options needed on maturity. Thus, the probability of maturing must be accounted for: \\[ \\begin{aligned} \\pi(0) &= {}_{n}p_{x} \\cdot P \\xi \\cdot p(0) \\\\ &= {}_{n}p_{x} \\cdot P \\xi \\cdot \\left(K^{*} e^{-rn} \\Phi [- d_{2}(0)] - S_{0} \\cdot \\Phi [-d_{1}(0)] \\right) \\\\ \\end{aligned} \\] Note Since \\(T_{x}\\) and \\(S_{x}\\) are independent processes, the risk neutral expectation (\"Q\") of future lifetime is equivalent to the true (\"P\") probabilities. \\[ E^{Q}(T_{x} \\gt n) = E^{P}(T_{x} \\gt n) = {}_{10}p_{x} \\] The above can be adjusted to reflect valuation at any time \\(t\\) : \\[ \\begin{aligned} \\pi(t) &= {}_{n-t}p_{x+t} \\cdot P \\xi \\cdot p(t) \\\\ &= {}_{n-t}p_{x+t} \\cdot P \\xi \\cdot \\left(K^{*} e^{-r(n-t)} \\Phi [-d_{2}(t)] - S_{t} \\cdot \\Phi [-d_{1}(t)] \\right) \\\\ \\\\ d_{1}(t) &= \\frac{\\ln \\frac{S_{t}}{K^{*}} + (r + \\frac{1}{2} \\sigma^{2}) (n-t)}{\\sigma \\sqrt{n-t}} \\\\ &= d_{2}(t) + \\sigma \\sqrt{n-t} \\\\ \\\\ d_{2}(t) &= \\frac{\\ln \\frac{S_{t}}{K^{*}} + (r - \\frac{1}{2} \\sigma^{2}) (n-t)}{\\sigma \\sqrt{n-t}} \\\\ &= d_{1}(t) - \\sigma \\sqrt{n-t} \\end{aligned} \\] Warning The above is obtained going through the same process , just that the valuation date is time \\(t\\) instead of time \\(0\\) , which affects the following: Time to Maturity : Changed from \\(n\\) to \\(n-t\\) Initial Stock Price : Changed from \\(S_{0}\\) to \\(S_{t}\\) CDF : Updated to reflect the above changes as well Note that \\(\\xi\\) does NOT depend on the time to maturity, thus does not need to be changed. Guaranteed Death Benefit \u00b6 Unlike the Maturity Benefit which is paid out at a fixed time (end of the policy term), the death benefit is paid out at an unknown time ( \\(T\\) ), depending on when the policyholder dies. Thus, the insurer needs an option for every possible time that the policyholder could die. The expression is the same as before, just that the constant \\(n\\) is replaced with the variable \\(T\\) to reflect the unknown time of payout: Valuation Time, \\(t\\) Payout Time, \\(T\\) The value of the guarantee for each possible period \\(n\\) can be expressed as the following: \\[ \\begin{aligned} P(0, T) &= K^{*} \\cdot e^{-rT} \\Phi [-d_{2}(0, T)] - S_{0} \\cdot \\Phi [-d_{1}(0, T)] \\\\ \\\\ d_{1}(0, T) &= \\frac{\\ln \\frac{S_{0}}{K^{*}} + (r + \\frac{1}{2} \\sigma^{2}) T}{\\sigma \\sqrt{T}} \\\\ &= d_{2}(0, T) + \\sigma \\sqrt{T} \\\\ \\\\ d_{2}(0, T) &= \\frac{\\ln \\frac{S_{0}}{K^{*}} + (r - \\frac{1}{2} \\sigma^{2}) T}{\\sigma \\sqrt{T}} \\\\ &= d_{1}(0, T) - \\sigma \\sqrt{T} \\end{aligned} \\] The value of the entire embedded option must reflect the expected number of options needed to hedge the risk in EACH period for ALL periods : \\[ \\pi(0) = \\sum^{n}_{1} P(0, T) \\cdot {}_{T-1}p_{x} \\cdot q_{x+T-1} \\] Note Similar to before, the above expression can be adjusted to determine the value of the guarantee at a specified period for reserving purposes. It can also be adjusted for benefits that are paid monthly as well. Note There are a few different notations that can be used: \\(P\\) : Price of a single put option \\(V\\) : Value of the number of put options needed to fully hedge the benefits \\(\\pi\\) : Value of the expected number of put options needed to fully hedge the benefits Funding \u00b6 The cost of the guarantee is passed down to the policyholder in one of two ways: Front-End Load : Deducted once on inception (% of single premium) Risk Premium : Deducted routinely from the fund as part of the management charges Consider the following parameters: \\(m\\) : Management Charge rate as a % of fund value \\(m^*\\) : Management Charge rate as a % of fund value (Excluding the Risk Premium) \\(c\\) : Risk Premium rate as a % of fund value \\[ m = m^* + c \\] Assuming a single premium policy, recall that the fund value at any point can be expressed as: \\[ \\text{FV}_{t} = P \\cdot (1-m)^{t} \\] The risk premium will only be charged if the policy is still in-force: \\[ \\text{RP}_{t} = \\text{FV}_{t} \\cdot c \\cdot {}_{t}p_{x} \\] Assuming that they are charged at the start of the period, it can be expressed as an annuity: \\[ \\begin{aligned} \\text{EPV Risk Premium} &= \\sum \\text{RP}_{t} \\\\ &= \\sum \\text{FV}_{t} \\cdot c \\cdot {}_{t}p_{x} \\\\ &= \\sum P \\cdot (1-m)^{t} \\cdot c \\cdot {}_{t}p_{x} \\\\ &= cP \\cdot \\sum (1-m)^{t} {}_{t}p_{x} \\\\ &= cP \\cdot \\sum v^{*} {}_{t}p_{x} \\\\ &= cP \\cdot \\ddot{a}_{x:\\enclose{actuarial}{n} \\mid i^{*}} \\\\ \\\\ v^{*} &= (1-m) \\\\ (1 + i^{*}) &= \\frac{1}{1-m} \\\\ i^{*} &= \\frac{m}{1-m} \\end{aligned} \\] Thus, the EPV of the risk premium should be equal to the cost of the options: \\[ \\begin{aligned} \\pi(0) &= \\text{EPV Risk Premium} \\\\ \\pi(0) &= cP \\cdot \\ddot{a}_{x:\\enclose{actuarial}{n} \\mid i^{*}} \\\\ c &= \\frac{\\pi(0)}{\\ddot{a}_{x:\\enclose{actuarial}{n} \\mid i^{*}}} \\end{aligned} \\] Hedging \u00b6 Given that the guaranteed benefits follow the payoff of a european put option, the insurer can perfectly hedge the risk by either: Purchasing a European Put Option with the specified parameters from a third party Creating a replicating portfolio of Stocks and Bonds Note Hedging GMMBs only require one put option while a GMDB requires multiple options, one expiring at each year. Note Traditional life insurance policies bring mortality risk to the insurer. This can be diversified by selling more policies, as each policyholder has a different mortality . Given a sufficiently large pool, the mortality of the entire pool becomes stable (Law of Large Numbers). However, equity linked insurance brings investment risk to the insurer. Every policyholder experiences the same investment risk from the market, thus it cannot be diversified via selling more policies . This is why there is a need for specific hedging procedures. External Hedging \u00b6 The first method involves purchasing the required options from an external third party . The cost of the option(s) are incurred as a pre-contract expense . With the option, the risk of the guarantee is effectively ceded to the option seller. Thus, the insurer should not incur any cost of guarantees . The main issue with this approach is the availability* of the option(s) with the specified parameters (Strike Price & Duration). Internal Hedging \u00b6 Alternatively, the insurer can create a replicating portfolio of stocks and zero-coupon bonds that will match the payoff of the required option(s). If the guarantee would be exercised, the insurer will liquidate the replicating portfolio to cover the cost. Each Put Option can be decomposed into a Stock and Zero-Coupon Bond: \\[ \\begin{aligned} p(0) &= K^{*} e^{-rn} \\Phi [- d_{2}(0)] - S_{0} \\cdot \\Phi [-d_{1}(0)] \\\\ &= \\text{Bond Value} + S_{0} \\cdot \\text{Number of Stocks} \\\\ \\\\ \\text{Bond Value} &= K^{*} e^{-rn} \\Phi [- d_{2}(0)] \\\\ \\text{Number of Stocks} &= - \\Phi [-d_{1}(0)] \\end{aligned} \\] For a put option, Stock Component is always negative : Short that amount of stocks Bond Component is always positive : Lend that amount of money Note The derivation can be found in the Option Greeks section of exam IFM. For the purposes of this exam, it is sufficient to memorize the formulas instead. The replicating portfolio is incurred at the start of the policy as is treated as a pre-contract expense . The replicating portfolio functionally acts as a reserve for the cost of the policy. Each period, part of the replicating portfolio will be liquidated (\u201creleased\u201d) to cover the expected guarantee costs for that period. The release occurs due to the portfolio needing to re-balanced every period: The portfolios value changes every period due to market movement It may become larger/smaller than what is required for future periods Thus, excesses are released while shortfalls are incurred as additional rebalancing costs \\[ \\begin{aligned} \\text{Heding Release}_{t+1} &= \\text{Initial Portfolio After Growth}_{t+1} \\\\ &- p_{x+t} \\cdot \\text{Expected Portfolio Required}_{t+1} \\\\ \\\\ \\text{Initial Portfolio After Growth}_{t+1} &= \\text{Stock Value}_{t} \\cdot (1+g) \\\\ &+ \\text{Bond Value}_{t} \\cdot (1+r) \\end{aligned} \\] Tip The expected required portfolio must reflect the following changes: Decreased time to expiration (Since moved one period later) Updated stock price (Since the stock has grown by one month) The rebalancing cost computation is similar to profit testing where the required portfolio is only needed for the policies that are STILL in-force at the end of the year. The replicating portfolio functionally acts as a reserve that is released over time. Comparison \u00b6 One key difference between the two approaches is their treatment in profit testing : External Hedge Internal Hedge Pre-contract cost to purchase option(s) Pre-contract cost to establish portfolio No other hedging costs incurred Rebalancing Costs Incurred Each Period No projectected cost of guarantee Cost of guarantee still borne by insurer One other method to mitigate the investment risk is to hold a reserve . IF the reserve is sufficiently large, it can withstand the shock of a poor investment and thus mitigate the risk completely. This method is usually a much cheaper compared to purchasing options, at the cost of the risk not being completely mitigated. Tip More generally , there are many questions in the exam which asks for the Pros and Cons of DIY vs outsourcing via hedging or buying an annuity etc. The following logic can be applied to a wide range of situations: Outsourcing eliminates the risk of negative experience, but also eliminates potential upside from positive experience Outsourcing is more expensive due to profit margins from the other party while DIY might be cheaper as they only involve the raw cost Outspurcing could also be cheaper as they have expertise/economies of scale while DIY might be more expensive due to inefficies Outsourcing frees up resources to handle other important matters while DIY includes the hassle of setting the system up and managing it Variable Annuities \u00b6 In the US, rather than having Equity Linked Insurance, they offer Variable Annuities instead. They are a form of Deferred Annuity where the premiums paid are invested into a mutual fund of their choice. At the end of the policy term, they will receive Receive the fund value in cash ( Maturity Benefit ) Convert the amount into a life annuity ( Annuitization ) The policyholder CANNOT withdraw any amount from fund during the deferral period Warning Despite having an Annuity in its name, it is actually a life insurance policy before it becomes annuitized. A death benefit is payable if the insured dies during the policy term. Once annuitized, the death benefit is no longer payable. The policy term above refers to the term of the variable annuity . The annuity will have a seperately specified term. Similar to Equity Linked insurance, variable annuities also offer guaranteed maturity and death benefits. However, they also offer additional more complex guarantee features than those described previously: Reset Option Lookback Option Rollover Guarantee Additionally, since they have an annuity component, there are additional annuity related guarantees : Guaranteed Income Benefits Guaranteed Withdrawal Benefits Reset Option \u00b6 Reset Options allow the policy to increase the guarantee amount to match the fund value at the time of exercising the option. It will only be exercised if the fund value has increased beyond the guaranteed amount: \\[ \\begin{aligned} \\text{Original Guarantee} &= \\lambda \\cdot P \\\\ \\text{Reset Guarantee} &= \\lambda \\cdot \\text{FV at time of exercise} \\end{aligned} \\] Note The same scaling factor is applied for the new guarantee. Functionally, it is changing the base of the guarantee . Policyholders have the flexibility to choose when to exercise the option, allowing them to time the market for a good amount. However, it comes with a \"cost\" of a minimum policy term post exercising : Less than \\(k\\) years left : Remaining term increased to \\(k\\) years More than \\(k\\) year left : No change to remaining term Info The minimum term might come attached with a Surrender Charge condition if any amount is withdrawn then. This is to ensure that the insurer can recoup the costs associated with increasing the gurantee. Apart from being competitive, there is another reason why such benefits are offered. If the fund value is significantly higher than the guaranteed amount, the policyholder might surrender the policy and reinvest in a new policy with a higher guarantee - incurring costs related to cancelling and administering a new policy. Offering the option achieves the same outcome with cost savings . However, such a complex guarantee does come with significant risks as well: If the entire market goes up, a significant number will exercise their reset option at once, resulting in a huge increase in liability for the insurer Similarly, they would mature at similar times (due to the minimum policy term), thus resulting in large concentration risks Lookback Option \u00b6 Lookback Options will increase the gurantee amount to match the fund value at specific intervals . Functionally, it is a reset option that will be automatically exercised (if in the money) every few years, but with no minimum policy term after. \\[ \\text{Lookback Guarantee} = \\max (\\text{FV}_{x}, \\text{FV}_{2x}, \\text{FV}_{3x}, \\dots) \\] While it provides more opportunities to reset the guarantee, it loses the flexibility to choose when it occurs. If the fund value increases during the intervals but decreases before the specified time, the peak will not be captured by the option. Info This option is sometimes referred to as a Step Up Guarantee , as the guarantee has the potentially to be stepped up at regular intervals (assuming the fund value continuously increases). Rollover Guarantee \u00b6 Rollover guarantees provide the following benefits at specified intervals: If the fund value is higher than the guarantee, the guarantee is increased to match the fund value If the fund value is lower than the guarantee, the fund value is increased to match the guarantee Info It is a essentially a lookback option with downside protection . It is also sometimes referred to as a Guaranteed Accumulation Benefit . One key consideration is that if the policy has a rollover guarantee, it must be projected seperately . This is because the capital injections at the specified intervals will modify the fund value, affecting subsequent growth and charges . Warning The capital injection by the insurer into the fund value is NOT considered part of the guaranteed benefits payout . It should be thought of as a cost to the insurer. Thus, if asked to determine the payout of the option, do NOT include any capital injections . Tip Generally speaking, this section is a relatively small portion of the entire exam. The key is to remember that all the features of the Variable Annuity covered essentially modify the existing guarantees in some way . Guaranteed Income Benefit \u00b6 A Guaranteed Income Benefit sets a minimum payout from the resulting annuity: \\[ \\text{Payout} = \\max (\\text{Payout}_\\text{Market}, \\text{Payout}_\\text{GIB}) \\] The annuitization process can be broken down into two components: Benefit Base : Amount to be annuitized Annuitization Rate : Annuity EPV Factor \\[ \\begin{aligned} \\text{fund value} &= \\text{Payout} \\cdot \\text{Annuity Factor} \\\\ \\text{Payout} &= \\frac{\\text{fund value}}{\\text{Annuity Factor}} \\\\ &= \\underbrace{\\text{fund value}}_{\\text{Benefit Base}} \\cdot \\underbrace{\\frac{1}{\\text{Annuity Factor}}}_{\\text{Annuitization Rate}} \\end{aligned} \\] The guarantee can specify a change to either or both of the components , which affects the minimum payout: Benefit Base : Accumulate at different rate from fund value (notional value; no charges) Annuitization Rate : Determined using different assumptions Warning Most questions provide a scenario where one of the components is better while the other is worse than their market counterparts. There is insufficient information from this to determine if the guarantee is better or not; the combined effect needs to be considered. Note Other guarantees (EG. Step Up) can also apply to the Benefit Base - which may be different from the guarantees on the actual fund value. The cost of this guarantee is the cost of actually fulfilling the payout - based on the insurer's true expectation of the future - which is based on the MARKET annuity factor : \\[ \\text{Cost of Guarantee} = \\text{Guaranteed Payout} \\cdot \\text{Market Annuity Factor} \\] Guaranteed Withdrawal Benefit \u00b6 Under this arrangement, the mechanism of the product changes entirely. Rather than receive the entire fund value upon maturity or annuitizing it, the policy will become an investment account that the policyholder can directly withdraw from after a specified waiting period . The Guaranteed Withdrawal Benefit specifies a minimum amount that can be withdrawn from the fund, even if the fund value has been depleted . The benefits are payable for life or for a fixed term, depending on the nature of the policy. While the fund has value , the policyholder can withdraw any addittional amount on top of the minimum, subject to the amount remaining the in the fund While the fund has no value , the policyholder can only withdraw the guaranteed amount Any shortfall in the fund value is funded by the insurer , which is the cost of the guarantee: \\[ \\text{Cost of Guarantee} = \\sum \\text{PV (Shortfall Funding)} \\] Note The key difference between GWB and GIB is that there is NO annuitization . Thus, if there are any excess funds in the account after the death of the policyholder, they will be paid out to the estate of the policyholder.","title":"Embedded Options"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#embedded-options","text":"","title":"Embedded Options"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#overview","text":"Due to developments in the financial market, insurance products had to evolve to remain relevant. This led to development of Equity Linked Insurance , where policy benefits were linked to the performance of an underlying mutual fund . In order to attract investors, these products usually comes with guarantees on their benefits, providing unlimited upside and limited downsides . However, this results in insurance companies bearing huge financial risks . Info These guarantees may not be part of the product features, but offered as a rider for additional cost. Traditional actuarial methods were not well suited to effectively deal with these risks, which led to the insolvency of many insurance companies in the 1990s when interest rates fell. This led to the creation of Market Consistent valuation basis to ensure that these risks are appropriately accounted for. As its name suggests, market consistent valuation involves valuing an asset at its observed market value . However, insurance contracts are almost never traded, thus does not have an observable market value. Instead, the insurance cashflows are translated into tradeable assets , where the value of the contract is equivalent to the price of the assets. The guarantees offered Equity Linked insurance can be translated into an Option , which is why it can be valued as an Embedded Option .","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#equity-linked-insurance","text":"The defining feature of Equity Linked insurance is that the policyholder's premiums are invested into units of one or more mutual funds of the policyholder's choice. The policy benefits are directly linked to the total value of the units in the policyholder's account. Warning It is a common mistake to confuse universal life (UL) and Equity Linked (EL) policies as both involve an account for the policyholder: Universal Life : Account is purely notional; known as Account Value Equity Linked : Account is actually invested in funds of choice; known as Fund Value","title":"Equity Linked Insurance"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#unit-vs-non-unit-account","text":"The key to understanding Equity Linked insurance is to understand the difference between: Unit Account : Owned by Policyholders; where premiums are invested Non-Unit Account : Owned by Company (Shareholders); where benefits and expenses are paid out from Every period, charges are posted to the unit account to cover the cost of the policy (Mortality & Expense charges, similar to UL). These charges then flow into the non-unit account as fee income , which is then used to cover the actual cost of the policy. Tip There are two distinct cashflows: Charges : Posted to Unit Account, flows into Non Unit Account as Income Cost : Posted to Non Unit Account, actual cost of the policy The amount charged to the policyholder and the actual are often different - where the charges are higher than the cost, allowing the insurer to earn the spread . For instance, the fund management charge might be $100 - but the insurer only incurs $70 of investment expenses (due to economies of scale), allowing them to earn the difference. Another intuitive way to think about it is that for all other types of policies , there exists only a non-unit account . The premiums paid might be more/less than the cost of the policy, the excess/shortfall is managed by the insurer's investments . The key difference is that the premiums are invested by the policyholder ; only what is needed is deducted each period. The investment risk is borne by the policyholder instead.","title":"Unit VS Non-Unit Account"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#premium-allocation","text":"Only the Allocated Premium is used to purchase units ; the remaining un allocated premium flows directly into the unit account. The proportion of premium that is allocated is controlled by the Allocation Rate : \\[ \\text{Allocated Premium} = \\text{Premium} \\cdot \\text{Allocation Rate} \\] Note It is possible for the allocation rate to be above 100%. The insurer will fund the additional amount as an outflow. When purchasing units, part of the value will be lost due to Transactional Costs such as the Bid Offer Spread : Bid Price : How much the fund manager is bidding for the unit (Sell price) Offer Price : How much the fund manager is asking for the unit (Buy price) The units are purchased based on the Offer Price but values at the Bid Price. However, Offer prices are always higher than bid prices, this leads to an immediate loss in value compared to the premium paid: \\[ \\text{Unit Value Purchased} = \\text{Allocated Premium} \\cdot (1 - \\text{Bid Offer Spread}) \\] Note Some questions might not use the term BO spread but rather a Premium Charge . In either case, the key concept from this section is to understand that not the entire premium is invested in the fund.","title":"Premium Allocation"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#fund-value","text":"For the purposes of this exam, the fund value in each period can be calculated as: \\[ \\text{FV} = (\\text{Allocated Premium} - \\text{Management Charge}) \\cdot (1 + \\text{Fund Growth}) \\] For the purposes of this exam, the methods and timing of the above cashflows will be specified by the question . The key component is the management charge timing: BOP : Based on fund value BEFORE or AFTER premium? EOP : Based on fund value BEFORE or AFTER growth? The above can be calculated in Excel and presented as the following:","title":"Fund Value"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#policy-benefits","text":"As mentioned previously, the benefits of the policy are tied to the value of the unit account: \\[ \\text{Death Benefit} = \\text{Surrender Benefit} = \\text{Fund Value} \\] However, due to the volatile nature of investing, some policyholders could end up with low fund values and hence low coverage. Thus, most policies offer a guaranteed minimum benefit that will kick in if the fund value is below a certain threshold . The most common variation is to offer a guaranteed minimum benefit based on some multiple \\(\\lambda\\) of the premiums paid to date : \\[ \\text{Minimum Benefit} = \\lambda \\cdot \\text{Premium Paid to Date} \\] The Guaranteed Benefit (GB) will be paid if the Fund Value (FV) is smaller than the guarantee. Thus, the benefit of the policy can be expressed as the following: \\[ \\begin{aligned} \\text{Policy Benefit} &= \\begin{cases} \\text{FV}, & \\text{FV} > \\text{GB} \\\\ \\text{GB}, & \\text{FV} <= \\text{GB} \\end{cases} \\\\ &= \\max (\\text{FV}, \\text{GB}, 0) \\end{aligned} \\] Thus, the insurer does not pay the full benefit amount - they pay the shortfall of the fund value compared to the benefit.","title":"Policy Benefits"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#profit-testing","text":"Profit testing is very different for Equity Linked Insurance compared to other products. There are several key differences: Only NON-UNIT account cashflows should be considered Reserves are typically not held for Equity Linked Insurance Warning There are a few points to take note of: The insurer receives only the unallocated portion of the premium The insurer receives the management charges as fee income The insurer only pays the shortfall in the benefit amounts The cost to the insurer INCLUDES any guarantees Note that even though both UL and EL policies pay the shortfall in the benefit amount, they are treated differently during profit testing due to the account set up. Warning Even though only the unallocated premiums flow into the profit calculation, the profit margin is calculated based on the Paid Premium . As mentioned previously, the expected benefit above is the expected payment from the insurer (the shortfall): \\[ \\begin{aligned} \\text{EDB} &= \\max (\\text{DB} - \\text{FV}, 0) \\cdot q_{x} \\\\ \\text{ESB} &= (\\text{SB} - \\text{FV}, 0) \\cdot w_{x} \\\\ \\text{EMB} &= \\max (\\text{SB} - \\text{FV}, 0) \\cdot p_{x} \\end{aligned} \\] Tip Notice that for surrender benefit, the shortfall is NOT floored at 0. This is because the insurer may impose surrender charges which will show up as a \"negative cost\" (the doube negative will cause it to become cash inflow ). Note Another quantity of interest is the Cost of Guarantee . It is similar to the cost of benefit, where it measures the contribution of the guarantee to the benefit: \\[ \\text{COG}_{t} = \\max (\\text{GB}_{t} - \\text{DB}_{t}, 0) \\cdot q_{x+t} \\] For most policies where the death benefit is simply the account value, the cost of guarantee and cost of benefit are the same . However, if there is a multiplier on top of the benefit (EG. 105% times the account value), then the two would deviate: \\[ \\begin{aligned} \\text{COG}_{t} &= \\max (\\text{GB}_{t} - 1.1 \\cdot \\text{FV}_{t}, 0) \\cdot q_{x+t-1} \\\\ \\text{COB}_{t} &= \\max (\\max (\\text{GB}_{t}, 1.1 \\cdot \\text{FV}_{t}) - \\text{FV}, 0) \\cdot q_{x+t-1} \\end{aligned} \\] The COG is typically calculated for all periods on issue: \\[ \\begin{aligned} \\text{COG}_{0} &= \\sum \\max (\\text{GB}_{t} - \\text{DB}_{t}, 0) \\cdot {}_{k \\mid}q_{x} \\\\ \\text{COG}_{0} &= \\sum {}_{k-1}p_{x} \\cdot \\text{COG}_{t} \\end{aligned} \\]","title":"Profit Testing"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#comparison-with-ul","text":"There are several key features that make UL or EL more competitive than the other: Premiums : UL \"invests\" lesser premiums into the account due to the COI Death Benefit : UL DB's tend to be higher due to the Corridoor & ADBs Downside Risk : UL's returns have a minimum guarantee; EL can be negative Upside Risk : UL's returns are only a portion of the actual return; EL will get the full return","title":"Comparison with UL"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#embedded-options_1","text":"With some manipulation, the above expression for policy benefit can be expressed in the form a European Put Option Payoff : Guaranteed Benefit is a fixed value , similar to the Strike Price fund value follows a stock price process (with charges applied) \\[ \\begin{aligned} \\text{Put Option Payoff}_{t} &= \\max (K - S_{t}, 0) \\\\ \\\\ \\text{Policy Benefit}_{t} &= \\max (\\text{FV}_{t}, \\text{GB}) \\\\ &= \\max (\\text{GB} - \\text{FV}_{t}, \\text{FV}_{t} - \\text{FV}_{t}) \\\\ &= \\max (\\text{GB} - \\text{FV}_{t}, 0) \\end{aligned} \\] Thus, the same framework used to evaluate european put options - Black Scholes Merton model - can be used to evaluate these guaranteed benefits as well. However, there is one key difference between the two that must be accounted for: Regular Option : Time to expiration (when the option is exercised) is fixed Embedded Option : Time is contingent on the survival/death of the policyholder Thus, the framework must be adjusted slightly to account for the probabilities of death/survival within the given timeframe. Note Please refer to the Derivatives Section of the IFM notes for more information on European Options and the Black Scholes model. Unless otherwise stated, all policies referenced in this section are assumed to be Single Premium policies with no additional top ups .","title":"Embedded Options"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#guaranteed-maturity-benefit","text":"The maturity benefit is paid at a known time (at the end of the policy term ), given that the policyholder is alive at that time . For simplicity, assume that the policy has a Guaranteed Minimum Maturity Benefit (GMMB) of some multiple of premiums paid : \\[ \\text{GMMB} = \\lambda P \\] Note The expression is simplified in this case since we assume a single premium policy. The fund value projection earlier assumes that the fund grows at a specified rate . An alternative method is to assume that it grows at the same rate as an underlying stock process . Thus, the fund value at maturity of time \\(n\\) can be expressed as: \\[ \\begin{aligned} \\text{FV}_{n} &= \\text{Opening Value} \\cdot \\text{Charges} \\cdot \\text{Growth} \\\\ &= P \\cdot (1 - \\text{IC}) \\cdot (1 - \\text{RC})^{n-1} \\cdot \\frac{S_{n}}{S_{0}} \\\\ &= P \\cdot \\xi \\cdot \\frac{S_{n}}{S_{0}} \\\\ \\\\ \\xi &= (1 - \\text{IC}) \\cdot (1 - \\text{RC})^{n-1} \\end{aligned} \\] Note The charges in this scenario have have been simplified to Initial Charges (IC) and Renewal Charges (RC). Both are charged as a percentage of the fund value at the time. They are then further combined into \\(\\xi\\) , since they have a fixed value (dependent only on policy term, which is fixed for each policy). Warning Some questions might provide the monthly rates instead of annual rates - do not blindly apply the formulas. The payoff of the option on maturity is denoted as \\(h(n)\\) and can be expressed as the following: \\[ \\begin{aligned} h(n) &= \\max (\\text{GB} - \\text{FV}_{t}, 0) \\\\ &= \\max \\left(\\lambda P - P \\cdot \\xi \\cdot \\frac{S_{n}}{S_{0}}, 0 \\right) \\\\ &= P \\xi \\cdot \\max \\left(\\frac{\\lambda}{\\xi} - \\frac{S_{n}}{S_{0}}, 0 \\right) \\\\ &= P \\xi \\cdot \\max \\left(\\frac{\\lambda}{\\xi} - S_{n}, 0 \\right) \\\\ &= P \\xi \\cdot \\max \\left(K^{*} - S_{n}, 0 \\right) \\\\ \\end{aligned} \\] Tip The above can be intepreted as requiring \\(P \\xi\\) european put options with strike price \\(\\frac{\\lambda}{\\xi}\\) . Under the Black Scholes Merton framework, the value of ONE of the above european put options on inception is the risk neutral expected present value of the payoff: \\[ \\begin{aligned} p(0) &= e^{-rn} \\cdot E^{Q}_{t} \\left[\\max (K^{*} - S_{n}, 0) \\right] \\\\ &= e^{-rn} \\cdot \\left(K^{*} \\cdot \\Phi [- d_{2}(0)] - S_{n} \\cdot \\Phi [-d_{1}(0)] \\right) \\\\ &= K^{*} e^{-rn} \\Phi [- d_{2}(0)] - S_{0} \\cdot \\Phi [-d_{1}(0)] \\\\ \\\\ d_{1}(0) &= \\frac{\\ln \\frac{S_{0}}{K} + (r + \\frac{1}{2} \\sigma^{2}) n}{\\sigma \\sqrt{n}} \\\\ &= d_{2}(0) + \\sigma \\sqrt{n} \\\\ \\\\ d_{2}(0) &= \\frac{\\ln \\frac{S_{0}}{K} + (r - \\frac{1}{2} \\sigma^{2}) n}{\\sigma \\sqrt{n}} \\\\ &= d_{1}(0) - \\sigma \\sqrt{n} \\end{aligned} \\] Tip \\(\\Phi(x)\\) represents the CDF of the standard normal distribution at \\(x\\) . It can be determined via the NORM.S.DIST Excel Function . Note In practice, investors take into account risk when looking for looking for returns (which affects asset prices). For instance, if the risk-free rate is \\(r\\) , then investors demand a risk premium for that them to take on an investment with risk . Under a risk neutral basis, it assumes that investors do NOT take into account risk. Thus, assets will be priced based on the risk free rate , rather than on the actual return (with a risk premium). \\[ \\begin{aligned} S_{n} &= S_{0} \\cdot e^{\\alpha n} = S_{0} \\cdot e^{rn} \\\\ \\therefore e^{-rn} \\cdot S_{n} &= S_{0} \\end{aligned} \\] The value of the entire embedded option is the expected number of european put options needed on maturity. Thus, the probability of maturing must be accounted for: \\[ \\begin{aligned} \\pi(0) &= {}_{n}p_{x} \\cdot P \\xi \\cdot p(0) \\\\ &= {}_{n}p_{x} \\cdot P \\xi \\cdot \\left(K^{*} e^{-rn} \\Phi [- d_{2}(0)] - S_{0} \\cdot \\Phi [-d_{1}(0)] \\right) \\\\ \\end{aligned} \\] Note Since \\(T_{x}\\) and \\(S_{x}\\) are independent processes, the risk neutral expectation (\"Q\") of future lifetime is equivalent to the true (\"P\") probabilities. \\[ E^{Q}(T_{x} \\gt n) = E^{P}(T_{x} \\gt n) = {}_{10}p_{x} \\] The above can be adjusted to reflect valuation at any time \\(t\\) : \\[ \\begin{aligned} \\pi(t) &= {}_{n-t}p_{x+t} \\cdot P \\xi \\cdot p(t) \\\\ &= {}_{n-t}p_{x+t} \\cdot P \\xi \\cdot \\left(K^{*} e^{-r(n-t)} \\Phi [-d_{2}(t)] - S_{t} \\cdot \\Phi [-d_{1}(t)] \\right) \\\\ \\\\ d_{1}(t) &= \\frac{\\ln \\frac{S_{t}}{K^{*}} + (r + \\frac{1}{2} \\sigma^{2}) (n-t)}{\\sigma \\sqrt{n-t}} \\\\ &= d_{2}(t) + \\sigma \\sqrt{n-t} \\\\ \\\\ d_{2}(t) &= \\frac{\\ln \\frac{S_{t}}{K^{*}} + (r - \\frac{1}{2} \\sigma^{2}) (n-t)}{\\sigma \\sqrt{n-t}} \\\\ &= d_{1}(t) - \\sigma \\sqrt{n-t} \\end{aligned} \\] Warning The above is obtained going through the same process , just that the valuation date is time \\(t\\) instead of time \\(0\\) , which affects the following: Time to Maturity : Changed from \\(n\\) to \\(n-t\\) Initial Stock Price : Changed from \\(S_{0}\\) to \\(S_{t}\\) CDF : Updated to reflect the above changes as well Note that \\(\\xi\\) does NOT depend on the time to maturity, thus does not need to be changed.","title":"Guaranteed Maturity Benefit"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#guaranteed-death-benefit","text":"Unlike the Maturity Benefit which is paid out at a fixed time (end of the policy term), the death benefit is paid out at an unknown time ( \\(T\\) ), depending on when the policyholder dies. Thus, the insurer needs an option for every possible time that the policyholder could die. The expression is the same as before, just that the constant \\(n\\) is replaced with the variable \\(T\\) to reflect the unknown time of payout: Valuation Time, \\(t\\) Payout Time, \\(T\\) The value of the guarantee for each possible period \\(n\\) can be expressed as the following: \\[ \\begin{aligned} P(0, T) &= K^{*} \\cdot e^{-rT} \\Phi [-d_{2}(0, T)] - S_{0} \\cdot \\Phi [-d_{1}(0, T)] \\\\ \\\\ d_{1}(0, T) &= \\frac{\\ln \\frac{S_{0}}{K^{*}} + (r + \\frac{1}{2} \\sigma^{2}) T}{\\sigma \\sqrt{T}} \\\\ &= d_{2}(0, T) + \\sigma \\sqrt{T} \\\\ \\\\ d_{2}(0, T) &= \\frac{\\ln \\frac{S_{0}}{K^{*}} + (r - \\frac{1}{2} \\sigma^{2}) T}{\\sigma \\sqrt{T}} \\\\ &= d_{1}(0, T) - \\sigma \\sqrt{T} \\end{aligned} \\] The value of the entire embedded option must reflect the expected number of options needed to hedge the risk in EACH period for ALL periods : \\[ \\pi(0) = \\sum^{n}_{1} P(0, T) \\cdot {}_{T-1}p_{x} \\cdot q_{x+T-1} \\] Note Similar to before, the above expression can be adjusted to determine the value of the guarantee at a specified period for reserving purposes. It can also be adjusted for benefits that are paid monthly as well. Note There are a few different notations that can be used: \\(P\\) : Price of a single put option \\(V\\) : Value of the number of put options needed to fully hedge the benefits \\(\\pi\\) : Value of the expected number of put options needed to fully hedge the benefits","title":"Guaranteed Death Benefit"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#funding","text":"The cost of the guarantee is passed down to the policyholder in one of two ways: Front-End Load : Deducted once on inception (% of single premium) Risk Premium : Deducted routinely from the fund as part of the management charges Consider the following parameters: \\(m\\) : Management Charge rate as a % of fund value \\(m^*\\) : Management Charge rate as a % of fund value (Excluding the Risk Premium) \\(c\\) : Risk Premium rate as a % of fund value \\[ m = m^* + c \\] Assuming a single premium policy, recall that the fund value at any point can be expressed as: \\[ \\text{FV}_{t} = P \\cdot (1-m)^{t} \\] The risk premium will only be charged if the policy is still in-force: \\[ \\text{RP}_{t} = \\text{FV}_{t} \\cdot c \\cdot {}_{t}p_{x} \\] Assuming that they are charged at the start of the period, it can be expressed as an annuity: \\[ \\begin{aligned} \\text{EPV Risk Premium} &= \\sum \\text{RP}_{t} \\\\ &= \\sum \\text{FV}_{t} \\cdot c \\cdot {}_{t}p_{x} \\\\ &= \\sum P \\cdot (1-m)^{t} \\cdot c \\cdot {}_{t}p_{x} \\\\ &= cP \\cdot \\sum (1-m)^{t} {}_{t}p_{x} \\\\ &= cP \\cdot \\sum v^{*} {}_{t}p_{x} \\\\ &= cP \\cdot \\ddot{a}_{x:\\enclose{actuarial}{n} \\mid i^{*}} \\\\ \\\\ v^{*} &= (1-m) \\\\ (1 + i^{*}) &= \\frac{1}{1-m} \\\\ i^{*} &= \\frac{m}{1-m} \\end{aligned} \\] Thus, the EPV of the risk premium should be equal to the cost of the options: \\[ \\begin{aligned} \\pi(0) &= \\text{EPV Risk Premium} \\\\ \\pi(0) &= cP \\cdot \\ddot{a}_{x:\\enclose{actuarial}{n} \\mid i^{*}} \\\\ c &= \\frac{\\pi(0)}{\\ddot{a}_{x:\\enclose{actuarial}{n} \\mid i^{*}}} \\end{aligned} \\]","title":"Funding"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#hedging","text":"Given that the guaranteed benefits follow the payoff of a european put option, the insurer can perfectly hedge the risk by either: Purchasing a European Put Option with the specified parameters from a third party Creating a replicating portfolio of Stocks and Bonds Note Hedging GMMBs only require one put option while a GMDB requires multiple options, one expiring at each year. Note Traditional life insurance policies bring mortality risk to the insurer. This can be diversified by selling more policies, as each policyholder has a different mortality . Given a sufficiently large pool, the mortality of the entire pool becomes stable (Law of Large Numbers). However, equity linked insurance brings investment risk to the insurer. Every policyholder experiences the same investment risk from the market, thus it cannot be diversified via selling more policies . This is why there is a need for specific hedging procedures.","title":"Hedging"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#external-hedging","text":"The first method involves purchasing the required options from an external third party . The cost of the option(s) are incurred as a pre-contract expense . With the option, the risk of the guarantee is effectively ceded to the option seller. Thus, the insurer should not incur any cost of guarantees . The main issue with this approach is the availability* of the option(s) with the specified parameters (Strike Price & Duration).","title":"External Hedging"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#internal-hedging","text":"Alternatively, the insurer can create a replicating portfolio of stocks and zero-coupon bonds that will match the payoff of the required option(s). If the guarantee would be exercised, the insurer will liquidate the replicating portfolio to cover the cost. Each Put Option can be decomposed into a Stock and Zero-Coupon Bond: \\[ \\begin{aligned} p(0) &= K^{*} e^{-rn} \\Phi [- d_{2}(0)] - S_{0} \\cdot \\Phi [-d_{1}(0)] \\\\ &= \\text{Bond Value} + S_{0} \\cdot \\text{Number of Stocks} \\\\ \\\\ \\text{Bond Value} &= K^{*} e^{-rn} \\Phi [- d_{2}(0)] \\\\ \\text{Number of Stocks} &= - \\Phi [-d_{1}(0)] \\end{aligned} \\] For a put option, Stock Component is always negative : Short that amount of stocks Bond Component is always positive : Lend that amount of money Note The derivation can be found in the Option Greeks section of exam IFM. For the purposes of this exam, it is sufficient to memorize the formulas instead. The replicating portfolio is incurred at the start of the policy as is treated as a pre-contract expense . The replicating portfolio functionally acts as a reserve for the cost of the policy. Each period, part of the replicating portfolio will be liquidated (\u201creleased\u201d) to cover the expected guarantee costs for that period. The release occurs due to the portfolio needing to re-balanced every period: The portfolios value changes every period due to market movement It may become larger/smaller than what is required for future periods Thus, excesses are released while shortfalls are incurred as additional rebalancing costs \\[ \\begin{aligned} \\text{Heding Release}_{t+1} &= \\text{Initial Portfolio After Growth}_{t+1} \\\\ &- p_{x+t} \\cdot \\text{Expected Portfolio Required}_{t+1} \\\\ \\\\ \\text{Initial Portfolio After Growth}_{t+1} &= \\text{Stock Value}_{t} \\cdot (1+g) \\\\ &+ \\text{Bond Value}_{t} \\cdot (1+r) \\end{aligned} \\] Tip The expected required portfolio must reflect the following changes: Decreased time to expiration (Since moved one period later) Updated stock price (Since the stock has grown by one month) The rebalancing cost computation is similar to profit testing where the required portfolio is only needed for the policies that are STILL in-force at the end of the year. The replicating portfolio functionally acts as a reserve that is released over time.","title":"Internal Hedging"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#comparison","text":"One key difference between the two approaches is their treatment in profit testing : External Hedge Internal Hedge Pre-contract cost to purchase option(s) Pre-contract cost to establish portfolio No other hedging costs incurred Rebalancing Costs Incurred Each Period No projectected cost of guarantee Cost of guarantee still borne by insurer One other method to mitigate the investment risk is to hold a reserve . IF the reserve is sufficiently large, it can withstand the shock of a poor investment and thus mitigate the risk completely. This method is usually a much cheaper compared to purchasing options, at the cost of the risk not being completely mitigated. Tip More generally , there are many questions in the exam which asks for the Pros and Cons of DIY vs outsourcing via hedging or buying an annuity etc. The following logic can be applied to a wide range of situations: Outsourcing eliminates the risk of negative experience, but also eliminates potential upside from positive experience Outsourcing is more expensive due to profit margins from the other party while DIY might be cheaper as they only involve the raw cost Outspurcing could also be cheaper as they have expertise/economies of scale while DIY might be more expensive due to inefficies Outsourcing frees up resources to handle other important matters while DIY includes the hassle of setting the system up and managing it","title":"Comparison"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#variable-annuities","text":"In the US, rather than having Equity Linked Insurance, they offer Variable Annuities instead. They are a form of Deferred Annuity where the premiums paid are invested into a mutual fund of their choice. At the end of the policy term, they will receive Receive the fund value in cash ( Maturity Benefit ) Convert the amount into a life annuity ( Annuitization ) The policyholder CANNOT withdraw any amount from fund during the deferral period Warning Despite having an Annuity in its name, it is actually a life insurance policy before it becomes annuitized. A death benefit is payable if the insured dies during the policy term. Once annuitized, the death benefit is no longer payable. The policy term above refers to the term of the variable annuity . The annuity will have a seperately specified term. Similar to Equity Linked insurance, variable annuities also offer guaranteed maturity and death benefits. However, they also offer additional more complex guarantee features than those described previously: Reset Option Lookback Option Rollover Guarantee Additionally, since they have an annuity component, there are additional annuity related guarantees : Guaranteed Income Benefits Guaranteed Withdrawal Benefits","title":"Variable Annuities"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#reset-option","text":"Reset Options allow the policy to increase the guarantee amount to match the fund value at the time of exercising the option. It will only be exercised if the fund value has increased beyond the guaranteed amount: \\[ \\begin{aligned} \\text{Original Guarantee} &= \\lambda \\cdot P \\\\ \\text{Reset Guarantee} &= \\lambda \\cdot \\text{FV at time of exercise} \\end{aligned} \\] Note The same scaling factor is applied for the new guarantee. Functionally, it is changing the base of the guarantee . Policyholders have the flexibility to choose when to exercise the option, allowing them to time the market for a good amount. However, it comes with a \"cost\" of a minimum policy term post exercising : Less than \\(k\\) years left : Remaining term increased to \\(k\\) years More than \\(k\\) year left : No change to remaining term Info The minimum term might come attached with a Surrender Charge condition if any amount is withdrawn then. This is to ensure that the insurer can recoup the costs associated with increasing the gurantee. Apart from being competitive, there is another reason why such benefits are offered. If the fund value is significantly higher than the guaranteed amount, the policyholder might surrender the policy and reinvest in a new policy with a higher guarantee - incurring costs related to cancelling and administering a new policy. Offering the option achieves the same outcome with cost savings . However, such a complex guarantee does come with significant risks as well: If the entire market goes up, a significant number will exercise their reset option at once, resulting in a huge increase in liability for the insurer Similarly, they would mature at similar times (due to the minimum policy term), thus resulting in large concentration risks","title":"Reset Option"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#lookback-option","text":"Lookback Options will increase the gurantee amount to match the fund value at specific intervals . Functionally, it is a reset option that will be automatically exercised (if in the money) every few years, but with no minimum policy term after. \\[ \\text{Lookback Guarantee} = \\max (\\text{FV}_{x}, \\text{FV}_{2x}, \\text{FV}_{3x}, \\dots) \\] While it provides more opportunities to reset the guarantee, it loses the flexibility to choose when it occurs. If the fund value increases during the intervals but decreases before the specified time, the peak will not be captured by the option. Info This option is sometimes referred to as a Step Up Guarantee , as the guarantee has the potentially to be stepped up at regular intervals (assuming the fund value continuously increases).","title":"Lookback Option"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#rollover-guarantee","text":"Rollover guarantees provide the following benefits at specified intervals: If the fund value is higher than the guarantee, the guarantee is increased to match the fund value If the fund value is lower than the guarantee, the fund value is increased to match the guarantee Info It is a essentially a lookback option with downside protection . It is also sometimes referred to as a Guaranteed Accumulation Benefit . One key consideration is that if the policy has a rollover guarantee, it must be projected seperately . This is because the capital injections at the specified intervals will modify the fund value, affecting subsequent growth and charges . Warning The capital injection by the insurer into the fund value is NOT considered part of the guaranteed benefits payout . It should be thought of as a cost to the insurer. Thus, if asked to determine the payout of the option, do NOT include any capital injections . Tip Generally speaking, this section is a relatively small portion of the entire exam. The key is to remember that all the features of the Variable Annuity covered essentially modify the existing guarantees in some way .","title":"Rollover Guarantee"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#guaranteed-income-benefit","text":"A Guaranteed Income Benefit sets a minimum payout from the resulting annuity: \\[ \\text{Payout} = \\max (\\text{Payout}_\\text{Market}, \\text{Payout}_\\text{GIB}) \\] The annuitization process can be broken down into two components: Benefit Base : Amount to be annuitized Annuitization Rate : Annuity EPV Factor \\[ \\begin{aligned} \\text{fund value} &= \\text{Payout} \\cdot \\text{Annuity Factor} \\\\ \\text{Payout} &= \\frac{\\text{fund value}}{\\text{Annuity Factor}} \\\\ &= \\underbrace{\\text{fund value}}_{\\text{Benefit Base}} \\cdot \\underbrace{\\frac{1}{\\text{Annuity Factor}}}_{\\text{Annuitization Rate}} \\end{aligned} \\] The guarantee can specify a change to either or both of the components , which affects the minimum payout: Benefit Base : Accumulate at different rate from fund value (notional value; no charges) Annuitization Rate : Determined using different assumptions Warning Most questions provide a scenario where one of the components is better while the other is worse than their market counterparts. There is insufficient information from this to determine if the guarantee is better or not; the combined effect needs to be considered. Note Other guarantees (EG. Step Up) can also apply to the Benefit Base - which may be different from the guarantees on the actual fund value. The cost of this guarantee is the cost of actually fulfilling the payout - based on the insurer's true expectation of the future - which is based on the MARKET annuity factor : \\[ \\text{Cost of Guarantee} = \\text{Guaranteed Payout} \\cdot \\text{Market Annuity Factor} \\]","title":"Guaranteed Income Benefit"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/7.%20Embedded%20Options/#guaranteed-withdrawal-benefit","text":"Under this arrangement, the mechanism of the product changes entirely. Rather than receive the entire fund value upon maturity or annuitizing it, the policy will become an investment account that the policyholder can directly withdraw from after a specified waiting period . The Guaranteed Withdrawal Benefit specifies a minimum amount that can be withdrawn from the fund, even if the fund value has been depleted . The benefits are payable for life or for a fixed term, depending on the nature of the policy. While the fund has value , the policyholder can withdraw any addittional amount on top of the minimum, subject to the amount remaining the in the fund While the fund has no value , the policyholder can only withdraw the guaranteed amount Any shortfall in the fund value is funded by the insurer , which is the cost of the guarantee: \\[ \\text{Cost of Guarantee} = \\sum \\text{PV (Shortfall Funding)} \\] Note The key difference between GWB and GIB is that there is NO annuitization . Thus, if there are any excess funds in the account after the death of the policyholder, they will be paid out to the estate of the policyholder.","title":"Guaranteed Withdrawal Benefit"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/","text":"Pensions \u00b6 Overview \u00b6 Pensions are an arrangement where an employer makes regular contributions into a fund as long as the employee has yet to retire. Upon retirement, contributions will stop and the fund will be drawn down to make regular payments to the employee as a form of retirement benefit as long as they are alive. For the purposes of this exam, there are two types of pensions to consider: Defined Contribution Defined Benefit Contribution is fixed Contribution is variable Benefit is variable Benefit is fixed Investment risk borne by Employee Investment risk borne by Employer Variable Benefits to Employee, Guaranteed Cost to Employers Guaranteed Benefits to Employee, Variable Cost to Employers Preferred by Employers Preferred by Employees Note Under a DB pension, if the fund performs worse than expected , the employer will vary their contributions to ensure that the target payout can be supported. Thus, the downside (risk) of DB plans is borne by the employer . Conversely, under a DC pension, the employer has no obligation to adjust their contributions, thus the loss in benefit is borne by the employee via lower benefit payments. Pensions are within the scope of the exam as they are a form of contingent cashflows - contingent on the retirement of the employee. Pensions are offered typically for the following reaosns: Attract new employees Retain existing employees Improve employee morale Negotiation with Unions Salary \u00b6 Key Properties \u00b6 Salary is the most important component for any pension related problem as all plan types rely on salary as the main driver for the benefits. Thus, it is important to have a strong understanding of how it is treated for this exam: Salary is paid monthly and is assumed to be level throughout the year Salary is paid in arrears ; at the end of the period However, most questions are concerned with the Annual Salary , denoted as \\(S_{x}\\) \\(S_{x}\\) is the salary earned by the employee WHILE they were age \\(x\\) For the purposes of this exam, an employee is assumed to retire exactly on the day that they reach the retirement age ( their birthday ). If an employee retires at age \\(y+1\\) , then the last salary they salary they would receive would be while they were aged \\(y\\) : Tip For most questions, the individual is assumed to retire at age 65 (Beginning of age 65; end of age 64). Thus, the final annual salary that the individual receives is \\(S_{64}\\) . When in doubt, always draw a timeline to visualize the salaries paid. Warning Some questions may specify that the individuals retire in the middle of their ages (EG. Age 64.5). In this case, the final salary is comprised of two components: 6 months of salary from \\(S_{63}\\) 6 months of salary from \\(S_{64}\\) The above is important because the salary while 63 is usually different while 64. Salary Growth \u00b6 For the purposes of the exam, assume that the salary of the employee grows over time . Thus, rather than provide a salary schedule, they will provide the salary at a specified age and then require students to manually compute the salary at the required ages. Most questions will assume that the annual salary grows annually , typically on the employee's birthday: \\[ S_{x+t} = S_{x} \\cdot (1+g)^{t} \\] Note Salary is still assumed to be level throughout the year. Thus, the monthly salary also grows at the same rate: \\[ \\begin{aligned} S_{x+t} &= S_{x} \\cdot (1+g)^{t} \\\\ &= 12 \\cdot \\text{Monthly Salary}_{x} \\cdot (1+g)^{t} \\\\ &= 12 \\cdot \\text{Monthly Salary}_{x+t} \\end{aligned} \\] However, there are some questions that assume that the monthly salary grows monthly instead. In these cases, the salary is no longer level throughout the year. Thus, the salary in the individual months must be summed up to obtain the annual salary in the year. \\[ \\begin{aligned} S_{x+t} &= \\text{Monthly Salary}_{x+t-1} \\cdot [1 + (1+j)^{1} + (1+j)^{2} + (1+j)^{3} + \\dots] \\\\ &= \\text{Monthly Salary}_{x+t-1} \\cdot s_{\\enclose{actuarial}{12} j} \\end{aligned} \\] Warning The accumulation function is for the payments made in arrears (not the usual payment in advance) that is usually used for ALTAM: \\[ s_{\\enclose{actuarial}{12}} = \\frac{(1+j)^{12} - 1}{i} \\] Continuous Salary \u00b6 In some rare cases, questions might provide the Salary Rate instead. In these cases, the salary also grows at a continuous rate . The same principles from before apply - the annual salary is the sum of all the salaries received in the year. In a continuous scenario, the salaries are received every instant. To sum across all instances, integration must be used: \\[ S_{x+t} = \\int^{n}_{n-1} \\text{Initial Salary Rate} \\cdot e^{\\delta t} \\] Replacement Ratio \u00b6 The Replacement Ratio is the percentage of FINAL salary that is replaced by the pension benefit: \\[ \\text{Replacement Ratio} = \\frac{\\text{Yearly Retirement Benefit}}{\\text{Final Yearly Salary}} \\] Most pension plans have a target replacement ratio is 50-70%. The ratio does not need to be close to 100% as people typically require less money while in retirement . If an individual has multiple pension plans (from different companies or personal plans), then the replacement ratio is calculated based on the TOTAL retirement benefit the individual receives. Contribution Ratio \u00b6 Similar to replacement ratio, one common metric is also the Contribution Ratio - which measures how much the employer has contributed to their pension relative to the salary in that year: \\[ \\text{Contribution Ratio} = \\frac{\\text{Contribution}}{\\text{Salary}} \\] Note For a DC plan, the contribution ratio is fixed . For a DB plan, the contribution in each period is different and must be determined. Warning For DB contributions, the salary is different from the salary used to compute the benefits, especially if using TUC method : Valuation Salary: Previous Salary Contribution Salary: Current Salary Defined Contribution \u00b6 Defined Contribution plans are a type of pension where both the employer ( and sometimes employee ) contribute a fixed amount into a fund which accumulates over time. Upon retirement, the fund value is then used to purchase a Life Annuity which will pay the pension benefit to the employee. Thus, the primary goal of these DC questions are to solve for the yearly retirement benefit that the employee will receive on retirement: \\[ \\begin{aligned} \\text{AV of Contributions} &= \\text{EPV DC Benefits}_{65} \\\\ &= \\text{Yearly DC Benefit} \\cdot \\ddot{a}^{(12)}_{65} \\end{aligned} \\] Warning Retirement benefits are typically paid on a monthly basis , which is why the payable 12 times a year annuity function is used. However, it is important to remember that the face value of the annuity is assumed to be a yearly cashflow . To obtain the monthly benefit, the resulting benefit must be divided by 12 . The contributions into the fund are defined as a proportion \\((c)\\) of the employee's salary each period: \\[ \\text{DC Contribution}_{t} = c \\cdot S_{x+t} \\] Due to the rather straightforward nature of DC plans, most questions from this topic will focus on DB plans instead. Accumulated Value \u00b6 The contribution each period changes as the employee's salary grows. Thus, it is NOT possible to use the typical accumulated value function from exam FM ( \\(s_{x:\\enclose{actuarial}{n}}\\) ). The best approach would be to use first principles via the Sum of Geometric Series . It must account for two dimensions: Each contribution is different due to Salary Increases Each contribution earns a different interest due to the Time Value of Money \\[ \\begin{aligned} \\text{Sum of AV of Contributions} &= a \\cdot \\frac{r^{n} - 1}{r - 1} \\\\ \\\\ a &= S_{x} \\cdot (1+i)^n \\\\ r &= (1+g) \\cdot v \\end{aligned} \\] The key is knowing HOW MANY years to accumulate: Salary Growth : Starts from 0, ends at \\(n-1\\) Accumulation : Starts from \\(n-1\\) , end at \\(0\\) In both cases, the total is still \\(n\\) times Tip The same principles apply for a continuous salary, simply convert the discrete components into continuous ones: Summation into Integration Rates into forces \\[ \\begin{aligned} \\text{Sum of AV of Contributions} &= \\int^{n}_{0} S_{x} \\cdot e^{\\delta_{i} t} \\cdot e^{(\\delta_{g} - \\delta_{i}) t} \\end{aligned} \\] Different Frequencies \u00b6 The above above only works if all components are on the same frequency: Salary Contributions Salary Growth Interest Accumulation The main issue comes when Contributions are made monthly but Growth is made yearly . Thus, an additional step must be done to bridge the two together: Since salary growth is only made yearly, the contributions within a given year are constant . Thus, the accumulated value of the monthly contributions at the end of the year can be expressed using the accumulation function : \\[ \\text{Yearly Contribution} = S_{x} \\cdot c \\cdot s_{\\enclose{actuarial}{12}} \\] The above yearly contribution can then be plugged into the usual geometric formula that to determine the accumulated value as at the valuation date. Tip This is similar in principle to the monthly salary growth situation earlier. The key is to gross up the components with different frequencies to match the rest. Defined Benefit \u00b6 Defined Benefit plans are a type pension where the retirement benefits are fixed, defined as some function of an employee's final salary upon retirement. It is the employer's duty to set aside and invest funds to ensure that they are able to pay out the benefit upon retirement. Thus, the primary goal of these DB questions are to solve for accumulated value needed (and hence contributions needed) to fulfill the pension liability: \\[ \\begin{aligned} \\text{DB Liability} &= \\text{EPV DB Benefits}_{65} \\\\ &= \\text{Yearly DB Benefit} \\cdot \\ddot{a}^{(12)}_{65} \\end{aligned} \\] Note Some pension plans also offer a death benefit, thus the accumulated value needed should also reflect the EPV of the death benefit as well. Typically, the benefit is a proportion \\((\\alpha)\\) of the employee's pensionable salary , for each year of service \\((n)\\) that the employee \\[ \\text{Yearly DB Benefit} = n \\cdot \\alpha \\cdot \\text{Pensionable Salary} \\] Pensionable Salary (PS) is a catch all term whose definition varies from plan to plan: Final Salary : Average Annual salary in the last year of employment Final Average Salary : Average annual salary over the last few years of employment Career Average Salary : Average annual salary over the entire span of employment Warning The above methodology is NOT exhaustive . It is important to understand the intuition behind how the benefit is determined so that the same principle can be applied to different question scenarios. Note If pensionable salary is the average salary across the entire employment: \\[ \\text{Total Salary} = n \\cdot \\text{Career Average Salary} \\] Some questions might instead provide the total earnings without the years of service; but note that they are equivalent in this scenario. Tip It is recommended to calculate the Final Salary and the Retirement Benefit seperately , as some questions can provide multiple scenarios which require reusing one or more components. Early Retirement \u00b6 As stated previously, the default retirement age is 65. However, some programs allow for employees to retire early and hence receive their pension earlier. There are two consequences as a result: The years of service and pensionable salary should reflect the earlier retirement age Reduction factor is linearly applied for the number of years brought forward \\[ \\begin{aligned} \\text{Benefit Reduction} &= (1 - n_\\text{Years Early} \\cdot \\text{Reduction Factor}) \\\\ \\text{Reduced Benefit} &= \\text{Early Benefit} \\cdot \\text{Reduction Factor} \\end{aligned} \\] Warning It is a common mistake to apply the reduction exponentially: \\[ \\text{Benefit Reduction} \\ne (1 + \\text{Reduction Factor})^{n_\\text{Years Early}} \\] This mistake can be avoided by understanding that the benefit determination itself is applied linearly, thus the reduction should be on the same basis. Accrued Benefits \u00b6 Legally speaking, an employer can cancel their pension program at any time. However, they must still make pension payments to employees based on the number of years they have worked up to the point of cancellation . Thus, the liability of a DB program is only the accrued (earned) portion of the retirement benefit at any point in time. To better understand this, consider the following situation: Employee joined the company at age \\(x\\) The employee is currently aged \\(y\\) with \\((y-x)\\) years of service Retirements occur upon exact age 65 \\[ \\begin{aligned} \\text{Pension Benefit} &= (65 - x) \\cdot \\alpha \\cdot \\text{PS} \\\\ &= [(y - x) + (65-y)] \\cdot \\alpha \\cdot \\text{PS} \\\\ &= \\underbrace{(y - x) \\cdot \\alpha \\cdot \\text{PS}}_{\\text{Accrued Benefit}} + \\underbrace{(65-y) \\cdot \\alpha \\cdot \\text{PS}}_{\\text{Future Service Benefit}} \\end{aligned} \\] However, this raises a question on the basis of the pensionable salary . If the years of service only reflects what is earned so far, should the salary follow the same logic? Thus, there are two different valuation basis: Projected Unit Credit Traditional Unit Credit Accounts for future salary increases Does not allow for future salary increases Projects salary to retirement age Uses salary as at current age More conservative Less conservative EG. PS = S(64), S(63), S(62) EG. PS = S(y-1), S(y-2), S(y-3) Warning The methodology only influences the Pensionable Salary component - the number of years remains based on the accrued amount. Note Naturally, this means that an employee who just joined the pension plan will have no actuarial liability as they do not have any prior years of service. Accrued Liability \u00b6 Ultimately, the benefit payments will only start on retirement. Thus, liability must be discounted back to the current valuation date to control for interest and decrements , known as the Accrued Liability : \\[ \\begin{aligned} {}_{x}V &= \\text{EPV DB Benefits}_{65} \\cdot v^{65-y} \\cdot \\frac{r_{65}}{\\ell_{y}} \\\\ &= (y - x) \\cdot \\alpha \\cdot \\text{PS} \\cdot \\ddot{a}^{(12)}_{65} \\cdot v^{65-y} \\cdot \\frac{r_{65}}{\\ell_{y}} \\end{aligned} \\] The probability of retirement is a combination of two probabilities: Probability of surviving till retirement age Probability of retiring at that age , given that they are alive at that age \\[ \\begin{aligned} \\text{P(Retire at 65)} &= \\text{P(Survive till 65)} \\cdot \\text{P(Retire at 65)} \\\\ &= \\frac{\\ell_{65}}{\\ell_{y}} \\cdot \\frac{r_{65}}{\\ell_{65}} \\\\ &= \\frac{r_{65}}{\\ell_{y}} \\end{aligned} \\] Tip Most questions will use the following phrasing: Decrements follow standard service table Mortality after retirement follows the Standard Ultimate Life Table This means that the retirement probabilities should be obtained from the SST, while the EPV factors should be obtained from the SULT. Note Some questions might provide a scenaro where the employee has already retired . In this case, the following aspects should be taken care of: The benefit is based on the actual salary as at retirement There is no need to discount interest or mortality as the employee has already retired and has started receiving payments Variable Retirement \u00b6 Up till this point, we have assumed that there is only one possible retirement age. In reality, individuals could retire across a range of ages . In particular, if retirements follow the Standard Service Table , then employees could retire from any age from 60 to 65 : Note To reduce complexity, most questions assume that the employee is currently aged 62 or 63 to reduce the range of early ages. Alternatively, their assumptions might follow an adjusted table such that all employees surviving to a specified age (EG. 62) retire at that age, reducing the range of ages. The accrued liability must account for all possibilities of retiring at each age: \\[ \\begin{aligned} {}_{x}V &= \\text{EPV DB Benefits}_{65} \\cdot v^{65-y} \\cdot \\frac{r_{65}}{\\ell_{y}} \\\\ &+ \\text{EPV DB Benefits}_{64} \\cdot v^{64-y} \\cdot \\frac{r_{64}}{\\ell_{y}} \\\\ &+ \\text{EPV DB Benefits}_{63} \\cdot v^{63-y} \\cdot \\frac{r_{63}}{\\ell_{y}} \\\\ &+ \\dots \\end{aligned} \\] If the PUC method is specified, then each scenario must use a different pensionable salary , based on the retirement age in that scenario: \\[ \\begin{aligned} {}_{x}V &= n \\cdot \\alpha \\cdot \\text{PS}_{65} \\cdot \\ddot{a}^{(12)}_{65} \\cdot v^{65-y} \\cdot \\frac{r_{65}}{\\ell_{y}} \\\\ &+ n \\cdot \\alpha \\cdot \\text{PS}_{64} \\cdot \\ddot{a}^{(12)}_{64} \\cdot v^{64-y} \\cdot \\frac{r_{64}}{\\ell_{y}} \\\\ &+ n \\cdot \\alpha \\cdot \\text{PS}_{63} \\cdot \\ddot{a}^{(12)}_{63} \\cdot v^{63-y} \\cdot \\frac{r_{63}}{\\ell_{y}} \\\\ &+ \\dots \\end{aligned} \\] Consequently, if the TUC method is specified, then all scenarios would be based on the same retirement benefit. Thus, the expression can be greatly simplified to: \\[ \\begin{aligned} {}_{x}V &= n \\cdot \\alpha \\cdot \\text{PS}_{y} \\\\ & \\cdot \\biggl(\\ddot{a}^{(12)}_{65} \\cdot v^{65-y} \\cdot \\frac{r_{65}}{\\ell_{y}} + \\ddot{a}^{(12)}_{64} \\cdot v^{64-y} \\cdot \\frac{r_{64}}{\\ell_{y}} + \\dots \\biggr) \\end{aligned} \\] Withdrawal \u00b6 Another possible scenario is that the employee leaves the company (hence withdrawing from the pension program) prior to their retirement. The company is still obligated pay the accrued benefits for the years that the employee has worked. Note If the employee joins a new company which also has a pension benefit, they will receive two sets of retirement benefits upon from both the old and new company. Allowing for withdrawals adds another layer of complexity to the calculation: Similar to variable retirements, the accrued liability must account for the possibility of withdrawing at every possible age : \\[ \\begin{aligned} {}_{x}V &= \\text{EPV Withdrawal}_{z} \\cdot v^{z-y} \\cdot \\frac{w_{z}}{\\ell_{y}} \\\\ &+ \\text{EPV Withdrawal}_{z+1} \\cdot v^{z+1-y} \\cdot \\frac{w_{z+1}}{\\ell_{y}} \\\\ &+ \\text{EPV Withdrawal}_{z+2} \\cdot v^{z+2-y} \\cdot \\frac{w_{z+2}}{\\ell_{y}} \\\\ &+ \\dots \\end{aligned} \\] Note However, due to the sheer number of possible retirements, most questions will most definitely limit the ages to a specified range . The EPV of withdrawals can be expressed as: \\[ \\text{EPV Withdraw}_{z} = \\text{EPV DB Benefits}_{65} \\cdot v^{65-z} \\cdot \\frac{\\ell_{65}}{\\ell_{z}} \\] Note In order to simplify this process, some questions might provide a specialized annuity factor that can directly calculate the EPV of future retirement benefits as at the withdrawal date . DB Funding \u00b6 As stated previously, it is the employer's duty to ensure that they have sufficient funds to cover the DB pension liability. Thus, employers hols a reserve equal to the accrued liability of the pension at the time of valuation (hence the notation). As the employees move towards retirement, the accrued liability increases, forcing the employer to increase their reserves . The extent of the reserve increase (after controlling for mortality and interest) is the amount that the contribution needed \\((C_{t})\\) for that period. \\[ \\begin{aligned} {}_{t}V + C_{t} &= {}_{t+1}V \\cdot vp_{x} \\\\ C_{t} &= {}_{t+1}V \\cdot vp_{x} - {}_{t}V \\end{aligned} \\] Note The contribution needed is sometimes referred to as the Normal Cost of the pension. Functionally, it is equivalent to the premium that the plan receives. Similarly, since there are two different valuation methodologies, there will be a different contribution amount for each method. PUC Method \u00b6 Using the PUC method, the pensionable salary is projected to retirement, regardless of the valuation year. Thus, the pensionable salary is constant across time . Thus, consider the following key ideas about how the two accrued liabilities are different: Both assume payments start on the same retirement age Both are on the same basis (discounted to the same time period) Both are based on the same pensionable salary Key difference is the years of service - liability as at \\(t+1\\) has earned one more year of salary Note Recall that the future accrued liability is controlled for interest and mortality. This means that the cashflows are discounted to the same time point . Given that there is only one key difference , the \\({}_{t+1}V\\) can be expressed as a function of \\({}_{t}V\\) : \\[ \\begin{aligned} vp_{x} \\cdot {}_{t+1}V &= vp_{x} \\cdot \\text{AB}_{t+1} \\cdot \\ddot{a}^{(12)}_{65} \\cdot v^{65-(y+1)} \\frac{r_{65}}{\\ell_{y+1}} \\\\ &= \\text{AB}_{t+1} \\cdot \\ddot{a}^{(12)}_{65} \\cdot v^{65-y} \\frac{r_{65}}{\\ell_{y}} \\\\ &= \\frac{n+1}{n} \\cdot \\text{AB}_{t} \\cdot \\ddot{a}^{(12)}_{65} \\cdot v^{65-y} \\frac{r_{65}}{\\ell_{y}} \\\\ &= \\frac{n+1}{n} \\cdot {}_{t}V \\\\ \\\\ \\text{AB}_{t+1} &= (n+1) \\cdot \\alpha \\cdot \\text{PS}_{65} \\\\ &= \\frac{n+1}{n} \\cdot n \\cdot \\alpha \\cdot \\text{PS}_{65} \\\\ &= \\frac{n+1}{n} \\cdot \\text{AB}_{t} \\\\ \\\\ \\therefore C_{t} &= {}_{t+1}V \\cdot vp_{x} - {}_{t}V \\\\ &= \\frac{n+1}{n} \\cdot {}_{t}V - {}_{t}V \\\\ &= \\left(\\frac{n+1}{n} - 1 \\right) \\cdot {}_{t}V \\\\ &= \\frac{1}{n} \\cdot {}_{t}V \\end{aligned} \\] Tip The above expression can be shown WITHOUT proof. This will greatly save time on the exam. Thus, the contribution under a PUC basis is the liability solely for an additional year of accruals - which is calculated as the average yearly liability of the current liability. TUC Method \u00b6 Using the TUC method, the pensionable salary is based on the current salary as at the valuation date. This means that the liability at \\(t+1\\) is based on a different pensionable salary compared to the liability at \\(t\\) . However, the approach is still the same. The goal is convert the re-express the liability at \\(t+1\\) as a function of the liability at \\(t\\) : \\[ \\begin{aligned} vp_{x} \\cdot {}_{t+1}V &= vp_{x} \\cdot \\text{AB}_{t+1} \\cdot \\ddot{a}^{(12)}_{65} \\cdot v^{65-(y+1)} \\frac{r_{65}}{\\ell_{y+1}} \\\\ &= \\text{AB}_{t+1} \\cdot \\ddot{a}^{(12)}_{65} \\cdot v^{65-y} \\frac{r_{65}}{\\ell_{y}} \\\\ &= (1+g) \\cdot \\frac{n+1}{n} \\cdot \\text{AB}_{t} \\cdot \\ddot{a}^{(12)}_{65} \\cdot v^{65-y} \\frac{r_{65}}{\\ell_{y}} \\\\ &= (1+g) \\cdot \\frac{n+1}{n} \\cdot {}_{t}V \\\\ \\\\ \\text{AB}_{t+1} &= (n+1) \\cdot \\alpha \\cdot \\text{PS}_{t+1} \\\\ &= (1+g) \\cdot \\frac{n+1}{n} \\cdot n \\cdot \\alpha \\cdot \\text{PS}_{t} \\\\ &= (1+g) \\cdot \\frac{n+1}{n} \\cdot \\text{AB}_{t} \\\\ \\\\ \\therefore C_{t} &= {}_{t+1}V \\cdot vp_{x} - {}_{t}V \\\\ &= (1+g) \\cdot \\frac{n+1}{n} \\cdot {}_{t}V - {}_{t}V \\\\ &= \\left((1+g) \\cdot \\frac{n+1}{n} - 1 \\right) \\cdot {}_{t}V \\\\ \\end{aligned} \\] Thus, the contribution under a TUC basis reflects two items: An additional year of accruals An additional year of salary increases for all previously earned benefits Comparison \u00b6 Questions tend to ask what is the difference between the contribution rates under PUC and TUC methods respectively. Firstly, it is important to understand that both methods result in the same total funding : The difference lies in the path taken to get there. Generally speaking, PUC contributions start higher but increase slower than TUC TUC contributes become higher around \\(\\frac{2}{3}\\) of the maximum employment period Thus, use the time to retirement as a sense check for which should be higher. Most questions give a scenario close to retirement, thus TUC should generally be higher. It is important to understand why the above occurs: Both methodologies result in contributions that increase over time as they must reflect the additional years of earned service over time PUC approach pre-pays for all future salary increases, which is why it starts higher TUC approach pays for salary increases as they come , which includes paying to upgrade all past accruals for the salary increase At older ages, the past accruals become extremely large , resulting in steep increases over time Note The main reason why the normal cost is needed is to cover for the additional year of accruals (due to years of service and/or salary). It is NOT due to interest or mortality - if the experience follows as assumed, the current fund value will be sufficient to support the liability at retirement. However, since the retirement benefit increases each year of service , the liability increases which is why there must be an additional normal contribution each period to account for the increase. This is why there are no normal contributions past retirement - because the retirement benefit is fixed . Mid Year Retirements \u00b6 Some questions provide a scenario where they assume retirements occur in the middle of ages instead - EG. employees who were supposed to retire at 63 instead retire at 63.5 . Warning When determining ACCRUED benefits, the years of service should still reflect the years as at the valuation date . It is only when determining the ACTUAL benefits, that the additional half year of service is accounted for. The key complication comes from a funding perspective. Using the previous formula, the mid-year retirement is NOT accounted for. Thus, if mid-year retirements are present, the following should be used instead: \\[ {}_{t}V + C_{t} = vp_{x} \\cdot {}_{t+1}V + v^{0.5} {}_{0.5}r_{x} \\cdot \\text{EPV at Mid Year Retirement} \\] The EPV of Mid Year Retirements should be calculated assuming the employee ACTUALLY retires at that time; no need accruals needed: Years of Service should reflect the actual number of years worked as at that time; additional half year \\(n+0.5\\) Salary should reflect the actual salary as at that time; average of \\(S_{n-1}\\) and \\(S_{n}\\) Note The intuition for the reserve recursion follows any other insurance product. The current reserve must be sufficient to cover any outflows in the period (mid-year retirements) and set-up the next reserve (accrued liability at \\(t+1\\) ). Any shortfall must be funded to ensure reserve adequacy. In particular, there would be a release in reserve to cover the expected outflow. However, due to the valuation basis, the reserve provision will always be insufficient to cover the expected outgo: Reserve provision uses \\(n\\) years of accrued service (as at valuation time) Expected outgo uses \\(n+0.5\\) years accrued of service (as at expected retirement) Thus, mid-year exits will increase the contribution for the year, all else equal. Unfortunately, there is no way to easily simplify the above expression to quickly determine the normal contribution. It will require manual calculation via first principles . Post Retirement \u00b6 The normal contribution is invested into a pension fund . The goal is that by retirement, the pension program is fully funded - the fund value is equal to the actuarial liability . During retirement, the pension fund is drawn down to pay for the pension benefits as they arise. Thus, the profit or loss of the program is the difference between the pension fund and the actuarial liability: Pension fund assets may grow at a different rate than what was assumed in liability calculation Pension members may have a different mortality than what was experienced The key difficulty is determining the pension fund value at a future time, after taking into account the withdrawals . The most intuitive method would be to: Assume that there are no withdrawals , grow the entire fund by the growth rate Determine the accumulated value of the withdrawals only during the time at the assumed growth rate Take the difference between the two \\[ \\text{FV}_{t+n} = \\text{FV}_{t} \\cdot (1+g)^{n} - \\text{Retirement Benefit} \\cdot \\ddot{s}_{\\enclose{actuarial}{n}} \\] Health Benefits \u00b6 Alongside pensions, employers sometimes also offer Health Benefits to their employees in retirement. Operationally, this is done by sponsoring a health insurance policy for the employee for as long as they live post retirement. Similarly, the goal is to determine how much an employer needs to contribute while an employee is working in order to fund the health benefit. Health Valuation \u00b6 Firstly, the expected amount of health insruance premiums payable must be determined. The key is to understand that health insurance premiums tend to increase over time due to: Age : Reflecting higher probability of claims; growth rate \\(a\\) Inflation : Reflecting higher cost of healthcare; growth rate \\(b\\) \\[ P_{65+t} = P_{65} \\cdot (1+a)^{t} \\cdot (1+b)^{t} \\] Warning The years compounded might not be the same for both factors. Consider an individual is currently aged \\(x\\) , but the salary provided is for an individual aged \\(x+t\\) . Attained Age Growth: \\(65 - x\\) Inflation Growth: \\(65 - (x+t)\\) Read the question carefully and apply the correct factors. \\[ \\begin{aligned} \\text{EPV Health Benefits}_{65} &= \\sum P_{65} \\cdot (1+a)^{k} \\cdot (1+b)^{k} \\cdot v^{k} \\cdot {}_{k}p_{x} \\\\ &= P_{65} \\sum \\left(\\frac{(1+a)(1+b)}{(1+i)} \\right)^{k} \\cdot {}_{k}p_{x} \\\\ &= P_{65} \\sum \\frac{1}{(1+i^{*})^{k}} \\cdot {}_{k}p_{x} \\\\ &= P_{65} \\cdot \\ddot{a}_{65 \\mid i^{*}} \\\\ \\\\ \\frac{1}{(1+i^{*})} &= \\frac{(1+a)(1+b)}{(1+i)} \\\\ (1+i^{*}) &= \\frac{(1+i)}{(1+a)(1+b)} \\\\ i^{*} &= \\frac{(1+i)}{(1+a)(1+b)} - 1 \\end{aligned} \\] Tip Depending on the information given, the initial retirement premium might already have some growth baked into it. Simply factorize out the excess to mimic the standard annuity formula: \\[ \\begin{aligned} \\text{EPV Health Benefits}_{x} &= P_{x} (1+a)^{10} (1+b)^{15} + P_{x} \\cdot v(1+a)^{11} (1+b)^{16} + P_{x} \\cdot v^{2} (1+a)^{12} (1+b)^{17} + \\dots \\\\ &= P_{x} (1+a)^{10} (1+b)^{15} [1 + v(1+a)(1+b) + v^{2} (1+a)^{2} (1+a)^{2} + \\dots] \\\\ &= P_{x} (1+a)^{10} (1+b)^{15} [1 + (1+i^{*})^{-1} + (1+i^{*})^{-2} + \\dots] \\\\ &= P_{x} (1+a)^{10} (1+b)^{15} \\cdot \\ddot{a}_{x \\mid i^{*}} \\end{aligned} \\] The value of the benefits as at the current date is known as the Actuarial Value of Total Health Benefits (AVTHB): \\[ \\text{AVTHB}_{x} = \\text{EPV}_{65} \\cdot v^{65-y} \\cdot \\frac{r_{65}}{\\ell_{x}} \\] Note that unlike Pensions, the AVTHB is NOT the accrued liability of the plan. Instead, the Accrued Liability is based on a linear accrual of the AVTHB: n: Number of years worked k: Number of years to retirement \\[ {}_{x}V = \\frac{n}{n+k} \\cdot \\text{AVTHB}_{x} \\] There are two methodologies for the accruals: Pro-Rata Method : Assume accrual to the EARLIEST possible retirement age Other Method : Assume accrual to EACH possible retirement age \\[ \\begin{aligned} {}_{x}V_{\\text{Pro Rata}} &= \\frac{n}{n+k_{\\text{Earliest}}} \\cdot \\text{AVTHB}_{\\text{Earliest}} \\\\ \\\\ {}_{x}V_{\\text{Other}} &= \\sum \\frac{n}{n+k} \\cdot \\text{AVTHB}_{x} \\end{aligned} \\] Warning This section is confusing because the concepts are similar to pensions but the execution is slightly different: Pensions : Accrual affects the pensionable benefit, which is included in the EPV Health : Accrual and EPV seperated into two steps Health Funding \u00b6 Similar to pensions, the employer holds a reserve equal to the Accrued Liability of the plan. Assuming Pro-Rata accruals, the contributions are determined using the same approach : \\[ \\begin{aligned} C_{t} &= {}_{t+1}V \\cdot vp_{x} - {}_{t}V \\\\ &= \\frac{n+1}{n+k} \\cdot \\text{AVTHB}_{x+1} \\cdot vp_{x} - \\frac{n}{n+k} \\cdot \\text{AVTHB}_{x} \\\\ &= \\frac{n+1}{n+k} \\cdot \\text{AVTHB}_{x} - \\frac{n}{n+k} \\cdot \\text{AVTHB}_{x} \\\\ &= \\frac{1}{n+k} \\cdot \\text{AVTHB}_{x} \\end{aligned} \\] Warning The formula LOOKS similar to the Pension one but it is different: Pensions : Years of service & accrued liability Health : Years to retirement & AVTHB Conceptually, is it similar to the pension contribution under a PUC basis - both are essentially just accounting for an additional year of accruals , since the health benefit does not change with accruals. If the accrued liabilities was accrued to each possible age , then the total contribution needed is the SUM of the contributions requried for each possible age: \\[ C_{t} = \\sum \\frac{1}{n+k} \\cdot \\text{AVTHB}_{x} \\] Note Mathematically, the above is true because the EPV to each retirement age is simply a linear sum of the pro-rata liability at each age. The contributions under a pro-rata method will always be higher than that of the each possible retirement age: Pro-rata : Plan must be fully funded by the earliest possible retirement age (EG. Benefits at 65 fully funded by 60 ) Other : Plan is funded up till the specified retiement age (EG. Benefits at 65 fully funded by 65 ) Under a pro-rata basis, there is less time to fund the plan, thus each contribution must be higher (all else equal)","title":"Pensions"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#pensions","text":"","title":"Pensions"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#overview","text":"Pensions are an arrangement where an employer makes regular contributions into a fund as long as the employee has yet to retire. Upon retirement, contributions will stop and the fund will be drawn down to make regular payments to the employee as a form of retirement benefit as long as they are alive. For the purposes of this exam, there are two types of pensions to consider: Defined Contribution Defined Benefit Contribution is fixed Contribution is variable Benefit is variable Benefit is fixed Investment risk borne by Employee Investment risk borne by Employer Variable Benefits to Employee, Guaranteed Cost to Employers Guaranteed Benefits to Employee, Variable Cost to Employers Preferred by Employers Preferred by Employees Note Under a DB pension, if the fund performs worse than expected , the employer will vary their contributions to ensure that the target payout can be supported. Thus, the downside (risk) of DB plans is borne by the employer . Conversely, under a DC pension, the employer has no obligation to adjust their contributions, thus the loss in benefit is borne by the employee via lower benefit payments. Pensions are within the scope of the exam as they are a form of contingent cashflows - contingent on the retirement of the employee. Pensions are offered typically for the following reaosns: Attract new employees Retain existing employees Improve employee morale Negotiation with Unions","title":"Overview"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#salary","text":"","title":"Salary"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#key-properties","text":"Salary is the most important component for any pension related problem as all plan types rely on salary as the main driver for the benefits. Thus, it is important to have a strong understanding of how it is treated for this exam: Salary is paid monthly and is assumed to be level throughout the year Salary is paid in arrears ; at the end of the period However, most questions are concerned with the Annual Salary , denoted as \\(S_{x}\\) \\(S_{x}\\) is the salary earned by the employee WHILE they were age \\(x\\) For the purposes of this exam, an employee is assumed to retire exactly on the day that they reach the retirement age ( their birthday ). If an employee retires at age \\(y+1\\) , then the last salary they salary they would receive would be while they were aged \\(y\\) : Tip For most questions, the individual is assumed to retire at age 65 (Beginning of age 65; end of age 64). Thus, the final annual salary that the individual receives is \\(S_{64}\\) . When in doubt, always draw a timeline to visualize the salaries paid. Warning Some questions may specify that the individuals retire in the middle of their ages (EG. Age 64.5). In this case, the final salary is comprised of two components: 6 months of salary from \\(S_{63}\\) 6 months of salary from \\(S_{64}\\) The above is important because the salary while 63 is usually different while 64.","title":"Key Properties"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#salary-growth","text":"For the purposes of the exam, assume that the salary of the employee grows over time . Thus, rather than provide a salary schedule, they will provide the salary at a specified age and then require students to manually compute the salary at the required ages. Most questions will assume that the annual salary grows annually , typically on the employee's birthday: \\[ S_{x+t} = S_{x} \\cdot (1+g)^{t} \\] Note Salary is still assumed to be level throughout the year. Thus, the monthly salary also grows at the same rate: \\[ \\begin{aligned} S_{x+t} &= S_{x} \\cdot (1+g)^{t} \\\\ &= 12 \\cdot \\text{Monthly Salary}_{x} \\cdot (1+g)^{t} \\\\ &= 12 \\cdot \\text{Monthly Salary}_{x+t} \\end{aligned} \\] However, there are some questions that assume that the monthly salary grows monthly instead. In these cases, the salary is no longer level throughout the year. Thus, the salary in the individual months must be summed up to obtain the annual salary in the year. \\[ \\begin{aligned} S_{x+t} &= \\text{Monthly Salary}_{x+t-1} \\cdot [1 + (1+j)^{1} + (1+j)^{2} + (1+j)^{3} + \\dots] \\\\ &= \\text{Monthly Salary}_{x+t-1} \\cdot s_{\\enclose{actuarial}{12} j} \\end{aligned} \\] Warning The accumulation function is for the payments made in arrears (not the usual payment in advance) that is usually used for ALTAM: \\[ s_{\\enclose{actuarial}{12}} = \\frac{(1+j)^{12} - 1}{i} \\]","title":"Salary Growth"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#continuous-salary","text":"In some rare cases, questions might provide the Salary Rate instead. In these cases, the salary also grows at a continuous rate . The same principles from before apply - the annual salary is the sum of all the salaries received in the year. In a continuous scenario, the salaries are received every instant. To sum across all instances, integration must be used: \\[ S_{x+t} = \\int^{n}_{n-1} \\text{Initial Salary Rate} \\cdot e^{\\delta t} \\]","title":"Continuous Salary"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#replacement-ratio","text":"The Replacement Ratio is the percentage of FINAL salary that is replaced by the pension benefit: \\[ \\text{Replacement Ratio} = \\frac{\\text{Yearly Retirement Benefit}}{\\text{Final Yearly Salary}} \\] Most pension plans have a target replacement ratio is 50-70%. The ratio does not need to be close to 100% as people typically require less money while in retirement . If an individual has multiple pension plans (from different companies or personal plans), then the replacement ratio is calculated based on the TOTAL retirement benefit the individual receives.","title":"Replacement Ratio"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#contribution-ratio","text":"Similar to replacement ratio, one common metric is also the Contribution Ratio - which measures how much the employer has contributed to their pension relative to the salary in that year: \\[ \\text{Contribution Ratio} = \\frac{\\text{Contribution}}{\\text{Salary}} \\] Note For a DC plan, the contribution ratio is fixed . For a DB plan, the contribution in each period is different and must be determined. Warning For DB contributions, the salary is different from the salary used to compute the benefits, especially if using TUC method : Valuation Salary: Previous Salary Contribution Salary: Current Salary","title":"Contribution Ratio"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#defined-contribution","text":"Defined Contribution plans are a type of pension where both the employer ( and sometimes employee ) contribute a fixed amount into a fund which accumulates over time. Upon retirement, the fund value is then used to purchase a Life Annuity which will pay the pension benefit to the employee. Thus, the primary goal of these DC questions are to solve for the yearly retirement benefit that the employee will receive on retirement: \\[ \\begin{aligned} \\text{AV of Contributions} &= \\text{EPV DC Benefits}_{65} \\\\ &= \\text{Yearly DC Benefit} \\cdot \\ddot{a}^{(12)}_{65} \\end{aligned} \\] Warning Retirement benefits are typically paid on a monthly basis , which is why the payable 12 times a year annuity function is used. However, it is important to remember that the face value of the annuity is assumed to be a yearly cashflow . To obtain the monthly benefit, the resulting benefit must be divided by 12 . The contributions into the fund are defined as a proportion \\((c)\\) of the employee's salary each period: \\[ \\text{DC Contribution}_{t} = c \\cdot S_{x+t} \\] Due to the rather straightforward nature of DC plans, most questions from this topic will focus on DB plans instead.","title":"Defined Contribution"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#accumulated-value","text":"The contribution each period changes as the employee's salary grows. Thus, it is NOT possible to use the typical accumulated value function from exam FM ( \\(s_{x:\\enclose{actuarial}{n}}\\) ). The best approach would be to use first principles via the Sum of Geometric Series . It must account for two dimensions: Each contribution is different due to Salary Increases Each contribution earns a different interest due to the Time Value of Money \\[ \\begin{aligned} \\text{Sum of AV of Contributions} &= a \\cdot \\frac{r^{n} - 1}{r - 1} \\\\ \\\\ a &= S_{x} \\cdot (1+i)^n \\\\ r &= (1+g) \\cdot v \\end{aligned} \\] The key is knowing HOW MANY years to accumulate: Salary Growth : Starts from 0, ends at \\(n-1\\) Accumulation : Starts from \\(n-1\\) , end at \\(0\\) In both cases, the total is still \\(n\\) times Tip The same principles apply for a continuous salary, simply convert the discrete components into continuous ones: Summation into Integration Rates into forces \\[ \\begin{aligned} \\text{Sum of AV of Contributions} &= \\int^{n}_{0} S_{x} \\cdot e^{\\delta_{i} t} \\cdot e^{(\\delta_{g} - \\delta_{i}) t} \\end{aligned} \\]","title":"Accumulated Value"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#different-frequencies","text":"The above above only works if all components are on the same frequency: Salary Contributions Salary Growth Interest Accumulation The main issue comes when Contributions are made monthly but Growth is made yearly . Thus, an additional step must be done to bridge the two together: Since salary growth is only made yearly, the contributions within a given year are constant . Thus, the accumulated value of the monthly contributions at the end of the year can be expressed using the accumulation function : \\[ \\text{Yearly Contribution} = S_{x} \\cdot c \\cdot s_{\\enclose{actuarial}{12}} \\] The above yearly contribution can then be plugged into the usual geometric formula that to determine the accumulated value as at the valuation date. Tip This is similar in principle to the monthly salary growth situation earlier. The key is to gross up the components with different frequencies to match the rest.","title":"Different Frequencies"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#defined-benefit","text":"Defined Benefit plans are a type pension where the retirement benefits are fixed, defined as some function of an employee's final salary upon retirement. It is the employer's duty to set aside and invest funds to ensure that they are able to pay out the benefit upon retirement. Thus, the primary goal of these DB questions are to solve for accumulated value needed (and hence contributions needed) to fulfill the pension liability: \\[ \\begin{aligned} \\text{DB Liability} &= \\text{EPV DB Benefits}_{65} \\\\ &= \\text{Yearly DB Benefit} \\cdot \\ddot{a}^{(12)}_{65} \\end{aligned} \\] Note Some pension plans also offer a death benefit, thus the accumulated value needed should also reflect the EPV of the death benefit as well. Typically, the benefit is a proportion \\((\\alpha)\\) of the employee's pensionable salary , for each year of service \\((n)\\) that the employee \\[ \\text{Yearly DB Benefit} = n \\cdot \\alpha \\cdot \\text{Pensionable Salary} \\] Pensionable Salary (PS) is a catch all term whose definition varies from plan to plan: Final Salary : Average Annual salary in the last year of employment Final Average Salary : Average annual salary over the last few years of employment Career Average Salary : Average annual salary over the entire span of employment Warning The above methodology is NOT exhaustive . It is important to understand the intuition behind how the benefit is determined so that the same principle can be applied to different question scenarios. Note If pensionable salary is the average salary across the entire employment: \\[ \\text{Total Salary} = n \\cdot \\text{Career Average Salary} \\] Some questions might instead provide the total earnings without the years of service; but note that they are equivalent in this scenario. Tip It is recommended to calculate the Final Salary and the Retirement Benefit seperately , as some questions can provide multiple scenarios which require reusing one or more components.","title":"Defined Benefit"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#early-retirement","text":"As stated previously, the default retirement age is 65. However, some programs allow for employees to retire early and hence receive their pension earlier. There are two consequences as a result: The years of service and pensionable salary should reflect the earlier retirement age Reduction factor is linearly applied for the number of years brought forward \\[ \\begin{aligned} \\text{Benefit Reduction} &= (1 - n_\\text{Years Early} \\cdot \\text{Reduction Factor}) \\\\ \\text{Reduced Benefit} &= \\text{Early Benefit} \\cdot \\text{Reduction Factor} \\end{aligned} \\] Warning It is a common mistake to apply the reduction exponentially: \\[ \\text{Benefit Reduction} \\ne (1 + \\text{Reduction Factor})^{n_\\text{Years Early}} \\] This mistake can be avoided by understanding that the benefit determination itself is applied linearly, thus the reduction should be on the same basis.","title":"Early Retirement"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#accrued-benefits","text":"Legally speaking, an employer can cancel their pension program at any time. However, they must still make pension payments to employees based on the number of years they have worked up to the point of cancellation . Thus, the liability of a DB program is only the accrued (earned) portion of the retirement benefit at any point in time. To better understand this, consider the following situation: Employee joined the company at age \\(x\\) The employee is currently aged \\(y\\) with \\((y-x)\\) years of service Retirements occur upon exact age 65 \\[ \\begin{aligned} \\text{Pension Benefit} &= (65 - x) \\cdot \\alpha \\cdot \\text{PS} \\\\ &= [(y - x) + (65-y)] \\cdot \\alpha \\cdot \\text{PS} \\\\ &= \\underbrace{(y - x) \\cdot \\alpha \\cdot \\text{PS}}_{\\text{Accrued Benefit}} + \\underbrace{(65-y) \\cdot \\alpha \\cdot \\text{PS}}_{\\text{Future Service Benefit}} \\end{aligned} \\] However, this raises a question on the basis of the pensionable salary . If the years of service only reflects what is earned so far, should the salary follow the same logic? Thus, there are two different valuation basis: Projected Unit Credit Traditional Unit Credit Accounts for future salary increases Does not allow for future salary increases Projects salary to retirement age Uses salary as at current age More conservative Less conservative EG. PS = S(64), S(63), S(62) EG. PS = S(y-1), S(y-2), S(y-3) Warning The methodology only influences the Pensionable Salary component - the number of years remains based on the accrued amount. Note Naturally, this means that an employee who just joined the pension plan will have no actuarial liability as they do not have any prior years of service.","title":"Accrued Benefits"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#accrued-liability","text":"Ultimately, the benefit payments will only start on retirement. Thus, liability must be discounted back to the current valuation date to control for interest and decrements , known as the Accrued Liability : \\[ \\begin{aligned} {}_{x}V &= \\text{EPV DB Benefits}_{65} \\cdot v^{65-y} \\cdot \\frac{r_{65}}{\\ell_{y}} \\\\ &= (y - x) \\cdot \\alpha \\cdot \\text{PS} \\cdot \\ddot{a}^{(12)}_{65} \\cdot v^{65-y} \\cdot \\frac{r_{65}}{\\ell_{y}} \\end{aligned} \\] The probability of retirement is a combination of two probabilities: Probability of surviving till retirement age Probability of retiring at that age , given that they are alive at that age \\[ \\begin{aligned} \\text{P(Retire at 65)} &= \\text{P(Survive till 65)} \\cdot \\text{P(Retire at 65)} \\\\ &= \\frac{\\ell_{65}}{\\ell_{y}} \\cdot \\frac{r_{65}}{\\ell_{65}} \\\\ &= \\frac{r_{65}}{\\ell_{y}} \\end{aligned} \\] Tip Most questions will use the following phrasing: Decrements follow standard service table Mortality after retirement follows the Standard Ultimate Life Table This means that the retirement probabilities should be obtained from the SST, while the EPV factors should be obtained from the SULT. Note Some questions might provide a scenaro where the employee has already retired . In this case, the following aspects should be taken care of: The benefit is based on the actual salary as at retirement There is no need to discount interest or mortality as the employee has already retired and has started receiving payments","title":"Accrued Liability"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#variable-retirement","text":"Up till this point, we have assumed that there is only one possible retirement age. In reality, individuals could retire across a range of ages . In particular, if retirements follow the Standard Service Table , then employees could retire from any age from 60 to 65 : Note To reduce complexity, most questions assume that the employee is currently aged 62 or 63 to reduce the range of early ages. Alternatively, their assumptions might follow an adjusted table such that all employees surviving to a specified age (EG. 62) retire at that age, reducing the range of ages. The accrued liability must account for all possibilities of retiring at each age: \\[ \\begin{aligned} {}_{x}V &= \\text{EPV DB Benefits}_{65} \\cdot v^{65-y} \\cdot \\frac{r_{65}}{\\ell_{y}} \\\\ &+ \\text{EPV DB Benefits}_{64} \\cdot v^{64-y} \\cdot \\frac{r_{64}}{\\ell_{y}} \\\\ &+ \\text{EPV DB Benefits}_{63} \\cdot v^{63-y} \\cdot \\frac{r_{63}}{\\ell_{y}} \\\\ &+ \\dots \\end{aligned} \\] If the PUC method is specified, then each scenario must use a different pensionable salary , based on the retirement age in that scenario: \\[ \\begin{aligned} {}_{x}V &= n \\cdot \\alpha \\cdot \\text{PS}_{65} \\cdot \\ddot{a}^{(12)}_{65} \\cdot v^{65-y} \\cdot \\frac{r_{65}}{\\ell_{y}} \\\\ &+ n \\cdot \\alpha \\cdot \\text{PS}_{64} \\cdot \\ddot{a}^{(12)}_{64} \\cdot v^{64-y} \\cdot \\frac{r_{64}}{\\ell_{y}} \\\\ &+ n \\cdot \\alpha \\cdot \\text{PS}_{63} \\cdot \\ddot{a}^{(12)}_{63} \\cdot v^{63-y} \\cdot \\frac{r_{63}}{\\ell_{y}} \\\\ &+ \\dots \\end{aligned} \\] Consequently, if the TUC method is specified, then all scenarios would be based on the same retirement benefit. Thus, the expression can be greatly simplified to: \\[ \\begin{aligned} {}_{x}V &= n \\cdot \\alpha \\cdot \\text{PS}_{y} \\\\ & \\cdot \\biggl(\\ddot{a}^{(12)}_{65} \\cdot v^{65-y} \\cdot \\frac{r_{65}}{\\ell_{y}} + \\ddot{a}^{(12)}_{64} \\cdot v^{64-y} \\cdot \\frac{r_{64}}{\\ell_{y}} + \\dots \\biggr) \\end{aligned} \\]","title":"Variable Retirement"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#withdrawal","text":"Another possible scenario is that the employee leaves the company (hence withdrawing from the pension program) prior to their retirement. The company is still obligated pay the accrued benefits for the years that the employee has worked. Note If the employee joins a new company which also has a pension benefit, they will receive two sets of retirement benefits upon from both the old and new company. Allowing for withdrawals adds another layer of complexity to the calculation: Similar to variable retirements, the accrued liability must account for the possibility of withdrawing at every possible age : \\[ \\begin{aligned} {}_{x}V &= \\text{EPV Withdrawal}_{z} \\cdot v^{z-y} \\cdot \\frac{w_{z}}{\\ell_{y}} \\\\ &+ \\text{EPV Withdrawal}_{z+1} \\cdot v^{z+1-y} \\cdot \\frac{w_{z+1}}{\\ell_{y}} \\\\ &+ \\text{EPV Withdrawal}_{z+2} \\cdot v^{z+2-y} \\cdot \\frac{w_{z+2}}{\\ell_{y}} \\\\ &+ \\dots \\end{aligned} \\] Note However, due to the sheer number of possible retirements, most questions will most definitely limit the ages to a specified range . The EPV of withdrawals can be expressed as: \\[ \\text{EPV Withdraw}_{z} = \\text{EPV DB Benefits}_{65} \\cdot v^{65-z} \\cdot \\frac{\\ell_{65}}{\\ell_{z}} \\] Note In order to simplify this process, some questions might provide a specialized annuity factor that can directly calculate the EPV of future retirement benefits as at the withdrawal date .","title":"Withdrawal"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#db-funding","text":"As stated previously, it is the employer's duty to ensure that they have sufficient funds to cover the DB pension liability. Thus, employers hols a reserve equal to the accrued liability of the pension at the time of valuation (hence the notation). As the employees move towards retirement, the accrued liability increases, forcing the employer to increase their reserves . The extent of the reserve increase (after controlling for mortality and interest) is the amount that the contribution needed \\((C_{t})\\) for that period. \\[ \\begin{aligned} {}_{t}V + C_{t} &= {}_{t+1}V \\cdot vp_{x} \\\\ C_{t} &= {}_{t+1}V \\cdot vp_{x} - {}_{t}V \\end{aligned} \\] Note The contribution needed is sometimes referred to as the Normal Cost of the pension. Functionally, it is equivalent to the premium that the plan receives. Similarly, since there are two different valuation methodologies, there will be a different contribution amount for each method.","title":"DB Funding"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#puc-method","text":"Using the PUC method, the pensionable salary is projected to retirement, regardless of the valuation year. Thus, the pensionable salary is constant across time . Thus, consider the following key ideas about how the two accrued liabilities are different: Both assume payments start on the same retirement age Both are on the same basis (discounted to the same time period) Both are based on the same pensionable salary Key difference is the years of service - liability as at \\(t+1\\) has earned one more year of salary Note Recall that the future accrued liability is controlled for interest and mortality. This means that the cashflows are discounted to the same time point . Given that there is only one key difference , the \\({}_{t+1}V\\) can be expressed as a function of \\({}_{t}V\\) : \\[ \\begin{aligned} vp_{x} \\cdot {}_{t+1}V &= vp_{x} \\cdot \\text{AB}_{t+1} \\cdot \\ddot{a}^{(12)}_{65} \\cdot v^{65-(y+1)} \\frac{r_{65}}{\\ell_{y+1}} \\\\ &= \\text{AB}_{t+1} \\cdot \\ddot{a}^{(12)}_{65} \\cdot v^{65-y} \\frac{r_{65}}{\\ell_{y}} \\\\ &= \\frac{n+1}{n} \\cdot \\text{AB}_{t} \\cdot \\ddot{a}^{(12)}_{65} \\cdot v^{65-y} \\frac{r_{65}}{\\ell_{y}} \\\\ &= \\frac{n+1}{n} \\cdot {}_{t}V \\\\ \\\\ \\text{AB}_{t+1} &= (n+1) \\cdot \\alpha \\cdot \\text{PS}_{65} \\\\ &= \\frac{n+1}{n} \\cdot n \\cdot \\alpha \\cdot \\text{PS}_{65} \\\\ &= \\frac{n+1}{n} \\cdot \\text{AB}_{t} \\\\ \\\\ \\therefore C_{t} &= {}_{t+1}V \\cdot vp_{x} - {}_{t}V \\\\ &= \\frac{n+1}{n} \\cdot {}_{t}V - {}_{t}V \\\\ &= \\left(\\frac{n+1}{n} - 1 \\right) \\cdot {}_{t}V \\\\ &= \\frac{1}{n} \\cdot {}_{t}V \\end{aligned} \\] Tip The above expression can be shown WITHOUT proof. This will greatly save time on the exam. Thus, the contribution under a PUC basis is the liability solely for an additional year of accruals - which is calculated as the average yearly liability of the current liability.","title":"PUC Method"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#tuc-method","text":"Using the TUC method, the pensionable salary is based on the current salary as at the valuation date. This means that the liability at \\(t+1\\) is based on a different pensionable salary compared to the liability at \\(t\\) . However, the approach is still the same. The goal is convert the re-express the liability at \\(t+1\\) as a function of the liability at \\(t\\) : \\[ \\begin{aligned} vp_{x} \\cdot {}_{t+1}V &= vp_{x} \\cdot \\text{AB}_{t+1} \\cdot \\ddot{a}^{(12)}_{65} \\cdot v^{65-(y+1)} \\frac{r_{65}}{\\ell_{y+1}} \\\\ &= \\text{AB}_{t+1} \\cdot \\ddot{a}^{(12)}_{65} \\cdot v^{65-y} \\frac{r_{65}}{\\ell_{y}} \\\\ &= (1+g) \\cdot \\frac{n+1}{n} \\cdot \\text{AB}_{t} \\cdot \\ddot{a}^{(12)}_{65} \\cdot v^{65-y} \\frac{r_{65}}{\\ell_{y}} \\\\ &= (1+g) \\cdot \\frac{n+1}{n} \\cdot {}_{t}V \\\\ \\\\ \\text{AB}_{t+1} &= (n+1) \\cdot \\alpha \\cdot \\text{PS}_{t+1} \\\\ &= (1+g) \\cdot \\frac{n+1}{n} \\cdot n \\cdot \\alpha \\cdot \\text{PS}_{t} \\\\ &= (1+g) \\cdot \\frac{n+1}{n} \\cdot \\text{AB}_{t} \\\\ \\\\ \\therefore C_{t} &= {}_{t+1}V \\cdot vp_{x} - {}_{t}V \\\\ &= (1+g) \\cdot \\frac{n+1}{n} \\cdot {}_{t}V - {}_{t}V \\\\ &= \\left((1+g) \\cdot \\frac{n+1}{n} - 1 \\right) \\cdot {}_{t}V \\\\ \\end{aligned} \\] Thus, the contribution under a TUC basis reflects two items: An additional year of accruals An additional year of salary increases for all previously earned benefits","title":"TUC Method"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#comparison","text":"Questions tend to ask what is the difference between the contribution rates under PUC and TUC methods respectively. Firstly, it is important to understand that both methods result in the same total funding : The difference lies in the path taken to get there. Generally speaking, PUC contributions start higher but increase slower than TUC TUC contributes become higher around \\(\\frac{2}{3}\\) of the maximum employment period Thus, use the time to retirement as a sense check for which should be higher. Most questions give a scenario close to retirement, thus TUC should generally be higher. It is important to understand why the above occurs: Both methodologies result in contributions that increase over time as they must reflect the additional years of earned service over time PUC approach pre-pays for all future salary increases, which is why it starts higher TUC approach pays for salary increases as they come , which includes paying to upgrade all past accruals for the salary increase At older ages, the past accruals become extremely large , resulting in steep increases over time Note The main reason why the normal cost is needed is to cover for the additional year of accruals (due to years of service and/or salary). It is NOT due to interest or mortality - if the experience follows as assumed, the current fund value will be sufficient to support the liability at retirement. However, since the retirement benefit increases each year of service , the liability increases which is why there must be an additional normal contribution each period to account for the increase. This is why there are no normal contributions past retirement - because the retirement benefit is fixed .","title":"Comparison"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#mid-year-retirements","text":"Some questions provide a scenario where they assume retirements occur in the middle of ages instead - EG. employees who were supposed to retire at 63 instead retire at 63.5 . Warning When determining ACCRUED benefits, the years of service should still reflect the years as at the valuation date . It is only when determining the ACTUAL benefits, that the additional half year of service is accounted for. The key complication comes from a funding perspective. Using the previous formula, the mid-year retirement is NOT accounted for. Thus, if mid-year retirements are present, the following should be used instead: \\[ {}_{t}V + C_{t} = vp_{x} \\cdot {}_{t+1}V + v^{0.5} {}_{0.5}r_{x} \\cdot \\text{EPV at Mid Year Retirement} \\] The EPV of Mid Year Retirements should be calculated assuming the employee ACTUALLY retires at that time; no need accruals needed: Years of Service should reflect the actual number of years worked as at that time; additional half year \\(n+0.5\\) Salary should reflect the actual salary as at that time; average of \\(S_{n-1}\\) and \\(S_{n}\\) Note The intuition for the reserve recursion follows any other insurance product. The current reserve must be sufficient to cover any outflows in the period (mid-year retirements) and set-up the next reserve (accrued liability at \\(t+1\\) ). Any shortfall must be funded to ensure reserve adequacy. In particular, there would be a release in reserve to cover the expected outflow. However, due to the valuation basis, the reserve provision will always be insufficient to cover the expected outgo: Reserve provision uses \\(n\\) years of accrued service (as at valuation time) Expected outgo uses \\(n+0.5\\) years accrued of service (as at expected retirement) Thus, mid-year exits will increase the contribution for the year, all else equal. Unfortunately, there is no way to easily simplify the above expression to quickly determine the normal contribution. It will require manual calculation via first principles .","title":"Mid Year Retirements"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#post-retirement","text":"The normal contribution is invested into a pension fund . The goal is that by retirement, the pension program is fully funded - the fund value is equal to the actuarial liability . During retirement, the pension fund is drawn down to pay for the pension benefits as they arise. Thus, the profit or loss of the program is the difference between the pension fund and the actuarial liability: Pension fund assets may grow at a different rate than what was assumed in liability calculation Pension members may have a different mortality than what was experienced The key difficulty is determining the pension fund value at a future time, after taking into account the withdrawals . The most intuitive method would be to: Assume that there are no withdrawals , grow the entire fund by the growth rate Determine the accumulated value of the withdrawals only during the time at the assumed growth rate Take the difference between the two \\[ \\text{FV}_{t+n} = \\text{FV}_{t} \\cdot (1+g)^{n} - \\text{Retirement Benefit} \\cdot \\ddot{s}_{\\enclose{actuarial}{n}} \\]","title":"Post Retirement"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#health-benefits","text":"Alongside pensions, employers sometimes also offer Health Benefits to their employees in retirement. Operationally, this is done by sponsoring a health insurance policy for the employee for as long as they live post retirement. Similarly, the goal is to determine how much an employer needs to contribute while an employee is working in order to fund the health benefit.","title":"Health Benefits"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#health-valuation","text":"Firstly, the expected amount of health insruance premiums payable must be determined. The key is to understand that health insurance premiums tend to increase over time due to: Age : Reflecting higher probability of claims; growth rate \\(a\\) Inflation : Reflecting higher cost of healthcare; growth rate \\(b\\) \\[ P_{65+t} = P_{65} \\cdot (1+a)^{t} \\cdot (1+b)^{t} \\] Warning The years compounded might not be the same for both factors. Consider an individual is currently aged \\(x\\) , but the salary provided is for an individual aged \\(x+t\\) . Attained Age Growth: \\(65 - x\\) Inflation Growth: \\(65 - (x+t)\\) Read the question carefully and apply the correct factors. \\[ \\begin{aligned} \\text{EPV Health Benefits}_{65} &= \\sum P_{65} \\cdot (1+a)^{k} \\cdot (1+b)^{k} \\cdot v^{k} \\cdot {}_{k}p_{x} \\\\ &= P_{65} \\sum \\left(\\frac{(1+a)(1+b)}{(1+i)} \\right)^{k} \\cdot {}_{k}p_{x} \\\\ &= P_{65} \\sum \\frac{1}{(1+i^{*})^{k}} \\cdot {}_{k}p_{x} \\\\ &= P_{65} \\cdot \\ddot{a}_{65 \\mid i^{*}} \\\\ \\\\ \\frac{1}{(1+i^{*})} &= \\frac{(1+a)(1+b)}{(1+i)} \\\\ (1+i^{*}) &= \\frac{(1+i)}{(1+a)(1+b)} \\\\ i^{*} &= \\frac{(1+i)}{(1+a)(1+b)} - 1 \\end{aligned} \\] Tip Depending on the information given, the initial retirement premium might already have some growth baked into it. Simply factorize out the excess to mimic the standard annuity formula: \\[ \\begin{aligned} \\text{EPV Health Benefits}_{x} &= P_{x} (1+a)^{10} (1+b)^{15} + P_{x} \\cdot v(1+a)^{11} (1+b)^{16} + P_{x} \\cdot v^{2} (1+a)^{12} (1+b)^{17} + \\dots \\\\ &= P_{x} (1+a)^{10} (1+b)^{15} [1 + v(1+a)(1+b) + v^{2} (1+a)^{2} (1+a)^{2} + \\dots] \\\\ &= P_{x} (1+a)^{10} (1+b)^{15} [1 + (1+i^{*})^{-1} + (1+i^{*})^{-2} + \\dots] \\\\ &= P_{x} (1+a)^{10} (1+b)^{15} \\cdot \\ddot{a}_{x \\mid i^{*}} \\end{aligned} \\] The value of the benefits as at the current date is known as the Actuarial Value of Total Health Benefits (AVTHB): \\[ \\text{AVTHB}_{x} = \\text{EPV}_{65} \\cdot v^{65-y} \\cdot \\frac{r_{65}}{\\ell_{x}} \\] Note that unlike Pensions, the AVTHB is NOT the accrued liability of the plan. Instead, the Accrued Liability is based on a linear accrual of the AVTHB: n: Number of years worked k: Number of years to retirement \\[ {}_{x}V = \\frac{n}{n+k} \\cdot \\text{AVTHB}_{x} \\] There are two methodologies for the accruals: Pro-Rata Method : Assume accrual to the EARLIEST possible retirement age Other Method : Assume accrual to EACH possible retirement age \\[ \\begin{aligned} {}_{x}V_{\\text{Pro Rata}} &= \\frac{n}{n+k_{\\text{Earliest}}} \\cdot \\text{AVTHB}_{\\text{Earliest}} \\\\ \\\\ {}_{x}V_{\\text{Other}} &= \\sum \\frac{n}{n+k} \\cdot \\text{AVTHB}_{x} \\end{aligned} \\] Warning This section is confusing because the concepts are similar to pensions but the execution is slightly different: Pensions : Accrual affects the pensionable benefit, which is included in the EPV Health : Accrual and EPV seperated into two steps","title":"Health Valuation"},{"location":"2.%20Actuarial%20Mathematics/3.%20ASA-ALTAM/8.%20Pensions/#health-funding","text":"Similar to pensions, the employer holds a reserve equal to the Accrued Liability of the plan. Assuming Pro-Rata accruals, the contributions are determined using the same approach : \\[ \\begin{aligned} C_{t} &= {}_{t+1}V \\cdot vp_{x} - {}_{t}V \\\\ &= \\frac{n+1}{n+k} \\cdot \\text{AVTHB}_{x+1} \\cdot vp_{x} - \\frac{n}{n+k} \\cdot \\text{AVTHB}_{x} \\\\ &= \\frac{n+1}{n+k} \\cdot \\text{AVTHB}_{x} - \\frac{n}{n+k} \\cdot \\text{AVTHB}_{x} \\\\ &= \\frac{1}{n+k} \\cdot \\text{AVTHB}_{x} \\end{aligned} \\] Warning The formula LOOKS similar to the Pension one but it is different: Pensions : Years of service & accrued liability Health : Years to retirement & AVTHB Conceptually, is it similar to the pension contribution under a PUC basis - both are essentially just accounting for an additional year of accruals , since the health benefit does not change with accruals. If the accrued liabilities was accrued to each possible age , then the total contribution needed is the SUM of the contributions requried for each possible age: \\[ C_{t} = \\sum \\frac{1}{n+k} \\cdot \\text{AVTHB}_{x} \\] Note Mathematically, the above is true because the EPV to each retirement age is simply a linear sum of the pro-rata liability at each age. The contributions under a pro-rata method will always be higher than that of the each possible retirement age: Pro-rata : Plan must be fully funded by the earliest possible retirement age (EG. Benefits at 65 fully funded by 60 ) Other : Plan is funded up till the specified retiement age (EG. Benefits at 65 fully funded by 65 ) Under a pro-rata basis, there is less time to fund the plan, thus each contribution must be higher (all else equal)","title":"Health Funding"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/1.%20Regression%20Overview/","text":"Linear Regression \u00b6 Population Regression Model \u00b6 Regression is a statistical model that relates a Dependent Variable (DV) to one or more Independent Variables (IV). The dependent variable is regressed on to the independent variable. They are fundamentally a function of the independent variables and several Regression Parameters , \\(\\beta\\) . The functional form of the regression is based on the relationship between the variables. The goal is to use a model that best captures the relationship between the variables. \\[ Y = f(X, \\beta) \\] Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) To be precise, for every set of IVs, the DV has a Conditional Distribution dependent on the given IV. For instance, the the \\(Y\\) could take any possible value (Marginal Distribution), but given these set of \\(X\\) , the possible values can be narrowed down to a certain range (conditional distribution). \\[ \\displaylines{ Y \\sim Distribution \\\\ Y|X \\sim Conditional~Distribution} \\] Thus, the output of the regression model is actually the Expected Value of the conditional distribution, \\(E(Y|X)\\) , for every possible \\(X\\) . \\[ E(Y|X) = f(X, \\beta) \\] The actual observations are unlikely to be exactly equal to its expectation, thus there is a difference between an observation and the corresponding regression output. It known as the Random Error Term which accounts for all other factors that affect the DV that are not captured in the regression. This means that the relationship between the \\(Y\\) and \\(X\\) is only approximate , as the true relationship is probably different due to the possibility of unaccounted variables. Note that the sign of the errors are significant - positive implies the actual value lies above the regression output while negative implies it lies below. \\[ \\varepsilon_i = y_i - f(x_i, \\beta) \\] This means that \\(Y\\) (not its expectation!) can be expressed as a sum of the regression model and the error terms: \\[ y_i = f(x_i,\\beta) + \\varepsilon_i \\] The Regression is known as the Systematic component as it is shared among all observations The Error is known as the Non-Systematic component as it is unique to each observation Sample Regression Model \u00b6 In practice, the population is unobservable hence it is impossible to construct the population regression model. Instead, a regression model is constructed from a sample instead, which aims to estimate the population model. \\[ \\hat{y} = f(X,\\hat{\\beta}) \\] Similarly, the output of this model can be compared to the actual observations. However, the resulting difference is known as the Residual of the model, which like all the other components, is an estimate for the Error term. \\[ \\hat{\\varepsilon_i} = y_i - \\hat{y_i} \\] There are several different methods to estimate the regression parameters, but they usually involve minimizing the residuals of the model, such that the resulting model best fits the given sample, which is why it is also known as the Fitted Regression Model . Hypothesis Testing \u00b6 Once a regression model has been fit, the next step is to determine if the relationship found in the sample is indicative of a relationship in the population. This can be determined through the following two-sided hypothesis test : Null Hypothesis: \\(\\beta_1 = 0\\) Alternative Hypothesis: \\(\\beta_1 \\ne 0\\) Under the null, the regression parameters are assumed to be 0, implying that there is no relationship between \\(Y\\) and \\(X\\) . The test should reject the null , proving that there IS a relationship between the DV and IVs. Prediction \u00b6 Once the best model has been determined, it can be used to make Predictions about future unobserved values. Let these future values be denoted by the subscript \\(*\\) . These unobserved DVs come from the population, thus can be expressed as a function of the population model: \\[ y_* = f(x_*, \\beta) + \\varepsilon_* \\] The corresponding values from the sample regression model is an estimate for this unobserved value: \\[ \\hat{y}_* = f(x_*, \\hat{\\beta}) \\] Like before, the predicted value is unlikely to be exactly equal to the actual value. Thus, the difference between both values can be measured as the Prediction Error : \\[ y_* - \\hat{y_*} = \\varepsilon_* + [f(x_*, \\beta) - f(x_*, \\hat{\\beta})] \\] The prediction error is thus made up of two components : Inherent error present in the DV ( \\(\\varepsilon_*\\) ) Error in estimating the population model ( \\(f(x_*, \\beta) - f(x_*, \\hat{\\beta})\\) ) Based on the distribution of the prediction error, a Prediction Interval at a given confidence level can be calculated to accompany the regression estimate, which is essentially a confidence interval for the predicted value . Note that the prediction intervals will always be wider than confidence intervals . This is because CIs only takes into the account the error in estimating the population model/parameters while PIs take into account the inherent error of the DV as well.","title":"Regression Overview"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/1.%20Regression%20Overview/#linear-regression","text":"","title":"Linear Regression"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/1.%20Regression%20Overview/#population-regression-model","text":"Regression is a statistical model that relates a Dependent Variable (DV) to one or more Independent Variables (IV). The dependent variable is regressed on to the independent variable. They are fundamentally a function of the independent variables and several Regression Parameters , \\(\\beta\\) . The functional form of the regression is based on the relationship between the variables. The goal is to use a model that best captures the relationship between the variables. \\[ Y = f(X, \\beta) \\] Independent Variable(s) Dependent Variables Variable used to make predictions Variable being predicted Free to change the value Depends on the value of indepenent variable Deterministic Random Variable Denoted as \\(X\\) Denoted as \\(Y\\) To be precise, for every set of IVs, the DV has a Conditional Distribution dependent on the given IV. For instance, the the \\(Y\\) could take any possible value (Marginal Distribution), but given these set of \\(X\\) , the possible values can be narrowed down to a certain range (conditional distribution). \\[ \\displaylines{ Y \\sim Distribution \\\\ Y|X \\sim Conditional~Distribution} \\] Thus, the output of the regression model is actually the Expected Value of the conditional distribution, \\(E(Y|X)\\) , for every possible \\(X\\) . \\[ E(Y|X) = f(X, \\beta) \\] The actual observations are unlikely to be exactly equal to its expectation, thus there is a difference between an observation and the corresponding regression output. It known as the Random Error Term which accounts for all other factors that affect the DV that are not captured in the regression. This means that the relationship between the \\(Y\\) and \\(X\\) is only approximate , as the true relationship is probably different due to the possibility of unaccounted variables. Note that the sign of the errors are significant - positive implies the actual value lies above the regression output while negative implies it lies below. \\[ \\varepsilon_i = y_i - f(x_i, \\beta) \\] This means that \\(Y\\) (not its expectation!) can be expressed as a sum of the regression model and the error terms: \\[ y_i = f(x_i,\\beta) + \\varepsilon_i \\] The Regression is known as the Systematic component as it is shared among all observations The Error is known as the Non-Systematic component as it is unique to each observation","title":"Population Regression Model"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/1.%20Regression%20Overview/#sample-regression-model","text":"In practice, the population is unobservable hence it is impossible to construct the population regression model. Instead, a regression model is constructed from a sample instead, which aims to estimate the population model. \\[ \\hat{y} = f(X,\\hat{\\beta}) \\] Similarly, the output of this model can be compared to the actual observations. However, the resulting difference is known as the Residual of the model, which like all the other components, is an estimate for the Error term. \\[ \\hat{\\varepsilon_i} = y_i - \\hat{y_i} \\] There are several different methods to estimate the regression parameters, but they usually involve minimizing the residuals of the model, such that the resulting model best fits the given sample, which is why it is also known as the Fitted Regression Model .","title":"Sample Regression Model"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/1.%20Regression%20Overview/#hypothesis-testing","text":"Once a regression model has been fit, the next step is to determine if the relationship found in the sample is indicative of a relationship in the population. This can be determined through the following two-sided hypothesis test : Null Hypothesis: \\(\\beta_1 = 0\\) Alternative Hypothesis: \\(\\beta_1 \\ne 0\\) Under the null, the regression parameters are assumed to be 0, implying that there is no relationship between \\(Y\\) and \\(X\\) . The test should reject the null , proving that there IS a relationship between the DV and IVs.","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/1.%20Regression%20Overview/#prediction","text":"Once the best model has been determined, it can be used to make Predictions about future unobserved values. Let these future values be denoted by the subscript \\(*\\) . These unobserved DVs come from the population, thus can be expressed as a function of the population model: \\[ y_* = f(x_*, \\beta) + \\varepsilon_* \\] The corresponding values from the sample regression model is an estimate for this unobserved value: \\[ \\hat{y}_* = f(x_*, \\hat{\\beta}) \\] Like before, the predicted value is unlikely to be exactly equal to the actual value. Thus, the difference between both values can be measured as the Prediction Error : \\[ y_* - \\hat{y_*} = \\varepsilon_* + [f(x_*, \\beta) - f(x_*, \\hat{\\beta})] \\] The prediction error is thus made up of two components : Inherent error present in the DV ( \\(\\varepsilon_*\\) ) Error in estimating the population model ( \\(f(x_*, \\beta) - f(x_*, \\hat{\\beta})\\) ) Based on the distribution of the prediction error, a Prediction Interval at a given confidence level can be calculated to accompany the regression estimate, which is essentially a confidence interval for the predicted value . Note that the prediction intervals will always be wider than confidence intervals . This is because CIs only takes into the account the error in estimating the population model/parameters while PIs take into account the inherent error of the DV as well.","title":"Prediction"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/10.%20Clustering/","text":"","title":"10. Clustering"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/2.%20Simple%20Linear%20Regression/","text":"Simple Linear Regression \u00b6 Simple Linear Regression (SLR) assumes a Linear Relationship between a Numeric DV and single continuous quantitative IV. The model is considered to be simple because it only contains a single independent variable. \\[ E(Y|X) = \\beta_0 + \\beta_1 X \\] \\(\\beta_0\\) \\(\\beta_1\\) Expected value of \\(Y\\) when \\(X = 0\\) Change in the expected value of \\(Y\\) given a one unit increase in \\(X\\) Intercept Parameter Slope Parameter Each observation can also be expressed as sum of the regression and its error term: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\] Ordinary Least Squares \u00b6 SLR parameters are estimated using the Ordinary Least Squares method, which minimizes the Sum of Squared Residuals of the fitted model. It is commonly referred to as the Residual Sum Squared (RSS). \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 \\] The minimization is solved through calculus by setting the partial derivatives of the RSS to 0: For the intercept parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_0} RSS &= 0 \\\\ -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum y_i - \\sum \\hat{\\beta}_0 - \\sum \\hat{\\beta}_1 x &= 0 \\\\ n\\bar{y} -n\\hat{\\beta}_0 - n\\hat{\\beta}_1 \\bar{x} &= 0 \\\\ \\end{aligned} \\] \\[\\therefore \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] For the slope parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_1} RSS &= 0 \\\\ -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum (y_i x_i) - \\hat{\\beta}_0 \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} + n\\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} &= \\hat{\\beta}_1 \\sum (x^2_i) - n\\bar{x}^2 \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 = \\frac{\\sum (x_i y_i) - n\\bar{x}\\bar{y}}{\\sum (x_i^2) - n\\bar{x}^2} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = r * \\frac{s_y}{s_x} \\] This results in the following fitted regression model, which can be graphically expressed as a Regression Line : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] Note that it should \\(\\hat{\\varepsilon}\\) in the above image, not \\(e_i\\) . OLS Properties \u00b6 By re-arranging the formula for \\(\\hat{\\beta}_0\\) , we can show that \\((\\bar{x}, \\bar{y})\\) always lies on the fitted regression model: \\[ \\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} \\] Additionally, since the parameters are estimated through minimization, the resulting model must always fulfil the two first order conditions . The model thus has \\(n-2\\) degrees of freedom to reflect these \"constraints\". \\(\\beta_0\\) FOC \\(\\beta_1\\) FOC \\(\\frac{\\partial}{\\partial \\beta_0} = -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\frac{\\partial}{\\partial \\beta_1} = -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\sum \\hat{\\varepsilon_i} = 0\\) \\(\\sum x_i \\hat{\\varepsilon_i} = 0\\) Residuals are negatively correlated Residuals and Independendent variables are uncorrelated Using the above results, we can also show the following that the mean of the regression outputs is equal to the mean of the population: \\[ \\begin{aligned} \\hat{\\varepsilon_i} &= y_i - \\hat{y_i} \\\\ \\sum \\hat{\\varepsilon_i} &= \\sum y_i - \\hat{y_i} \\\\ 0 &= n\\bar{y} - n\\bar{\\hat{y}} \\\\ \\bar{\\hat{y}} &= \\bar{y} \\\\ \\end{aligned} \\] Goodness of Fit \u00b6 Ideally, the regression should fit the sample closely, having as small as residuals as possible . The size of all the residuals in the model can be summarized through the RSS. The lower the RSS, the better the fit of the model. Recall that the residuals naturally sum to 0 under OLS - the residuals are thus squared to remove the sign so that they can be summed together. \\[ RSS = \\sum (y_i - \\hat{y})^2 \\] However, the SSR on its own is hard to intepret as there is no indication of how low or high it actually is. Thus, the Total Sum of Squares (TSS) can be used as a benchmark for the RSS as it is at least equal to or higher than the RSS. The TSS represents the RSS for a Null Regression - a model with containing only the intercept parameter . The output of this regression is always the sample mean \\(\\bar{y}\\) , which is used for the computation of its residuals. It represents the worst possible model which thus has the highest possible RSS . The lower the RSS compared to the TSS, the better the fit of the model. \\[ TSS = \\sum (y_i - \\bar{y}) \\] Null Model \u00b6 Consider a regression with only the intercept; \\(\\beta_1 = 0\\) . It is known as the Null Model as there are no independent variables used. \\[ y = \\beta_0 \\] We can estimate \\(\\hat{\\beta_0}\\) using OLS, which results in the following result: \\[ \\begin{aligned} -2 \\sum (y_i - \\hat{\\beta_0}) &= 0 \\\\ n \\bar{y} - n \\hat{\\beta_0} &= 0 \\\\ \\hat{\\beta_0} &= \\bar{y} \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{y} = \\bar{y} \\] Sum of Squares \u00b6 The TSS can be further decomposed into two more parts for analysis: \\[ \\begin{aligned} TSS &= \\sum (y_i - \\bar{y})^2 \\\\ TSS &= \\sum[(y_i - \\hat{y}) + (\\hat{y}-\\bar{y})]^2 \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 2 \\sum((y_i - \\hat{y})(\\hat{y}-\\bar{y})) \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 0 \\\\ TSS &= RSS + RegSS \\end{aligned} \\] Residual SS (RSS) Regression SS (RegSS) \\(\\sum(\\hat{y}-\\bar{y})^2\\) \\(\\sum(y_i - \\hat{y})^2\\) Variation of the observed values about the regression Variation of the regression output about the sample mean Variation explained by the regression Variation unexplained by the regression Note that it can also be expressed in terms of the Slope Parameter : \\[ \\begin{align} RegSS &= \\sum(\\hat{y}-\\bar{y})^2 \\\\ &= \\sum(\\hat{\\beta}_0 + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum(\\bar{y} - \\beta_1 \\bar{x} + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum[\\hat{\\beta}_1 (x_i - \\bar{x})]^2 \\\\ &= \\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2 \\\\ \\end{align} \\] The Coefficient of Determination \\(R^2\\) can also be used to demonstrate goodness of fit. It measures the proportion of variation explained by the regression model: \\[ R^2 = \\frac{RegSS}{TSS} = 1 - \\frac{RSS}{TSS} \\] Building off the above expression, it can also be expressed in terms of the Sample Correlation : \\[ \\begin{align} R^2 &= \\frac{\\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\right)^2 \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^4} \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x}) \\sum (y_i - \\bar{y})}\\right)^2 \\\\ &= r_{y,x}^2 \\end{align} \\] Degrees of Freedom \u00b6 The TSS is based on the naive model with only the intercept parameter, thus, it is subject to the single contraint of all residuals summing to 0. The TSS thus has \\(n-1\\) degrees of freedom . The RSS is based on the SLR with both the intercept and slope parameter, thus it is subject to an additional constraint of the sumproduct of all residuals and independent variables being 0. The RSS thus has \\(n-2\\) degrees of freedom. The sum of the RSS and RegSS is equal to the TSS, thus the sum of their degrees of freedom must also be equal to that of the TSS. By working backwards, the RegSS thus has only \\(1\\) degree of freedom . Mean Squared \u00b6 The division of any Sum of Square (TSS, RSS, RegSS) by its Degrees of Freedom is known as the Mean Squared (MS), which is a measure of its average variance . The MS of the TSS is the Unbiased Estimator for Population Variance , which is why this process is known as the Analysis of Variance , as it decomposes the variance of \\(Y\\) into its constituent components: \\[ s = \\frac{TSS}{n-1} \\] The MS of the RSS is known the Mean Squared Residuals , often also referred to as the Mean Squared Error , as it is an estimate for the population variance of the error \\(\\sigma^2\\) : \\[ MS_{\\text{Residuals}} = \\frac{RSS}{n-2} \\] The MS of the RegSS is known as the Mean Squared Regression , which represents the proportion of variance explained per \\(X\\) used. \\[ MS_{\\text{Regression}} = \\frac{RegSS}{1} \\] F Statistic \u00b6 The ANOVA parameters can be used to conduct a hypothesis test to determine if there is in fact a relationship between \\(X\\) and \\(Y\\) : \\(H_0\\) : \\(\\beta_1 = 0\\) \\(H_1\\) : \\(\\beta_1 \\ne 0\\) Under the null hypothesis, there should be no difference between the assumed model and a null model as both only contain the intercept parameter, thus \\(TSS = RSS\\) , where \\(RegSS = 0\\) . Thus, the F-statistic is testing for the equality of variance between the TSS and RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_1 \\ne 0\\) . The F-statistic can be constructed using the sum of squares: \\[ \\begin{aligned} F &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{RegSS/1}{RSS/(n-2)} \\\\ &= \\frac{(TSS - RSS)/1}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{R^2}{1-R^2}, \\text{divide both by TSS} \\end{aligned} \\] Similar to the variance test, it can be shown that under the null, this test-statistic follows an F distribution with \\(1\\) and \\(n-2\\) degrees of freedom: \\[ \\begin{aligned} T &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{\\sigma^2_{RegSS} \\frac{MS_{Reg}}{\\sigma^2_{RegSS}}}{\\sigma^2_{RSS} \\frac{MS_{RSS}}{\\sigma^2_{RSS}}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\frac{1 * MS_{Reg}}{\\sigma^2_{RegSS}} * \\frac{1}{1}}{\\frac{(n-2) * MS_{RegSS}}{\\sigma^2_{RegSS}}* \\frac{1}{n-2}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\chi_1}{\\chi_{n-2}} * (n-2) \\\\ &= 1 * F_{1, n-2} * (n-2) \\\\ &= F_{1, n-2} * (n-2) \\end{aligned} \\] \\[ \\therefore F \\sim F_{1, n-2} \\] ANOVA Table \u00b6 All the above information is then summarized in a table for convenience, known as the ANOVA Table : Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(1\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-2\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - Statistical Inference \u00b6 Sampling Distributions \u00b6 Since the errors are assumed to be normally distributed, then \\(Y\\) is assumed to be normally distributed as well. Since \\(Y\\) is a linear combination of the regression parameters, then the parameters (& their estimates) are normally distributed as well. Both estimates can be expressed in another form that makes it more convenient to find their expectation & variances. \\[ \\begin{aligned} \\hat{\\beta}_1 &= \\frac{\\sum [(x_i - \\bar{x})(y_i - \\bar{y})]}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})y_i}{\\sum (x^2_i - \\bar{x})} - \\frac{\\bar{y} \\sum (x_i - \\bar{x})}{\\sum (x^2_i - \\bar{x})} \\\\ &= \\sum \\frac{(x_i - \\bar{x})}{(x^2_i - \\bar{x})}* y_i - 0 \\\\ &= \\sum w_i * y_i \\\\ \\\\ \\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\ &= \\frac{1}{n} \\sum y_i - \\bar{x} \\sum w_i * y_i \\\\ &= \\sum y_i (\\frac{1}{n} - \\bar{x}w_i) \\end{aligned} \\] \\(w_i\\) is a sort of \"weight\" parameter of the sum of squares. It has three interesting properties that makes it useful: \\[ \\begin{aligned} \\sum w_i &= \\frac{\\sum (x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{n\\bar{x}-n\\bar{x}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{0}{\\sum (x^2_i - \\bar{x})} \\\\ &= 0 \\\\ \\\\ \\sum w_i x_i &= \\frac{\\sum x_i(x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x^2_i - \\bar{x} \\sum x_i)}{\\sum x^2_i - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - \\bar{x}(n \\bar{x})}{\\sum x^2_i - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - 2n\\bar{x}^2 + n\\bar{x}^2} \\\\ &=\\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - n\\bar{x}^2} \\\\ &= 1 \\\\ \\\\ \\sum w_i^2 &= \\frac{\\sum (x_i - \\bar{x})^2}{\\sum (x_i - \\bar{x})^4} \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i) &= n(\\frac{1}{n}) - \\bar{x} \\sum w_i \\\\ &= 1 - 0 \\\\ &= 1 \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i &= \\frac{1}{n} \\sum x_i - \\bar{x} \\sum w_i x_i \\\\ &= \\frac{1}{n} (n\\bar{x}) - \\bar{x} (1) \\\\ &= \\bar{x} - \\bar{x} \\\\ &= 0 \\end{aligned} \\] Using this, the Expectation & Variance can be determined: \\[ \\begin{aligned} E(\\hat{\\beta}_1) &= \\sum w_i E(y_i) \\\\ &= \\sum w_i E(\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum w_i + \\beta_1 \\sum w_i x_i \\\\ &= \\beta_0 (0) + \\beta_1 (1) \\\\ &= \\beta_1\\\\ \\\\ Var(\\hat{\\beta}_1) &= Var(\\sum w_i y_i) \\\\ &= \\sum w_i^2 Var (y_i) \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2} \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}) \\] \\[ \\begin{aligned} E(\\hat{\\beta}_0) &= \\sum (\\frac{1}{n} - \\bar{x}w_i) E(y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i) (\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum (\\frac{1}{n} - \\bar{x}w_i) + \\beta_1 \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i \\\\ &= \\beta_0 (1) + \\beta_1 (0) \\\\ &= \\beta_0 \\\\ \\\\ Var(\\hat{\\beta}_0) &= Var(\\sum (\\frac{1}{n} - \\bar{x}w_i)y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i)^2 Var (y_i) \\\\ &= \\sigma^2 \\sum (\\frac{1}{n^2} -\\frac{2\\bar{x}w_i}{n} + \\bar{x}^2 w_i^2) \\\\ &= \\sigma^2 (\\sum \\frac{1}{n^2} - \\frac{2\\bar{x}}{n} \\sum w_i + \\bar{x}^2 \\sum w_i^2) \\\\ &= \\sigma^2 [n(\\frac{1}{n^2}) - \\frac{2\\bar{x}}{n} (0) + \\bar{x}^2 (\\frac{1}{\\sum (x_i - \\bar{x})^2})] \\\\ &= \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2}) \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_0 \\sim N(\\beta_0, \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2})) \\] Hypothesis Testing \u00b6 Since the regression parameters are normally distributed, a z-statistic can also be used to conduct the tests. However, since the population variance is not known, a t-statistic is used instead: \\[ \\begin{aligned} t &= \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\end{aligned} \\] Since the population variance is estimated by the MSE which has \\(n-2\\) degrees of freedom, the corresponding chi-squared and hence t-distribution has \\(n-2\\) degrees of freedom as well. \\[ \\begin{aligned} \\hat{Var}(\\hat{\\beta_1}) &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} * \\frac{\\frac{1}{n-1}}{\\frac{1}{n-1}} \\\\ &= \\frac{MS_{RSS}}{(n-1) s^2} \\end{aligned} \\] \\[ t \\sim t_{n-2} \\] Since the square of the t-statistic is the F-statistic, both are equivalent ways of doing so and will always lead to the same conclusions. \\[ t^2 \\sim F_{1, n-2} \\] Confidence Intervals \u00b6 Since the distribution of t-statistic is known, it can be used to determine the confidence interval of the estimate: \\[ P\\left(\\text{Margin of Error} < \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} < \\text{Margin of Error}\\right) = 1 - \\alpha \\] \\[ \\text{Confidence Interval} = \\hat{\\beta}_1 \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_1}} \\] Prediction Intervals \u00b6 Consider the Prediction Error of the SLR model: \\[ y_* - \\hat{y_*} = \\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\] Since both \\(y_*\\) and \\(\\hat{y_*}\\) are normally distributed, the prediction errors are normally distributed as well: \\[ \\begin{aligned} E(y_* - \\hat{y_*}) &= E[\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)]] \\\\ &= 0 + \\beta_0 + \\beta_1 E(x_*) - \\beta_0 - \\beta_1 E(x_*) \\\\ &= 0 \\\\ \\\\ Var(y_* - \\hat{y_*}) &= Var(\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= Var(\\varepsilon_*) + Var[(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= ... \\\\ &= \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}] \\end{aligned} \\] \\[ \\therefore y_* - \\hat{y_*} \\sim N\\left(0, \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}]\\right) \\] Similar to before, since the population variance is unknown, it can be approximated using the MSE. Thus, a t-statistic constructed from this sample will have a t-distribution , allowing the following prediction interval to be calculated: \\[ \\text{Prediction Interval} = \\hat{y}_* \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\] Notice that the standard error of the prediction interval increases as \\(x_*\\) moves further away \\(\\bar{x}\\) , indicating that the predictions become less accurate for those values.","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/2.%20Simple%20Linear%20Regression/#simple-linear-regression","text":"Simple Linear Regression (SLR) assumes a Linear Relationship between a Numeric DV and single continuous quantitative IV. The model is considered to be simple because it only contains a single independent variable. \\[ E(Y|X) = \\beta_0 + \\beta_1 X \\] \\(\\beta_0\\) \\(\\beta_1\\) Expected value of \\(Y\\) when \\(X = 0\\) Change in the expected value of \\(Y\\) given a one unit increase in \\(X\\) Intercept Parameter Slope Parameter Each observation can also be expressed as sum of the regression and its error term: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\]","title":"Simple Linear Regression"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/2.%20Simple%20Linear%20Regression/#ordinary-least-squares","text":"SLR parameters are estimated using the Ordinary Least Squares method, which minimizes the Sum of Squared Residuals of the fitted model. It is commonly referred to as the Residual Sum Squared (RSS). \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 \\] The minimization is solved through calculus by setting the partial derivatives of the RSS to 0: For the intercept parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_0} RSS &= 0 \\\\ -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum y_i - \\sum \\hat{\\beta}_0 - \\sum \\hat{\\beta}_1 x &= 0 \\\\ n\\bar{y} -n\\hat{\\beta}_0 - n\\hat{\\beta}_1 \\bar{x} &= 0 \\\\ \\end{aligned} \\] \\[\\therefore \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\] For the slope parameter, \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\beta_1} RSS &= 0 \\\\ -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] &= 0 \\\\ \\sum (y_i x_i) - \\hat{\\beta}_0 \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\sum x_i - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} + n\\hat{\\beta}_1 \\bar{x}^2 - \\hat{\\beta}_1 \\sum x^2_i &= 0 \\\\ \\sum (y_i x_i) - n\\bar{x}\\bar{y} &= \\hat{\\beta}_1 \\sum (x^2_i) - n\\bar{x}^2 \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 = \\frac{\\sum (x_i y_i) - n\\bar{x}\\bar{y}}{\\sum (x_i^2) - n\\bar{x}^2} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = r * \\frac{s_y}{s_x} \\] This results in the following fitted regression model, which can be graphically expressed as a Regression Line : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] Note that it should \\(\\hat{\\varepsilon}\\) in the above image, not \\(e_i\\) .","title":"Ordinary Least Squares"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/2.%20Simple%20Linear%20Regression/#ols-properties","text":"By re-arranging the formula for \\(\\hat{\\beta}_0\\) , we can show that \\((\\bar{x}, \\bar{y})\\) always lies on the fitted regression model: \\[ \\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} \\] Additionally, since the parameters are estimated through minimization, the resulting model must always fulfil the two first order conditions . The model thus has \\(n-2\\) degrees of freedom to reflect these \"constraints\". \\(\\beta_0\\) FOC \\(\\beta_1\\) FOC \\(\\frac{\\partial}{\\partial \\beta_0} = -2 \\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\frac{\\partial}{\\partial \\beta_1} = -2 \\sum x_i [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)] = 0\\) \\(\\sum \\hat{\\varepsilon_i} = 0\\) \\(\\sum x_i \\hat{\\varepsilon_i} = 0\\) Residuals are negatively correlated Residuals and Independendent variables are uncorrelated Using the above results, we can also show the following that the mean of the regression outputs is equal to the mean of the population: \\[ \\begin{aligned} \\hat{\\varepsilon_i} &= y_i - \\hat{y_i} \\\\ \\sum \\hat{\\varepsilon_i} &= \\sum y_i - \\hat{y_i} \\\\ 0 &= n\\bar{y} - n\\bar{\\hat{y}} \\\\ \\bar{\\hat{y}} &= \\bar{y} \\\\ \\end{aligned} \\]","title":"OLS Properties"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/2.%20Simple%20Linear%20Regression/#goodness-of-fit","text":"Ideally, the regression should fit the sample closely, having as small as residuals as possible . The size of all the residuals in the model can be summarized through the RSS. The lower the RSS, the better the fit of the model. Recall that the residuals naturally sum to 0 under OLS - the residuals are thus squared to remove the sign so that they can be summed together. \\[ RSS = \\sum (y_i - \\hat{y})^2 \\] However, the SSR on its own is hard to intepret as there is no indication of how low or high it actually is. Thus, the Total Sum of Squares (TSS) can be used as a benchmark for the RSS as it is at least equal to or higher than the RSS. The TSS represents the RSS for a Null Regression - a model with containing only the intercept parameter . The output of this regression is always the sample mean \\(\\bar{y}\\) , which is used for the computation of its residuals. It represents the worst possible model which thus has the highest possible RSS . The lower the RSS compared to the TSS, the better the fit of the model. \\[ TSS = \\sum (y_i - \\bar{y}) \\]","title":"Goodness of Fit"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/2.%20Simple%20Linear%20Regression/#null-model","text":"Consider a regression with only the intercept; \\(\\beta_1 = 0\\) . It is known as the Null Model as there are no independent variables used. \\[ y = \\beta_0 \\] We can estimate \\(\\hat{\\beta_0}\\) using OLS, which results in the following result: \\[ \\begin{aligned} -2 \\sum (y_i - \\hat{\\beta_0}) &= 0 \\\\ n \\bar{y} - n \\hat{\\beta_0} &= 0 \\\\ \\hat{\\beta_0} &= \\bar{y} \\\\ \\end{aligned} \\] \\[ \\therefore \\hat{y} = \\bar{y} \\]","title":"Null Model"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/2.%20Simple%20Linear%20Regression/#sum-of-squares","text":"The TSS can be further decomposed into two more parts for analysis: \\[ \\begin{aligned} TSS &= \\sum (y_i - \\bar{y})^2 \\\\ TSS &= \\sum[(y_i - \\hat{y}) + (\\hat{y}-\\bar{y})]^2 \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 2 \\sum((y_i - \\hat{y})(\\hat{y}-\\bar{y})) \\\\ TSS &= \\sum(y_i - \\hat{y})^2 + \\sum(\\hat{y}-\\bar{y})^2 + 0 \\\\ TSS &= RSS + RegSS \\end{aligned} \\] Residual SS (RSS) Regression SS (RegSS) \\(\\sum(\\hat{y}-\\bar{y})^2\\) \\(\\sum(y_i - \\hat{y})^2\\) Variation of the observed values about the regression Variation of the regression output about the sample mean Variation explained by the regression Variation unexplained by the regression Note that it can also be expressed in terms of the Slope Parameter : \\[ \\begin{align} RegSS &= \\sum(\\hat{y}-\\bar{y})^2 \\\\ &= \\sum(\\hat{\\beta}_0 + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum(\\bar{y} - \\beta_1 \\bar{x} + \\hat{\\beta}_1 x - \\bar{y})^2 \\\\ &= \\sum[\\hat{\\beta}_1 (x_i - \\bar{x})]^2 \\\\ &= \\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2 \\\\ \\end{align} \\] The Coefficient of Determination \\(R^2\\) can also be used to demonstrate goodness of fit. It measures the proportion of variation explained by the regression model: \\[ R^2 = \\frac{RegSS}{TSS} = 1 - \\frac{RSS}{TSS} \\] Building off the above expression, it can also be expressed in terms of the Sample Correlation : \\[ \\begin{align} R^2 &= \\frac{\\hat{\\beta}^2_1 \\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\right)^2 \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^4} \\frac{\\sum(x_i - \\bar{x})^2}{\\sum (y_i - \\bar{y}) ^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})^2 (y_i - \\bar{y})^2}{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2} \\\\ &= \\left(\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x}) \\sum (y_i - \\bar{y})}\\right)^2 \\\\ &= r_{y,x}^2 \\end{align} \\]","title":"Sum of Squares"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/2.%20Simple%20Linear%20Regression/#degrees-of-freedom","text":"The TSS is based on the naive model with only the intercept parameter, thus, it is subject to the single contraint of all residuals summing to 0. The TSS thus has \\(n-1\\) degrees of freedom . The RSS is based on the SLR with both the intercept and slope parameter, thus it is subject to an additional constraint of the sumproduct of all residuals and independent variables being 0. The RSS thus has \\(n-2\\) degrees of freedom. The sum of the RSS and RegSS is equal to the TSS, thus the sum of their degrees of freedom must also be equal to that of the TSS. By working backwards, the RegSS thus has only \\(1\\) degree of freedom .","title":"Degrees of Freedom"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/2.%20Simple%20Linear%20Regression/#mean-squared","text":"The division of any Sum of Square (TSS, RSS, RegSS) by its Degrees of Freedom is known as the Mean Squared (MS), which is a measure of its average variance . The MS of the TSS is the Unbiased Estimator for Population Variance , which is why this process is known as the Analysis of Variance , as it decomposes the variance of \\(Y\\) into its constituent components: \\[ s = \\frac{TSS}{n-1} \\] The MS of the RSS is known the Mean Squared Residuals , often also referred to as the Mean Squared Error , as it is an estimate for the population variance of the error \\(\\sigma^2\\) : \\[ MS_{\\text{Residuals}} = \\frac{RSS}{n-2} \\] The MS of the RegSS is known as the Mean Squared Regression , which represents the proportion of variance explained per \\(X\\) used. \\[ MS_{\\text{Regression}} = \\frac{RegSS}{1} \\]","title":"Mean Squared"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/2.%20Simple%20Linear%20Regression/#f-statistic","text":"The ANOVA parameters can be used to conduct a hypothesis test to determine if there is in fact a relationship between \\(X\\) and \\(Y\\) : \\(H_0\\) : \\(\\beta_1 = 0\\) \\(H_1\\) : \\(\\beta_1 \\ne 0\\) Under the null hypothesis, there should be no difference between the assumed model and a null model as both only contain the intercept parameter, thus \\(TSS = RSS\\) , where \\(RegSS = 0\\) . Thus, the F-statistic is testing for the equality of variance between the TSS and RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_1 \\ne 0\\) . The F-statistic can be constructed using the sum of squares: \\[ \\begin{aligned} F &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{RegSS/1}{RSS/(n-2)} \\\\ &= \\frac{(TSS - RSS)/1}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{R^2}{1-R^2}, \\text{divide both by TSS} \\end{aligned} \\] Similar to the variance test, it can be shown that under the null, this test-statistic follows an F distribution with \\(1\\) and \\(n-2\\) degrees of freedom: \\[ \\begin{aligned} T &= \\frac{MS_{RegSS}}{MS_{RSS}} \\\\ &= \\frac{\\sigma^2_{RegSS} \\frac{MS_{Reg}}{\\sigma^2_{RegSS}}}{\\sigma^2_{RSS} \\frac{MS_{RSS}}{\\sigma^2_{RSS}}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\frac{1 * MS_{Reg}}{\\sigma^2_{RegSS}} * \\frac{1}{1}}{\\frac{(n-2) * MS_{RegSS}}{\\sigma^2_{RegSS}}* \\frac{1}{n-2}} \\\\ &= \\frac{\\sigma^2_{RegSS}}{\\sigma^2_{RSS}} * \\frac{\\chi_1}{\\chi_{n-2}} * (n-2) \\\\ &= 1 * F_{1, n-2} * (n-2) \\\\ &= F_{1, n-2} * (n-2) \\end{aligned} \\] \\[ \\therefore F \\sim F_{1, n-2} \\]","title":"F Statistic"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/2.%20Simple%20Linear%20Regression/#anova-table","text":"All the above information is then summarized in a table for convenience, known as the ANOVA Table : Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(1\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-2\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) -","title":"ANOVA Table"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/2.%20Simple%20Linear%20Regression/#statistical-inference","text":"","title":"Statistical Inference"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/2.%20Simple%20Linear%20Regression/#sampling-distributions","text":"Since the errors are assumed to be normally distributed, then \\(Y\\) is assumed to be normally distributed as well. Since \\(Y\\) is a linear combination of the regression parameters, then the parameters (& their estimates) are normally distributed as well. Both estimates can be expressed in another form that makes it more convenient to find their expectation & variances. \\[ \\begin{aligned} \\hat{\\beta}_1 &= \\frac{\\sum [(x_i - \\bar{x})(y_i - \\bar{y})]}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x_i - \\bar{x})y_i}{\\sum (x^2_i - \\bar{x})} - \\frac{\\bar{y} \\sum (x_i - \\bar{x})}{\\sum (x^2_i - \\bar{x})} \\\\ &= \\sum \\frac{(x_i - \\bar{x})}{(x^2_i - \\bar{x})}* y_i - 0 \\\\ &= \\sum w_i * y_i \\\\ \\\\ \\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\ &= \\frac{1}{n} \\sum y_i - \\bar{x} \\sum w_i * y_i \\\\ &= \\sum y_i (\\frac{1}{n} - \\bar{x}w_i) \\end{aligned} \\] \\(w_i\\) is a sort of \"weight\" parameter of the sum of squares. It has three interesting properties that makes it useful: \\[ \\begin{aligned} \\sum w_i &= \\frac{\\sum (x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{n\\bar{x}-n\\bar{x}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{0}{\\sum (x^2_i - \\bar{x})} \\\\ &= 0 \\\\ \\\\ \\sum w_i x_i &= \\frac{\\sum x_i(x_i - \\bar{x})}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sum (x^2_i - \\bar{x} \\sum x_i)}{\\sum x^2_i - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - \\bar{x}(n \\bar{x})}{\\sum x^2_i - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2} \\\\ &= \\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - 2n\\bar{x}^2 + n\\bar{x}^2} \\\\ &=\\frac{\\sum x^2_i - n\\bar{x}^2}{\\sum x^2_i - n\\bar{x}^2} \\\\ &= 1 \\\\ \\\\ \\sum w_i^2 &= \\frac{\\sum (x_i - \\bar{x})^2}{\\sum (x_i - \\bar{x})^4} \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i) &= n(\\frac{1}{n}) - \\bar{x} \\sum w_i \\\\ &= 1 - 0 \\\\ &= 1 \\\\ \\\\ \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i &= \\frac{1}{n} \\sum x_i - \\bar{x} \\sum w_i x_i \\\\ &= \\frac{1}{n} (n\\bar{x}) - \\bar{x} (1) \\\\ &= \\bar{x} - \\bar{x} \\\\ &= 0 \\end{aligned} \\] Using this, the Expectation & Variance can be determined: \\[ \\begin{aligned} E(\\hat{\\beta}_1) &= \\sum w_i E(y_i) \\\\ &= \\sum w_i E(\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum w_i + \\beta_1 \\sum w_i x_i \\\\ &= \\beta_0 (0) + \\beta_1 (1) \\\\ &= \\beta_1\\\\ \\\\ Var(\\hat{\\beta}_1) &= Var(\\sum w_i y_i) \\\\ &= \\sum w_i^2 Var (y_i) \\\\ &= \\frac{1}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2} \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{\\sum (x_i - \\bar{x})^2}) \\] \\[ \\begin{aligned} E(\\hat{\\beta}_0) &= \\sum (\\frac{1}{n} - \\bar{x}w_i) E(y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i) (\\beta_0 + \\beta_1 x_i) \\\\ &= \\beta_0 \\sum (\\frac{1}{n} - \\bar{x}w_i) + \\beta_1 \\sum (\\frac{1}{n} - \\bar{x}w_i)x_i \\\\ &= \\beta_0 (1) + \\beta_1 (0) \\\\ &= \\beta_0 \\\\ \\\\ Var(\\hat{\\beta}_0) &= Var(\\sum (\\frac{1}{n} - \\bar{x}w_i)y_i) \\\\ &= \\sum (\\frac{1}{n} - \\bar{x}w_i)^2 Var (y_i) \\\\ &= \\sigma^2 \\sum (\\frac{1}{n^2} -\\frac{2\\bar{x}w_i}{n} + \\bar{x}^2 w_i^2) \\\\ &= \\sigma^2 (\\sum \\frac{1}{n^2} - \\frac{2\\bar{x}}{n} \\sum w_i + \\bar{x}^2 \\sum w_i^2) \\\\ &= \\sigma^2 [n(\\frac{1}{n^2}) - \\frac{2\\bar{x}}{n} (0) + \\bar{x}^2 (\\frac{1}{\\sum (x_i - \\bar{x})^2})] \\\\ &= \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2}) \\end{aligned} \\] \\[ \\therefore \\hat{\\beta}_0 \\sim N(\\beta_0, \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2})) \\]","title":"Sampling Distributions"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/2.%20Simple%20Linear%20Regression/#hypothesis-testing","text":"Since the regression parameters are normally distributed, a z-statistic can also be used to conduct the tests. However, since the population variance is not known, a t-statistic is used instead: \\[ \\begin{aligned} t &= \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\end{aligned} \\] Since the population variance is estimated by the MSE which has \\(n-2\\) degrees of freedom, the corresponding chi-squared and hence t-distribution has \\(n-2\\) degrees of freedom as well. \\[ \\begin{aligned} \\hat{Var}(\\hat{\\beta_1}) &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} \\\\ &= \\frac{MS_{RSS}}{\\sum (x_i - \\bar{x})^2} * \\frac{\\frac{1}{n-1}}{\\frac{1}{n-1}} \\\\ &= \\frac{MS_{RSS}}{(n-1) s^2} \\end{aligned} \\] \\[ t \\sim t_{n-2} \\] Since the square of the t-statistic is the F-statistic, both are equivalent ways of doing so and will always lead to the same conclusions. \\[ t^2 \\sim F_{1, n-2} \\]","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/2.%20Simple%20Linear%20Regression/#confidence-intervals","text":"Since the distribution of t-statistic is known, it can be used to determine the confidence interval of the estimate: \\[ P\\left(\\text{Margin of Error} < \\frac{\\hat{\\beta_1} - \\beta_1}{\\hat{\\sigma}_{\\hat{\\beta_1}}} < \\text{Margin of Error}\\right) = 1 - \\alpha \\] \\[ \\text{Confidence Interval} = \\hat{\\beta}_1 \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_1}} \\]","title":"Confidence Intervals"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/2.%20Simple%20Linear%20Regression/#prediction-intervals","text":"Consider the Prediction Error of the SLR model: \\[ y_* - \\hat{y_*} = \\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\] Since both \\(y_*\\) and \\(\\hat{y_*}\\) are normally distributed, the prediction errors are normally distributed as well: \\[ \\begin{aligned} E(y_* - \\hat{y_*}) &= E[\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)]] \\\\ &= 0 + \\beta_0 + \\beta_1 E(x_*) - \\beta_0 - \\beta_1 E(x_*) \\\\ &= 0 \\\\ \\\\ Var(y_* - \\hat{y_*}) &= Var(\\varepsilon_* + [(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= Var(\\varepsilon_*) + Var[(\\beta_0 + \\beta_1 x_*) - (\\hat{\\beta_0} + \\hat{\\beta_1}x_*)] \\\\ &= ... \\\\ &= \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}] \\end{aligned} \\] \\[ \\therefore y_* - \\hat{y_*} \\sim N\\left(0, \\sigma^2 [1 + \\frac{1}{n} + \\frac{(x_* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}]\\right) \\] Similar to before, since the population variance is unknown, it can be approximated using the MSE. Thus, a t-statistic constructed from this sample will have a t-distribution , allowing the following prediction interval to be calculated: \\[ \\text{Prediction Interval} = \\hat{y}_* \\pm t_{n-2, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\] Notice that the standard error of the prediction interval increases as \\(x_*\\) moves further away \\(\\bar{x}\\) , indicating that the predictions become less accurate for those values.","title":"Prediction Intervals"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/3.%20Multiple%20Linear%20Regression/","text":"Multiple Linear Regression \u00b6 The natural extension of the SLR model is to include more than one independent variable , which thus results in the more generalized Multiple Linear Regression (MLR) model. \\[ E(Y|X_1, ... X_p) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_{p}X_{p} \\] Unlike the SLR which studies how each individual IV influences the DV, the goal of the MLR model is to study how all the IVs operate together to influence the DV. \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) when \\(X_1 = X_2 = ... = 0\\) Change in \\(E(Y)\\) given a one unit increase in \\(X_j\\) , holding all other \\(X\\) 's constant Intercept Parameter \"Slope\" Parameter For avoidance of doubt, the subscript \\(i\\) will be used to denote observations while \\(j\\) will be used to denote independent variables. Every observation can also be expressed as the sum of the regression and the error term. However, due to the multi-dimensional nature of the model, it is commonly expressed in matrix notation: \\[ \\begin{aligned} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} &= \\begin{pmatrix} 1 & x_{11} & x_{12} & ... & x_{1p} \\\\ 1 & x_{21} & x_{22} & ... & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & ... & x_{np} \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix} + \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} \\\\ \\boldsymbol{y} &= \\boldsymbol{X\\beta + \\varepsilon} \\end{aligned} \\] Ordinary Least Squares \u00b6 Similar to the SLR model, the regression parameters can be found by minimizing the sum of squared residuals: \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_p x_ip)]^2 \\] There are \\(p+1\\) FOC equations to solve through the minimization, with the additional one reflecting the intercept parameter. It is difficult to algebraically solve this system of equations, thus there is no closed form tsolution for each individual paramater. Instead, here is a vector solution for all of the parameters: \\[ \\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta_0} \\\\ \\hat{\\beta_1} \\\\ \\vdots \\\\ \\hat{\\beta_p} \\end{pmatrix} = (\\boldsymbol{X'X})^{-1}\\boldsymbol{X'y} \\] Note that since there are \\(p+1\\) equations that must be solved, the model has \\(n-p+1\\) degrees of freedom. Following the same logic, there must be at least \\(p+1\\) observations in order to solve the equations and hence construct the model. This results in the following fitted regression model, which can be graphically expressed as a Regression Plane : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_P x_ip \\] Manual Computation \u00b6 The tricky part is that \\((\\boldsymbol{X'X})^{-1}\\) is hard to compute by manually , except for the special case where \\(p=1\\) (SLR). Thus, it is likely that the parameters will be provided by the question. If required to compute them manually, then \\((\\boldsymbol{X'X})^{-1}\\) is likely to be provided. The remaining \\(\\boldsymbol{X'y}\\) still needs to be computed and put together to obtain the regression parameters. However, if the model has \\(p=2\\) but no intercept , then \\((\\boldsymbol{X'X})^{-1}\\) is a 2 x 2 Matrix whose inverse can be easily calculated. Similarly, if \\((\\boldsymbol{X'X})^{-1}\\) is a Diagonal Matrix , its inverse can be easily calculated as well. Goodness of Fit \u00b6 The ANOVA for MLR follows the same intuition as the SLR version, adjusted for the new degrees of freedom: Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(p\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-(p+1)\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - The coefficient of determination still represents the proportion of variance explained by the regression, but has a slightly different formula: \\[ R^2 = \\frac{RegSS}{TSS} = r_{y,\\hat{y}}^2 \\] The multiple IVs of the model are now captured through \\(\\hat{y}\\) instead of \\(x\\) directly. However, the hypothesis test under the MLR is vastly different from the SLR version. Instead of testing if an individual IV is useful, it tests if all the IVs are collectively useful in helping to explain the DV. \\[ \\begin{aligned} H_0 &: \\beta_1 = \\beta_2 = ... = \\beta_p = 0 \\\\ H_1 &: \\text{At least one } \\beta_j \\text{ is non-zero} \\end{aligned} \\] Thus, rejecting the null hypothesis implies that at least one of the IVs used is useful, but does not provide much insight into which of them are useful. Partial F Test \u00b6 A partial F-test can be used to precisely determine which of the IVs are useful in explaining the DV. A regular F test compares the null model with no IVs to the desired model with all the IVs. If the sum of squares are significantly different, then it implies that the additional IVs are jointly useful in explaining \\(Y\\) . The partial F test generalizes this idea. Instead of considering a null model with no IVs, a Reduced Model with a limited number IVs ( \\(q\\) ) is considered instead. Consequently, the desired model is known as the Full Model with all \\(p\\) IVs, where \\(q < p\\) . \\[ \\begin{aligned} H_0: \\beta_{p-q+1} = ... = \\beta_p = 0 \\\\ H_1: \\beta_{p-q+1} = ... = \\beta_p \\ne 0 \\end{aligned} \\] The test is also commonly referred to as the Generalized F test, where the models are referred to as the Restricted and Unrestricted Models. The difference in the RSS between the Full and Reduced Model is known as the Extra Sum of Squares (ExtraSS) . It represents the contribution of the missing variables in explaining the variance of \\(Y\\) . Under the null hypothesis, there should be no difference between the two RSS, and thus \\(ExtraSS = 0\\) . \\[ ExtraSS = RSS_{Reduced} - RSS_{Full} \\] Thus, the Partial F-statistic is testing for the equality of variance between the two RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_{p-q+1} = ... = \\beta_p \\ne 0\\) . \\[ \\begin{aligned} F &= \\frac{MS_{ExtraSS}}{MS_{RSS_{Full}}} \\\\ &= \\frac{ExtraSS/q}{RSS/(n-2)} \\\\ &= \\frac{(RSS_{Reduced} - RSS_{Full})/q}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{(1- R_{Reduced}^2) - (1 - R_{Full}^2)}{1-RSS_{Full}^2}, \\text{divide both by TSS} \\\\ &= (n-2) * \\frac{R_{Full}^2 - R_{Reduced}^2}{1-RSS_{Full}^2} \\end{aligned} \\] Statistical Inference \u00b6 Sampling Distributions \u00b6 Similar to SLR, the regression parameters are normally distributed as well. However, since there are multiple regression parameters, they collectively follow a multivariate normal distribution . \\[ \\hat{\\beta} \\sim N_{p+1}(\\beta, \\sigma^2 (\\boldsymbol{X'X})^{-1}) \\] The variance of the distribution is known as the Variance Covariance Matrix , which provides the covariances between every possible pair of regression parameters. Since the covariance of a variable with itself is its variance, the diagonals are the respective variances of the parameters. Note that the first element of the diagonal is the intercept, thus the variance of the jth IV is the (j+1)th element of the diagonal . \\[ Var(\\hat{\\beta}) = \\begin{pmatrix} Var(\\hat{\\beta}_0) & Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_0, \\hat{\\beta}_p) \\\\ Cov(\\hat{\\beta}_1, \\hat{\\beta}_0) & Var(\\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ Cov(\\hat{\\beta}_p, \\hat{\\beta}_0) & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) & ... & Var(\\hat{\\beta}_p) \\end{pmatrix} \\] Note that the covariances are symmetrical about the diagonal - \\(Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = Cov(\\hat{\\beta}_1, \\hat{\\beta}_0)\\) . Hypothesis Testing \u00b6 Similar to SLR, the t-test can be used to test for the significance of an individual IV, but the intepretation of the test is different from the SLR case. It tests the usefulness of an individual IV in the presence of the other predictors . \\[ \\begin{aligned} H_0: \\beta_j = 0 \\\\ H_1: \\beta_j \\ne 0 \\end{aligned} \\] \\[ t(\\beta_j) = \\frac{\\hat{\\beta_j} - \\beta_j}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\] Recall that the variance of the jth IV si the (j+1)th element of the variance covariance matrix. \\[ t(\\beta_j) \\sim t_{n-p-1} \\] However, this leads to several odd results which needs to be accounted for: Predictor is not significant individually but significant when taken alone . Predictors are not significant individually but significant when taken together . There are now two possible ways to test for the significance of IVs: Conduct a single F-test to test for the joint significance of all IVs Conduct multiple t-tests to test for the joint significance of each IV The problem with the multiple t-test approach lies with the type I error of the tests. For \\(\\alpha = 0.05\\) , the probability of correctly rejecting the null is \\(0.95\\) . Assuming that all the tests are independent, the probability of correctly rejecting all the nulls is \\(0.95^p\\) , which drastically decreases with the number of tests conducted. Given enough predictors, this means that the probability drops to approximately 0, which means that there is bound to be a wrongly rejected null; a type I error is guaranteed even though it was supposed to be limited at a 0.05 chance. The Bonferroni Correction is a method of adjusting \\(\\alpha\\) of each hypothesis test such that the overall type I error is kept at its desired level. However, this has the consequence of increasing the probability of type II errors, which is why it is not popular. The F-test has the advantage of controlling the type I error regardless of the number of predictors , which is why it is preferred for hypothesis testing in the MLR. Confidence Intervals \u00b6 Similar to SLR, the confidence intervals can be constructed using the distribution of the test-statistic: \\[ \\text{Confidence Interval} = \\hat{\\beta}_j \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_j}} \\] Prediction Intervals \u00b6 Unlike in SLR, it is difficult to determine the distribution of the prediction error. Thus, the final result can be found below: \\[ \\begin{aligned} \\text{Prediction Interval} &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\\\ &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\sqrt{s^2 [1 + x'_* (\\boldsymbol{X'X})^-1 * x_*]} \\end{aligned} \\] Despite the result looking more complicated, the key takeaway remains the same - the further away \\(x_*\\) is from \\(\\bar{x}\\) , the greater Variations of MLR \u00b6 Qualitative IV \u00b6 The discussion so far has mostly focused on Quantitative IVs, thus this section will explore Qualitative IVs. They can only take values from a list of pre-defined values, known as the Levels of the variable. In a regression context, most qualitative IVs are represented in the form of a Dummy Variable which can only take two possible levels - Yes (1) or No (0). Note that there are other ways of encoding a dummy variable (-1/0/1 etc), but the principles stay large the same. \\[ \\text{Dummy Variable} = \\begin{cases} 1, & \\text{First Level} \\\\ 0, & \\text{Second Level} \\end{cases} \\] In general, \\(n-1\\) dummy variables are needed to represent a qualitative variable with \\(n\\) levels. This is because the status of the last level can be deduced from the other dummy variables . Thus, including a seperate dummy variable for this last level is redundant and will lead to the problem of Collinearity , which will be explored in a later section. For instance, consider four levels (North, East, South & West), represented by the three dummy variables ( \\(N, E , S\\) ). If any of the variables are 1, they represent their respective direction (North, East & South). If all of them are 0, then the direction is the remaining level (West). The last remaining level is often referred to as the Baseline Level as it is the default level of the variable when all other dummies are 0. Any level can be used as the baseline, but the parameters will differ across models with different baselines. \\[ E(Y|X) = \\beta_0 + \\beta_{1, North} x_{North} + \\beta_{2, East} * x_{East} + \\beta_{3, South} * x_{South} \\] \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) at the baseline level Change in \\(E(Y)\\) from the baseline to the chosen level \\(X_1 = X_2 = ... = X_j = 0\\) \\(X_1 = X_2 = ... = 0; X_j = 1\\) Dummy variables are usually used in conjunction with quantitative ones . This essentially creates a \"seperate\" regression model for each of the levels. For the simplest case of one quantiative and one dummy, \\(\\beta_2\\) is the difference in the intercept of the two resulting SLR models. \\[ E(Y|X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + \\beta_1 x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\] Interaction Model \u00b6 So far, it was assumed that each IV had an independent effect on the DV. However, IVs may interact to produce a joint effect on the DV, where the effect of one IV depends on the value of another IV. For instance, the production of a factory may depend on the number of Machines and Workers. However, the more machines there are, the greater the effect of an additional worker . Thus, this interaction effect can be captured through an Interaction Variable , which is the product of both IVs: \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\\\ &= \\beta_0 + (\\beta_1 + \\beta_3 x_2)x_1 + \\beta_2 x_2 \\\\ &= \\beta_0 + \\beta_1 x_1 + (\\beta_2 + \\beta_3 x_1) x_2 \\\\ \\end{aligned} \\] A one unit increase in \\(x_1\\) will increase E(Y) by \\(\\beta_1 + \\beta_3 x_2\\) , which depends on the value of \\(x_2\\) as well, which is is why they interact with each another. Phrased another way, for every one unit increase in \\(x_2\\) , the change in E(Y) from a unit increase in \\(x_1\\) increases by \\(\\beta_3\\) . Something unusual to take note of is that the Interaction Variable tests significant but the constituent variables do not. In this case, it is common practice to retain both the interaction and the consistuent variables in the model. This is practice is known as the Hierarchical Principle . Models containing dummy variables can also have an interaction effect. Building off the example from the previous section, \\(\\beta_2\\) is still the difference in the intercept but with the new \\(\\beta_3\\) being the difference in slopes of the two resulting SLR models. \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3)x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\] Piecewise Model \u00b6 If the DV has an abrupt change in behaviour over different values of the IVs, it can be accounted for the through the use of a Piecewise Regression . The first method of creating a piecewise regression involves the use of an Indicator Function . It is essentially a dummy variable which depends on the value of the other IVs. \\[ z_{\\{x>=c\\}} = \\begin{cases} 0, & x < c \\\\ 1, & x \\ge c \\end{cases} \\] \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 z(x-c) \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2c) + (\\beta_1 + \\beta_2) x_1, & x \\ge c \\end{cases} \\end{aligned} \\] Note that \\(z(x-c)\\) is treated as a distinct IV and hence can be equivalently expressed as \\(x_2\\) ; the full notation is used here for clarity. \\(c\\) is the value at which the DV abruptly changes in behaviour, known as a Kink in the graph, which continuously connects the two regression lines. The other method is to use an interaction variable instead. Similar to how the interaction variables resulted in the model to \"split\", the model now splits at \\(x = c\\) , resulting in a non-continuous gap. \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2z + \\beta_3 zx \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2) + (\\beta_1 + \\beta_3) x_1, & x \\ge c \\end{cases} \\end{aligned} \\] Polynomial Model \u00b6 If the relationship between the DV and IV is complex (non-linear), then a Polynomial Regression can be used to better model the relationship between the two. \\[ E(Y|X) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 \\] Note that it is the same IV used in the regression, just with additional powers. Although a polynomial regression may capture the true relationship better, the regression parameters become hard to intepret. The partial derivatives can no longer be intepreted as holding other IVs constant as each IV is dependent on the same quantity, just to different powers. \\[ \\frac{\\partial E(Y|X)}{\\partial x} = \\beta_1 + 2\\beta_2 x + ... + m \\beta_m x^{m-1} \\]","title":"Multiple Linear Regression"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/3.%20Multiple%20Linear%20Regression/#multiple-linear-regression","text":"The natural extension of the SLR model is to include more than one independent variable , which thus results in the more generalized Multiple Linear Regression (MLR) model. \\[ E(Y|X_1, ... X_p) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_{p}X_{p} \\] Unlike the SLR which studies how each individual IV influences the DV, the goal of the MLR model is to study how all the IVs operate together to influence the DV. \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) when \\(X_1 = X_2 = ... = 0\\) Change in \\(E(Y)\\) given a one unit increase in \\(X_j\\) , holding all other \\(X\\) 's constant Intercept Parameter \"Slope\" Parameter For avoidance of doubt, the subscript \\(i\\) will be used to denote observations while \\(j\\) will be used to denote independent variables. Every observation can also be expressed as the sum of the regression and the error term. However, due to the multi-dimensional nature of the model, it is commonly expressed in matrix notation: \\[ \\begin{aligned} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} &= \\begin{pmatrix} 1 & x_{11} & x_{12} & ... & x_{1p} \\\\ 1 & x_{21} & x_{22} & ... & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & ... & x_{np} \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix} + \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} \\\\ \\boldsymbol{y} &= \\boldsymbol{X\\beta + \\varepsilon} \\end{aligned} \\]","title":"Multiple Linear Regression"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/3.%20Multiple%20Linear%20Regression/#ordinary-least-squares","text":"Similar to the SLR model, the regression parameters can be found by minimizing the sum of squared residuals: \\[ RSS = \\sum y_i - \\hat{y} =\\sum [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_p x_ip)]^2 \\] There are \\(p+1\\) FOC equations to solve through the minimization, with the additional one reflecting the intercept parameter. It is difficult to algebraically solve this system of equations, thus there is no closed form tsolution for each individual paramater. Instead, here is a vector solution for all of the parameters: \\[ \\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta_0} \\\\ \\hat{\\beta_1} \\\\ \\vdots \\\\ \\hat{\\beta_p} \\end{pmatrix} = (\\boldsymbol{X'X})^{-1}\\boldsymbol{X'y} \\] Note that since there are \\(p+1\\) equations that must be solved, the model has \\(n-p+1\\) degrees of freedom. Following the same logic, there must be at least \\(p+1\\) observations in order to solve the equations and hence construct the model. This results in the following fitted regression model, which can be graphically expressed as a Regression Plane : \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i1 + \\hat{\\beta}_2 x_i2 + ... + \\hat{\\beta}_P x_ip \\]","title":"Ordinary Least Squares"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/3.%20Multiple%20Linear%20Regression/#manual-computation","text":"The tricky part is that \\((\\boldsymbol{X'X})^{-1}\\) is hard to compute by manually , except for the special case where \\(p=1\\) (SLR). Thus, it is likely that the parameters will be provided by the question. If required to compute them manually, then \\((\\boldsymbol{X'X})^{-1}\\) is likely to be provided. The remaining \\(\\boldsymbol{X'y}\\) still needs to be computed and put together to obtain the regression parameters. However, if the model has \\(p=2\\) but no intercept , then \\((\\boldsymbol{X'X})^{-1}\\) is a 2 x 2 Matrix whose inverse can be easily calculated. Similarly, if \\((\\boldsymbol{X'X})^{-1}\\) is a Diagonal Matrix , its inverse can be easily calculated as well.","title":"Manual Computation"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/3.%20Multiple%20Linear%20Regression/#goodness-of-fit","text":"The ANOVA for MLR follows the same intuition as the SLR version, adjusted for the new degrees of freedom: Source Sum of Squares Degrees of Freedom Mean Square F-Statistic Regression RegSS \\(p\\) \\(MS_{RegSS}\\) \\(\\frac{RegSS}{MSE}\\) Error RSS \\(n-(p+1)\\) \\(MSE\\) - Total TSS \\(n-1\\) \\(s^2\\) - The coefficient of determination still represents the proportion of variance explained by the regression, but has a slightly different formula: \\[ R^2 = \\frac{RegSS}{TSS} = r_{y,\\hat{y}}^2 \\] The multiple IVs of the model are now captured through \\(\\hat{y}\\) instead of \\(x\\) directly. However, the hypothesis test under the MLR is vastly different from the SLR version. Instead of testing if an individual IV is useful, it tests if all the IVs are collectively useful in helping to explain the DV. \\[ \\begin{aligned} H_0 &: \\beta_1 = \\beta_2 = ... = \\beta_p = 0 \\\\ H_1 &: \\text{At least one } \\beta_j \\text{ is non-zero} \\end{aligned} \\] Thus, rejecting the null hypothesis implies that at least one of the IVs used is useful, but does not provide much insight into which of them are useful.","title":"Goodness of Fit"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/3.%20Multiple%20Linear%20Regression/#partial-f-test","text":"A partial F-test can be used to precisely determine which of the IVs are useful in explaining the DV. A regular F test compares the null model with no IVs to the desired model with all the IVs. If the sum of squares are significantly different, then it implies that the additional IVs are jointly useful in explaining \\(Y\\) . The partial F test generalizes this idea. Instead of considering a null model with no IVs, a Reduced Model with a limited number IVs ( \\(q\\) ) is considered instead. Consequently, the desired model is known as the Full Model with all \\(p\\) IVs, where \\(q < p\\) . \\[ \\begin{aligned} H_0: \\beta_{p-q+1} = ... = \\beta_p = 0 \\\\ H_1: \\beta_{p-q+1} = ... = \\beta_p \\ne 0 \\end{aligned} \\] The test is also commonly referred to as the Generalized F test, where the models are referred to as the Restricted and Unrestricted Models. The difference in the RSS between the Full and Reduced Model is known as the Extra Sum of Squares (ExtraSS) . It represents the contribution of the missing variables in explaining the variance of \\(Y\\) . Under the null hypothesis, there should be no difference between the two RSS, and thus \\(ExtraSS = 0\\) . \\[ ExtraSS = RSS_{Reduced} - RSS_{Full} \\] Thus, the Partial F-statistic is testing for the equality of variance between the two RSS - if there is a significant difference in the variance of the two, then the null should be rejected and thus \\(\\beta_{p-q+1} = ... = \\beta_p \\ne 0\\) . \\[ \\begin{aligned} F &= \\frac{MS_{ExtraSS}}{MS_{RSS_{Full}}} \\\\ &= \\frac{ExtraSS/q}{RSS/(n-2)} \\\\ &= \\frac{(RSS_{Reduced} - RSS_{Full})/q}{RSS/(n-2)} \\\\ &= (n-2) * \\frac{(1- R_{Reduced}^2) - (1 - R_{Full}^2)}{1-RSS_{Full}^2}, \\text{divide both by TSS} \\\\ &= (n-2) * \\frac{R_{Full}^2 - R_{Reduced}^2}{1-RSS_{Full}^2} \\end{aligned} \\]","title":"Partial F Test"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/3.%20Multiple%20Linear%20Regression/#statistical-inference","text":"","title":"Statistical Inference"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/3.%20Multiple%20Linear%20Regression/#sampling-distributions","text":"Similar to SLR, the regression parameters are normally distributed as well. However, since there are multiple regression parameters, they collectively follow a multivariate normal distribution . \\[ \\hat{\\beta} \\sim N_{p+1}(\\beta, \\sigma^2 (\\boldsymbol{X'X})^{-1}) \\] The variance of the distribution is known as the Variance Covariance Matrix , which provides the covariances between every possible pair of regression parameters. Since the covariance of a variable with itself is its variance, the diagonals are the respective variances of the parameters. Note that the first element of the diagonal is the intercept, thus the variance of the jth IV is the (j+1)th element of the diagonal . \\[ Var(\\hat{\\beta}) = \\begin{pmatrix} Var(\\hat{\\beta}_0) & Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_0, \\hat{\\beta}_p) \\\\ Cov(\\hat{\\beta}_1, \\hat{\\beta}_0) & Var(\\hat{\\beta}_1) & ... & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ Cov(\\hat{\\beta}_p, \\hat{\\beta}_0) & Cov(\\hat{\\beta}_1, \\hat{\\beta}_p) & ... & Var(\\hat{\\beta}_p) \\end{pmatrix} \\] Note that the covariances are symmetrical about the diagonal - \\(Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = Cov(\\hat{\\beta}_1, \\hat{\\beta}_0)\\) .","title":"Sampling Distributions"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/3.%20Multiple%20Linear%20Regression/#hypothesis-testing","text":"Similar to SLR, the t-test can be used to test for the significance of an individual IV, but the intepretation of the test is different from the SLR case. It tests the usefulness of an individual IV in the presence of the other predictors . \\[ \\begin{aligned} H_0: \\beta_j = 0 \\\\ H_1: \\beta_j \\ne 0 \\end{aligned} \\] \\[ t(\\beta_j) = \\frac{\\hat{\\beta_j} - \\beta_j}{\\hat{\\sigma}_{\\hat{\\beta_1}}} \\] Recall that the variance of the jth IV si the (j+1)th element of the variance covariance matrix. \\[ t(\\beta_j) \\sim t_{n-p-1} \\] However, this leads to several odd results which needs to be accounted for: Predictor is not significant individually but significant when taken alone . Predictors are not significant individually but significant when taken together . There are now two possible ways to test for the significance of IVs: Conduct a single F-test to test for the joint significance of all IVs Conduct multiple t-tests to test for the joint significance of each IV The problem with the multiple t-test approach lies with the type I error of the tests. For \\(\\alpha = 0.05\\) , the probability of correctly rejecting the null is \\(0.95\\) . Assuming that all the tests are independent, the probability of correctly rejecting all the nulls is \\(0.95^p\\) , which drastically decreases with the number of tests conducted. Given enough predictors, this means that the probability drops to approximately 0, which means that there is bound to be a wrongly rejected null; a type I error is guaranteed even though it was supposed to be limited at a 0.05 chance. The Bonferroni Correction is a method of adjusting \\(\\alpha\\) of each hypothesis test such that the overall type I error is kept at its desired level. However, this has the consequence of increasing the probability of type II errors, which is why it is not popular. The F-test has the advantage of controlling the type I error regardless of the number of predictors , which is why it is preferred for hypothesis testing in the MLR.","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/3.%20Multiple%20Linear%20Regression/#confidence-intervals","text":"Similar to SLR, the confidence intervals can be constructed using the distribution of the test-statistic: \\[ \\text{Confidence Interval} = \\hat{\\beta}_j \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{\\hat{\\beta_j}} \\]","title":"Confidence Intervals"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/3.%20Multiple%20Linear%20Regression/#prediction-intervals","text":"Unlike in SLR, it is difficult to determine the distribution of the prediction error. Thus, the final result can be found below: \\[ \\begin{aligned} \\text{Prediction Interval} &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\hat{\\sigma}_{y_* - \\hat{y_*}} \\\\ &= \\hat{y}_* \\pm t_{n-p-1, \\frac{\\alpha}{2}} * \\sqrt{s^2 [1 + x'_* (\\boldsymbol{X'X})^-1 * x_*]} \\end{aligned} \\] Despite the result looking more complicated, the key takeaway remains the same - the further away \\(x_*\\) is from \\(\\bar{x}\\) , the greater","title":"Prediction Intervals"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/3.%20Multiple%20Linear%20Regression/#variations-of-mlr","text":"","title":"Variations of MLR"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/3.%20Multiple%20Linear%20Regression/#qualitative-iv","text":"The discussion so far has mostly focused on Quantitative IVs, thus this section will explore Qualitative IVs. They can only take values from a list of pre-defined values, known as the Levels of the variable. In a regression context, most qualitative IVs are represented in the form of a Dummy Variable which can only take two possible levels - Yes (1) or No (0). Note that there are other ways of encoding a dummy variable (-1/0/1 etc), but the principles stay large the same. \\[ \\text{Dummy Variable} = \\begin{cases} 1, & \\text{First Level} \\\\ 0, & \\text{Second Level} \\end{cases} \\] In general, \\(n-1\\) dummy variables are needed to represent a qualitative variable with \\(n\\) levels. This is because the status of the last level can be deduced from the other dummy variables . Thus, including a seperate dummy variable for this last level is redundant and will lead to the problem of Collinearity , which will be explored in a later section. For instance, consider four levels (North, East, South & West), represented by the three dummy variables ( \\(N, E , S\\) ). If any of the variables are 1, they represent their respective direction (North, East & South). If all of them are 0, then the direction is the remaining level (West). The last remaining level is often referred to as the Baseline Level as it is the default level of the variable when all other dummies are 0. Any level can be used as the baseline, but the parameters will differ across models with different baselines. \\[ E(Y|X) = \\beta_0 + \\beta_{1, North} x_{North} + \\beta_{2, East} * x_{East} + \\beta_{3, South} * x_{South} \\] \\(\\beta_0\\) \\(\\beta_j\\) \\(E(Y)\\) at the baseline level Change in \\(E(Y)\\) from the baseline to the chosen level \\(X_1 = X_2 = ... = X_j = 0\\) \\(X_1 = X_2 = ... = 0; X_j = 1\\) Dummy variables are usually used in conjunction with quantitative ones . This essentially creates a \"seperate\" regression model for each of the levels. For the simplest case of one quantiative and one dummy, \\(\\beta_2\\) is the difference in the intercept of the two resulting SLR models. \\[ E(Y|X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + \\beta_1 x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\]","title":"Qualitative IV"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/3.%20Multiple%20Linear%20Regression/#interaction-model","text":"So far, it was assumed that each IV had an independent effect on the DV. However, IVs may interact to produce a joint effect on the DV, where the effect of one IV depends on the value of another IV. For instance, the production of a factory may depend on the number of Machines and Workers. However, the more machines there are, the greater the effect of an additional worker . Thus, this interaction effect can be captured through an Interaction Variable , which is the product of both IVs: \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\\\ &= \\beta_0 + (\\beta_1 + \\beta_3 x_2)x_1 + \\beta_2 x_2 \\\\ &= \\beta_0 + \\beta_1 x_1 + (\\beta_2 + \\beta_3 x_1) x_2 \\\\ \\end{aligned} \\] A one unit increase in \\(x_1\\) will increase E(Y) by \\(\\beta_1 + \\beta_3 x_2\\) , which depends on the value of \\(x_2\\) as well, which is is why they interact with each another. Phrased another way, for every one unit increase in \\(x_2\\) , the change in E(Y) from a unit increase in \\(x_1\\) increases by \\(\\beta_3\\) . Something unusual to take note of is that the Interaction Variable tests significant but the constituent variables do not. In this case, it is common practice to retain both the interaction and the consistuent variables in the model. This is practice is known as the Hierarchical Principle . Models containing dummy variables can also have an interaction effect. Building off the example from the previous section, \\(\\beta_2\\) is still the difference in the intercept but with the new \\(\\beta_3\\) being the difference in slopes of the two resulting SLR models. \\[ E(Y|X) = \\begin{cases} (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3)x_1, & x_2 = 1 \\\\ \\beta_0 + \\beta_1 x_1, & x_2 = 0 \\end{cases} \\]","title":"Interaction Model"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/3.%20Multiple%20Linear%20Regression/#piecewise-model","text":"If the DV has an abrupt change in behaviour over different values of the IVs, it can be accounted for the through the use of a Piecewise Regression . The first method of creating a piecewise regression involves the use of an Indicator Function . It is essentially a dummy variable which depends on the value of the other IVs. \\[ z_{\\{x>=c\\}} = \\begin{cases} 0, & x < c \\\\ 1, & x \\ge c \\end{cases} \\] \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 z(x-c) \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2c) + (\\beta_1 + \\beta_2) x_1, & x \\ge c \\end{cases} \\end{aligned} \\] Note that \\(z(x-c)\\) is treated as a distinct IV and hence can be equivalently expressed as \\(x_2\\) ; the full notation is used here for clarity. \\(c\\) is the value at which the DV abruptly changes in behaviour, known as a Kink in the graph, which continuously connects the two regression lines. The other method is to use an interaction variable instead. Similar to how the interaction variables resulted in the model to \"split\", the model now splits at \\(x = c\\) , resulting in a non-continuous gap. \\[ \\begin{aligned} E(Y|X) &= \\beta_0 + \\beta_1 x_1 + \\beta_2z + \\beta_3 zx \\\\ &= \\begin{cases} \\beta_0 + \\beta_1 x_1, & x < c \\\\ (\\beta_0 - \\beta_2) + (\\beta_1 + \\beta_3) x_1, & x \\ge c \\end{cases} \\end{aligned} \\]","title":"Piecewise Model"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/3.%20Multiple%20Linear%20Regression/#polynomial-model","text":"If the relationship between the DV and IV is complex (non-linear), then a Polynomial Regression can be used to better model the relationship between the two. \\[ E(Y|X) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 \\] Note that it is the same IV used in the regression, just with additional powers. Although a polynomial regression may capture the true relationship better, the regression parameters become hard to intepret. The partial derivatives can no longer be intepreted as holding other IVs constant as each IV is dependent on the same quantity, just to different powers. \\[ \\frac{\\partial E(Y|X)}{\\partial x} = \\beta_1 + 2\\beta_2 x + ... + m \\beta_m x^{m-1} \\]","title":"Polynomial Model"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/","text":"Gauss Markov Theorem \u00b6 Using OLS, the estimated regression parameters will always be unbiased under certain assumptions. The Gauss Markov Theorem extends this, which states that under certain assumptions, the OLS estimators will have the lowest variance among all possible linear unbiased estimators. In a statistics context, they are said to be the most efficient among all other linear unbiased estimators. In a regression context, the OLS estimators are said to be the Best Linear Unbiased Estimators (BLUE). This section will go over the various assumptions for both OLS and Gauss Markov Theorem. It will also cover the diagnostics to determine if the assumptions have been violated. The assumptions needed for OLS and the Gauss Markov theorem are often mixed up with each other as the assumptions needed for OLS are also needed for the theorem. This set of notes makes a clear distinction between the two. OLS Assumptions \u00b6 #1: Linearity \u00b6 Linear regression is a model where the relationship between the DV and IVs are linear. Thus, the regression parameters must be linear , but NOT the DV or IV. This means that the model is still considered a \"Linear Regression\" even after a transformation of the DV and/or IV. \\[ \\begin{aligned} y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\\\ y_i &= \\beta_0 + \\beta_1 x^2 + \\beta_2 x^3 \\\\ \\ln y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\end{aligned} \\] #2: Exogenity \u00b6 Exogenity refers to how a variable comes from outside the model and is thus independent of any other variables within the model . In a regression context, this comes in the form of the errors having a conditional mean of 0 , ensuring that the errors are random and thus not related to the IVs. \\[ E(\\varepsilon_i | x_{ij}) = 0 \\\\ \\] \"Endo\" and \"Exo\" in Greek means \"In\" and \"Out\" respectively, which is how the meaning of the words were derived. There are two key implications of Exogenity: By the Law of Total Expectations, the unconditional expectation of the error is also 0. By the Linearity of Conditional Expectations, the expectation of the product of the Error and IVs is 0. \\[ \\begin{aligned} E(\\varepsilon_i) = 0 \\\\ E(\\varepsilon_i x_{ij}) = 0 \\end{aligned} \\] Following these two implications, it can be shown that the Covariance between the Error and IVs are also 0, which is another consequence of independence (NOT the other way around). \\[ \\begin{aligned} Cov (\\varepsilon_i, x_{ij}) &= E(\\varepsilon_i x_{ij}) - E(\\varepsilon_i) * E(x_{ij}) \\\\ &= 0 - 0 * E(x_{ij}) \\\\ &= 0 \\end{aligned} \\] Without exogenity, the regression parameters would reflect the effect of both the IV and the unmodelled variable within the error term. This causes the OLS estimate to be biased , known as the Omitted Variable Bias . Since the unmodelled variable confounds the results of the regression, it is known as a Confounding Variable . Residual Analysis \u00b6 Since the errors are unobservable, the residuals are used to estimate the errors. If the fitted model is adequate - all relavent IVs are included in the right form, then the residuals should closely resemble the errors and therefore be structureless (random). However, if there are patterns in the residuals , it indicates that there is additional information that can be used to improve the model and thus should be included. Due to the OLS, the correlation between residuals and existing IVs will always be 0 indicating no linear relationship . To check for unmodelled non-linear relationships , a Residual Plot of the IVs against the Residuals can be used. For instance, if the residual plot shows a quadractic pattern (curve), then a quadractic IV should be added into the model. Mathematically, it can be expressed as a function of the existing estimates: \\[ \\begin{aligned} \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\varepsilon}_i \\\\ \\hat{\\varepsilon_i} &= \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\text{ (From residual plot)} \\\\ \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= (\\hat{\\beta}_0 + \\hat{\\gamma}_0) + (\\hat{\\beta}_1 + \\hat{\\gamma}_1)x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= \\hat{\\beta}_0' + \\hat{\\beta}_1'x + \\hat{\\beta}_2' x^2 \\end{aligned} \\] #3: No Perfect Collinearity \u00b6 Collinearity refers to when an IV can be expressed as a linear combination of one or more other IVs . Perfect collinearity is an extreme case where an IV can be perfectly expressed as a combination of another. Technically speaking, Collinearity refers to one to one variable relationship, while Multicollinearity refers to one to many variable relationship, hence \"Multi\". Variable is a multiple of another: \\(x_1 = cx_2\\) Variable differs by a constant from another: \\(x_1 = x_2 \\pm c\\) Variable is an affine transformation of another: Sum of several variables is fixed: Dummy Variable Trap The issue with perfect collinearity is that it affects the linear algebra used to solve for the coefficients (EG. Two equations to solve for three unknowns). There will be no unique solutions - many different values for the coefficients could work equally well. Imperfect Collinearity \u00b6 Imperfect Collinearity is a less extreme case where an IV is highly (but not perfectly) correlated with one or more other IVs. Recall that correlation refers to the extent of a Linear relationship. Non-linear relationships between variables are fine (EG. Polynomial Regression). This means that including the IV does not bring much additional predctive power into the model as its effects are already captured through related predictors and thus can be removed from the model. Unlike in the perfect case, high collinearity does not prevent OLS from finding a solution. However, the intepretation of the variables become complicated: The original intepretation of coefficients \"holding other variables constant\" is no longer true as highly correlated variables tend to move together. Thus, it is hard to seperate the effects of an individual variable . Consequently, OLS has difficulty estimating these coefficients, which could result in weird meaningless estimates ; EG. Large positive coefficient but large negative for its correlated counterpart. This also results in higher standard errors for the coefficients of correlated variables. This reduces the magnitude of t-statistic , which results in more false negatives , failing to reject the null when it should. This results in important variables being omitted from the regression. Technically speaking, there is nothing wrong with collinearity if the purpose of the model is solely for prediction. However, if the purpose of the model was to establish causality, then collinearity poses a problem as it interferes with statistical inference. Detecting Collinearity \u00b6 The simplest way to detect collinearity is through a Scatterplot or Correlation Matrix , which shows the correlations between pairs of variables . A correlation of 0.8 and higher is typically considered high enough where the collinearity becomes problematic. However, the issue is that this method can only detect collinearity between pairs of variable at a time. In order to detect collinearity among three or more variables ( multicollinearity ), then the Variance Inflation Factor (VIF) should be used instead. \\[ VIF = \\frac{1}{1-R^2_j} \\] The VIF is derived from the variance of the regression coefficient. As mentioned previously, under the presence of collinearity, the standard error and hence variance of the coefficient increases (\"inflated\"). The extent of the increase is known as the VIF. \\[ \\hat{Var}(\\hat{\\beta_1}) = \\frac{MS_{RSS}}{(n-1) s^2} * \\frac{1}{1-R^2_j} \\] The \\(R^2_j\\) in the VIF is the coefficient of determination of a model where the jth IV is regressed against all other IVs . A high \\(R^2_j\\) means that the IV is well explained by the other IVs (high correlation), which indicates the presence of collinearity. Generally, a \\(VIF > 10\\) is deemed to have severe collinearity . #4: No Extreme Outliers \u00b6 Outliers are observations with unusual values of the DV relative to fitted regression model. The last OLS assumption is that there are no extreme outliers in the dataset used to create the regression model. Generally, as long as the DV and IVs have a positive and finite Kurtosis , then the probability of such observations occuring are low. Outliers are problematic as OLS is sensitive to outliers . Extreme outliers have large residuals which receive more weight in the optimization process, which causes the resulting model to accomodate it (when it should not), causing the resulting coefficients to be biased. Identifying Outliers \u00b6 By definition, Outliers have unusually large residuals . In order to gauge what is considered a \"large residual\", the residuals are standardized for comparison. Standardization requires knowledge of the sampling distribution of the residuals . Given that the errors have a constant variance of \\(\\sigma^2\\) , the variance of the residuals can be shown to be: \\[ var(\\hat{\\varepsilon}_i) = \\sigma^2 (1-h_{ii}) \\] \\(h_ii\\) is known as the Leverage of the observation, which will be covered in the following section. The sampling distribution can then be determined: \\[ \\hat{\\varepsilon}_i ~ N(0, \\sigma^2 (1-h_{ii})) \\] Thus, the standardized residuals are the raw residuals scaled by their standard error: \\[ \\hat{\\varepsilon}_i^{\\text{standardized}} = \\frac{\\hat{\\varepsilon}_i}{\\sqrt{\\sigma^2 (1-h_{ii})}} \\] In practice, the variance of the errors are unknown, thus it is estimated using the Sample Variance of the residuals instead. Residuals with standardized values of larger than 2 or 3 are considered large and thus can be considered as an outlier. High Leverage Points \u00b6 While outliers are unusual points of the DV, High Leverage observations have unusual values of the IV relative to the majority of the values. It is easy to identify high leverage points when there is only one IV through a scatterplot - simply find the observation that is away from the rest. It becomes much more complicated when there are multiple IVs. The observation's values for each of the IVs could be in the common range of each IV, but unusual when taken collectively : The leverage of the observation can be determined from the MLR: \\[ \\begin{aligned} \\hat{\\boldsymbol{y}} &= \\boldsymbol{X}\\boldsymbol{\\beta} \\\\ &= \\boldsymbol{X}[(X'X)^{-1}y] \\\\ &= \\boldsymbol{H}\\boldsymbol{y} \\end{aligned} \\] \\(\\boldsymbol{H}\\) is known as the Hat Matrix as it puts a hat on y in the notation. The leverage of the \\(ith\\) observation is the \\(ith\\) diagonal element of the matrix, \\(h_{ii}\\) . An observation is considered to have high leverage if its leverage is greater than three times the average leverage: \\[ h_{ii} \\gt 3 \\left(\\frac{p+1}{n}\\right) \\] Influential Points \u00b6 The effect of Outliers and High leverage points can both be summarized into a concept known as Influence . An observation is influential if the exclusion of the observation from the regression leads to significantly differently results. In general the process involves three steps: Fit the original model with all \\(n\\) observations; determine the \\(j-th\\) fitted value \\(\\hat{y}_j\\) Fit an adjusted model with omitting the \\(i-th\\) observation, determine the \\(j-th\\) fitted value \\(\\hat{y}_{j(i)}\\) Calculate the change in the \\(j-th\\) fitted value This process has to be repeated for all fitted values for all observations . Cook's Distance summarizes the effect of the \\(i-th\\) observation on the whole model: \\[ D_i = \\frac{\\sum^n_{j=1} (\\hat{y}_j - \\hat{y}_{j(i)})^2}{(p+1) \\cdot MS_{Residuals}} \\] This method of computation requires \\(n+1\\) datasets - 1 dataset with all the observation and \\(n\\) datasets with the \\(i-th\\) observation omitted. It is also extremely time consuming to have to fit a model to each dataset. An alternative method of determining Cook's Distance is to make use of both the Outliers and Leverage: \\[ D_i = \\frac{1}{p+1} \\cdot (e^{\\text{standardized}})^2 \\cdot \\frac{h_{ii}}{1-h_{ii}} \\] Thus, an observation must be unusual in BOTH the DV and IV in order to be considered influential. Outliers and High leverage points are necessary but not sufficient conditions to be influential. Gauss Markov Assumptions \u00b6 #1: Conditional Homoscedasticity \u00b6 Homoscedasticity refers to the error terms having constant variance while Heteroscedasticity refers to having non-constant variance. \\[ var(\\varepsilon_i | x_i) = \\sigma^2 \\] Under homoscedasticity, the sampling distribution of the estimates are easily derived and thus it can be shown that they are the most efficient estimators. The same cannot be proven under heteroscedasticity. Note that that the OLS estimates are still unbiased; they are just not the most efficient. Identifying Heteroscedasticity \u00b6 Similar to before, if the model is adequate, then the residuals should resemble the errors and have constant variance . Thus, this can be easily determined through a residual plot of the Residuals against the fitted values. If the points are equally spread out about the mean (0) and show no pattern , then homoscedasticity is present. However, if the points show an increasing or decreasing variance (typically in the shape of a funnel ), then heteroscedasticity is present. Alternatively, a hypothesis test can be conducted to determine if heteroscedasticity is present, known as the Bresuch Pagan Test . \\[ \\begin{aligned} H_0 &: \\sigma^2 \\\\ H_1 &: \\sigma^2 + \\boldsymbol{Z\\gamma} \\end{aligned} \\] The test-statistic is computed as follows: Compute the squared standardized residuals from the original model Regress them onto the variables in Z (LOL need to change this part) Compute the RegSS of the new regression \\[ T = \\frac{RegSS}{2} \\] The test-statistic follows a chi-square distribution with \\(q\\) degrees of freedom, where \\(q\\) is the number of variables in \\(\\boldsymbol{Z}\\) . \\[ T \\sim \\chi^2_q \\] Dealing with Heteroscedasticity \u00b6 If prior information is known about the structure of the data, then the most intuitive method would be to incorporate that information into the data. \\[ Var(\\varepsilon_i) = \\frac{\\sigma^2}{w_i} \\] If no prior information is known, then the Heteroscedasticity can be reduced by using a Variance Stabilizing Transformation , such as the Log or Squareroot . Note that since they require positive data, a constant can be added to each term before the transformation to ensure that the values are positive. It is out of the scope for this set of notes to show why these help to stabilize variance. Alternatively, if there is only mild heteroscedasticity in the data, then OLS can be used but with an adjustment to the standard errors of the coefficients, known as heteroscedastic-robust standard errors . Due to complexity of the computations, it will not be covered in this set of notes. However, the general idea is that an weighted estimate of the variance covariance matrix is computed and the standard errors are computed from there. #2: No Serial Correlation \u00b6 If errors are correlated with one another, it is known as Serial Correlation or Autocorrelation . It implies that there are other unmodelled factors that can be used for prediction, which would imply the current model specification to be wrong. Thus, for the SLR model to be true, the errors must be independent of one another . \\[ Cov(\\varepsilon_i,\\varepsilon_j) = 0 \\] Confidence Intervals and PI are narrower than it should be > 95% PI is actually < 95%> P values lower > Appear statisticlaly significant when they shld not be Time series tends to have errors that are positively correlated, which is why it has its own dedicated section No Serial Correlation > Outcome of zero conditional mean, but most likely in time series data Error Distribution \u00b6 Although not needed for OLS estimation or Guass Markov, the errors of the regression are usually assumed to be normally distributed . \\[ \\varepsilon \\sim N(0, \\sigma^2) \\] If the errors are normally distributed, then it follows that \\(\\beta\\) is normally distributed as well since they are linear and additive. This greatly eases the computation needed to determine the sampling distribution for statistical inference. Q-Q Plots \u00b6 Since the errors are normally distributed, the residuals should be normally distributed as well. This can be verified using a Quantile-Quantile Plot (QQ Plot) , which compares the quantiles of two distributions. The first distribution is plotted on the x-axis while the second on the y-axis. If the quantiles are the same (same distribution), then the points should lie on \\(y = x\\) , the 45 degree line.","title":"Gauss Markov Theorem"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#gauss-markov-theorem","text":"Using OLS, the estimated regression parameters will always be unbiased under certain assumptions. The Gauss Markov Theorem extends this, which states that under certain assumptions, the OLS estimators will have the lowest variance among all possible linear unbiased estimators. In a statistics context, they are said to be the most efficient among all other linear unbiased estimators. In a regression context, the OLS estimators are said to be the Best Linear Unbiased Estimators (BLUE). This section will go over the various assumptions for both OLS and Gauss Markov Theorem. It will also cover the diagnostics to determine if the assumptions have been violated. The assumptions needed for OLS and the Gauss Markov theorem are often mixed up with each other as the assumptions needed for OLS are also needed for the theorem. This set of notes makes a clear distinction between the two.","title":"Gauss Markov Theorem"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#ols-assumptions","text":"","title":"OLS Assumptions"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#1-linearity","text":"Linear regression is a model where the relationship between the DV and IVs are linear. Thus, the regression parameters must be linear , but NOT the DV or IV. This means that the model is still considered a \"Linear Regression\" even after a transformation of the DV and/or IV. \\[ \\begin{aligned} y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\\\ y_i &= \\beta_0 + \\beta_1 x^2 + \\beta_2 x^3 \\\\ \\ln y_i &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\end{aligned} \\]","title":"#1: Linearity"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#2-exogenity","text":"Exogenity refers to how a variable comes from outside the model and is thus independent of any other variables within the model . In a regression context, this comes in the form of the errors having a conditional mean of 0 , ensuring that the errors are random and thus not related to the IVs. \\[ E(\\varepsilon_i | x_{ij}) = 0 \\\\ \\] \"Endo\" and \"Exo\" in Greek means \"In\" and \"Out\" respectively, which is how the meaning of the words were derived. There are two key implications of Exogenity: By the Law of Total Expectations, the unconditional expectation of the error is also 0. By the Linearity of Conditional Expectations, the expectation of the product of the Error and IVs is 0. \\[ \\begin{aligned} E(\\varepsilon_i) = 0 \\\\ E(\\varepsilon_i x_{ij}) = 0 \\end{aligned} \\] Following these two implications, it can be shown that the Covariance between the Error and IVs are also 0, which is another consequence of independence (NOT the other way around). \\[ \\begin{aligned} Cov (\\varepsilon_i, x_{ij}) &= E(\\varepsilon_i x_{ij}) - E(\\varepsilon_i) * E(x_{ij}) \\\\ &= 0 - 0 * E(x_{ij}) \\\\ &= 0 \\end{aligned} \\] Without exogenity, the regression parameters would reflect the effect of both the IV and the unmodelled variable within the error term. This causes the OLS estimate to be biased , known as the Omitted Variable Bias . Since the unmodelled variable confounds the results of the regression, it is known as a Confounding Variable .","title":"#2: Exogenity"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#residual-analysis","text":"Since the errors are unobservable, the residuals are used to estimate the errors. If the fitted model is adequate - all relavent IVs are included in the right form, then the residuals should closely resemble the errors and therefore be structureless (random). However, if there are patterns in the residuals , it indicates that there is additional information that can be used to improve the model and thus should be included. Due to the OLS, the correlation between residuals and existing IVs will always be 0 indicating no linear relationship . To check for unmodelled non-linear relationships , a Residual Plot of the IVs against the Residuals can be used. For instance, if the residual plot shows a quadractic pattern (curve), then a quadractic IV should be added into the model. Mathematically, it can be expressed as a function of the existing estimates: \\[ \\begin{aligned} \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\varepsilon}_i \\\\ \\hat{\\varepsilon_i} &= \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\text{ (From residual plot)} \\\\ \\hat{y_i} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\gamma}_0 + \\hat{\\gamma}_1 x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= (\\hat{\\beta}_0 + \\hat{\\gamma}_0) + (\\hat{\\beta}_1 + \\hat{\\gamma}_1)x + \\hat{\\gamma}_2 x^2 \\\\ \\hat{y_i} &= \\hat{\\beta}_0' + \\hat{\\beta}_1'x + \\hat{\\beta}_2' x^2 \\end{aligned} \\]","title":"Residual Analysis"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#3-no-perfect-collinearity","text":"Collinearity refers to when an IV can be expressed as a linear combination of one or more other IVs . Perfect collinearity is an extreme case where an IV can be perfectly expressed as a combination of another. Technically speaking, Collinearity refers to one to one variable relationship, while Multicollinearity refers to one to many variable relationship, hence \"Multi\". Variable is a multiple of another: \\(x_1 = cx_2\\) Variable differs by a constant from another: \\(x_1 = x_2 \\pm c\\) Variable is an affine transformation of another: Sum of several variables is fixed: Dummy Variable Trap The issue with perfect collinearity is that it affects the linear algebra used to solve for the coefficients (EG. Two equations to solve for three unknowns). There will be no unique solutions - many different values for the coefficients could work equally well.","title":"#3: No Perfect Collinearity"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#imperfect-collinearity","text":"Imperfect Collinearity is a less extreme case where an IV is highly (but not perfectly) correlated with one or more other IVs. Recall that correlation refers to the extent of a Linear relationship. Non-linear relationships between variables are fine (EG. Polynomial Regression). This means that including the IV does not bring much additional predctive power into the model as its effects are already captured through related predictors and thus can be removed from the model. Unlike in the perfect case, high collinearity does not prevent OLS from finding a solution. However, the intepretation of the variables become complicated: The original intepretation of coefficients \"holding other variables constant\" is no longer true as highly correlated variables tend to move together. Thus, it is hard to seperate the effects of an individual variable . Consequently, OLS has difficulty estimating these coefficients, which could result in weird meaningless estimates ; EG. Large positive coefficient but large negative for its correlated counterpart. This also results in higher standard errors for the coefficients of correlated variables. This reduces the magnitude of t-statistic , which results in more false negatives , failing to reject the null when it should. This results in important variables being omitted from the regression. Technically speaking, there is nothing wrong with collinearity if the purpose of the model is solely for prediction. However, if the purpose of the model was to establish causality, then collinearity poses a problem as it interferes with statistical inference.","title":"Imperfect Collinearity"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#detecting-collinearity","text":"The simplest way to detect collinearity is through a Scatterplot or Correlation Matrix , which shows the correlations between pairs of variables . A correlation of 0.8 and higher is typically considered high enough where the collinearity becomes problematic. However, the issue is that this method can only detect collinearity between pairs of variable at a time. In order to detect collinearity among three or more variables ( multicollinearity ), then the Variance Inflation Factor (VIF) should be used instead. \\[ VIF = \\frac{1}{1-R^2_j} \\] The VIF is derived from the variance of the regression coefficient. As mentioned previously, under the presence of collinearity, the standard error and hence variance of the coefficient increases (\"inflated\"). The extent of the increase is known as the VIF. \\[ \\hat{Var}(\\hat{\\beta_1}) = \\frac{MS_{RSS}}{(n-1) s^2} * \\frac{1}{1-R^2_j} \\] The \\(R^2_j\\) in the VIF is the coefficient of determination of a model where the jth IV is regressed against all other IVs . A high \\(R^2_j\\) means that the IV is well explained by the other IVs (high correlation), which indicates the presence of collinearity. Generally, a \\(VIF > 10\\) is deemed to have severe collinearity .","title":"Detecting Collinearity"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#4-no-extreme-outliers","text":"Outliers are observations with unusual values of the DV relative to fitted regression model. The last OLS assumption is that there are no extreme outliers in the dataset used to create the regression model. Generally, as long as the DV and IVs have a positive and finite Kurtosis , then the probability of such observations occuring are low. Outliers are problematic as OLS is sensitive to outliers . Extreme outliers have large residuals which receive more weight in the optimization process, which causes the resulting model to accomodate it (when it should not), causing the resulting coefficients to be biased.","title":"#4: No Extreme Outliers"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#identifying-outliers","text":"By definition, Outliers have unusually large residuals . In order to gauge what is considered a \"large residual\", the residuals are standardized for comparison. Standardization requires knowledge of the sampling distribution of the residuals . Given that the errors have a constant variance of \\(\\sigma^2\\) , the variance of the residuals can be shown to be: \\[ var(\\hat{\\varepsilon}_i) = \\sigma^2 (1-h_{ii}) \\] \\(h_ii\\) is known as the Leverage of the observation, which will be covered in the following section. The sampling distribution can then be determined: \\[ \\hat{\\varepsilon}_i ~ N(0, \\sigma^2 (1-h_{ii})) \\] Thus, the standardized residuals are the raw residuals scaled by their standard error: \\[ \\hat{\\varepsilon}_i^{\\text{standardized}} = \\frac{\\hat{\\varepsilon}_i}{\\sqrt{\\sigma^2 (1-h_{ii})}} \\] In practice, the variance of the errors are unknown, thus it is estimated using the Sample Variance of the residuals instead. Residuals with standardized values of larger than 2 or 3 are considered large and thus can be considered as an outlier.","title":"Identifying Outliers"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#high-leverage-points","text":"While outliers are unusual points of the DV, High Leverage observations have unusual values of the IV relative to the majority of the values. It is easy to identify high leverage points when there is only one IV through a scatterplot - simply find the observation that is away from the rest. It becomes much more complicated when there are multiple IVs. The observation's values for each of the IVs could be in the common range of each IV, but unusual when taken collectively : The leverage of the observation can be determined from the MLR: \\[ \\begin{aligned} \\hat{\\boldsymbol{y}} &= \\boldsymbol{X}\\boldsymbol{\\beta} \\\\ &= \\boldsymbol{X}[(X'X)^{-1}y] \\\\ &= \\boldsymbol{H}\\boldsymbol{y} \\end{aligned} \\] \\(\\boldsymbol{H}\\) is known as the Hat Matrix as it puts a hat on y in the notation. The leverage of the \\(ith\\) observation is the \\(ith\\) diagonal element of the matrix, \\(h_{ii}\\) . An observation is considered to have high leverage if its leverage is greater than three times the average leverage: \\[ h_{ii} \\gt 3 \\left(\\frac{p+1}{n}\\right) \\]","title":"High Leverage Points"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#influential-points","text":"The effect of Outliers and High leverage points can both be summarized into a concept known as Influence . An observation is influential if the exclusion of the observation from the regression leads to significantly differently results. In general the process involves three steps: Fit the original model with all \\(n\\) observations; determine the \\(j-th\\) fitted value \\(\\hat{y}_j\\) Fit an adjusted model with omitting the \\(i-th\\) observation, determine the \\(j-th\\) fitted value \\(\\hat{y}_{j(i)}\\) Calculate the change in the \\(j-th\\) fitted value This process has to be repeated for all fitted values for all observations . Cook's Distance summarizes the effect of the \\(i-th\\) observation on the whole model: \\[ D_i = \\frac{\\sum^n_{j=1} (\\hat{y}_j - \\hat{y}_{j(i)})^2}{(p+1) \\cdot MS_{Residuals}} \\] This method of computation requires \\(n+1\\) datasets - 1 dataset with all the observation and \\(n\\) datasets with the \\(i-th\\) observation omitted. It is also extremely time consuming to have to fit a model to each dataset. An alternative method of determining Cook's Distance is to make use of both the Outliers and Leverage: \\[ D_i = \\frac{1}{p+1} \\cdot (e^{\\text{standardized}})^2 \\cdot \\frac{h_{ii}}{1-h_{ii}} \\] Thus, an observation must be unusual in BOTH the DV and IV in order to be considered influential. Outliers and High leverage points are necessary but not sufficient conditions to be influential.","title":"Influential Points"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#gauss-markov-assumptions","text":"","title":"Gauss Markov Assumptions"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#1-conditional-homoscedasticity","text":"Homoscedasticity refers to the error terms having constant variance while Heteroscedasticity refers to having non-constant variance. \\[ var(\\varepsilon_i | x_i) = \\sigma^2 \\] Under homoscedasticity, the sampling distribution of the estimates are easily derived and thus it can be shown that they are the most efficient estimators. The same cannot be proven under heteroscedasticity. Note that that the OLS estimates are still unbiased; they are just not the most efficient.","title":"#1: Conditional Homoscedasticity"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#identifying-heteroscedasticity","text":"Similar to before, if the model is adequate, then the residuals should resemble the errors and have constant variance . Thus, this can be easily determined through a residual plot of the Residuals against the fitted values. If the points are equally spread out about the mean (0) and show no pattern , then homoscedasticity is present. However, if the points show an increasing or decreasing variance (typically in the shape of a funnel ), then heteroscedasticity is present. Alternatively, a hypothesis test can be conducted to determine if heteroscedasticity is present, known as the Bresuch Pagan Test . \\[ \\begin{aligned} H_0 &: \\sigma^2 \\\\ H_1 &: \\sigma^2 + \\boldsymbol{Z\\gamma} \\end{aligned} \\] The test-statistic is computed as follows: Compute the squared standardized residuals from the original model Regress them onto the variables in Z (LOL need to change this part) Compute the RegSS of the new regression \\[ T = \\frac{RegSS}{2} \\] The test-statistic follows a chi-square distribution with \\(q\\) degrees of freedom, where \\(q\\) is the number of variables in \\(\\boldsymbol{Z}\\) . \\[ T \\sim \\chi^2_q \\]","title":"Identifying Heteroscedasticity"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#dealing-with-heteroscedasticity","text":"If prior information is known about the structure of the data, then the most intuitive method would be to incorporate that information into the data. \\[ Var(\\varepsilon_i) = \\frac{\\sigma^2}{w_i} \\] If no prior information is known, then the Heteroscedasticity can be reduced by using a Variance Stabilizing Transformation , such as the Log or Squareroot . Note that since they require positive data, a constant can be added to each term before the transformation to ensure that the values are positive. It is out of the scope for this set of notes to show why these help to stabilize variance. Alternatively, if there is only mild heteroscedasticity in the data, then OLS can be used but with an adjustment to the standard errors of the coefficients, known as heteroscedastic-robust standard errors . Due to complexity of the computations, it will not be covered in this set of notes. However, the general idea is that an weighted estimate of the variance covariance matrix is computed and the standard errors are computed from there.","title":"Dealing with Heteroscedasticity"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#2-no-serial-correlation","text":"If errors are correlated with one another, it is known as Serial Correlation or Autocorrelation . It implies that there are other unmodelled factors that can be used for prediction, which would imply the current model specification to be wrong. Thus, for the SLR model to be true, the errors must be independent of one another . \\[ Cov(\\varepsilon_i,\\varepsilon_j) = 0 \\] Confidence Intervals and PI are narrower than it should be > 95% PI is actually < 95%> P values lower > Appear statisticlaly significant when they shld not be Time series tends to have errors that are positively correlated, which is why it has its own dedicated section No Serial Correlation > Outcome of zero conditional mean, but most likely in time series data","title":"#2: No Serial Correlation"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#error-distribution","text":"Although not needed for OLS estimation or Guass Markov, the errors of the regression are usually assumed to be normally distributed . \\[ \\varepsilon \\sim N(0, \\sigma^2) \\] If the errors are normally distributed, then it follows that \\(\\beta\\) is normally distributed as well since they are linear and additive. This greatly eases the computation needed to determine the sampling distribution for statistical inference.","title":"Error Distribution"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/4.%20Gauss%20Markov%20Theorem/#q-q-plots","text":"Since the errors are normally distributed, the residuals should be normally distributed as well. This can be verified using a Quantile-Quantile Plot (QQ Plot) , which compares the quantiles of two distributions. The first distribution is plotted on the x-axis while the second on the y-axis. If the quantiles are the same (same distribution), then the points should lie on \\(y = x\\) , the 45 degree line.","title":"Q-Q Plots"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/5.%20Statistical%20Learning/","text":"Statistical Learning \u00b6 The regression concepts covered in the previous sections are widely considered to be traditional applied statistics . In a contemporary context, regression is just one of the many methods that fall under Statistical Learning . It is a framework of harnessing data to gain an understanding of how the data is related to one another and/or how a group of variables can be used to accurately predict another. Similar to regression, the relationship between variables can be expressed as a combination of a Signal Function and a Noise term: \\[ y = f(x) + \\varepsilon \\] The two terms originate from Engineering, where Signal refers to the meaningful component of the data while Noise refers to the random variation that inteferes with the signal. Statistical learning models are distinguished based on their signal function. There are two main kinds of models: Parametric Models Non-Parametric Assumes DV follows a specific functional form Does not assume any functional form Function parameters determined from the data - Does not require a large amount of data Requires a large amount of data to work well EG. Linear Regression EG. Clustering | May not fit the data well | Fits the data well | | Described by parameters | No parameters used | | Simple to implement | Requires large amount of data to implement | | Risk that assumed function is wrong | Does not make any assumption about data | They can also be further split according to: Supervised Learning Unsupervised Learning Specified DV to supervise the learning No specific variable chosen Inference/Prediction with respect to the DV Inference/Prediction for all variables EG. Linear Regression EG. Clustering Regression vs classification classification - classifying the observations to a certian level Model Accuracy \u00b6 Since the end goal of the model is to make predictions on new unobserved data, the quality of the model should be evaluated against its performance on unobserved data as well. The observed data used to create the model is known as the Training Data as it helps train the signal function to identify relationships between variables, while the unobserved data used to evaluate the model is known as the Test Data . The quality of the model can then be quantified by the extent to which the model predictions match the data . Similar to regression, this quantity is known as the Error of the model and is summarized through the Mean Square Error (MSE) statistic. The MSE is the average of the sum of squared errors and can be calculated for both the training and test data: \\[ \\begin{aligned} \\text{Training MSE} &= \\frac{[y_i - \\hat{f}(x_0)]^2}{n_{training}} \\\\ \\text{Test MSE} &= \\frac{[y_0 - \\hat{f}(x_0)]^2}{n_{test}} \\end{aligned} \\] Note that this is different from the MSE defined in ANOVA, where the MS is divded by its degrees of freedom rather than the number of observations. The MSE defined in this section is a general concept while the ANOVA MSE is a purely regression concept. The Training MSE reflects the goodness of fit of the model while the Test MSE reflects its prediction accuracy. As alluded to earlier, the goal of statistical learning is to choose the model with the lowest Test MSE . In general, the training MSE should always be smaller than the testing MSE, which is why they are not interchangeable. This is because all models are trained to match the training data to various extents, thus they should naturally have relatively smaller errors. On the flipside, test MSE should always be higher because the model is likely to have mistakenly captured some of the noise in the training data that do not generalize to the test data, resulting in a higher testing error. The extent of the difference is dependent on how well the model fits the training data; the extent to which it learns from it: High Flexibility/Complexity - Tends to overfit the training data; matches data too much; learns too much Low Flexibility/Complexity - Tends to underfit the training data; matches data too little; learns too little This is not to say that low flexibility models are better. In fact, some level of flexibility is needed for the model to pick up most of the signals in the training data, but not too much such that the noise is captured as well. Thus, the test MSE generally decreases with flexibility up till a certain point, following which it increases, forming a U shaped curve : Bias Variance Tradeoff \u00b6 The test MSE can be better understood by decomposing it into its consistuent commponents: \\[ \\text{MSE} = \\text{Bias}[\\hat{f}(x_0)]^2 + \\text{Variance}[\\hat{f}(x_0)] \\] The Bias of the model (also known as the Accuracy ) is the difference in expected value of the estimated signal function and the true signal function. More complex models are better able to capture the signal in the data, thus tends to have a lower bias. The Variance of the model (also known as Precision ) is the change in the estimated signal function across different datasets. Ideally, the model should have low variance such that it would be relatively stable across different training data. While more complex models are better able to capture signals, this makes them prone to overfitting and hence more sensitive to differences in the training data , leading to higher variance. Note that although Precision & Accuracy are both synonyms in English, they have distinct meanings in statistics. Ideally, a model should have both low bias and low variance. However, as explained above, there is an inherent tension between Bias and Variance due to the complexity of the model, known as the Bias-Variance Tradeoff . A relatively simple model (underfitted) tends to have a high bias but low variance . As the complexity increases, the bias initially decreases more than the variance increases , causing the test MSE to fall. At some point, the model becomes too complex (overfitted), where the increase in variance outweighs the fall in bias , resulting in the U-shaped curve as seen previously. Thus, the goal is to find an optimal balance in between Bias and Variance where the test MSE is minimized. Resampling Methods \u00b6 Validation Set \u00b6 LOO Cross Validation \u00b6 K fold Cross Validation \u00b6 Model Selection \u00b6 Feature Selection \u00b6 Forward Stepwise Selection \u00b6 Backward Stepwise Selection \u00b6 Stepwise Selection \u00b6 Shrinkage Methods \u00b6","title":"Statistical Learning"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/5.%20Statistical%20Learning/#statistical-learning","text":"The regression concepts covered in the previous sections are widely considered to be traditional applied statistics . In a contemporary context, regression is just one of the many methods that fall under Statistical Learning . It is a framework of harnessing data to gain an understanding of how the data is related to one another and/or how a group of variables can be used to accurately predict another. Similar to regression, the relationship between variables can be expressed as a combination of a Signal Function and a Noise term: \\[ y = f(x) + \\varepsilon \\] The two terms originate from Engineering, where Signal refers to the meaningful component of the data while Noise refers to the random variation that inteferes with the signal. Statistical learning models are distinguished based on their signal function. There are two main kinds of models: Parametric Models Non-Parametric Assumes DV follows a specific functional form Does not assume any functional form Function parameters determined from the data - Does not require a large amount of data Requires a large amount of data to work well EG. Linear Regression EG. Clustering | May not fit the data well | Fits the data well | | Described by parameters | No parameters used | | Simple to implement | Requires large amount of data to implement | | Risk that assumed function is wrong | Does not make any assumption about data | They can also be further split according to: Supervised Learning Unsupervised Learning Specified DV to supervise the learning No specific variable chosen Inference/Prediction with respect to the DV Inference/Prediction for all variables EG. Linear Regression EG. Clustering Regression vs classification classification - classifying the observations to a certian level","title":"Statistical Learning"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/5.%20Statistical%20Learning/#model-accuracy","text":"Since the end goal of the model is to make predictions on new unobserved data, the quality of the model should be evaluated against its performance on unobserved data as well. The observed data used to create the model is known as the Training Data as it helps train the signal function to identify relationships between variables, while the unobserved data used to evaluate the model is known as the Test Data . The quality of the model can then be quantified by the extent to which the model predictions match the data . Similar to regression, this quantity is known as the Error of the model and is summarized through the Mean Square Error (MSE) statistic. The MSE is the average of the sum of squared errors and can be calculated for both the training and test data: \\[ \\begin{aligned} \\text{Training MSE} &= \\frac{[y_i - \\hat{f}(x_0)]^2}{n_{training}} \\\\ \\text{Test MSE} &= \\frac{[y_0 - \\hat{f}(x_0)]^2}{n_{test}} \\end{aligned} \\] Note that this is different from the MSE defined in ANOVA, where the MS is divded by its degrees of freedom rather than the number of observations. The MSE defined in this section is a general concept while the ANOVA MSE is a purely regression concept. The Training MSE reflects the goodness of fit of the model while the Test MSE reflects its prediction accuracy. As alluded to earlier, the goal of statistical learning is to choose the model with the lowest Test MSE . In general, the training MSE should always be smaller than the testing MSE, which is why they are not interchangeable. This is because all models are trained to match the training data to various extents, thus they should naturally have relatively smaller errors. On the flipside, test MSE should always be higher because the model is likely to have mistakenly captured some of the noise in the training data that do not generalize to the test data, resulting in a higher testing error. The extent of the difference is dependent on how well the model fits the training data; the extent to which it learns from it: High Flexibility/Complexity - Tends to overfit the training data; matches data too much; learns too much Low Flexibility/Complexity - Tends to underfit the training data; matches data too little; learns too little This is not to say that low flexibility models are better. In fact, some level of flexibility is needed for the model to pick up most of the signals in the training data, but not too much such that the noise is captured as well. Thus, the test MSE generally decreases with flexibility up till a certain point, following which it increases, forming a U shaped curve :","title":"Model Accuracy"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/5.%20Statistical%20Learning/#bias-variance-tradeoff","text":"The test MSE can be better understood by decomposing it into its consistuent commponents: \\[ \\text{MSE} = \\text{Bias}[\\hat{f}(x_0)]^2 + \\text{Variance}[\\hat{f}(x_0)] \\] The Bias of the model (also known as the Accuracy ) is the difference in expected value of the estimated signal function and the true signal function. More complex models are better able to capture the signal in the data, thus tends to have a lower bias. The Variance of the model (also known as Precision ) is the change in the estimated signal function across different datasets. Ideally, the model should have low variance such that it would be relatively stable across different training data. While more complex models are better able to capture signals, this makes them prone to overfitting and hence more sensitive to differences in the training data , leading to higher variance. Note that although Precision & Accuracy are both synonyms in English, they have distinct meanings in statistics. Ideally, a model should have both low bias and low variance. However, as explained above, there is an inherent tension between Bias and Variance due to the complexity of the model, known as the Bias-Variance Tradeoff . A relatively simple model (underfitted) tends to have a high bias but low variance . As the complexity increases, the bias initially decreases more than the variance increases , causing the test MSE to fall. At some point, the model becomes too complex (overfitted), where the increase in variance outweighs the fall in bias , resulting in the U-shaped curve as seen previously. Thus, the goal is to find an optimal balance in between Bias and Variance where the test MSE is minimized.","title":"Bias Variance Tradeoff"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/5.%20Statistical%20Learning/#resampling-methods","text":"","title":"Resampling Methods"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/5.%20Statistical%20Learning/#validation-set","text":"","title":"Validation Set"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/5.%20Statistical%20Learning/#loo-cross-validation","text":"","title":"LOO Cross Validation"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/5.%20Statistical%20Learning/#k-fold-cross-validation","text":"","title":"K fold Cross Validation"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/5.%20Statistical%20Learning/#model-selection","text":"","title":"Model Selection"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/5.%20Statistical%20Learning/#feature-selection","text":"","title":"Feature Selection"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/5.%20Statistical%20Learning/#forward-stepwise-selection","text":"","title":"Forward Stepwise Selection"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/5.%20Statistical%20Learning/#backward-stepwise-selection","text":"","title":"Backward Stepwise Selection"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/5.%20Statistical%20Learning/#stepwise-selection","text":"","title":"Stepwise Selection"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/5.%20Statistical%20Learning/#shrinkage-methods","text":"","title":"Shrinkage Methods"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/6.%20Generalized%20Linear%20Models/","text":"","title":"6. Generalized Linear Models"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/7.%20Time%20Series%20Models/","text":"","title":"7. Time Series Models"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/8.%20Tree%20Models/","text":"","title":"8. Tree Models"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/9.%20Principal%20Component%20Analysis/","text":"","title":"9. Principal Component Analysis"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/Review%20of%20Statistical%20Theory/","text":"Review of Statistical Theory \u00b6 This section assumes some basic knowledge on Random Variables , which can be found under another set of notes covering a Review of Probability Theory . Overview of Statistics \u00b6 Statistics is a discipline revolving around data. A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters . However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample . Attributes that summarize or describe the sample are known as Statistics . Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters. We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat ( \\(x\\) ) while their corresponding sample statistics are written with the hat ( \\(\\hat{x}\\) ). Common Sample Statistics \u00b6 The Mean is the average of the population. Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) Mean is a measure of centrality But if there are outliers, then median is better The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance . Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) Covariance is a measure of the linear relationship between two variables: Positive Covariance - Variables move in the same direction Negative Covariance - Variables move in opposite directions Population Covariance Sample Covariance \\(\\sigma_{x, y} = \\mu_{xy} - \\mu_x \\mu_y\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1 . Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x} \\sum(y_i - \\bar{y})}\\) Properties of Estimators Biased Consistent Efficient Degrees of freedom Sampling Distribution \u00b6 Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate . Due to measurement error, a different sample would be drawn each time, thus leading to a different point estimate . If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic. There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic being measured, sampling method etc. The standard deviation of this sampling distribution is known as the Standard Error of the statistic: \\[ \\sigma_{\\theta} = \\sqrt {\\sigma^2_{\\theta}} \\] A special case is the Sample Mean . If the population is normally distributed, then it is normally distributed as well. Regardless of the population distribution, it is also approximately normally distributed through the Central Limit Theorem or Law of Large Numbers . \\[ \\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\] Following that, we can compute the standard error: \\[ \\sigma_{\\bar{x}} = \\sqrt \\frac{\\sigma^2_{\\bar{x}}}{n} = \\frac{\\sigma}{\\sqrt{n}} \\] However, the population variance is usually unknown. Thus, it can be approximated using the Sample Variance, which is an unbiased estimator for it. The result is known as an Estimate for the Standard Error : \\[ \\hat{\\sigma}_{\\bar{x}} = \\sqrt \\frac{s^2}{n} = \\frac{s}{\\sqrt{n}} \\] Note that only the sample variance is an unbiased estimator for the population variance. Although it may look like it, the sample SD is NOT an unbiased estimator for the population SD. Confidence Interval \u00b6 Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval . The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value . In other words, if a large number of confidence intervals constructed in the same manner were to be made, \\((1-\\alpha)%\\) of them would contain the true value. \\[ P(- \\text{Margin of Error} < \\theta < \\text{Margin of Error}) = 1 - \\alpha \\] The Margin of Error represents the range of values on either side of the point estimate that the true value could lie. For instance, for a 95% confidence interval, the confidence lies within the 0.025 and 0.975 percentile of the sampling distribution. The margin of error can be calculated by finding the corresponding values of the sampling distribution at these percentiles. Consider the 95% confidence interval for the Sample Mean , which is normally distributed. For convenience, it is usually normalized such that it will become a Standard Normal Distribution : \\[ \\begin{aligned} P(-1.96 < \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} < 1.96) &= 0.95 \\\\ P(\\bar{x} - 1.96 \\frac{\\sigma}{\\sqrt n} < \\mu < \\bar{x} + 1.96 \\frac{\\sigma}{\\sqrt n}) &= 0.95 \\\\ \\end{aligned} \\] \\[ \\therefore \\text{Margin of Error} = \\bar{x} + Z_{\\frac{\\alpha}{2}} * \\frac{\\sigma}{\\sqrt n} \\] Hypothesis Testing \u00b6 Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a Hypothesis which is a conjecture about the population parameters: Null Hypothesis - What is currently believed to be true Alternative Hypothesis - What is to be proven A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis. Assuming the Null Hypothesis is true , the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme . If the p-value is smaller than a pre-determined level of Statistical Significance ( \\(\\alpha\\) ), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected. Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot. Thus, the hypotheses are usually constructed such that the two hypothesis are complementary , such that rejecting the null allows acceptance of the alternative, leading to a definitive insight. In layman terms, a hypothesis test is a test of extremeness . Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true. We distinguish between the two mathematically through \\(\\alpha\\) . It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\) , then it is likely that the observation was not due to chance but instead because the null was false . Alternatively, instead of comparing p-value to \\(alpha\\) , the test-statistic and the corresponding value of \\(\\alpha\\) on the sampling distribution can be used. It is known as the Critical Value , which represents the boundary of Reject Null Do not Reject Null p-value smaller than \\(\\alpha\\) p-value smaller than \\(\\alpha\\) test-statistic larger than critical value test-statistic smaller than critical value Let the random variable \\(T\\) denote the test statistics. There are many different kinds of test statistics depending on the distribution and what is being investigated. Z-statistic \u00b6 The most simple test statistic involve the Sample Mean , which is normally distributed: \\[ T = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] If the population variance is known , then the test statistic has a Standard Normal Distribution and thus the test-statistic is known as a Z-Statistic . \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\sqrt n \\\\ &= Z * \\sqrt n \\\\ \\therefore T &\\sim N(0,1) \\end{aligned} \\] t-statistic \u00b6 If the population variance is unknown, then it will be approximated by the Sample Variance . Through algebraic manipulation, the test statistic can still be expressed in the form of a Z variable, but with an additional term: \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\\\ &= \\frac{\\bar{x} - \\mu}{s} * \\sqrt n \\\\ &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\frac{\\sigma}{s} * \\sqrt n \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{s^2}{\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{(n-1)s^2}{(n-1)\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt {\\frac{(n-1)s^2}{\\sigma^2} * \\frac{1}{n-1}} \\\\ \\end{aligned} \\] The additional term can be shown to have a Chi-Squared Distribution of \\(n-1\\) degrees of freedom, which by definition is the sum of \\(n-1\\) independent standard normal variables: \\[ \\begin{aligned} \\chi^2_n &= Z^2 \\\\ &= \\sum \\left(\\frac {\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac {(x_i - \\bar{x}) + (\\bar{x} - \\mu)}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac{x_i - \\bar{x}}{\\sigma}\\right)^2 + \\sum \\left(\\frac{\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\frac{1}{\\sigma^2} \\sum (x_i - \\bar{x})^2 + \\sum Z^2 + 0 \\\\ &= \\frac{(n-1)}{\\sigma^2} \\frac{\\sum (x_i - \\bar{x})^2}{n-1} + \\chi^2_1 \\\\ &= \\frac{(n-1)s^2}{\\sigma^2} + \\chi^2_1 \\\\ \\end{aligned} \\] \\[\\therefore \\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\] Thus, the test statistic is the ratio of a Standard Normal Variable to the squareroot of a Chi Squared Variable (divided by its degrees of freedom). By definition, this test statistic has a t-distribution with the same degrees of freedom and is known as the t-statistic : \\[ \\begin{align*} T &= \\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\\\ &= t_{n-1} * \\sqrt{n} \\\\ \\end{align*} \\] \\[\\therefore T \\sim t_{n-1}\\] Despite the slightly convoluted proof, the t-distribution is simply a standard normal distribution with heavier tails . This means that extreme values are slightly more likely, which is meant to account for the increase in variability due to the use of the sample variance rather than the population variance. F-statistic \u00b6 The square of t-statisic has an F-Distribution , which is defined as the ratio of two independent chi-square variables (divided by their respective degrees of freedom). It has two dimensions for its degree of freedom, reflecting the two chi-square variables. \\[ F_{m,n} = \\frac{\\frac{\\chi_m}{m}}{\\frac{\\chi_n}{n}} \\] The square of a t-statistic follows an F-distribution because: The square of the standard normal variable in the numerator becomes a \\(\\chi_1\\) variable The squareroot is removed in the denominator, becoming a \\(\\chi_{n-1}\\) over its degree of freedom \\[ \\begin{aligned} t_{n-1}^2 &= \\left(\\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\right)^2 \\\\ &= \\frac{\\chi_1}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= \\frac{\\frac{\\chi_1}{1}}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= F_{1,n-1} * n \\end{aligned} \\] \\[ \\therefore t_{n-1}^2 \\sim F_{1, n-1} \\] Thus, the square of the t-statistic is known as the F-statistic , which is usually used to test for the equality of variance . Maximum Likelihood Estimation \u00b6 If the population distribution is known , there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics, known as Maximum Likelihood Estimation (MLE). There are an infinite number of variations of the distribution that could have resulted in the sample, each with different parameters . Technically speaking, any set of parameters could have resulted in the sample. However, the goal of MLE is to find the set of parameters that are most likely to result in the sample ; in other words, the probability of obtaining this sample is the highest with this set of paramters than any other set. The probability of obtaining the sample is known as its Likelihood : \\[ L(\\theta \\mid x) = P_{\\theta} (X = x) \\] Warning Likelihood functions and PMF/PDFs are often confused with one another as they involve the same expression. The key is understanding what is given and what is random , which results in the subtle but differing notation: PMF/PDF : Given parameters, outcomes are random; \\(P_{X}(x)\\) Likelihood : Given outcomes, parameters are random; \\(P_{\\theta}(x)\\) Assuming that the sample is iid, the likehood for the entire sample is the product of the likelihood for each observation , known as the Likelihood Function : \\[ L(\\theta) = \\prod P_{\\theta}(X = x_i) \\] Note For discrete distributions, \\(X = 0\\) is a valid observation and thus should be considered as well. The goal is to find the parameters that maximizes the likelihood function through calculus: \\[ \\frac{d}{d\\theta} L(\\theta) = 0 \\] In practice, especially when dealing with multiple parameters, the likelihood function is complicated to work with. Thus, a log transformation is often applied to simplify it, turning the product into a summation . This is known as the Log-Likelihood Function . Since the logarithm transform is monotonic, both the likelihood and log-likelihood functions share the same maximum . \\[ \\begin{aligned} \\ell (\\theta) &= \\ln L(\\theta) \\\\ \\therefore \\frac{d}{d\\theta} \\ell (\\theta) &= 0 \\end{aligned} \\] Practical Tips \u00b6 Some distributions have complicated PMF/PDFs that make working with them more complicated. Most questions will usually have some method to simplify the likelihood function . The first tip is to understand that since the likelihood function will be logarithm transformed and then differentiated, factors that contains ONLY constants can be dropped since they will inevitably be removed later: \\[ \\begin{aligned} L(\\theta) &= a * x \\\\ \\ell (\\theta) &= \\ln a + \\ln x \\\\ \\ell' (\\theta) &= \\frac{1}{x} \\\\ \\\\ \\therefore L(\\theta) \\propto x \\end{aligned} \\] The next tip is that if the parameters are embedded in the power of some constant, they should be combined together: \\[ \\begin{aligned} L(\\theta) &= a^{\\theta} \\cdot b^{\\theta} \\cdot c^{\\theta} \\\\ &= (a \\cdot b \\cdot c)^{\\theta} \\end{aligned} \\] The reverse also applies...divide power, may not be in the same term, could come from another term However, if the above terms for some reason are added instead of multiplied , then a substituition method would be better: \\[ \\begin{aligned} L(\\theta) &= e^{-\\frac{k}{100}} - \\left(e^{-\\frac{k}{100}} \\right)^2 \\\\ L(\\theta) &= p - p^2 \\end{aligned} \\] Note If multiple parameters are being estimated, then the likelihood function is partially differentiated to each variable instead The MLE parameters are then the combination of parameters that maximizes the function. Method of Moments \u00b6 An alternative method for estimating population parameters is the Method of Moments (MOM). It is based on the Law of Large Numbers , which states that the sample mean converges to the population mean (first raw moment), given a sufficiently large sample size . Thus, by equating the sample raw moments to the population raw moments , up to the number of parameters to estimate, we can solve for an estimate of the parameters. Note This process can be repeated for as many parameters there are: One Parameter : First moments equated Two Parameters : First & Second moment equated Most MLE questions will only require single parameter estimation. If two parameters are given, then there is usually some way to simplify it . \\[ \\begin{aligned} E(X^k) &= \\bar{x} \\\\ &= \\frac{\\sum x^k_i}{n} \\end{aligned} \\] Note Needless to say, \\(\\bar{x}\\) represents the average of the quantity being modelled , NOT the number of observations. If there are 10 observations of 2 claims, then we must compute the number of claims as \\(10 \\cdot 2 = 20\\) . The main advantage of this method is that it is computationally simpler than MLE. For certain known distributions, the MOM estimate and MLE estimate are the same, thus MOM can be used as a shortcut for MLE .","title":"Review of Statistical Theory"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/Review%20of%20Statistical%20Theory/#review-of-statistical-theory","text":"This section assumes some basic knowledge on Random Variables , which can be found under another set of notes covering a Review of Probability Theory .","title":"Review of Statistical Theory"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/Review%20of%20Statistical%20Theory/#overview-of-statistics","text":"Statistics is a discipline revolving around data. A Population refers to the theoretical set of all possible data of the event of interest. The goal of statistics is to determine certain attributes that summarizes or describes the population, known as Parameters . However, it is impossible to study the entire population at once, thus a subset of the population is studied instead, known as the Sample . Attributes that summarize or describe the sample are known as Statistics . Ideally, the sample is representative of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used estimate population parameters. We distinguish between the two (when they have the same notation) through the Hat accent (^) - Population Parameters are their written without the hat ( \\(x\\) ) while their corresponding sample statistics are written with the hat ( \\(\\hat{x}\\) ).","title":"Overview of Statistics"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/Review%20of%20Statistical%20Theory/#common-sample-statistics","text":"The Mean is the average of the population. Population Mean Sample Mean \\(\\mu = \\sum\\limits_{i=i}^n x_i * p(x_i)\\) \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=i}^n x_i\\) Mean is a measure of centrality But if there are outliers, then median is better The Variance measures the spread of values about the mean. However, the units of Variance are unintuitive, thus we consider the Standard Deviation for more practical purposes, which is the square root of the variance . Population Variance Sample Variance \\(\\sigma^2 = \\sum\\limits_{i=i}^n (x_i - \\mu)^2 * p(x_i)\\) \\(s^2 = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x})^2\\) Covariance is a measure of the linear relationship between two variables: Positive Covariance - Variables move in the same direction Negative Covariance - Variables move in opposite directions Population Covariance Sample Covariance \\(\\sigma_{x, y} = \\mu_{xy} - \\mu_x \\mu_y\\) \\(\\hat{\\sigma}_{x, y} = \\frac{1}{n-1} \\sum\\limits_{i=i}^n (x_i - \\bar{x}) * (y_i - \\bar{y})\\) However, there are two issues with Covariance - the units are unintuitive (similar to variance) and there is no benchmark as to what constitutes a strong/weak relationship. Thus, the Correlation is an adjusted measure of the relationship between -1 and 1 . Population Correlation Sample Correlation \\(\\rho = \\frac{\\sigma_{x,y}}{\\sigma_x * \\sigma_y}\\) \\(r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x} \\sum(y_i - \\bar{y})}\\) Properties of Estimators Biased Consistent Efficient Degrees of freedom","title":"Common Sample Statistics"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/Review%20of%20Statistical%20Theory/#sampling-distribution","text":"Whenever a sample is drawn from a population and a statistic is calculated, it is known as a Point Estimate . Due to measurement error, a different sample would be drawn each time, thus leading to a different point estimate . If this process were to be repeated a large number times, the probability distribution of the resulting point estimates is known as the Sampling Distribution of the statistic. There is no rule surrounding the sampling distribution - it depends on the distribution of the population, statistic being measured, sampling method etc. The standard deviation of this sampling distribution is known as the Standard Error of the statistic: \\[ \\sigma_{\\theta} = \\sqrt {\\sigma^2_{\\theta}} \\] A special case is the Sample Mean . If the population is normally distributed, then it is normally distributed as well. Regardless of the population distribution, it is also approximately normally distributed through the Central Limit Theorem or Law of Large Numbers . \\[ \\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\] Following that, we can compute the standard error: \\[ \\sigma_{\\bar{x}} = \\sqrt \\frac{\\sigma^2_{\\bar{x}}}{n} = \\frac{\\sigma}{\\sqrt{n}} \\] However, the population variance is usually unknown. Thus, it can be approximated using the Sample Variance, which is an unbiased estimator for it. The result is known as an Estimate for the Standard Error : \\[ \\hat{\\sigma}_{\\bar{x}} = \\sqrt \\frac{s^2}{n} = \\frac{s}{\\sqrt{n}} \\] Note that only the sample variance is an unbiased estimator for the population variance. Although it may look like it, the sample SD is NOT an unbiased estimator for the population SD.","title":"Sampling Distribution"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/Review%20of%20Statistical%20Theory/#confidence-interval","text":"Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a point estimate, a range of estimates is used, known as a Confidence Interval . The interval is made a chosen Confidence Level which represents the proportion of confidence intervals that will contain the true value . In other words, if a large number of confidence intervals constructed in the same manner were to be made, \\((1-\\alpha)%\\) of them would contain the true value. \\[ P(- \\text{Margin of Error} < \\theta < \\text{Margin of Error}) = 1 - \\alpha \\] The Margin of Error represents the range of values on either side of the point estimate that the true value could lie. For instance, for a 95% confidence interval, the confidence lies within the 0.025 and 0.975 percentile of the sampling distribution. The margin of error can be calculated by finding the corresponding values of the sampling distribution at these percentiles. Consider the 95% confidence interval for the Sample Mean , which is normally distributed. For convenience, it is usually normalized such that it will become a Standard Normal Distribution : \\[ \\begin{aligned} P(-1.96 < \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} < 1.96) &= 0.95 \\\\ P(\\bar{x} - 1.96 \\frac{\\sigma}{\\sqrt n} < \\mu < \\bar{x} + 1.96 \\frac{\\sigma}{\\sqrt n}) &= 0.95 \\\\ \\end{aligned} \\] \\[ \\therefore \\text{Margin of Error} = \\bar{x} + Z_{\\frac{\\alpha}{2}} * \\frac{\\sigma}{\\sqrt n} \\]","title":"Confidence Interval"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/Review%20of%20Statistical%20Theory/#hypothesis-testing","text":"Hypothesis Testing is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a Hypothesis which is a conjecture about the population parameters: Null Hypothesis - What is currently believed to be true Alternative Hypothesis - What is to be proven A Test Statistic is then calculated, which would quantify the behaviour of population such that it would distinguish the null and alternative hypothesis. Assuming the Null Hypothesis is true , the sampling distribution of the test statistic is determined. From the sampling distribution, the p-value is calculated, which is the probability of observing the calculated sample statistic or more extreme . If the p-value is smaller than a pre-determined level of Statistical Significance ( \\(\\alpha\\) ), then the test is \"successful\" and the null hypothesis is rejected. If not, then the test \"fails\" and the null is not rejected. Note that it rejecting the null hypothesis does NOT mean that the alternative hypothesis is accepted - EG. Rejected then hypothesis of a large paw print being from a bear does not mean that it is from Bigfoot. Thus, the hypotheses are usually constructed such that the two hypothesis are complementary , such that rejecting the null allows acceptance of the alternative, leading to a definitive insight. In layman terms, a hypothesis test is a test of extremeness . Assuming the null hypothesis is true, how extreme (low probability) is the observed sample? If the p-value is sufficiently low, it means that the sample is rare - it could be due to pure chance or that it is actually not rare because the null is not true. We distinguish between the two mathematically through \\(\\alpha\\) . It is the probability of a False Positive - that the result obtained was by pure chance. It is typically set at 5%, which means that 5% of all statistics calculated are expected to be extreme under the null. Thus, if the p-value of the sample is smaller than \\(\\alpha\\) , then it is likely that the observation was not due to chance but instead because the null was false . Alternatively, instead of comparing p-value to \\(alpha\\) , the test-statistic and the corresponding value of \\(\\alpha\\) on the sampling distribution can be used. It is known as the Critical Value , which represents the boundary of Reject Null Do not Reject Null p-value smaller than \\(\\alpha\\) p-value smaller than \\(\\alpha\\) test-statistic larger than critical value test-statistic smaller than critical value Let the random variable \\(T\\) denote the test statistics. There are many different kinds of test statistics depending on the distribution and what is being investigated.","title":"Hypothesis Testing"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/Review%20of%20Statistical%20Theory/#z-statistic","text":"The most simple test statistic involve the Sample Mean , which is normally distributed: \\[ T = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\] If the population variance is known , then the test statistic has a Standard Normal Distribution and thus the test-statistic is known as a Z-Statistic . \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\sqrt n \\\\ &= Z * \\sqrt n \\\\ \\therefore T &\\sim N(0,1) \\end{aligned} \\]","title":"Z-statistic"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/Review%20of%20Statistical%20Theory/#t-statistic","text":"If the population variance is unknown, then it will be approximated by the Sample Variance . Through algebraic manipulation, the test statistic can still be expressed in the form of a Z variable, but with an additional term: \\[ \\begin{aligned} T &= \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\\\ &= \\frac{\\bar{x} - \\mu}{s} * \\sqrt n \\\\ &= \\frac{\\bar{x} - \\mu}{\\sigma} * \\frac{\\sigma}{s} * \\sqrt n \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{s^2}{\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt \\frac{(n-1)s^2}{(n-1)\\sigma^2} \\\\ &= Z\\sqrt n \\div \\sqrt {\\frac{(n-1)s^2}{\\sigma^2} * \\frac{1}{n-1}} \\\\ \\end{aligned} \\] The additional term can be shown to have a Chi-Squared Distribution of \\(n-1\\) degrees of freedom, which by definition is the sum of \\(n-1\\) independent standard normal variables: \\[ \\begin{aligned} \\chi^2_n &= Z^2 \\\\ &= \\sum \\left(\\frac {\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac {(x_i - \\bar{x}) + (\\bar{x} - \\mu)}{\\sigma}\\right)^2 \\\\ &= \\sum \\left(\\frac{x_i - \\bar{x}}{\\sigma}\\right)^2 + \\sum \\left(\\frac{\\bar{x} - \\mu}{\\sigma}\\right)^2 \\\\ &= \\frac{1}{\\sigma^2} \\sum (x_i - \\bar{x})^2 + \\sum Z^2 + 0 \\\\ &= \\frac{(n-1)}{\\sigma^2} \\frac{\\sum (x_i - \\bar{x})^2}{n-1} + \\chi^2_1 \\\\ &= \\frac{(n-1)s^2}{\\sigma^2} + \\chi^2_1 \\\\ \\end{aligned} \\] \\[\\therefore \\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\] Thus, the test statistic is the ratio of a Standard Normal Variable to the squareroot of a Chi Squared Variable (divided by its degrees of freedom). By definition, this test statistic has a t-distribution with the same degrees of freedom and is known as the t-statistic : \\[ \\begin{align*} T &= \\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\\\ &= t_{n-1} * \\sqrt{n} \\\\ \\end{align*} \\] \\[\\therefore T \\sim t_{n-1}\\] Despite the slightly convoluted proof, the t-distribution is simply a standard normal distribution with heavier tails . This means that extreme values are slightly more likely, which is meant to account for the increase in variability due to the use of the sample variance rather than the population variance.","title":"t-statistic"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/Review%20of%20Statistical%20Theory/#f-statistic","text":"The square of t-statisic has an F-Distribution , which is defined as the ratio of two independent chi-square variables (divided by their respective degrees of freedom). It has two dimensions for its degree of freedom, reflecting the two chi-square variables. \\[ F_{m,n} = \\frac{\\frac{\\chi_m}{m}}{\\frac{\\chi_n}{n}} \\] The square of a t-statistic follows an F-distribution because: The square of the standard normal variable in the numerator becomes a \\(\\chi_1\\) variable The squareroot is removed in the denominator, becoming a \\(\\chi_{n-1}\\) over its degree of freedom \\[ \\begin{aligned} t_{n-1}^2 &= \\left(\\frac{Z}{\\sqrt \\frac{\\chi^2_{n-1}}{n-1}} * \\sqrt n \\right)^2 \\\\ &= \\frac{\\chi_1}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= \\frac{\\frac{\\chi_1}{1}}{\\frac{\\chi^2_{n-1}}{n-1}} * n \\\\ &= F_{1,n-1} * n \\end{aligned} \\] \\[ \\therefore t_{n-1}^2 \\sim F_{1, n-1} \\] Thus, the square of the t-statistic is known as the F-statistic , which is usually used to test for the equality of variance .","title":"F-statistic"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/Review%20of%20Statistical%20Theory/#maximum-likelihood-estimation","text":"If the population distribution is known , there is an alternative method of estimating the parameters apart from calculating the corresponding sample statistics, known as Maximum Likelihood Estimation (MLE). There are an infinite number of variations of the distribution that could have resulted in the sample, each with different parameters . Technically speaking, any set of parameters could have resulted in the sample. However, the goal of MLE is to find the set of parameters that are most likely to result in the sample ; in other words, the probability of obtaining this sample is the highest with this set of paramters than any other set. The probability of obtaining the sample is known as its Likelihood : \\[ L(\\theta \\mid x) = P_{\\theta} (X = x) \\] Warning Likelihood functions and PMF/PDFs are often confused with one another as they involve the same expression. The key is understanding what is given and what is random , which results in the subtle but differing notation: PMF/PDF : Given parameters, outcomes are random; \\(P_{X}(x)\\) Likelihood : Given outcomes, parameters are random; \\(P_{\\theta}(x)\\) Assuming that the sample is iid, the likehood for the entire sample is the product of the likelihood for each observation , known as the Likelihood Function : \\[ L(\\theta) = \\prod P_{\\theta}(X = x_i) \\] Note For discrete distributions, \\(X = 0\\) is a valid observation and thus should be considered as well. The goal is to find the parameters that maximizes the likelihood function through calculus: \\[ \\frac{d}{d\\theta} L(\\theta) = 0 \\] In practice, especially when dealing with multiple parameters, the likelihood function is complicated to work with. Thus, a log transformation is often applied to simplify it, turning the product into a summation . This is known as the Log-Likelihood Function . Since the logarithm transform is monotonic, both the likelihood and log-likelihood functions share the same maximum . \\[ \\begin{aligned} \\ell (\\theta) &= \\ln L(\\theta) \\\\ \\therefore \\frac{d}{d\\theta} \\ell (\\theta) &= 0 \\end{aligned} \\]","title":"Maximum Likelihood Estimation"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/Review%20of%20Statistical%20Theory/#practical-tips","text":"Some distributions have complicated PMF/PDFs that make working with them more complicated. Most questions will usually have some method to simplify the likelihood function . The first tip is to understand that since the likelihood function will be logarithm transformed and then differentiated, factors that contains ONLY constants can be dropped since they will inevitably be removed later: \\[ \\begin{aligned} L(\\theta) &= a * x \\\\ \\ell (\\theta) &= \\ln a + \\ln x \\\\ \\ell' (\\theta) &= \\frac{1}{x} \\\\ \\\\ \\therefore L(\\theta) \\propto x \\end{aligned} \\] The next tip is that if the parameters are embedded in the power of some constant, they should be combined together: \\[ \\begin{aligned} L(\\theta) &= a^{\\theta} \\cdot b^{\\theta} \\cdot c^{\\theta} \\\\ &= (a \\cdot b \\cdot c)^{\\theta} \\end{aligned} \\] The reverse also applies...divide power, may not be in the same term, could come from another term However, if the above terms for some reason are added instead of multiplied , then a substituition method would be better: \\[ \\begin{aligned} L(\\theta) &= e^{-\\frac{k}{100}} - \\left(e^{-\\frac{k}{100}} \\right)^2 \\\\ L(\\theta) &= p - p^2 \\end{aligned} \\] Note If multiple parameters are being estimated, then the likelihood function is partially differentiated to each variable instead The MLE parameters are then the combination of parameters that maximizes the function.","title":"Practical Tips"},{"location":"3.%20Predictive%20Analytics/1.%20ASA-SRM/Review%20of%20Statistical%20Theory/#method-of-moments","text":"An alternative method for estimating population parameters is the Method of Moments (MOM). It is based on the Law of Large Numbers , which states that the sample mean converges to the population mean (first raw moment), given a sufficiently large sample size . Thus, by equating the sample raw moments to the population raw moments , up to the number of parameters to estimate, we can solve for an estimate of the parameters. Note This process can be repeated for as many parameters there are: One Parameter : First moments equated Two Parameters : First & Second moment equated Most MLE questions will only require single parameter estimation. If two parameters are given, then there is usually some way to simplify it . \\[ \\begin{aligned} E(X^k) &= \\bar{x} \\\\ &= \\frac{\\sum x^k_i}{n} \\end{aligned} \\] Note Needless to say, \\(\\bar{x}\\) represents the average of the quantity being modelled , NOT the number of observations. If there are 10 observations of 2 claims, then we must compute the number of claims as \\(10 \\cdot 2 = 20\\) . The main advantage of this method is that it is computationally simpler than MLE. For certain known distributions, the MOM estimate and MLE estimate are the same, thus MOM can be used as a shortcut for MLE .","title":"Method of Moments"}]}