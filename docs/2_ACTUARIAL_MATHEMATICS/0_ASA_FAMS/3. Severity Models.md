# **Severity Models**

## **Severity Distributions**

The **size of the loss** is usually known as **Loss Severity** and is typically modelled with a continuous distribution.

Let $X$ be the random variable denoting the size of the loss.

This section will cover *most* of the relevant severity distributions, but similarly note that there is **no need to memorize** anything as they are all provided in the formula sheet.

### **Normal Distribution**

The **Normal Distribution** is used to model processes where **outcomes close to the mean are common** and become increasingly less common as it strays further from it.

The unique property of the normal distribution in that it has **two parameters**, which are already its **Mean** $\mu$ and **Variance** $\sigma^2$:

$$
\begin{aligned}
    X &\sim  N(\mu, \sigma^2) \\
    \\
    f(x) &= \frac{1}{\sigma \sqrt{2\pi}} \cdot e^{-\frac{(x-\mu)^2}{2\sigma^2}} \\
    \\
    E(X) &= \mu \\
    Var (X) &= \sigma^2 \\
\end{aligned}
$$

Another interesting property of the normal distribution is that:

* Roughly 68.3% of the data is **within 1 SD** of the average ($\mu - 1\sigma \lt x \lt \mu + 1\sigma$)
* Roughly 95.5% of the data is **within 2 SD** of the average ($\mu - 2\sigma \lt x \lt \mu + 2\sigma$)
* Roughly 99.7% of the data is **within 3 SD** of the average ($\mu - 3\sigma \lt x \lt \mu + 3\sigma$)

<!-- Obtained from W3 Schools -->
![Normal Distribution](Assets/3.%20Severity%20Models.md/Normal%20Distribution.png){.center}

The sum of $k$ independent normal random variables is still normal:

$$
\begin{aligned}
    X_k &\sim N(\mu, \sigma^2) \\
    X &= X_1 + X_2 + \dots + X_k \\
    \therefore X &\sim N(\mu_1 + \mu_2 + ... + \mu_k, \sigma^2_1 + \sigma^2_2 + ... + \sigma_k)
\end{aligned}
$$

Another key property is that they are **symmetrical about their mean**:

$$
\begin{aligned}
    P(Z \lt -a)
    &= P(Z \gt a) \\
    &= 1 - P(Z \lt a)
\end{aligned}
$$

<!-- Obtained from Coaching Actuaries -->
![Symmetrical](Assets/3.%20Severity%20Models.md/Symmetrical.png){.center}

#### **Standard Normal Distribution**

If the parameters of the normal distribution are 0 and 1 respectively, then it is known as the **Standard Normal Distribution**, specially denoted by $Z$.

$$
    Z \sim N(0, 1)
$$

<!-- Obtained from Scribbr -->
![Standard Normal Distribution](Assets/3.%20Severity%20Models.md/Standard%20Normal%20Distribution.png){.center}

Any normal distribution can be converted into a standard normal through **Standardization**:

$$
    Z = \frac{X - E(X)}{\sqrt{Var (X)}}
$$

!!! Note

    An easy way to remember is that the above transformation sets the mean to 0; $E(X) - E(X) = 0$ and sets the variance to 1; $\frac{Var(X)}{Var(X)} = 1$.

The CDF of a standard normal distribution is denoted by $\Phi$:

$$
    P(Z \le z) = \Phi(z)
$$

!!! Tip

    The standard normal distribution is **symmetrical about the origin**. This allows us to convert negative values to positive ones:

    $$
    \begin{aligned}
        P(Z \lt -z)
        &= P(Z \gt z) \\
        &= 1 - P(Z \lt z) \\
        &= 1 - \Phi(z)
    \end{aligned}
    $$

    <!-- Obtained from Coaching Actuaries -->
    ![Standard Normal Symmetry](Assets/3.%20Severity%20Models.md/Standard%20Normal%20Symmetry.png){.center}

Since the exam provides values for the **CDF of Z**, most calculations involving a normal distribution will have first have to be standardized and then calculated using its CDF:

$$
\begin{aligned}
    P(X \le x)
    &= P \left(\frac{X - E(X)}{\sqrt{Var (X)}} \le \frac{x - E(X)}{\sqrt{Var (X)}} \right) \\
    &= P \left(Z \le \frac{x - E(X)}{\sqrt{Var (X)}} \right) \\
    &= \Phi \left(\frac{x - E(X)}{\sqrt{Var (X)}} \right)
\end{aligned}
$$

The reverse is also common whereby the **probability is provided** to solve for the value of Z and hence the value of the underlying variable. The issue is that most of the time, the probability provided **will not be exactly equal** to the values on the table. In such a case, the **midpoint between two values of Z** will be taken:

* Given: $P = 0.95$
* Table Value: $Z = 1.64, P = 0.9495$
* Table Value: $Z = 1.65, P = 0.9505$
* Midpoint: $Z = 1.645$

#### **Lognormal Distribution**

A **exponential transformed** normal distribution is known as the **Lognormal Distribution** as it has a logarithm applied to it.

It shares the **same two parameters as a regular normal distribution**, but they are **NOT the mean and variance** of the lognormal distribution.

$$
\begin{aligned}
    Y &= e^X \\
    Y &\sim \text{Lognormal} (\mu, \sigma^2) \\
    \\
    f(y) &= \frac{1}{x\sigma \sqrt{2\pi}} \cdot e^{-\frac{(\ln x - \mu)^2}{2\sigma^2}} \\
    \\
    E(Y) &= e^{\mu + 0.5 \sigma^2} \\
    Var (Y) &= E(Y)^2 \cdot (e^{\sigma^2} - 1)
\end{aligned}
$$

!!! Warning

    Given how similar the two distributions are, it is easy to mix them up.

    It is a common mistake to think that the lognormal parameters are its mean and variance. Note that since it is transformed from a normal distribution, it **must use its parameters**, but they are NOT its mean and variance.

    Another common mistake is thinking that the PDFs are identical, given how similar they look. However, there are **two key differences**:

    1. The denominator has an extra $x$ multiplied to it
    2. The $x$ in the exponent has become $\ln x$

The CDF of the log distribution can also be calculated through the **standard normal distribution**:

$$
\begin{aligned}
    P(Y \le y)
    &= P(e^X \le y) \\
    &= P(X \le \ln y) \\
    &= P \left(\frac{X - \mu}{\sigma} \le \frac{\ln y - \mu}{\sigma} \right) \\
    &= P \left(Z \le \frac{\ln y - \mu}{\sigma} \right) \\
    &= \Phi \left(\frac{\ln y - \mu}{\sigma} \right)
\end{aligned}
$$

Note that since the sum of independent normal variables is still normal, then the **product** of indepdent lognormal variables is **still lognormal**:

$$
\begin{aligned}
    X &= X_1 + X_2 + \dots + X_k \\
    X &\sim N(\mu_1 + \mu_2 + ... + \mu_k, \sigma^2_1 + \sigma^2_2 + ... + \sigma_k) \\
    \\
    Y
    &= e^X \\
    &= e^{X_1 + X_2 + \dots + X_k} \\
    &= e^{X_1} \cdot e^{X_2} \cdot (\dots) \cdot e^{X_k} \\
    \therefore Y &\sim \text{Lognormal}(\mu_1 + \mu_2 + ... + \mu_k, \sigma^2_1 + \sigma^2_2 + ... + \sigma_k)
\end{aligned}
$$

### **Exponential Distribution**

The **Exponential Distribution** is used to model the **time taken** for the **first event** to occur in a **Poisson Process**.

It has a single parameter $\lambda$, which represents the **rate** at which events occur, which can be LOOSELY thought of as the "probability" of the event occuring per unit time:

$$
\begin{aligned}
    X &\sim \text{Exponential} (\lambda) \\
    \\
    f(x) &= 1 - \lambda \cdot e^{-\lambda x} \\
    F(x) &= 1 - e^{-\lambda x} \\
    \\
    E(x) &= \frac{1}{\lambda} \\
    Var (x) &= \frac{1}{\lambda^2}
\end{aligned}
$$

Similar to the geometric distribution, the exponential distribution is **Memoryless**. Recall that the intuition is that the rate that the event occurs **remains constant over time**.

$$
\begin{aligned}
    P(X > m + x \mid X \ge m) &= P(X > x) \\
    E(X > m + x \mid X \ge m) &= E(X) \\
    Var(X > m + x \mid X \ge m) &= Var(X)
\end{aligned}
$$

### **Alternative Parameterization**

Given that the Exponential and Poisson Distributions are related, the exponential distribution can also be **parameterized in terms the poisson parameter $\mu$**.

In fact, the **rate is simply the inverse of the mean**:

$$
\begin{aligned}
    \lambda &= \frac{1}{\mu} \\
    \therefore X &\sim \text{Exponential} \left(\frac{1}{\mu} \right) \\
    \\
    f(x) &= 1 - \lambda \cdot e^{-\frac{x}{\mu}} \\
    F(x) &= 1 - e^{-\frac{x}{\mu}} \\
    \\
    E(x) &= \mu \\
    Var (x) &= \mu^2
\end{aligned}
$$

This version of the exponential distribution will be more useful for the exam as it is provided on the formula sheet. Note that the formula sheet uses $\theta$ instead of $\mu$, which is what will be used for all other exponential related distributions.

#### **Gamma Distribution**

The **Gamma Distribution** is used to model the **total time** till the $\alpha$-th event occurs in a Poisson Process.

<!-- Obtained from Wikipedia -->
![Poisson Process](Assets/3.%20Severity%20Models.md/Poisson%20Process.png){.center}

It has two parameters:

* $\alpha$: The number of events
* $\theta$: Mean

$$
\begin{aligned}
    X &\sim \text{Gamma}(\alpha, \theta) \\
    \\
    f(x) &= \frac{\theta e^{-\theta x} \cdot (\theta x)^{\alpha - 1}}{\Gamma(\alpha)} \\
    \Gamma(\alpha) &= (\alpha - 1)! \\
    \\
    E(X) &= \frac{\alpha}{\theta} \\
    Var (X) &= \frac{\alpha}{\theta^2}
\end{aligned}
$$

The CDF is hard to evaluate, thus we can determine it **intuitively** using the poisson process and poisson distribution. If $x$ is the time taken for the $\alpha$-th event to occur, then it means that $\alpha-1$ events occurred in the timespan of $x$.

Given that events occur at a rate of $\theta$, it means that **on average only $\theta \cdot x$ occur** in that period of time.

Thus, the probability of waiting $x$ for the $\alpha$-th event to occur is **equivalent to the probability that only $\alpha-1$ events occur in that time**:

$$
\begin{aligned}
    X &\sim \text{Gamma} (\alpha, \theta) \\
    N &\sim \text{Poisson} (\frac{x}{\theta}) \\
    \\
    \therefore P(X \le x) &= P(N \le \alpha - 1)
\end{aligned}
$$

The gamma distribution is actually the sum of $\alpha$ independent exponential variables with the same rate ($\theta$):

$$
\begin{aligned}
    Y_k &\sim \text{Exponential} (\theta) \\
    X &= Y_1 + Y_2 + \dots + Y_k \\
    X &\sim \text{Gamma} (\alpha, \theta)
\end{aligned}
$$

!!! Tip

    An exponential distribution is a **special case** of a gamma distribution with $\alpha = 1$.

Thus, the sum of independent gamma distributions is still gamma:

$$
\begin{aligned}
    X_k &\sim \text{Gamma} (\alpha_k, \theta) \\
    X &= X_1 + X_2 + \dots + X_k \\
    X &\sim \text{Gamma} (\alpha_1 + \alpha_2 + \dots + \alpha_k, \theta)
\end{aligned}
$$

#### **Weibull Distribution**

Similar to the Exponential Distribution, the Weibull Distribution is used to the model the time taken for the **first event** to occur in a poisson process. However, the difference is that unlike the exponential distribution, the **rate of the event occuring changes over time**.

!!! Info

    It is commonly used to model **machine survival**, where the event of interest is the failure of the machine. In other words, it models the probability of the machine failing.

It has two parameters:

* $\theta' = \frac{1}{\mu^{\frac{1}{k}}}$: The rate at which events occur
* $k$: The behaviour of the rate over time

The rate changes depending on the value of $k$:

* **Larger than 1** $(k \gt 1)$: Rate **increases** over time
* **Smaller than 1** $(k \lt 1)$ Rate **decreases** over time

Note that if $k=1$, it means that the rate that events occur is **constant** over time, which is simply **equivalent to an exponential distribution**. Thus, the weibull distribution is simply a transformed exponential distribution:

$$
    Y = X^{\frac{1}{k}}
$$

!!! Tip

    The exponential distribution is a **special case** of the weibull distribution where $k=1$.

$$
\begin{aligned}
    Y &\sim \text{Weibull} \left(\theta', k \right) \\
    Y &\sim \text{Weibull} \left(\frac{1}{\mu^{\frac{1}{k}}}, k \right) \\
    \\
    f(y) &= \frac{k \left(\frac{y}{\theta'} \right)^k \cdot e^{- \left(\frac{y}{\theta'} \right)^k}}{y} \\
    F(y) &= 1 - e^{\left(\frac{y}{\theta'} \right)^k} \\
    \\
    E(Y) &= \\
    Var (Y) &= \\
\end{aligned}
$$

### **Beta Distribution**

The **Beta Distribution** is a **probability distribution of probabilities**. In other words, it represents the distribution of all possible probabilities when we are unsure of the actual probability.

Although out of scope for this exam, in Bayesian Inference, the Beta distribution is the *Conjugate Prior* to the Binomial Distribution. In other words, the beta distribution **models the probability parameter in the binomial distribution**.

<!-- Obtained from Stack Exchange -->
![Beta Binomial](Assets/3.%20Severity%20Models.md/Beta%20Binomial.png){.center}

It has only two parameters, which represent the number of **prior** successes and failures:

* $a$: There are $a-1$ **prior successes** before the experiment
* $b$: There are $b-1$ **prior failures** before the experiment

$$
\begin{aligned}
    X &\sim \text{Beta} (a, b) \\
    \\
    f(x) &= f(x) = c \cdot x^{a-1} \cdot (1-x)^{b-1} \\
    \\
    E(X) &= \frac{a}{a+b} \\
    Var (X) &= \frac{ab}{(a+b)^2 \cdot (a+b+1)}
\end{aligned}
$$

Although the above is more common, the beta distribution can also be expressed **more generally** with a third parameter $\theta$:

$$
\begin{aligned}
    X &\sim \text{Beta} (a, b, \theta) \\
    \\
    f(x) &= \frac{\Gamma (a+b)}{\Gamma (a) \Gamma (b)} \left(\frac{x}{\theta} \right)^a \left(1 - \frac{x}{\theta} \right)^{b-1} \cdot \frac{1}{x} \\
    \\
    E(X) &= \frac{a}{a+b} \cdot \theta \\
    Var (X) &= \frac{ab}{(a+b)^2 \cdot (a+b+1)} \cdot \theta^2
\end{aligned}
$$

!!! Tip

    The "normal" beta distribution is a special case of the Generalized Beta Distribution with $\theta = 1$.

#### **Uniform Distribution**

The **Uniform Distribution** is used to model events where every outcome has an **equal probability of occuring**.

It has two parameters, which respectively represent the **lower and upper bound** of the possible outcomes:

$$
\begin{aligned}
    X &\sim \text{Uniform} (a, b) \\
    \\
    f(x) &= \frac{1}{b-a} \\
    F(x) &= \frac{x - a}{b - a} \\
    \\
    E(X) &= \frac{a+b}{2} \\
    Var (X) &= \frac{(a-b)^2}{12}
\end{aligned}
$$

The key property of a uniform distribution is how the parameters change when it is **shifted**:

$$
\begin{aligned}
    X &\sim \text{Uniform} (a, b) \\
    X \mid X \gt d &\sim \text{Uniform} (d, b) \\
    X-d &\sim \text{Uniform} (a-d, b-d) \\
    \therefore X-d \mid X \gt d &\sim \text{Uniform} (0, b-d)
\end{aligned}
$$

!!! Warning

    Although they look very similar, this should not be confused with the memoryless property of a distribution.

<!-- Obtained from Coaching Actuaries -->
![Uniform Transformation](Assets/3.%20Severity%20Models.md/Uniform%20Transformation.png){.center}

Note that the $(0,\theta)$ uniform distribution can also be thought of as a Generalized Beta Distribution with parameters $a = b = 1$. In other words, there are **no prior successes or failures**, thus each outcome has an **equal probability**.

### **Pareto Distribution**

The **Pareto Distribution** is used to model phenomena where few items account for a lot of it and a lot of items account for little of it.

For instance, the **Pareto Principle** is used to describe wealth inequality: A small 20% of people account for a large 80% of wealth while a large 80% of people account for 20% of wealth.

In statistical terms, it means that **large outcomes are rare** (20% of people, 80% of wealth) while **small outcomes are common** (80% of people, 20% of wealth).

It has two parameters:

* $\alpha$:
* $\theta$: Minimum value

$$
\begin{aligned}
    X &\sim \text{Pareto} (\alpha, \theta) \\
    \\
    f(x) &= \frac{\alpha \theta^{\alpha}}{(x+\theta)^{\alpha+1}} \\
    \\
    E(X) &= \frac{\theta}{\alpha - 1} \\
    Var(X) &= E(X)^2 \cdot \frac{\alpha}{\alpha - 2}
\end{aligned}
$$

The key property of a pareto distribution is how the parameters change when it is **shifted**:

$$
\begin{aligned}
    X &\sim \text{Pareto} (\alpha, \theta) \\
    X-d \mid x \gt d &\sim \text{Pareto} (\alpha, \theta + d)
\end{aligned}
$$

!!! Note

    This is similar to the shifting property of the uniform distribution.

    Thus the same warning applies -- although it looks similar, this should not be confused with the memoryless property of a distribution.

#### **Single Parameter Pareto**

If a pareto distribution is *added* by a constant, then the resulting distribution is known as a **Single Parameter Pareto Distribution**:

$$
\begin{aligned}
    Y = X + \theta \\
    \\
    X &\sim \text{Pareto}(\alpha, \theta) \\
    Y &\sim \text{SP Pareto}(\alpha, \theta)
\end{aligned}
$$

!!! Info

    $\theta$ is **not really a parameter** as it must be determined in advance, which is why it it known as a Single Parameter Pareto Distribution.

### **Parametric Distributions**

All the above distributions are those that are more commonly tested on the exam. However, many other distributions are also provided in the formula sheet but are much less likely to be tested due to their complexity.

**Parametric Distributions** are distributions that are completely determined by a set of quantities known as Parameters. By assigning all possible numerical values to the parameters, the resulting set of distributions is known as a **Parametric Distribution Family**.

<!-- Obtained from Coaching Actuaries -->
![Parametric Families](Assets/3.%20Severity%20Models.md/Parametric%20Families.png){.center}

An important parametric distribution family is the **Linear Exponential Family** (LEF).

A distribution $f(x, \theta)$ is part of the LEF if it fulfils the following criteria:

1. Its support **does not** depend on $\theta$
2. Its distribution function is in following form, where $p$, $q$ and $r$ are **generic functions**

$$
\begin{aligned}
    f(x, \theta) &= \frac{p(x) \cdot e^{r(\theta) \cdot x}}{q(\theta)} \\
    \\
    E(X) &= \frac{q'(\theta)}{r'(\theta) \cdot q(\theta)} \\
    Var (X) &= \frac{E'(X)}{r'(\theta)}
\end{aligned}
$$

Some examples of distributions belonging to the LEF:

1. Discrete: Poisson, Binomial & Negative Binomial
2. Gamma & Normal

Consider adding example, only p(x) can contain x, every other function cannot

#### **Scale Distribution**

If a parametric distribution, when **scaled by a constant** is still within the **same family of distributions**, then is it known as a **Scale Distribution**.

Scale Distributions typically have a special **Scale Parameter**, which fulfils the following conditions:

1. Scale parameter is multiplied by the same constant
2. All other parameters remain unchanged

All other parameters are typically known as **Shape Parameters**. For the purposes of the exam, $\theta$ is used to specially denote **Scale Parameters**.

However, note that there are multiple ways to parameterize a distribution and multiple ways to denote them, thus not all questions will follow the exam tables. Thus, it is important to practice how to identify them.

<!-- Obtained from Coaching Actuaries -->
![Parameter Types](Assets/3.%20Severity%20Models.md/Parameter%20Types.png){.center}

For any of the above scale distributions, if scaled by a constant $c$, their parameters get changed in the following manner:

$$
\begin{aligned}
    X &\sim \text{Distribution} (\text{Shape Parameters}, \theta) \\
    cX &\sim \text{Distribution} (\text{Shape Parameters}, c\theta) \\
    \\
    X &\sim \text{Normal} (\mu, \sigma^2) \\
    cX &\sim \text{Normal} (c\mu, (c\sigma)^2) \\
    \\
    X &\sim \text{Lognormal} (\mu, \sigma^2) \\
    cX &\sim \text{Lognormal} (\mu + \ln c, \sigma^2)
\end{aligned}
$$
