# **Frequency Models**

## **Frequency Distributions**

The **number of claims** is usually known as the **Loss Frequency** and is typically modelled with a discrete distribution.

Let $N$ be the random variable denoting the number of claims.

This section will cover all the relevant frequency distributions, but note that there is **no need to memorize** anything as they are all provided in the formula sheet.

### **Poisson Distribution**

The **Poisson Distribution** is used to count the number of random events in a **specified unit of space or time**.

It only has **one parameter** $\mu$, the **mean** number of occurrences in the specified unit of space or time. The key property is that its **mean and variance are both equal** to $\mu$.

$$
\begin{aligned}
    N &\sim \text{Poisson}(\mu) \\
    \\
    p_n &= \frac{e^{-\mu} \cdot \mu^k}{k!} \\
    \\
    E(N) &= \mu \\
    Var (N) &= \mu
\end{aligned}
$$

The sum of $k$ independent poisson random variables is **still poisson**:

$$
\begin{aligned}
    N_k &\sim \text{Poisson}(\mu_k) \\
    N &= N_1 + N_2 + ... + N_k \\
    \therefore N &\sim \text{Poisson} (\mu_1 + \mu_2 + ... + \mu_k)
\end{aligned}
$$

### **Bernoulli Distribution**

The **Bernoulli Distribution** is used to determine the outcome of a **Bernoulli Trial**. They are experiments with only **two possible outcomes**.

For a **Standard Bernoulli Distribution**, the two outcomes are **Successes and Failures**, denoted by 1 and 0 respectively:

$$
\begin{aligned}
    X &\sim \text{Bernoulli} (q) \\
    \\
    p_x &=
    \begin{cases}
        \text{Success } (1),& \text{Probability} = q, \\
        \text{Failure } (0),& \text{Probability} = 1-q
    \end{cases} \\
    \\
    E(X) &= q \\
    Var (X) &= q(1-q)
\end{aligned}
$$

However, the **Bernoulli Shortcut** generalizes this for **any two mutually exclusive outcomes**, denoted by $a$ and $b$ respectively:

$$
\begin{aligned}
    Y &= (a-b)X + b \\
    \\
    Y &=
    \begin{cases}
        \text{Outcome 1 } (a),& \text{Probability} = q, \\
        \text{Outcome 2 } (b),& \text{Probability} = 1-q
    \end{cases} \\
    \\
    E(Y)
    &= E[(a-b)X + b] \\
    &= (a-b)q + b \\
    \\
    Var (Y)
    &= Var[(a-b)X + b] \\
    &= (a-b)^2 \cdot q(1-q)
\end{aligned}
$$

Note that there is no need to memorize the mean and variance for the bernoulli shortcut - they can be easily derived from the standard distribution.

#### **Binomial Distribution**

The **Binomial Distribution** is used to count the **number of successes** out of a **fixed number of independent Standard Bernoulli Trials**.

It has **two parameters**:

1. The **number** of independent trials, $m$
2. The **probability** of success for each trial, $q$

$$
\begin{aligned}
    N &\sim \text{Binomial} (m, q) \\
    \\
    p_n &= {m \choose k} q^k (1-q)^{1-k} \\
    \\
    E(N) &= nq \\
    Var (N) &= nq(1-q)
\end{aligned}
$$

The binomial distribution is actually the **sum of $m$ independent standard bernoulli variables** with the *same probability*:

$$
\begin{aligned}
    X_k &\sim \text{Bernoulli} (q) \\
    N &= X_1 + X_2 + ... + X_m \\
    \therefore N &\sim \text{Binomial} (m, q)
\end{aligned}
$$

!!! Tip

    A bernoulli distribution is simply a binomial distribution with $m=1$.

Thus, the sum of $k$ independent binomial variables with the *same probability* will **still binomial**:

$$
\begin{aligned}
    N_k &\sim \text{Binomial}(m, q) \\
    N &= N_1 + N_2 + ... + N_k \\
    \therefore N &\sim \text{Binomial} (m_1 + m_2 + ... + m_k, q)
\end{aligned}
$$

This is intuitive, because the sum of binomial variables is actually the **sum of even more bernoulli variables**, which as shown previously, will follow the binomial distribution.

### **Geometric Distribution**

The **Geometric Distribution** can be understood in one of two ways, with respect to a **sequence of independent bernoulli trials**:

1. **Number of trials** needed to get the **first success**, $N \in \set{1, 2, 3, ...}$
2. **Number of failures** needed to get the **first success**, $F \in \set{0, 1, 2, ...}$

If not explicitly stated, the two intepretations can be distinguished from their supports - there must be at least 1 trial but there can be 0 failures. It is useful to **remember just one intepretation** and understand how they are related:

$$
    F = N-1
$$

The **second intepretation is preferred**, since that is what is provided in the formula sheet. Regardless, the Geometric distribution only has **one parameter**, the probability of success:

$$
\begin{aligned}
    N &\sim \text{Geom} (q) \\
    \\
    p_k &= (1-q)^{k} \cdot q \\
    \\
    E(N) &= \frac{1-p}{p} \\
    Var (N) &= \frac{1-p}{p^2}
\end{aligned}
$$

The key property is that it is **Memoryless**. Intuitively, the geometric distribution is "waiting" for the first success to occur.

However, it **does not matter how many failures have occurred**, the probability of the first success occuring in another $k$ failures is **independent of the history of the process**; it remains constant over time. The distribution "forgets" (memoryless) the current state that the process is in.

$$
\begin{aligned}
    P(N > m + n \mid N \ge m) &= P(N > n) \\
    E(N > m + n \mid N \ge m) &= E(N) \\
    Var(N > m + n \mid N \ge m) &= Var(N)
\end{aligned}
$$

#### **Negative Binomial Distribution**

The **Negative Binomial Distribution** is used to count the **number of failures** to get a **fixed number of successes**, with respect to a sequence of independent bernoulli trials.

It has two parameters:

1. The number of successes, $r$
2. The probability of success, $q$

$$
\begin{aligned}
    N &\sim \text{Negative Binomial} (r, q) \\
    \\
    p_n &= {{k+r-1} \choose {k}} (1-p)^k p^{r} \\
    \\
    E(N) &= \frac{r(1-p)}{p} \\
    Var (N) &= \frac{r(1-p)}{p^2}
\end{aligned}
$$

!!! Info

    The distribution is also referred to as **Pascals Distribution**, but is most commonly called the "negative" binomial because it uses a **negative integer in the binomial coefficient**:

    $$
    \begin{aligned}
        {n \choose k}
        &= \frac{n!}{k! \cdot (n-k)!} \\
        \\
        {{k+r-1} \choose {k}}
        &= \frac{(k+r-1)!}{k! \cdot (r-1)!} \\
        &= \frac{(k+r-1) \cdot (k+r-2) \cdot \dots \cdot (r)}{k!} \\
        &= (-1)^k \cdot \frac{(-r) \cdot (-r-1) \cdot \dots \cdot (-r-k+1)}{k!} \\
        &= (-1)^k {-r \choose k}
    \end{aligned}
    $$

The negative binomial distribution is actually the **sum of $r$ i.i.d. geometric variables** with the *same probability*:

$$
\begin{aligned}
    X_k &\sim \text{Geom} (q) \\
    N &= X_1 + X_2 + ... + X_r \\
    \therefore N &\sim \text{Negative Binomial} (r, q)
\end{aligned}
$$

!!! Tip

    A geometric distribution is simply a negative binomial distribution with $r=1$.

Thus, following the same logic as before, the sum of $k$ independent negative binomial variables with the same probability will **still be negative binomial**:

$$
\begin{aligned}
    N_k &\sim \text{Negative Binomial}(r, q) \\
    N &= N_1 + N_2 + ... + N_k \\
    \therefore N &\sim \text{Negative Binomial} (r_1 + r_2 + ... + r_k, q)
\end{aligned}
$$

### **Poisson Gamma Mixture**

Consider a poisson distribution with mean $\mu$ where $\mu$ follows a gamma distribution. The unconditional distribution actually follows a **Negative Binomial Distribution** with the same parameters as the gamma distribution.

$$
\begin{aligned}
    N \mid \mu &\sim \text{Poisson} (\mu) \\
    \mu &\sim \text{Gamma} (\alpha, \theta) \\
    \therefore N &\sim \text{Negative Binomial}(r = \alpha, \beta = \theta)
\end{aligned}
$$

The above result is a **Continuous Mixture** - it is the result of a mixture of an **infinite number of poisson distributions**. Continuous Mixtures are beyond the scope of this exam, which is why the proof is not shown. However, this specific result is important to know.

## **(a, b, 0) Class**

ONLY the distributions discussed above fall into the **(a, b, 0) class** of distributions, as their **PMFs follow the same recursion**:

$$
\begin{aligned}
    \frac{p_{n}}{p_{n-1}} &= a + \frac{b}{n} \\
    p_n &= \left(a + \frac{b}{n} \right) p_{n-1}
\end{aligned}
$$

$a$ and $b$ are constants that are **unique to each distribution** while the "0" comes from the fact that the recursion starts from $n-1=0$.

!!! Tip

    The parameters for each distribution can also be determined from the values of $a$ and $b$.

<!-- Obtained from MBFinan Exam C -->
![ab0 class](Assets/2.%20Frequency%20Models.md/ab0%20class.png){.center}

Notice that each distribution has a **different signs** for $a$. Thus, given the recursive equation, the **underlying distribution can be determined** based on the sign of $a$.

### **Choosing Distributions**

Given a sample of data, we need to decide **which distribution** best represents it. The general idea is that each distribution has some **unique characteristics** - if the dataset matches those characteristics, then the distribution should be used.

The first method involves the **Mean and Variance** of the distribution. Notice that for all three distributions, the mean and variance have a **different relative size to one another**. Thus, by comparing the **Sample Mean and Unbiased Sample Variance** of the dataset, it can be matched to the corresponding distribution.

<!-- Obtained from Coaching Actuaries -->
![MeanVariance](Assets/2.%20Frequency%20Models.md/MeanVariance.png){.center}

!!! Warning

    It is almost impossible (whether in practice or theory) to find a sample mean and variance that is *exactly equal* to one another.

    Generally speaking, if they are sufficiently close together, then they can be assumed to be equal. However, being "sufficiently close together" is arbitrary.

    Thus, a better way would be to **compare the ratio** of the two values - if the ratio is **around 1.00**, then it can be concluded that the **two values are equal**.

The second method involves the sign of $a$ in the recursion process. Since all three distributions have different signs for $a$, their probabilities will **move differently** as $n$ increases. Thus, by observing how the probability of the sample changes, it can be matched to the corresponding distribution.

However, we must first **multiply the ratio of the probabilities by $n$** in order to obtain a smooth linear line that clearly exhibits the relationships:

$$
\begin{aligned}
    \frac{p_{n}}{p_{n-1}} &= a + \frac{b}{n} \\
    \frac{n \cdot p_{n}}{p_{n-1}} &= an + b
\end{aligned}
$$

<!-- Obtained from Coaching Actuaries -->
![Recursion Slope](Assets/2.%20Frequency%20Models.md/Recursion%20Slope.png){.center}

## **(a, b, 1) Class**

When using the (a, b, 0) class of distributions to model experiments, one common problem is that the **probability at $n=0$ does not match experience**.

Thus, there is a need to **modify** the distribution such that the **probability at 0 is at the desired level**. This modification is known as the **Zero-Modification**.

!!! Note

    If the desired level is 0, then the modification is known as a **Zero-Truncation**, which is a special case of the zero-modification. This is because this **truncates (removes) the possibility of 0** from the random variable.

This is done by **directly changing** the probability at 0 to the desired level. However, this modification alone would cause the sum of probabilities to deviate from 1, thus the probabilities at all other levels of the support must be **scaled such that they sum to 1**.

Let $N^M$ be the zero-modified distribution and $p^{M}_{n}$ be its PMF.

$$
    p^{M}_{n} = \frac{1-p^{M}_{0}}{1-p_{0}} \cdot p_n
$$

If $N$ is a poisson random variable, then $N^M$ is known as a **Zero-Modified Poisson Random Variable**. However, it is important to note that $N^M$ does NOT follow a poisson distribution - it has its own unique distribution.

Thus, by applying the previous result, the moments of the new distribution can be calculated:

$$
\begin{aligned}
    E \left(N^M \right)
    &= 0 \cdot p^{M}_{0} + 1 \cdot p^{M}_{1} + 2 \cdot p^{M}_{2} + ... \\
    &= 0 \cdot \frac{1-p^{M}_{0}}{1-p_{0}} \cdot p_0 + 1 \cdot \frac{1-p^{M}_{0}}{1-p_{0}} \cdot p_1 + 2 \cdot \frac{1-p^{M}_{0}}{1-p_{0}} \cdot p_2 + ... \\
    &= \frac{1-p^{M}_{0}}{1-p_{0}} (0 \cdot p_0 + 1 \cdot p_1 + 2 \cdot p_2 + ...) \\
    &= \frac{1-p^{M}_{0}}{1-p_{0}} \cdot E(N) \\
    \\
    \therefore E \left[\left(N^M \right)^k \right] &= \frac{1-p^{M}_{0}}{1-p_{0}} \cdot E \left[N^k \right]
\end{aligned}
$$

Note that this result ONLY applies to **raw moments**. In order to calculate the central moments, express them in terms of raw moments and then calculate them from the above conversion.

Even after modification, the PMFs can still follow the same recursion:

$$
\begin{aligned}
    \frac{p^{M}_{n}}{p^{M}_{n-1}}
    &= \frac{\frac{1-p^{M}_{0}}{1-p_{0}} \cdot p_{n}}{\frac{1-p^{M}_{0}}{1-p_{0}} \cdot p_{n-1}} \\
    &= \frac{p_{n}}{p_{n-1}} \\
    &= a + \frac{b}{n}
\end{aligned}
$$

However, since the distribution is zero-modified, the recursion can **only start from $k-1=1$**, which is why they are known as the **(a, b, 1) class** of distributions. The **domain of the recursion** is what allows us to distinguish the two classes of distributions.

!!! Warning

    There are only 4 distributions under the (a, b, 0) class: Poisson, Binomial, Geometric & Negative Binomial.

    It is *seemingly intuitive* to think that their 0-modified versions are the only members of the (a, b, 1) class. However, the **Logarithmic Distribution** is also a member of the (a, b, 1).
