# **Review of Probability Theory**

## **Basic Probability**

Probability is the study of **Experiments** whose results cannot be predicted with certainty. The result of such an experiment is known as its **Outcome**.

The **Sample Space** $\left(\Omega \right)$ is the *set* of ALL possible outcomes from an experiment. The **Event Space** $(E)$ is a *subset* of the sample space, representing only the outcomes that we are interested in studying. Conversely, its **Complement** $(E^c)$ is the set of all OTHER outcomes not inside $E$.

The probability of the event occuring is the ratio of the **number of elements** in the event to the sample space. It is a measure of the *chance* that the outcome of the experiment is inside the event space.

$$
    P(E) = \frac{n(E)}{n\left(\Omega \right)}
$$

Consider the probability of rolling an odd number on a standard dice:

* **Experiment** - Rolling a dice
* **Outcome** - The number showed on the dice
* **Sample Space** - ${1, 2, 3, 4, 5, 6}$
* **Event Space** - ${1, 3, 5}$
* **Complement** - ${2, 4, 6}$
* **Probability of Event** - $\frac{3}{6}$
* **Probability of Complement** - $\frac{3}{6}$

Within the same experiment, there may be multiple events of interest. For any two events A and B, its **Union** $(A \cup B)$ is the set with outcomes that are **either in A or B** while their **Intersection** $(A \cap B)$ is the set with outcomes that are **in BOTH A and B**.

If both A and B have no outcomes in common $(A \cap B = \emptyset)$, then they are said to be **Mutually Exclusive**. Naturally, an event and its complement are always mutually exclusive.

!!! Warning

    The following *seems intuitive*, but is actually a common mistake:

    $$
        (A \cap B)^c \ne A^c \cap B^c
    $$

    This is properly explained through **De-morgans Law**:

    <!-- Obtained from OnlineMathLearning -->
    ![DeMorgan](Assets/1.%20Review%20of%20Probability%20Theory.md/DeMorgan.png){.center}

    It can be easily remembered by applying the complement to all components of the expression, **including the intersection/union symbol**:

    $$
    \begin{aligned}
        \cap^c &= \cup \\
        \cup^c &= \cap
    \end{aligned}
    $$

### **Probability Axioms**

**Axiom 1** states that all probabilities must be **non-negative**:

$$
    P(E) \ge 0
$$

**Axiom 2** states that probability of the Sample Space is exactly equal to 1:

$$
    P(\Omega) = 1
$$

**Axiom 3** states the probability of a union of **mutually exclusive** events is equal to the sum of their probabilities, known as **Countable Additivity**.

$$
\begin{aligned}
    A \cap B &= \emptyset \\
    P(A \cup B) &= P(A) + P(B)
\end{aligned}
$$

Based on these axioms, several other important properties can also be deduced:

* **Monoticity**: $A \subset B \rightarrow P(A) \le P(B)$
* **Empty Set**: $P(\emptyset) = 0$
* **Complement Rule**: $P(E^c) = 1 - P(E)$
* **Numeric Bound**: $0 \le P(E) \le 1$
* **Sum Rule**:  $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

### **Conditional Probability**

Conditional Probabilities are denoted by $P(A \mid B)$, which is the probability of event A occuring **given that event B has already occurred**.

The intuition is best understood by considering the following - Given that event B has already occured, what is the probability that event A also occurs?

The event space is $A \cap B$, as we are interested in the probability that both A and B occur. However, since event B has already occured, the **sample space is no longer all possible outcomes but rather only the event space for B**!

$$
\begin{aligned}
    P(A \mid B)
    &= \frac{n(A \cap B)}{n(B)} \\
    &= \frac{\frac{n(A \cap B)}{n(\Omega)}}{\frac{n(B)}{n(\Omega)}} \\
    &= \frac{P(A \cap B)}{P(B)}
\end{aligned}
$$

Following this expression, the **probability of an intersection** of two events is given by:

$$
    P(A \cap B) = P(A \mid B) \cdot P(B) = P(B \mid A) \cdot P(A)
$$

<!-- Obtained from Probability Course -->
![Conditional Probability](Assets/1.%20Review%20of%20Probability%20Theory.md/Conditional%20Probability.png){.center}

Most experiments involving conditional probabilities are multi-staged experiments, which are best visualized using **Probability Trees**:

![Probability Trees](Assets/1.%20Review%20of%20Probability%20Theory.md/Probability%20Tree.png){.center}

Instead of calculating conditional probabilities from scratch, some questions provide the conditional probability $P(A \mid B)$ (or the components to do so!) and ask us to **find the reverse** - $P(B \mid A)$.

$$
    P(B \mid A) = \frac{P(B \cap A)}{P(A)}
$$

The formula is the same as before, but the issue is that the unconditional probability of event A is usually not given. This problem is accounted for in **Bayes Theorem**:

$$
\begin{aligned}
    A &= (A \cap B) + (A \cap B^c) \\
    P(A) &= P(A \cap B) + P(A \cap B^c) \\
    P(A) &= P(A \mid B) \cdot P(B) + P(A \mid B^c) \cdot P(B^c) \\
    \\
    \therefore P(B \mid A) &= \frac{P(A \mid B) \cdot P(B)}{P(A \mid B) \cdot P(B) + P(A \mid B^c) \cdot P(B^c)}
\end{aligned}
$$

Note that if the Conditional Probability of A given B is the same as the unconditional probability of A, then **events A and B are independent**; B has no effect on A.

Thus, the **probability of an intersection** of two independent events is simply their product:

$$
\begin{aligned}
    P(A \mid B) &= \frac{P(A \cap B)}{P(B)} \\
    P(A) &= \frac{P(A \cap B)}{P(B)} \\
    P(A \cap B) &= P(A) \cdot P(B)
\end{aligned}
$$

## **Random Variables**

Unlike rolling a dice, the outcome of most experiments are **non-numeric**, which makes them hard to work with. For instance, the outcomes of a coin toss are "Heads" and "Tails".

A **Random Variable** is a **many to one function** that *maps* each outcome to a single real number. Each outcome must have only one corresponding number, but different outcomes can have the same value.

!!! Note

    Although the mapping is deterministic, the underlying experiment is still random which is why it is still a "random" variable.

<!-- Obtained from Helping With Math -->
![Many to one function](Assets/1.%20Review%20of%20Probability%20Theory.md/Many%20to%20One%20Function.png){.center}

The **range of possible values** that the random variable can take is known as its **Support**. They are broadly categorized based on its support:

<center>

| Discrete | Continuous |
|:-:|:-:|
| Countable Support | Uncountable Support |
| 1, 2, 3, 4, ... | 1, 1.1, 1.01, 1.001, ... |

</center>

Random variables are denoted using **upper case letters** (X, Y, Z) while their corresponding values are denoted using **lower case letters** (x, y, z) and their appropriate **subscripts**.

The notation $X(s) = x_1$ denotes that the random variable $X$ maps the outcome $s$ to the value of $x_1$. Thus, the **corresponding probability** is denoted by $P(X = x_1)$.

!!! Note

    If the subscript is omitted, then $P(X=x)$ is a general expression that describes the **entire distribution** of $X$, not just a single probability.

<!-- Obtained from DSA201 Notes -->
![Random Variable](Assets/1.%20Review%20of%20Probability%20Theory.md/Random%20Variable.png){.center}

### **Probability Distributions**

Similar to how a random variable maps the outcomes to a real number, a **Probability Distribution** is a function that maps the outcomes to its **probability of occurrence**.

For **Discrete Random Variables**, their distribution is described using a **Probability Mass Function** (PMF). The PMF provides the probability that the random variable is **exactly equal** to some value $(X = x_1)$.

It is typically denoted in **lower case** and sometimes includes a **subscript of the random variable** when working with multiple to distinguish them from one another.

$$
\begin{aligned}
    P(X = a) &= p(a) \\
    \\
    P(X = a) &= p_X(a) \\
    P(Y = a) &= p_Y(a)
\end{aligned}
$$

Since it is a probability measure, the **sum of the PMF** over the support of the random variable **must be equal to 1** (Probability Axiom).

$$
    \sum_{x \in \text{Support}} p(x) = 1
$$

PMFs can be represented in three main ways - Functions, Tables or Histograms.

For **Continuous Random Variables**, their distribution is described using a **Probability Density Function** (PDF). The PDF is a non-negative function where the **area under it** provides the probability that the random variable takes on some **range of values** $(a \le X \le b)$.

Similarly, it is typically denoted in lower case and includes a subscript when working with multiple random variables:

$$
\begin{aligned}
    P(a \le X \le b) = \int^b_a f(x) \\
    \\
    P(a \le X \le b) = \int^b_a f_X(x) \\
    P(a \le Y \le b) = \int^b_a f_Y(y)
\end{aligned}
$$

<!-- Obtained from BYJU -->
![Probability Density Function](Assets/1.%20Review%20of%20Probability%20Theory.md/Probability%20Density%20Function.png){.center}

Similarly, since the area is a probability measure, the **total area** under the graph **must be equal to 1**:

$$
    P(-\infty \le X \le \infty) = \int^{\infty}_{-\infty} f(x) = 1
$$

!!! Note

    $\infty$ is used as a catch all for the upper and lower bound of the random variable. If the actual bounds are known, then using them instead is more appropriate.

Additionally, note that the **probability of a specific value** for a continuous RV is 0. This is because there is an **infinite number of possible values**, thus the probability of a specific value (EG. 1.45679383920) is **infinitely small** such that it is assumed to be 0.

$$
    P(X = a) = \int^{a}_{a} f(x) = 0
$$

The **Cumulative Distribution Function** (CDF) is the probability that the random variable is **less than or equal** to some value $X \le t$.

It is typically denoted in **upper case** to distinguish it from the PDF and includes subscripts as well when working with multiple random variables.

$$
\begin{aligned}
    F(t) &= P(X < t) \\
    \\
    F_X(t) &= P(X < t) \\
    F_Y(t) &= P(Y < t)
\end{aligned}
$$

For discrete variables, the CDF is the **sum of all probabilities before the specified value**. Following this, the **difference of consecutive CDFs** allows us to obtain the PMFs at that value:

$$
\begin{aligned}
    F(t) &= \sum_{x \le t} p(x) \\
    p(x_i) &= F(x_i) - F(x_{i-1})
\end{aligned}
$$

For continuous variables, the CDF is the **integral from the lower bound to the specified value**. However, instead of integrating with respect to an actual value, it is better to integrate with respect to a dummy variable $t$ to obtain a **general expression for the CDF**, allowing it to be easily calculated for any value.

Although the CDF is very useful, it **can only be used to calculate probabilities starting from the lower bound**. When probabilities starting from other ranges are needed, the **PDF can be obtained from the CDF by differentiating it** and then re-integrating with different limits.

$$
\begin{aligned}
    F(t) &= \int^{t}_{-\infty} f(x) dx \\
    \\
    F(t) &= \int^{t}_{-\infty} f(t) dt \\
    f(t) &= F'(t)
\end{aligned}
$$

!!! Note

    The integral of the PDF can lead also lead to the **Survival Function**, as shown in the [Survival Model Section](../ASA-FAML/1. Survival Models.md).

### **Piecewise Distributions**

The probability distribution may be defined using a **Piecewise Function**, which means that it is defined via multiple **sub-functions** where each applies to different intervals in the domain:

$$
\begin{aligned}
    f(x)
    &=
    \begin{cases}
        f_{1}(x),& 0 \lt x \lt t_1 \\
        f_{2}(x),& t_1 \lt x \lt t_2 \\
        \vdots
    \end{cases}
\end{aligned}
$$

When calculating probabilities, we must **choose the appropriate PDF** based on the range of the probability:

$$
\begin{aligned}
    P(X \lt t_{0.5}) &= \int^{t_{0.5}}_{0} f_{1}(x) \\
    P(X \gt t_{1.5}) &= \int^{t_{2}}_{t_{1.5}} f_{2}(x)
\end{aligned}
$$

If the required range **spans across multiple intervals**, then the probability should be split according to the intervals:

$$
\begin{aligned}
    P(t_{0.5} \lt X \lt t_{1.5})
    &= P(t_{0.5} \lt X \lt t_{1}) + P(t_{1} \lt X \lt t_{1.5}) \\
    &= \int^{t_{1}}_{t_{0.5}} f_{1}(x) + \int^{t_{1.5}}_{t_{1}} f_{2}(x)
\end{aligned}
$$

Thus, this means that the CDF for a piecewise distribution must **fully integrate all "earlier" PDFs**:

$$
\begin{aligned}
    P(X \lt t)
    &= P(0 \lt X \lt t_{1}) + P(t_{1} \lt X \lt t) \\
    &= \int^{t_{1}}_{0} f_{1}(x) + \int^{t}_{t_{1}} f_{2}(x)
\end{aligned}
$$

## **Moments**

The **Moments** of a distribution are quantities that describe **characteristics of the distribution**.

### **First Moment**

**Raw Moments** are calculated with respect to the **origin**. The **n-th raw moment** is calculated as the following:

$$
\begin{aligned}
    E(X^n) &= \int x^n \cdot f(x) dx \\
    \mu'_k &= \sum x^n \cdot p(x)
\end{aligned}
$$

The **first raw moment** is known as the **Mean**, which is a measure of the **Centrality** of the distribution. It is commonly denoted as $\mu$, without any super or subscripts.

Note that the **mean of a constant is the constant itself**:

$$
    E(c) = c
$$

### **Second Moment**

**Central Moments** are calculated with respect to the **mean**. The **n-th central moment** is calculated as the following:

$$
\begin{aligned}
    E[(X - \mu)^n] &= \int (x - \mu)^n \cdot f(x) dx \\
    \mu_k&= \sum (x - \mu)^n \cdot p(x)
\end{aligned}
$$

The **second central moment** is known as the **Variance**, which is a measure of the **Spread** of the distribution about the mean.

Since calculating central moments directly is complicated, it can be simplified to an expression involving raw moments:

$$
\begin{aligned}
    Var(X)
    &= E[(X - \mu)^2] \\
    &= E(X^2 - 2\mu X + \mu^2) \\
    &= E(X^2) - 2\mu^2 + \mu^2 \\
    &= E(X^2) - \mu^2 \\
    &= E(X^2) - [E(X)]^2
\end{aligned}
$$

Note that the **variance of a constant is 0** as a constant cannot change:

$$
    Var(c) = 0
$$

<!-- Obtained from Coaching Actuaries -->
![Variance](Assets/1.%20Review%20of%20Probability%20Theory.md/Variance.png){.center}

However, one problem with variance is that it uses squared units, which makes it hard to intepret. Thus, the **squareroot of the variance** is used instead, known as the **Standard Deviation**.

$$
    \sigma = \sqrt{Var(X)}
$$

Similarly, standard deviation cannot be used to compare data with **different units**. Thus, the **Coefficient of Variation** is used instead, which is a **unitless measure** of the spread of the distribution.

$$
    CV(X) = \frac{\sigma}{\mu}
$$

### **Third Moment**

The **third central moment** is **Skewness**, which is a measure of the **symmetry** of distribution about the mean. Being left/right skewed means that the distribution has a "longer tail" on that side, which implies that **values on the opposite side are more likely to occur**.

!!! Tip

    Skewness is also sometimes referred to as being **Positively or Negatively Skewed**. An easy way to remember is that positive values occur to the *right* of the origin, hence is the same as being right skewed; vice-versa.

$$
\begin{aligned}
    \text{Skewness}
    &= \frac{E[(X - \mu)^3]}{\sigma^3} \\
    &= \frac{E[(X - \mu)^3]}{(\sigma^2)^\frac{3}{2}} \\
    &= \frac{E(X^3) - 3 E(X^2) \cdot E(X) + 2 [E(X)]^3}{(E(X^2) - [E(X)]^2)^\frac{3}{2}}
\end{aligned}
$$

<!-- Obtained from Coaching Actuaries -->
![Skewness](Assets/1.%20Review%20of%20Probability%20Theory.md/Skewness.png){.center}

### **Fourth Moment**

The fourth central moment is **Kurtosis**, which is a measure of the **flatness** of the distribution, typically with respect to the normal distribution. It is indicative of the **likelihood of producing extreme values** (outliers).

The normal distribution has a kurtosis of 3. If a distribution has a **kurtosis greater than 3**, then it is flatter and hence **more likely to produce outliers** as compared to the normal distribution.

$$
\begin{aligned}
    \text{Kurtosis}
    &= \frac{E[(X - \mu)^4]}{\sigma^4} \\
    &= \frac{E[(X - \mu)^4]}{(\sigma^2)^2} \\
    &= \frac{E(X^4) - 4 E(X^3) \cdot E(X) + 6 E(X^2) \cdot [E(X)]^2 - 3 [E(X)]^4}{(E(X^2) - [E(X)]^2)^2}
\end{aligned}
$$

<!-- Obtained from AnalystPrep -->
![Kurtosis](Assets/1.%20Review%20of%20Probability%20Theory.md/Kurtosis.png){.center}

### **Tail Weights**

Another method of determining the likelihood of outliers to occur is through the **Tail Weight** of the distribution. Distributions with a **heavier tails are more likely to produce outliers**.

!!! Warning

    Although both Tail Weight and Kurtosis are measures of the likelihood of outliers, there is **no link between them**, despite what some authors might suggest.

A distribution is said to have a heavier tail than another if it has **STRICTLY more density** at the "ends" of the distributions, past a certain threshold:

!!! Note

    For the purposes of this exam, we only consider the **right tail** of the distribution for large outliers.

<!-- Obtained from Coaching Actuaries -->
![Tail Weight](Assets/1.%20Review%20of%20Probability%20Theory.md/Tail%20Weight.png){.center}

There are a few methods to compare the tail weight of the distribution, but for the purposes of this exam, only the **Existence of Moments** method will be considered.

A distribution is **light tailed** if has **finite RAW moments** for *all* positive values of $k$:

$$
    E(X^k) \lt \infty, \ k \gt 0
$$

Conversely, a distribution is **heavy tailed** if it DOES NOT have finite raw moments for all positive values of $k$. From another perspective, it only has them up till a **certain threshold** $k \lt a$:

$$
    E(X^k) \lt \infty, \ k \lt a
$$

Consider the following two distributions:

<center>

| Exponential | Pareto |
| :-: | :-: |
| Raw moments exists for $k \gt 0$ | Raw moments exists for $-1 \lt k \lt \alpha$ |
| Unlimited number of finite raw moments | Limited number of finite raw moments |
| Light tailed | Heavy tailed |

</center>

Consider the formula for raw moments:

$$
    E(X^k) = \int x^k \cdot f(x)
$$

As $k$ increases, large values of $x$ will be raised by large values, resulting in extremely large values.

If the densities (probability of occurring) at this portion of the distribution are **sufficiently large**, then the **resulting value would be too large** $E \left[X^k\right] = \infty$, resulting in a **limited number of finite raw moments**.

This means that the fewer finite raw moments, the **heavier the tail** and hence **higher the probability of outliers**.

### **Statistical Metrics**

Apart from the moment of the distribution, there are some other **Statistical Metrics** that provide useful information.

The **Mode** is the value of the random variable that maximises the PMF or PDF. It is the **most likely outcome** of the experiment (loosely speaking for continuous variables).

The **Median** is the value of the random variable that **seperates the upper and lower half** of the probability distribution.

For **discrete variables**, the median $M$ is the **smallest value** such that the following two expressions are satisfied:

$$
\begin{aligned}
    P(X \le M) &\ge 0.5 \\
    P(X \ge M) &\ge 0.5
\end{aligned}
$$

For **continuous variables**, the median $M$ is found by **solving the CDF**:

$$
\begin{aligned}
    F(M) &= 0.5 \\
    M &= F^{-1}(0.5)
\end{aligned}
$$

!!! Note

    The key difference is that the continuous median is the value that **exactly seperates** the distribution while the discrete median **approximately splits** it, depending on the PMF.

The **Percentile** is the value of the random variable below which a **certain percentage of observations fall**. For instance, the 85th percentile is the value below which 85% of the observations fall.

The **median is the 50th percentile**; in other words when $p = 0.5$. Thus, the previous formulas can be generalized for any $p$:

$$
\begin{aligned}
    P(X \le M^{\text{Discrete}}) &\ge p \\
    P(X \ge M^{\text{Discrete}}) &\ge p \\
    \\
    M^{\text{Continuous}} &= F^{-1}(p)
\end{aligned}
$$

!!! Tip

    Remember that the **CDF for a piecewise distribution** is calculated differently.

The 25th, 50th & 75th percentile are known as the first, second & third **Quartiles** $(q)$ respectively. The difference between the 3rd and 1st quartile is known as the **Inter Quartile Range**.

$$
    IQR = q_3 - q_1
$$

### **Shifting, Scaling & Transformation**

An existing random variable $X$ can be adjusted in order to make a new random variable $Y$.

If a constant $a$ has been **multiplied** to the random variable, then it has been **Scaled** by $a$:

$$
\begin{aligned}
    Y &= aX \\
    \\
    F_Y(y)
    &= P(Y \le y) \\
    &= P(aX \le y) \\
    &= P \left(X \le \frac{y}{a} \right) \\
    &= F_X \left(\frac{y}{a} \right) \\
    \\
    f_Y(y)
    &= \frac{d}{dy} F_Y(y) \\
    &= \frac{1}{a} \cdot f_X \left(\frac{y}{a} \right)
\end{aligned}
$$

If a constant $b$ is **added** to the random variable instead, then it has been **Shifted** by $b$:

$$
\begin{aligned}
    Y &= X + b \\
    \\
    F_Y(y)
    &= P(Y \le y) \\
    &= P(X + b \le y) \\
    &= P(X \le y - b) \\
    &= F_X (y - b) \\
    \\
    f_Y(y)
    &= \frac{d}{dy} F_Y(y) \\
    &= f_X (y - b)
\end{aligned}
$$

For both shifting and scaling, the expectation and variance can be easily determined if that of the original is known as well:

$$
\begin{aligned}
    E(cX) &= c \cdot E(X) \\
    E(X+c)
    &= E(X) + E(c) \\
    &= E(X) + c
    \\
    Var(cX) &= c^2 \cdot Var(x) \\
    Var(X+c)
    &= Var(X) + Var(c) \\
    &= Var(X)
\end{aligned}
$$

If the random variable has been **raised by a power** of $\frac{1}{c}$ where $c \ne 1$, then it has been **Power Transformed**:

$$
\begin{aligned}
    Y &= X^{\frac{1}{c}} \\
    \\
    F_Y(y)
    &= P(Y \le y) \\
    &= P(X^{\frac{1}{c}} \le y) \\
    &= P(X \le y^c) \\
    &= F_X (y^c) \\
    \\
    f_Y(y)
    &= \frac{d}{dy} F_Y(y) \\
    &= cy^{c-1}\cdot f_X (y - b)
\end{aligned}
$$

If the random variable has been **exponentiated**, then it has also been **Exponential Transformed**:

$$
\begin{aligned}
    Y &= e^X \\
    \\
    F_Y(y)
    &= P(Y \le y) \\
    &= P(e^X \le y) \\
    &= P(X \le \ln y) \\
    &= F_X (\ln y) \\
    \\
    f_Y(y)
    &= \frac{d}{dy} F_Y(y) \\
    &= \frac{1}{y} \cdot f_X (\ln y)
\end{aligned}
$$

For both types of transformations, there is no simple method of determining the mean and variance. The various raw moments must be **manually determined via integration**.

## **Joint Distributions**

Consider a scenario where there are **two or more random variables** $(X_1, X_2, \dots)$ in the **same probability space** such that we are interested in some function of them $Y = g(x_1, x_2, \dots)$.

!!! Note

    For the purposes of this exam, only **Linear Combinations** (sums) of random variables will be considered.

For instance, the experiment could be studying the **sum of rolling two dice**. There would be **two random variables** $(X_1, X_2)$, each denoting the value of their respective dice. Thus, the sum of the two dice would be $Y = X_1 + X_2$.

Consider the probability of obtaining a sum of 3 - consider ALL **combination of values** of the two underlying variables will result in it:

$$
    P(Y = 3) = P(X_1 = 1, X_2 = 2) + P(X_1 = 2, X_2 = 1)
$$

This can be more generally expressed as the **Joint Distribution** of the two variables:

$$
\begin{aligned}
    P(X = x, Y = y) &= P_{X,Y} (x,y) \\
    P(a \lt X \lt b, c \lt Y \lt d) &= \int^{d}_{c} \int^{b}_{a} f_{X,Y} (x,y) \ dx dy
\end{aligned}
$$

The **individual distributions** within this shared probability space is known as the **Marginal Distribution** and is obtained by "integrating out" the other variable:

$$
\begin{aligned}
    f_{X}(x) &= \int^{d}_{c} f_{X,Y} (x,y) \ dy \\
    f_{Y}(y) &= \int^{b}_{a} f_{X,Y} (x,y) \ dx
\end{aligned}
$$

### **Joint Domain**

One of the main concerns when dealing with joint distributions is the domain of the distribution.

Each random variable is defined on their own domain, but the joint distribution is only defined in the **domain in which two variables coincide**.

For a joint distribution of **discrete variables**, it is simply a matter of finding **all possible combinations** of the two variables. For a **two variable** distribution, this can be easily visualized via a **Contingency Table**:

<!-- Obtained from Natbanting -->
![Contingency Table](Assets/1.%20Review%20of%20Probability%20Theory.md/Contingency%20Table.png){.center}

The table sometimes lists the *frequency* of observations while sometimes it lists their *probability*. In either case, by finding the **cells of interest**, the probability can be obtained.

For a joint distribution of **continuous variables**, it is best visualized **graphically**. For a two variable distribution:

$$
    1 \lt X_1 \lt 5, \ 2 \lt X_2 \lt 8
$$

<!-- Self made from Desmos -->
![Domain Graph](Assets/1.%20Review%20of%20Probability%20Theory.md/Domain%20Graph.png){.center}

Let $Y$ be the sum of the two random variables. Consider the following probability:

$$
\begin{aligned}
    Y &= X_1 + X_2 \\
    P(Y \lt 5)
    &= P(X_1 + X_2 \lt 5) \\
    &= P(X_2 \lt 5 - X_1)
\end{aligned}
$$

Thus, we need to find the domain of the above inequality **WITHIN the joint domain**. This can be done by **plotting the graph of the upper/lower bound** (simply change the inequality to an equal sign).

The domain can be intepreted in one of two ways:

1. $X_1$ to line first: $1 \lt X_1 \lt 3, \ 2 \lt y \lt 5-x$
2. $X_2$ to line first: $1 \lt X_1 \lt 5-y, \ 2 \lt y \lt 4$

Using these domains, the double integration can be performed to obtain the probability:

$$
\begin{aligned}
   P(X_2 \lt 5 - X_1)
   &= \int^{1}_{3} \int^{5-x}_{2} f_{X,Y}(x,y) \ dy dx \\
   &= \int^{2}_{4} \int^{5-y}_{1} f_{X,Y}(x,y) \ dx dy
\end{aligned}
$$

!!! Tip

    Recall that when performing a double integration, the **INNER integral is evaluated first**.

    The inner and outer integral are interchangeable, thus set the inner integral such that it will **ease the integration**.

    In this case, the integral with an algebraic expression should always be evaluated first, ensuring that the **final result is a probability** and not an expression.

<!-- Self made from Desmos -->
![Domain Graph](Assets/1.%20Review%20of%20Probability%20Theory.md/Domain%20Graph%20Shape.png){.center}

If there is a **Kink** in the shape, then the area should be broken into two **smaller areas without any kinks** in their shape. The overall probability is the sum of the areas:

$$
    P(X_2 \lt 8 - X_1) = \text{Orange Area} + \text{Purple Area}
$$

<!-- Self made from Desmos -->
![Domain Graph](Assets/1.%20Review%20of%20Probability%20Theory.md/Domain%20Graph%20Kink.png){.center}

### **Joint Moments**

Given that $Y = X_1 + X_2$, its mean and variance is the following:

$$
\begin{aligned}
    E(Y) &= E(X_1) + E(X_2) \\
    \\
    Var (Y) &= Var (X_1) + Var (X_2) + 2 \cdot Cov (X,Y)
\end{aligned}
$$

The last term is known as the **Covariance**, which is the **first central moment** about of the joint distribution. It measures the **linear relationship** between variables:

* **Positive Covariance**: Both variables move in the **same direction**
* **Negative Covariance**: Both variables move in **opposite directions**

$$
\begin{aligned}
    Cov (X_1, X_2)
    &= E[(X_1 - E(X_1))(X_2 - E(X_2))] \\
    &= E(X_1 \cdot X_2) - E(X_1) \cdot E(X_2) \\
    \\
    E(X_1 \cdot X_2)
    &= \int \int (x_1 \cdot x_2) \cdot f_{X_1, X_2} (x_1, x_2) \ dx_1 dx_2
\end{aligned}
$$

Note that it has some interesting properties:

1. $Cov (X_1, c) = 0$
2. $Cov (X_1, X_2) = Var (X_1)$
3. $Cov (c \cdot X_1, X_2) = c \cdot Cov (X_1,X_2)$
4. $Cov (X_1 + c, X_2) = Cov (X_1, X_2) + Cov (c, X_2)$

However, covariance is both **limitless and squared units**, which makes hard to use as a metric for comparison.

We can overcome these problems by scaling the covariance down by the individual standard deviations, obtaining the **Correlation** of the two variables:

$$
    Corr (X_1, X_2) = \frac{Cov (X_1, X_2)}{SD(X_1), SD(X_2)}
$$

<!-- Obtained from Coaching Actuaries -->
![Correlation](Assets/1.%20Review%20of%20Probability%20Theory.md/Correlation.png){.center}

If $X_1$ and $X_2$ are **Independent**, then their distributions and moments become the following:

$$
\begin{aligned}
    P(X_1 = x_1, X_2 = x_2) &= P(X_1 = x_1) \cdot P(X_2 = x_2) \\
    f_{X_1,X_2} (x_1,x_2) &= f_{X_1}(x_1) \cdot f_{X_2}(x_2) \\
    \\
    E(X_1 \cdot X_2) &= E(X_1) \cdot E(X_2) \\
    Var (X_1 + X_2) &= Var (X_1) + Var (X_2)
\end{aligned}
$$

!!! Warning

    If the domain of the random variables naturally contain other variables, then they are not independent, even if the above conditions are met.

Note that having 0 covariance is a **consequence of independence**, but is NOT indicative of it. Variables may also have 0 covariance when they have a **non-linear relationship**.

$$
\begin{aligned}
    Cov (X_1, X_2)
    &= E(X_1 \cdot X_2) - E(X_1) \cdot E(X_2) \\
    &= E(X_1) \cdot E(X_2) - E(X_1) \cdot E(X_2) \\
    &= 0 \\
    \\
    \therefore Corr (X_1, X_2) &= 0
\end{aligned}
$$

### **Generating Functions**

A useful way to analyze the sum of **independent random variables** is to convert the PDF/PMF of the individual distributions into a **Generating Function**.

The first kind is known as a **Moment Generating Function** (MGF):

$$
\begin{aligned}
    M_{X}(t)
    &= E(e^{tX}) \\
    &= \sum e^{tx} \cdot p_{X}(x) \\
    &= \int e^{tx} \cdot f_{X}(x)
\end{aligned}
$$

As its name suggests, the MGF can be used to **calculate the raw moments** of the distribution. To obtain the **n-th raw moment**,

1. **Differentiate** the MGF k times
2. **Evaluate** the expression at $t=0$

$$
    E \left(X^n \right) = \frac{d^n}{dt^n} \cdot M_{X}(0)
$$

However, the main benefit of MGFs is that they **uniquely identify** a distribution. If two random variables have the **same MGF**, then they have the **same distribution**.

This becomes especially useful when dealing with **sums of independent random variables**. By determining the MGF of the combination, its **exact distribution** can be determined.

$$
\begin{aligned}
    Y &= X_1 + X_2 + ... + X_n \\
    \\
    M_Y(t)
    &= E(e^{tY}) \\
    &= E(e^{t(X_1 + X_2 + ... + X_n)}) \\
    &= E(e^{tX_1} \cdot e^{tX_2} \cdot ... \cdot e^{tX_n}) \\
    &= \prod E(e^{tx}) \\
    &= \prod M_{X}(t)
\end{aligned}
$$

The other kind is known as a **Probability Generating Function** (PGF), which only applies for sums of **discrete variables**:

$$
\begin{aligned}
    P_{X}(t)
    &= E(t^x) \\
    &= \sum t^X \cdot p(x)
\end{aligned}
$$

!!! Note

    The PGF is denoted in upper case $P$ while the PMF is denoted in lower case $p$.

As its name suggests, the PGF can be used to calculate the **individual probabilities** of the distribution. To obtain the **probability of the n-th value**,

1. **Differentiate** the PGF k times
2. **Divide** the expression by n factorial
3. **Evaluate** the expression at $t=0$

$$
    P(X = k) = \frac{d^n}{dt^n} \cdot \frac{P_{X}(0)}{n!}
$$

Similarly, the PGF uniquely identifies the distribution and can be used in all the same ways as an MGF in this regard:

$$
\begin{aligned}
    Y &= X_1 + X_2 + ... + X_n\\
    \\
    P_Y(t)
    &= E(t^Y) \\
    &= E(t^{X_1 + X_2 + ... + X_n}) \\
    &= E(t^{X_1} \cdot t^{X_2} \cdot ... \cdot t^{X_n}) \\
    &= \prod E(t^{x}) \\
    &= \prod P_{X}(t)
\end{aligned}
$$

Given how similar the two are, they can be easily converted to and from one another:

$$
\begin{aligned}
    P_X(t)
    &= E(t^X) \\
    &= E(e^{\ln t^X}) \\
    &= M_X(\ln t^x) \\
    \\
    M_X(t)
    &= E(e^tX) \\
    &= E[(e^t)^x] \\
    &= P_X(e^t)
\end{aligned}
$$

### **Conditional Distributions**

If a random variable $X$ is **conditioned on a range of values of itself**, then it has the **Conditional Distribution** $(X \mid X>j)$:

$$
\begin{aligned}
    P(X = x \mid X > j) &= \frac{P(X = x)}{P(X > j)} \\
    f_{X \mid X>j} (x) &= \frac{f_X(x)}{P(X > j)}
\end{aligned}
$$

Similarly, a random variable can be conditional on **ANOTHER random variable**, resulting in the **Conditional Distribution** $(X \mid Y)$.

$$
\begin{aligned}
    P(X = x \mid Y = y) &= \frac{P(X = x, Y = y)}{P(Y = y)} \\
    f_{X \mid Y} (x,y) &= \frac{f_{X,Y}(x,y)}{f_{Y}(y)} \\
    \text{Conditional} &= \frac{\text{Joint}}{\text{Marginal}}
\end{aligned}
$$

Any calculations involving the above distributions must be solved via **first principles**.

However, most problems require us to find the **Marginal Distribution** given only the conditional distributions:

* $X$ is the random variable denoting the test grades (A, B, C)
* $Y$ is the random variable denoting the gender (M, F)

The teacher would like to find the marginal distribution of test scores $(X)$, but only has the **conditional distribution** of the scores of the students for each gender $(X \mid Y)$ and the proportion of the Genders $(Y)$.

The **Law of Total Probability** can be used to determine the **marginal probability** of $X$:

$$
\begin{aligned}
    P(X = a)
    &= E_Y[P(X = a \mid Y)] \\
    &= \sum_{y} P(X = a \mid y) \cdot p(Y = y)
\end{aligned}
$$

Note that this is equivalent to adding up the final probabilities from the relevant branches from a probability tree:

<!-- Obtained from Cleanpng -->
![Law of Total Probability](Assets/1.%20Review%20of%20Probability%20Theory.md/Law%20of%20Total%20Probability.png){.center}

Naturally, this also means that the **marginal CDF** can be obtained in similar fashion:

$$
\begin{aligned}
    F_X(a)
    &= E_Y[F_{X \mid Y}(a)] \\
    &= \sum_{y} F_{X \mid Y}(a) \cdot p(Y = y)
\end{aligned}
$$

Following the same logic, the **Law of Total Expectation** can be used to determine the **marginal expectation** of $x$:

$$
\begin{aligned}
    E(X)
    &= E_Y[E_X(X \mid Y)] \\
    &= \sum_{y} E_X(X \mid Y) \cdot p(Y = y)
\end{aligned}
$$

!!! Warning

    Since $E(X)$ is a constant, a **common mistake** is thinking that $E_X(X \mid Y)$ is a constant as well since they are both expectations.

    The issue is that it is conditional on $Y$, which is still random, which makes the conditional **expectation still random**.

The **Law of Total Variance** can be used to determine the **marginal variance** of $x$. However, unlike the previous two, it is NOT simply the expectation of the conditional variance:

$$
\begin{aligned}
    Var (X) &= E_Y [Var_X(X \mid Y)] + Var_Y[E_X(X \mid Y)] \\
    \\
    E_Y [Var_X(X \mid Y)] &= \sum_{y} Var_X(X \mid Y) \cdot p(Y = y) \\
    \\
    Var_Y[E_X(X \mid Y)]
    &= E_Y[E_X(X \mid Y)^2] - (E_Y[E_X(X \mid Y)])^2 \\
    &= \sum_{y} E_X(X \mid Y)^2 \cdot p(Y = y) - \sum_{y} E_X(X \mid Y) \cdot p(Y = y)
\end{aligned}
$$

Alternatively, the Marginal Variance can be **directly calculated** using the typical formula of $E(X^2) - [E(X)]^2$, where the **two marginal expectations** are calculated using the law of total expectation.

Alternatively once more, if the conditional distribution $Y$ only has two outcomes, then the **Bernoulli Shortcut** (covered in a later section) can be used to quickly compute the value of $Var_Y[E_X(X \mid Y)]$.

!!! Tip

    The discrete case was shown in this section due to its simplicity. All the same concepts apply to the continuous variables as well - simply **replace the summation & PMFs with integrals and PDFs**.

### **Mixture Distributions**

A mixture distribution is a distribution whose values can be intepreted as being derived from an underlying set of **other random variables**.

In an insurance context, a Homeowners Insurance claim could be from a fire, burglary or liability accident. To model it, we could use a mixture that is made up of the basic distributions used to individually model each type of accident.

If the mixture contains a countable number of other distributions, then it is known as a **Discrete Mixture**. Otherwise, it is known as a **Continuous Mixture**.

!!! Warning

    It is a common mistake to think that a discrete mixture is only made up of discrete distributions, vice-versa.

    Any type of distribution can be included in a mixture; the classification is based on the number of distributions.

The random variable $X$ is a **k-point mixture** if its **probability functions** can be expressed as the **weighted average** of the probability functions of the $k$ distributions $X_1, X_2, ... X_k$:

$$
\begin{aligned}
    F_X(x) &= w_1 \cdot F_{X_1}(x) + w_2 \cdot F_{X_2}(x) + ... + w_k \cdot F_{X_k}(x) \\
    f_X(x) &= w_1 \cdot f_{X_1}(x) + w_2 \cdot f_{X_2}(x) + ... + w_k \cdot f_{X_k}(x)
\end{aligned}
$$

!!! Note

    For this exam, questions will usually only use **2 or 3 point mixtures**.

$w$ represents the **mixing weights**, such that $w_1 + w_2 + ... + w_k = 1$. It can be intepreted that $Y$ **follows the distribution** of $X_1$ $100w_1 \%$ of the time, follows the distribution of $X_2$ $100w_2 \%$ of the time etc.

!!! Warning

    Another common mistake is confusing mixtures with **Linear Combinations** of random variables:

    $$
        X \ne w_1 X_1 + w_2 X_2 + ... + w_k X_k
    $$

    In a linear combination, $X$ **neither follows the distribution** of any of the $X_k$. Furthermore, since $w_k$ are not weights, they can be **any real number and do not need to sum to 1**.

The mixing weights can also be thought of as **Discrete Probabilities** that come from a random variable $Y$ representing the $k$ underlying distributions with the support $\set{1, 2, ..., k}$ where $P(Y = k) = w_k$.

Thus, we can think of the overall mixture $X$ as an **unconditional distribution** while each of the underlying distributions are conditional distributions $X \mid Y$. This allows us to make use of the all the **previous results** from the conditional distributions:

$$
\begin{aligned}
    P(X = a) &= E_Y[P(X = a \mid Y)] \\
    F_X(a) &= E_Y[F_{X \mid Y}(a)] \\
    E(X) &= E_Y[E_X(X \mid Y)] \\
    Var (X) &= E_Y [Var_X(X \mid Y)] + Var_Y[E_X(X \mid Y)]
\end{aligned}
$$

In terms of the weights, for a simple **two point mixture**:

$$
\begin{aligned}
    P(X = a) &= P(X = a \mid Y=1) \cdot w_1 + P(X = a \mid Y=2) \cdot w_2 \\
    F_X(a) &= F_{X \mid Y=1}(a) \cdot w_1 + F_{X \mid Y=2}(a) \cdot w_2 \\
    E(X) &= E_X(X \mid Y=1) \cdot w_1 + E_X(X \mid Y=2)  \cdot w_2
\end{aligned}
$$

<!-- TBC
Tail Weights
Empirical Distribution
 -->