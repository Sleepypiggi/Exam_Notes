# **Aggregate Models**

## **Collective Risk Model**

The combination of the Frequency Models $(N)$ and the Severity Models $(X)$ results in an **Aggregate Model** $(S)$, which represents the **total losses** for all policies.

The main type of aggregate model is known as the **Collective Risk Model**, which is a **sum of an unknown number of random variables**.

$$
\begin{aligned}
    S &= X_1 + X_2 + ... + X_N
\end{aligned}
$$

!!! Info

    In probability theory, this is known as a **Compound Distribution** where $N$ is known as the **Primary Distribution** and $X$ is known as the **Secondary Distribution**.

It makes the following assumptions:

1. **Losses are iid**: $X_1, X_2, ..., X_N$ are iid
2. **Frequency and Severity are independent**: $X$ and $N$ are independent

## **Moments**

Another perspective is that the collective risk model follows the **conditional distribution** $S \mid N$. Thus, we can compute the probability, expectation and variance using previously found results.

!!! Note

    It can also be thought of as a mixture distribution where the mixing weights are the probability that the there that number of losses:

    $$
        w_k = P(N=K)
    $$
    
    Thus, **Compound Distributions and Mixture Distributions are essentially the same concept**.

Starting with the **Law of Total Expectation**,

$$
\begin{aligned}
    E(S) &= E_{N} [E_{S}(S \mid N)] \\
    \\
    E_{S}(S \mid N)
    &= E(X_1 + X_2 + \dots + X_N \mid N) \\
    &= E(X_1 \mid N) + E(X_2 \mid N) + \dots + E(X_N \mid N) \\
    &= E(X_1) + E(X_2) + \dots + E(X_N), \ \text{Independence} \\
    &= E(X) + E(X)+ \dots + E(X), \ \text{Identical} \\
    &= N \cdot \underbrace{E(X)}_{\text{Constant}} \\
    \\
    \therefore E(S)
    &= E_{N} [E_{S}(S \mid N)] \\
    &= E_{N} [N \cdot E(X)] \\
    &= E(X)\cdot E(N)
\end{aligned}
$$

<!--
If each policy has a deductible then just E(N) E(X-d)+
-->

!!! Note

    This result is **highly intuitive** as $\text{Risk} = \text{Frequency} \cdot \text{Severity}$.

Moving on to the **Law of Total Variance**:

$$
\begin{aligned}
    Var (S)
    &= E_N [Var (S \mid N)] + Var_{N} [E(S \mid N)] \\
    \\
    Var (S \mid N)
    &= Var (X_1 + X_2 + ... + X_N \mid N) \\
    &= N \cdot \underbrace{Var(X)}_\text{Constant}, \ \text{IID} \\
    \\
    Var (S)
    &= E_N [Var (S \mid N)] + Var_{N} [E(S \mid N)] \\
    &= E [N \cdot Var(X)] + Var_{N} [N \cdot E(X)] \\
    &= Var(X) \cdot E(N) + [E(X)]^2 \cdot Var(N)
\end{aligned}
$$

!!! Tip

    If the **mean and variance of the frequency distribution are equal** (EG. Poisson Distribution), then the Variance can be simplified:

    $$
    \begin{aligned}
        Var (S)
        &= Var(X) \cdot E(N) + [E(X)]^2 \cdot Var(N) \\
        &= E(N) \cdot [Var(X) + [E(X)]^2] \\
        &= E(N) \cdot E(X^2)
    \end{aligned}
    $$

## **Probability**

The probability of $S$ is **difficult to derive**. Thus, for the purposes of this exam, we will assume a **Discrete Severity** for this portion. By the **Law of Total Probability**:

!!! Note

    If both the Frequency and Severity models are discrete, then the Aggrgegate Loss is discrete as well.

$$
\begin{aligned}
    P(S = s)
    &= E_{N} [P(S = s \mid N = n)] \\
    &= \sum P(S = s \mid N = n) \cdot P(N = n)
\end{aligned}
$$

The term within the above summation is known as a **Convolution**, as it is the probability of a sum of random variables. Since there are $n$ random variables, it is known as the **n-fold Convolution**.

$$
    P(S = s \mid N = n) = P(X_1 + X_2 + \dots + X_n = s)
$$

For simplicity, consider a two fold convolution:

$$
\begin{aligned}
    P(S = s \mid n = 2)
    &= \sum P(X_1 = x, X_2 = s - x_1) \\
    &= \sum P(X_1 = x) \cdot P(X_2 = s - x_1) \\
    \\
    \therefore P(S = s)
    &= P(N = n) \cdot \sum P(X_1 = x) \cdot P(X_2 = s - x_1)
\end{aligned}
$$

A convolution is simply the **probability of all possible combinations** of the random variables that result in the specified value (Recall the two dice example).

However, one implicit assumption for this to work is that the **individual severities cannot be 0**. The reason is because it would lead to **endless possibilities** for each case.

Consider $S=1$:

* $1 + 0 + 0 + \dots + 0 = 1$
* $0 + 1 + 0 + \dots + 0 = 1$
* $0 + 0 + 1 + \dots + 0 = 1$
* $0 + 0 + 0 + \dots + 1 = 1$

However, then how is $S=0$ determined, since there are **no sums that result in 0**? There is now **only ONE way** for aggregate losses to be 0 - if there are **no claims at all**.

$$
    P(S = 0) = P(n = 0)
$$

## **Generating Functions**

An alternative method to calculate the moments would be to use the **MGF**:

$$
    M_{S}(t) = M_{N} [\ln M_{X}(t)]
$$

Similarly, if severity is discrete, then an alternative method to calculate its probability would be to use its **PGF**:

$$
    P_{S}(t) = P_{N} [P_{X}(t)]
$$

These methods tend to be more **time consuming**, thus they are preferred only when calculating higher moments or large values of $s$.

## **Normal Approximation**

Another method of determining the probability would be to simply **approximate** it. This is especially useful when the individual losses are continuous, such that the previous methods would not work.

By the **Central Limit Theorem**, as the number of random variables increase, their **sum is approximately normal distributed**:

$$
    S \sim (E[S], Var[S])
$$

Instead of using the normal distribution, the **Lognormal Distribution** can be used as well. However, we need to **first solve for its parameters**:

$$
\begin{aligned}
    e^{\mu + 0.5\sigma^2} &= E(S) \\
    \left(e^{\mu + 0.5\sigma^2} \right)^2 \cdot (e^{\sigma^2}-1) &= Var (S) \\
    \\
    \therefore S &\sim \text{Log Normal} (\mu, \sigma^2)
\end{aligned}
$$

!!! Warning

    Although the notation is the same, **do NOT confuse** the lognormal parameters with the normal parameters.

### **Continuity Correction**

We may still want to use the approximation even when losses are discrete as they may still be **too complicated** to compute.

However, this may lead to some inaccuracies as we are using a continuous distribution to approximate a discrete distribution.

Under a discrete distribution, the following are complements:

$$
    P(X \le n) + P(X \gt n+1) = 1
$$

However, the same cannot be said for a continuous distribution, as there is some density between $n$ and $n+1$ that is not accounted for:

$$
    P(S \le n) + P(S \gt n+1) \ne 1
$$

Thus, **Continuity Correction** attempts to account for this by reducing the missing density. The general idea is to adjust the range such that the **midpoint of the two values**:

$$
\begin{aligned}
    P(S \le n) &= P \left(S \le \frac{n+(n+1)}{2} \right) \\
    P(S \ge n) &= P \left(S \ge \frac{n+(n-1)}{2} \right)
\end{aligned}
$$

Another concept to remember when going from Continuous to Discrete is that a continuous $S \lt n$ is a discrete $S \le (n-1)$ since there are a countable number of values, vice versa.

<!-- Obtained from Coaching Actuaries -->
![Continuity Correction](Assets/5.%20Aggregate%20Models.md/Continuity%20Correction.png){.center}

If required to approximate the **probability at a specified value**, then the above method will not work as a continuous distribution cannot find the probability at a single point.

Thus, it will be converted to a **range around the specified value** instead, with the **midpoint value used on both sides** of the specified value:

<!-- Obtained from Coaching Actuaries -->
![Continuity Correction Special](Assets/5.%20Aggregate%20Models.md/Continuity%20Correction%20Special.png){.center}

!!! Tip

    If the discrete distribution does not support consecutive integers $\set{0, 1, 2, \dots}$, then simply take the midpoint of whatever **consecutive values** in its support $\set{100, 200, 300}$.

## **Non-Direct Frequency**

Questions may not directly the frequency distribution of interest.

For instance, we may be interesting in modelling aggregate losses from earthquake insurance.

We are ultimately interested in the total number of claims. However, this has to account for two aspects:

* **Number of Claims per earthquake**: $N \sim \text{Distribution}$
* **Number of Earthquakes** -- $N^* \sim \text{Distribution}$


The first step is to follow the **usual steps** to find the **aggregate losses for one earthquake**, $S$. The aggregate loss for all possible earthquakes is thus represented as:

$$
    S^* = S_1 + S_2 + \dots + S_{N^*}
$$

Repeating the same steps as before, assuming **independence between earthquakes**,

$$
\begin{aligned}
    E(S^*)
    &= E_{N^*} [E(S^* \mid N^*)] \\
    &= E_{N^*} [N^* \cdot E(S)] \\
    &= E(N^*) \cdot E(S) \\
    \\
    \text{Var} (S^*)
    &= E_{N^*}[\text{Var}(S \mid N^*)] + \text{Var}_{N^*} [E(S \mid N^*)] \\
    &= E_{N^*}[N^* \cdot \text{Var}(S)] + \text{Var}_{N^*} [N^* \cdot E(S)] \\
    &= E(N^*) \cdot \text{Var}(S) + \text{Var}(N^*) \cdot [E(S)]^2
\end{aligned}
$$

!!! Warning

    Always read the question carefully - there could be questions that provide the distribution of the number of earthquakes as a **red herring** but only want aggregate losses from a single earthquake.

Alternatively, a question may provide the **exact number of earthquakes**, $N^*$. The aggregate loss is then a sum of a **known number of variables**:

$$
    S^* = S_1 + S_2
$$

Thus, the moments are simply scaled by that number:

$$
\begin{aligned}
    E(S^*) &= N^* \cdot E(S) \\
    \text{Var}(S^*) &= N^* \cdot \text{Var}(S)
\end{aligned}
$$

## **Non-Independence**

If frequency and severity are **NOT independent**, then the all the above methods cannot be used. This would mean that the **distribution of $X$ depends on the number of claims**.

Questions of this nature usually deal with **Discrete Severity**:

<!-- Obtained from Coaching Actuaries -->
![SOA FAM-S Sample Question 10](Assets/5.%20Aggregate%20Models.md/Non-Independence.png){.center}

We must first determine **all possible values of $S$** by considering all possible combinations of $X$ and $N$:

$$
\begin{aligned}
    S &= 0 \\
    S &= 25 = 1 \cdot 25 \\
    S &= 150 = 1 \cdot 150 \\
    S &= 100 = 2 \cdot 50 \\
    S &= 250 = 1 \cdot 50 + 1 \cdot 200 \\
    S &= 400 = 2 \cdot 200
\end{aligned}
$$

!!! Warning

    It is a common mistake to **forget to include $S = 0$**. As mentioned previously, it occurs only when $N=0$.

    A good way to check if there are any missing combinations or wrongly computed probabilities would be to ensure that all the **probabilities sum to 1**.

This process is a **"reverse" of a convolution** -- rather than finding the combinations that lead to a particular value, we are finding all possible values that could result from the combinations.

The probability of each value of $S$ is its convolution. Using this distribution, its **mean and variance** can be determined.

!!! Warning

    It is a common mistake to compute the **wrong probability** for cases when there are two or more claims.

    In the above example for $S = 250$, there are actually two possible combinations:

    * First Loss $50$, Second Loss $200$
    * First Loss $200$, Second Loss $50$

    Thus, the associated probability for this value is:

    $$
    \begin{aligned}
        P(S = 250)
        &= P(N = 2) \cdot [P(X_1 = 50, X_2 = 200 \mid N = 2) + P(X_1 = 200, X_2 = 50 \mid N = 2)] \\
        &= P(N = 2) \cdot 2 \cdot P(X = 50, X = 200 \mid N = 2) \\
        &= P(N = 2) \cdot 2 \cdot P(X = 50 \mid N = 2) \cdot P(X = 200 \mid N = 2) \\
        &= \frac{1}{5} \cdot 2 \cdot \frac{2}{3} \cdot \frac{1}{3} \\
        &= \frac{4}{45}
    \end{aligned}
    $$

## **Policy Modifications**

### **Aggregate Deductibles**

Similar to individual losses, we can also consider policy modifications to aggregate losses. For the purposes of this exam, we mainly consider deductibles.

As before, our primary concern is the **expected payment**:

$$
\begin{aligned}
    E(S-d)_{+} &= E(S) - E(S \land d)
\end{aligned}
$$

!!! Info

    Recall that Stop Loss Reinsurance is modelled as a deductible applied to Aggregate Losses.

    The perspective has flipped in this scenario; the primary insurer experiences the limited loss $S \land d$ and the reinsurer pays $S-d$.

    Thus, the expected value of the payment is known as often known as the **Pure Premium for Stop Loss Insurance**, as it represents the reinsurer's pure loss.

The problem is that the **distribution of $S$ is not known**, thus $E(S \land d)$ is difficult to obtain. Thus, for these types of questions, the underlying **severity is assumed to be discrete**, allowing the distribution of $S$ to be **determined manually** like in the non-independence case.

It can then be adjusted to obtain the distribution $S \land d$, following which all the usual calculations can proceeed.

### **Aggregate Payments**

The discussion so far has solely been on Aggregate *Losses*. However, if each policy has a deductible, then there is a need to consider **Aggregate Payments** instead.

Both types of aggregate payments (per loss and per payment) can be used to determine aggregate payments:

If considering **Payments per Losses** $(Y^L)$, then as its name suggests, the appropriate frequency to use is the **Loss Frequency** $(N)$.

$$
\begin{aligned}
    S &= Y^L_{1} + Y^L_{2} + \dots + Y^L_{N} \\
    \\
    E(S) &= E(N) \cdot E(Y^L) \\
    Var (S) &= Var(Y^L) \cdot E(N) + \left[E \left(Y^L \right) \right]^2 \cdot Var (N)
\end{aligned}
$$

If considering **Payments per Payments** $(Y^P)$, then as its name suggests, the appropriate frequency to use is the **Payment Frequency** $(N^{*})$.

$$
\begin{aligned}
    S &= Y^P_{1} + Y^P_{2} + \dots + Y^P_{N^{*}} \\
    \\
    E(S) &= E(N^{*}) \cdot E(Y^P) \\
    Var (S) &= Var(Y^P) \cdot E(N^{*}) + \left[E \left(Y^P \right) \right]^2 \cdot Var (N^{*})
\end{aligned}
$$

The key is to **match the frequency distribution with the severity**: Loss & Loss Frequency; Payments & Payment Frequency.

### **Payment Frequency**

Imposing a deductible naturally results in **fewer payments than losses** as not all losses result in a payment.

The **Payment Frequency** $(N^{*})$ is thus an adjusted version of the loss distribution that only takes into account the losses that result in a payment.

Consider a **Bernoulli Indicator variable** that takes on the value 1 if a payment will be made and 0 if it will not:

$$
\begin{aligned}
    I
    &=
    \begin{cases}
        1 \ (\text{Paid}),& X \gt d \\
        0 \ (\text{Not Paid}),& X \le d
    \end{cases} \\
    \\
    v &= P(X \gt d) \\
    P_I(t) &= 1 + v(t-1)
\end{aligned}
$$

$N^{*}$ is the **sum of $N$ idendepent indicator variables**:

$$
    N^{*} = I_1 + I_2 + \dots + I_N
$$

It is essentially a **Compound Distribution** with $N$ being the primary distribution and $I$ being the secondary distribution. Thus, the PGF of $N^{*}$ can be determined:

$$
\begin{aligned}
    P_{N^*}
    &= P_{N}[P_{I}(t)] \\
    &= P_{N}[1 + p(t-1)]
\end{aligned}
$$

By expanding simplfying the expression, the distribution of $N^{*}$ can be determined:

$$
\begin{aligned}
    N &\sim \text{Poisson}(\mu) \\
    \\
    P_{N}[1 + p(t-1)]
    &= e^{\mu(1 + v(z-1) -1)} \\
    &= e^{\mu (v(z-1))} \\
    &= e^{v\mu (z-1)} \\
    \\
    \therefore N^{*} &\sim \text{Poisson}(v \cdot \mu)
\end{aligned}
$$

Repeating the process for the other two members of the (a, b, 0) class,

$$
\begin{aligned}
   N &\sim \text{Binomial} (n, p) \\
   \therefore N^{*} &\sim \text{Binomial} (n, vp) \\
   \\
   N &\sim \text{Negative Binomial} (r, \theta) \\
   \therefore N^{*} &\sim \text{Negative Binomial} (r, v \theta)
\end{aligned}
$$
