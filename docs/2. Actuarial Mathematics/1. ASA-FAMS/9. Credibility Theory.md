# **Credibility Theory**

## **Overview**

**Credibility Theory** is a formal method of determining how much weight we should place on different sources of data when estimating future outcomes.

Consider two sources of data:

* **Internal Data**: More specific but smaller sample size
* **External Data**: Too general but larger sample size

**Credibility** $(Z)$ is the **weight** placed on our own **internally sourced data**. The overall estimate is the weighted average of the two sources:

$$
    \text{Overall Estimate} = Z \cdot \text{Internal Estimate} + (1-Z) \cdot \text{External Estimate}
$$

There are two unique properties of the credibility factor:

1. As the number of internal observations increases, the credibility factor increases
2. As the number of internal observations increases, the **rate of increase** decreases

$$
\begin{aligned}
    \frac{dZ}{dn} \gt 0 \\
    \frac{d^2Z}{dn^2} \lt 0
\end{aligned}
$$

## **Full Credibility**

The internal observations have **Full Credibility** when $Z=1$. This occurs when there are a **large number** of observations. The goal is to **quantify** what constitutes a "large" sample size.

Generally speaking, a source is credible if its **sample mean is equal to its theoretical mean**. The **smallest number of observations** where this occurs is what constitutes a sufficiently large sample size.

In practice, it is unreasonable to expect that the sample mean would be *exactly* equal to the theoretical mean. Thus, we instead expect that they will be *close* to one another, within some **small margin of error**, equivalent to some **percentage of the theoretical mean**:

$$
    |\bar{x} - E(X)| \le k \cdot E(X)
$$

Additionally, since $\bar{x}$ is a random variable, it is unreasonable to expect that it will *always* be within this margin of error. Thus, we instead expect that they will be close together *most* of the time, with some **minimum probability**:

$$
    P(|\bar{x} - E(X)| \le k \cdot E(X)) \ge p
$$

Thus, full credibility is dictated by two parameters:

* $k$: Permitted fluctuation; usually **small & close to 0**
* $p$: Minimum probability; usually **large & close to 1**

Given that there are a large number of observations, the distribution of the sample mean is **approximately normal via the Central Limit Theorem**:

<!-- Obtained from Coaching Actuaries -->
![CLT](Assets/9.%20Credibility%20Theory.md/CLT.png){.center}

Using the normal distribution, we can set up an **equation to solve for the minimum number of observations** $n$:

$$
\begin{aligned}
    \bar{x} \sim N(\mu_{\bar{x}}, \sigma^2_{\bar{x}}) \\
    \\
    P(|\bar{x} - E(X)| \le kE(X)) &= p \\
    P(-kE(X) \le \bar{x} - E(X) \le kE(X)) &= p \\
    P \left(\frac{-kE(X)}{\sqrt {Var(\bar{x})}} \le \frac{\bar{x} - E(\bar{x})}{Var (\bar{x})} \le \frac{kE(X)}{\sqrt {Var(\bar{x})}} \right) &= p \\
    P \left(Z \le \frac{kE(X)}{\sqrt {Var(\bar{x})}} \right) &= 1 - \frac{1-p}{2} \\
    \Phi \left(\frac{kE(X)}{\sqrt {Var(\bar{x})}} \right) &= \frac{1+p}{2}
\end{aligned}
$$

!!! Note

    Since we are solving for the **minimum** number of observations, we consider the **minimum case** to have full credibility - where the probability is EXACTLY $p$.

    This means that each of the areas in the two tails are $\frac{1-p}{2}$.

We then solve the equation to obtain the following expression:

$$
\begin{aligned}
    \Phi \left(\frac{kE(X)}{\sqrt {Var(\bar{x})}} \right) &= \frac{1+p}{2} \\
    \frac{kE(X)}{\sqrt {Var(\bar{x})}} &= z_{(1+p)/2} \\
    \frac{kE(X)}{\frac{\sqrt {Var(X)}}{\sqrt{n}}} &= z_{(1+p)/2} \\
    \sqrt{n} &= z_{(1+p)/2} \cdot \frac{1}{k} \cdot \frac{\sqrt {Var(X)}}{E(X)} \\
    \sqrt{n} &= \frac{z_{(1+p)/2}}{k} \cdot CV_{X} \\
    n &= \left(\frac{z_{(1+p)/2}}{k} \cdot CV_{X} \right)^2
\end{aligned}
$$

!!! Tip

    Although it is not hard to derive this expression on the spot, it is a rather long and tedious proof. Thus, it is recommended to **memorize** this formula to save time.

### **Number of Exposures**

In an actuarial context, credibility is often used when analyzing **aggregate claims data** ($S$). Each observation is called an **Exposure**, thus we are solving for the minimum **Number of Exposures**.

!!! Note

    Exposures are calculated based on a **per unit, per year** basis. For instance, two exposures could refer to either of the following scenarios:

    * Two Insureds for one year
    * One Insureds for two years

If using the collective risk model, the mean and variance of the aggregate model can be **decomposed into that of the underlying Frequency and Severity models**:

$$
\begin{aligned}
    n_{e}
    &= \left(\frac{z_{(1+p)/2}}{k} \cdot \frac{\sqrt{Var(S)}}{E(S)} \right)^2 \\
        &= \left(\frac{z_{(1+p)/2}}{k} \right)^2 \cdot \frac{Var (S)}{E(S)^2} \\
    &= \left(\frac{z_{(1+p)/2}}{k} \right)^2 \cdot \frac{Var(X) \cdot E(N) + [E(X)]^2 \cdot Var (N)}{E(N)^2 \cdot E(X)^2} \\
\end{aligned}
$$

### **Number of Claims**

Alternatively, the number of exposures can also be expressed in terms of **number of claims**.

If each exposure is a policyholder and the **average number of claims** per policyholder is given, then the number of claims is simply:

$$
\begin{aligned}
    n_c
    &= n_e \cdot E(N) \\
    &= \left(\frac{z_{(1+p)/2}}{k} \right)^2 \cdot \frac{Var(X) \cdot E(N) + [E(X)]^2 \cdot Var (N)}{E(N)^2 \cdot E(X)^2} \cdot E(N) \\
    &= \left(\frac{z_{(1+p)/2}}{k} \right)^2 \cdot \frac{Var(X) \cdot E(N) + [E(X)]^2 \cdot Var (N)}{E(N) \cdot E(X)^2} \\
    &= \left(\frac{z_{(1+p)/2}}{k} \right)^2 \cdot \left(\frac{Var(X)}{E(X)^2} + \frac{Var (N)}{E(N)} \right) \\
    &= \left(\frac{z_{(1+p)/2}}{k} \right)^2 \cdot \left( \left(\frac{SD(X)}{E(X)} \right)^2 + \frac{Var (N)}{E(N)} \right) \\
    &= \left(\frac{z_{(1+p)/2}}{k} \right)^2 \cdot \left( CV_{X}^2 + \frac{Var (N)}{E(N)} \right)
\end{aligned}
$$

Notice that the number of claims nicely **cleanly seperates out** the Severity and Frequency components. Thus, this allows credibility to be determined based on **ONLY frequency or severity data** by setting the **other component to 0**:

$$
\begin{aligned}
    n^{\text{Frequency}}_c &= \left(\frac{z_{(1+p)/2}}{k} \right)^2 \cdot \left(0 + \frac{Var (N)}{E(N)} \right) \\
    n^{\text{Severity}}_c &= \left(\frac{z_{(1+p)/2}}{k} \right)^2 \cdot \left(CV_{X}^2 + 0 \right)
\end{aligned}
$$

!!! Warning

    The wording for this section can be extremely confusing. Most questions will use the following format:

    "Calculate the **number of claims/exposures** needed for **aggregate loss/frequency/severity** to be within $k$% of the true value with $p$ probability."

    If only frequency or severity are specified, then the above formulas are used. Note that Frequency and severity can be also be referred to as **Number of Claims or Claims Size**.
    
    However, if frequency is expressed as the number of claims, the following **tricky question** can be produced:

    "Calculate the **number of claims** needed for the **total number of claims** to be within $k$% of the true value with $p$ probability."

We can then express them in terms of the number of exposures:

$$
\begin{aligned}
    n_c &= n_e \cdot E(N) \\
    n_e &= \frac{n_c}{E(N)}
\end{aligned}
$$

The intuition is that each exposure is expected to make $E(N)$ claims.

!!! Tip

    Given that the number of claims method can be used for Aggregate, Frequency & Severity data, it is much better to remember to always **start with the number of claims** and then convert to the number of exposures if needed.

### **Shortcuts**

Note that if frequency follows a **poisson distribution**, a simplification can be made:

$$
\begin{aligned}
    N &\sim \text{Poisson}(N) \\
    \\
    E(N) &= \text{Var(N)} \\
    \frac{E(N)}{Var(N)} &= 1 \\
    \\
    \therefore n_c
    &= \left(\frac{z_{(1+p)/2}}{k} \right)^2 \cdot \left(CV_{X}^2 + 1 \right)
\end{aligned}
$$

Similarly, if severity follows an **exponential distribution**, a simplification can be made:

$$
\begin{aligned}
    X &\sim \text{Exponential} \\
    \\
    E(X) &= \theta \\
    \text{Var}(X) &= \theta^2 \\
    \frac{\text{Var}(X)}{E(X)^2} &= 1 \\
    \\
    \therefore n_c
    &= \left(\frac{z_{(1+p)/2}}{k} \right)^2 \cdot \left(1 + \frac{Var (N)}{E(N)} \right)
\end{aligned}
$$

## **Partial Credibility**

If there are **insufficient observations**, then the data is said to have **Partial Credibility** $(Z \lt 1)$.

The appropriate factor to use can be determined via the **square root rule**, where $n'$ denotes the actual number of exposures or claims in the data:

$$
    Z = \sqrt{\frac{n'_{e}}{n_e}} = \sqrt{\frac{n'_{c}}{n_c}}
$$