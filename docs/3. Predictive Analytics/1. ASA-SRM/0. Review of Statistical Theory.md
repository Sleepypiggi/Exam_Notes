# **Review of Statistical Theory**

This section assumes some basic knowledge on **Probability Theory**, which can be found under another set of notes covering a [Review of Probability Theory](../../2. Actuarial Mathematics/ASA-FAMS/1. Review of Probability Theory.md).

## **Overview of Statistics**

Statistics is a discipline revolving around data.

A **Population** refers to the *theoretical* set of **all possible** data of the event of interest. The goal of statistics is to determine certain attributes that **summarizes or describes** the population, known as **Parameters**.

However, it is impossible to study the entire population at once, thus a **subset of the population** is studied instead, known as **Sample**. Attributes that summarize or describe the sample are known as **Statistics**.

!!! Info

    In practice, there are many different sampling methods that may result in vastly different samples. That is beyond the scope of this exam.

Ideally, the sample is **representative** of the population, which means that findings from the sample can be applied to the population as a whole. This means that the sample statistics can be used **estimate** population parameters.

We distinguish between the two (when they have the same notation) through the **Hat accent** (^) - Population Parameters are their written **without the hat** ($x$) while their corresponding sample statistics are written **with the hat** ($\hat{x}$).

!!! Info

    For more common statistics, they typically have their own unique notation. It is important to recognise both notations.

## **Sample Statistics**

### **Central Tendency**

The first set of Parameters & Statistics are concerned with **Central Tendency**, which is a measure of where the centre of the distribution is, to be used as a **representative value**.

The most common measure is the **Mean**, which is the sum of all observations divided by the number of observations:

$$
\begin{array}{|c|c|}
\hline
    \text{Population Mean} &
    \text{Sample Mean} \\
\hline
    \displaystyle{\mu_{x} = \sum x_{i} \cdot P(x_{i})} &
    \displaystyle{\bar{x} = \frac{1}{n} \sum x_{i}} \\
\hline
\end{array}
$$

!!! Note

    The above is more formally known as the **Arithmetic Mean**. There is also the **Geometric Mean**, which uses the **n-th root of the product** of the observations:

    $$
        \bar{x}_{\text{Geom}} = \left(\Pi x_{i} \right)^{\frac{1}{n}}
    $$

    The two are best applied in the following situations:

    * **Change Linearly**: Arithmetic average (Think Arithmetic Series)
    * **Change Exponentially**: Geometric Average (Think Geometric Series)

One problem with the Mean is that it is **sensitive to outliers**. An extremely large outlier could cause the mean to be well above or below the rest of the observations, reducing its reliability.

Thus, the **Median** is considered as well, which is the **midpoint of the population**, where **at least half the observations** is at least above or below the value:

$$
\begin{array}{|c|c|}
\hline
    \text{Population Median} &
    \text{Sample Median} \\
\hline
    \displaystyle{P(X \le m) \ge \frac{1}{2}} & \displaystyle{m_{\text{Odd}} = x_{\frac{n+1}{2}}} \\
    \displaystyle{P(X \ge m) \le \frac{1}{2}} & \displaystyle{m_{\text{Even}} = \frac{x_{\frac{n}{2}} + x_{\frac{n}{2} + 1}}{2}} \\
\hline
\end{array}
$$

!!! Warning

    The formula for the sample median assumes that the sample is **sorted in ascending order**.

The last common measure is the **Mode** of the population, which is the **most common value** in the distribution or sample.

!!! Info

    The term mode came from "Maximum Ordinate Frequency".

<!-- Obtained from Ledidi -->
![CENTRAL_TENDENCY](Assets/0.%20Review%20of%20Statistical%20Theory.md/CENTRAL_TENDENCY.png){.center}

### **Dispersion**

The next set of parameters and statistics are concerned with **Dispersion**, which is a measure of how **stretched or squeezed** a distribution is.

<!-- Obtained from Six Sigma -->
![SIX_SIGMA](Assets/0.%20Review%20of%20Statistical%20Theory.md/DISPERSION.png){.center}

The most common measure is the **Variance**, which measures the **spread** of values about the mean:

$$
\begin{array}{|c|c|}
\hline
    \text{Population Variance} &
    \text{Sample Variance} \\
\hline
    \displaystyle{\sigma^{2} = \frac{\sum (x_{i} - \mu)^{2}}{N}} &
    \displaystyle{s^{2} = \frac{\sum (x_{i} - \bar{x})^{2}}{n-1}} \\
\hline
\end{array}
$$

!!! Note

    The variance has the following key properties which makes it desirable:
    
    * Differences are squared thus **outliers are emphasised**, giving **better sense** of spread
    * Consequently, the spread is **positive only**, ensuring **no offsetting** effects
    * **Other Mathematical properties** which makes it **easier to manipulate** (EG. Variance of sum of independent variables is the sum of the individual variances)

!!! Note

    The variance is essentially the mean of the squared deviations from the mean. Thus, it is sometimes also referred to as the **Mean Squared**.

    However, it is not truly the "mean" as the division is by $n-1$ rather than $n$. This is a result of **Bessel's Correction**, which ensures that the resulting sample variance is an **Unbiased Estimator** of the population variance. It is not necessary to know the proof for the purposes of this exam.

    Additionally, Bessel's Correction **only affects the sample variance** - the resulting sample SD is a BIASED estimator.

However, the units of the variance are squared, which makes it hard to **hard to interpret**. Thus, the **Standard Deviation** (SD) which is the square root of the variance is also considered:

$$
\begin{array}{|c|c|}
\hline
    \text{Population Standard Deviation} &
    \text{Sample Standard Deviation} \\
\hline
    \displaystyle{\sigma = \sqrt{\frac{\sum (x_{i} - \mu)^{2}}{N}}} &
    \displaystyle{s = \sqrt{\frac{\sum (x_{i} - \bar{x})}{n-1}}} \\
\hline
\end{array}
$$

Similarly, the SD may **not be meaningful** to be used to **compare** the dispersion across different groups due to **different units and scale**. Thus, the **Coefficient of Variation** (CV) is measured, which is the **ratio of the SD relative to the mean**:

$$
\begin{array}{|c|c|}
\hline
    \text{Population Coefficient of Variation} &
    \text{Sample Coefficient of Variation} \\
\hline
    \displaystyle{\text{CV} = \frac{\sigma}{\mu}} &
    \displaystyle{\hat{\text{CV}} = \frac{s}{\bar{x}}} \\
\hline
\end{array}
$$

!!! Warning

    Naturally, the CV is not useful in situations where the **Mean is close to 0**, as the CV will **approach infinity**.

The last common measure is the **Interquartile Range** (IQR). It is the **difference** between the 75th and 25th Percentiles:

$$
    \text{IQR} = P_{75} - P_{25} = Q_{3} - Q_{1}
$$

!!! Info

    The 25th, 50th and 75th Percentiles are specially known as **Quartiles**, hence the naming. They are denoted by $Q_{i}$.

The IQR is most often used to **identify outliers**. It is generally accepted that fall outside of the "fences" are considered outliers:

$$
\begin{aligned}
    \text{Lower Fence} &= Q_{1} - 1.5 \cdot \text{IQR}
    \text{Upper Fence} &= Q_{3} + 1.5 \cdot \text{IQR}
\end{aligned}
$$

The IQR and the "fences" can be illustrated using a **Box-plot**:

<!-- Obtained from Scribbor -->
![BOXPLOT](Assets/0.%20Review%20of%20Statistical%20Theory.md/BOXPLOT.png)

!!! Warning
    
    There is actually **no formal definition for what is considered an outlier** in statistics. There is also **no strong mathematical basis** for why the fence was set at that level; it was a reasonable range that became popularized.

### **Relationships**

**Covariance** is a measure of the **linear relationship** between two variables:

* **Positive Covariance** - Variables move in the **same direction**
* **Negative Covariance** - Variables move in **opposite directions**

$$
\begin{array}{|c|c|}
\hline
    \text{Population Covariance} &
    \text{Sample Covariance} \\
\hline
    \displaystyle{\sigma_{x, y} = \text{Cov}(X, Y) = \mu_{x, y} - \mu_{x} \cdot \mu_{y}} &
    \displaystyle{\hat{\sigma}_{x, y} = \frac{1}{n-1} \sum (x_{i} - \bar{x}) \cdot (y_{i} - \bar{y})} \\
\hline
\end{array}
$$

However, there are two issues with Covariance:

1. Units are **unintuitive** (similar to variance)
2. **No benchmark** as to what constitutes a strong/weak relationship

Thus, the **Correlation** is an adjusted measure of the relationship **between -1 and 1**, which accounts for the above issues:

$$
\begin{array}{|c|c|}
\hline
    \text{Population Correlation} &
    \text{Sample Correlation} \\
\hline
    \displaystyle{\rho = \text{Corr}(X, Y) = \frac{\sigma_{x, y}}{\sigma_{x} \cdot \sigma_{y}}} &
    \displaystyle{r = \frac{\sum (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum(x_{i} - \bar{x}) \cdot \sum(y_{i} - \bar{y})}} \\
\hline
\end{array}
$$

The relationship between the variables can also be visualized using a Scatterplot of the data:

<!-- Obtained from Latest Quality -->
![SCATTEPLOT](Assets/0.%20Review%20of%20Statistical%20Theory.md/SCATTERPLOT.png){.center}

## **Sampling Distribution**

Whenever a sample is drawn from a population and a statistic is calculated, it is known as a **Point Estimate**.

Due to measurement errors, a different sample would be drawn each time, thus leading to a **different point estimate**. If this process were to be repeated a large number times, the **probability distribution** of the resulting point estimates is known as the **Sampling Distribution** of the statistic.

There is **no rule** surrounding the sampling distribution - it depends on the distribution of the population, statistic being measured, sampling method etc.

!!! Warning

    It may be hard to understand initially, but the distribution of sample statistics is **itself a population**. It is **different from the underlying population** that is being sampled.

!!! Note

    A well-known sampling distribution is that of the **Sample Mean**. Given a large enough sample size, it can be shown that the Sample Mean is approximately **normally distributed**.

    This is a result of the **Central Limit Theorem**. The technical proof is out of scope for the purposes of this exam.

<!-- Obtained from Claus Wike -->
![SAMPLING_DISTRIBUTION](Assets/0.%20Review%20of%20Statistical%20Theory.md/SAMPLING_DISTRIBUTION.png){.center}

### **Key Properties**

The **Error** of an estimate is the difference between the Sample Estimate and the Population Parameter:

$$
    \epsilon = \hat{\theta} - \theta
$$

The Bias of an estimator is the **expected value** of the error:

$$
    B(\hat{\theta}) = E(\hat{\theta} - \theta)
$$

An estimator is **unbiased if and only if the Bias is 0**. It can be understood that - **on average**, the estimator produces an **estimate that is equal to the true value**; individual estimates may still have errors.

!!! Info

    Biasness is a property of the **Estimator, not the estimate**. Thus, the term “Biased Estimate” should not be used in a statistical context.
    
The Variance of an Estimator (**Sampling Variance**) is the spread of estimates about its mean:

$$
    \mathrm{Var}(\hat{\theta}) = E[\hat{\theta} - E(\hat{\theta})]^{2}
$$

!!! Note

    The Standard Deviation of the Estimator (**Sampling Deviation**) is the square root of the sampling variance. It is also known as the **Standard Error** of the estimator.

A similar measure is known as the **Mean Squared Error** (MSE), which is the spread of estimates **about the Population Parameter**. It can be shown to be the **combination of the Bias and Variance** of the estimator:

$$
\begin{aligned}
    \text{MSE}(\hat{\theta})
    &= E[\hat{\theta} - \theta]^{2} \\
    &= \mathrm{Var}(\hat{\theta} - \theta) + [E(\hat{\theta} - \theta)]^{2} \\
    &= \mathrm{Var}(\hat{\theta}) + \text{Bias}^{2}(\hat{\theta})
\end{aligned}
$$

!!! Tip

    The proof for the above relationship is based on the relationship between the Mean and Variance:

    $$
    \begin{aligned}
        \mathrm{Var}(X) &= E(X^{2}) - [E(X)]^{2} \\
        X &= \theta - \hat{\theta} \\
        \\
        \therefore \mathrm{Var} &= \text{MSE} - \text{Bias}^{2} \\
        \therefore \text{MSE} &= \mathrm{Var} + \text{Bias}^{2}
    \end{aligned}
    $$

Variance and MSE are **both measuring the dispersion** of the estimator. The key difference is the reference point:

* **Variance**: Relative to the average of the estimator
* **MSE**: Relative to the population parameter

!!! Warning

    As seen in the above formula, MSE is not just about the spread of the estimates. An estimator which produces **clustered estimates that are far from the true value** has low variance, but **high bias and hence high MSE**.

    To have a low MSE, the estimator must have both a low bias AND low variance:

    <!-- Obtained from Scott Fortman Roe -->
    ![BIAS_VARIANCE](Assets/0.%20Review%20of%20Statistical%20Theory.md/BIAS_VARIANCE.png){.center}

Generally speaking, **estimators with lower MSE are preferred**. Note that this could also mean using a slightly biased estimator with low variance compared to using a unbiased estimator with high variance. There is **no clear rule given the two dimensional nature of MSE**.

### **Degrees of Freedom**

Degrees of freedom ($\nu$) refer to the **number of independent pieces of data that are free to vary** in the calculation of a statistic **without violating any of the constraints** of said statistic.

Number of independent pieces of data typically refers to the number of observations used; **sample size**. It is then usually **reduced by the number of constraints** imposed during the calculation:

$$
    \nu = n - \text{Constraints}
$$

To illustrate, consider the two most basic statistics:

<center>

|    **Sample Mean**     |      **Sample Variance**       |
| :--------------------: | :----------------------------: |
|     No constraints     | Constrained by the sample mean |
| $n$ degrees of freedom |    $n-1$ degrees of freedom    |

</center>

When computing the variance, the sample mean is **already a fixed quantity**. The average of the observations used in the sample **MUST be equal to the sample mean** (constraints). Thus, only $n-1$ points are able to freely vary, the last observation MUST force the average to be the sample mean; not free to vary.

!!! Tip

    This can be better understood as one degree of freedom being "used up" to determine the sample mean, thus all subsequent calculations **involving the sample mean** has one less degree of freedom.

### **Confidence Interval**

Given that there is only one true value for the population parameter and a whole distribution of estimators, it is unlikely that a point estimate will be equal to the population parameter. Thus, instead of a single point estimate, a range is used, known as a **Confidence Interval**.

The interval is made a chosen **Confidence Level** which represents the **proportion of confidence intervals that will contain the true value**. In other words, if a large number of confidence intervals constructed in the **same manner** were to be made, $(1-\alpha)%$ of them would contain the true value.

$$
    P(\text{Lower Bound} < \theta < \text{Upper Bound}) = 1 - \alpha
$$

!!! Warning

    A common misinterpretation is that there is a $(1-\alpha)%$ probability that the population parameter lies within a specific interval. This is WRONG. Once an interval is determined, the population parameter either lies within or outside that interval; it is **not a probability**.

    Thus, the confidence level is measuring the **reliability of the estimation procedure**, not a specific estimate.

    <!-- Obtained from Clausewike -->
    ![CONFIDENCE_INTERVAL_PROPORTION](Assets/0.%20Review%20of%20Statistical%20Theory.md/CONFIDENCE_INTERVAL_PROPORTION.png){.center}

The **Margin of Error** represents the range of values on *either side* of the point estimate that the true value could lie. The most common confidence level is 95%, which means that **95% of the density of the sampling distribution** lies within the interval.

The margin of errors are thus the corresponding **percentiles** at the boundary of the confidence interval. For a 95% confidence interval, the remaining 5% density is split equally in two:

$$
\begin{aligned}
    \text{Lower Bound} &= P_{0 + \frac{1+\alpha}{2}} \\
    \text{Upper Bound} &= P_{100 - \frac{1+\alpha}{2}} \\
    \therefore \text{Confidence Interval} &= \text{(Lower Bound, Upper Bound)}
\end{aligned}
$$

<!-- Obtained from Analyst Prep -->
![CONFIDENCE_INTERVAL](Assets/0.%20Review%20of%20Statistical%20Theory.md/CONFIDENCE_INTERVAL.png){.center}

!!! Tip

    The confidence interval will **gradually shrink to 0 as the sample size approaches the population size**. Intuitively, this is because if the sample = population, then there is no more uncertainty; the resulting statistic IS the population parameter!

If the sampling distribution is normal, then the distribution is symmetrical about the mean.

$$
    \text{Confidence Interval} = \hat{\theta} \pm \text{Percentile} \cdot \text{Standard Error}
$$

## **Hypothesis Testing**

**Hypothesis Testing** is a formal method of making inferences about the population parameters based on the sample statistic. It starts with a **Hypothesis** which is a conjecture about the population parameters:

* **Null Hypothesis** ($H_{0}$) - What is currently believed to be true
* **Alternative Hypothesis** ($H_{0}$) - What is to be proven

The key idea is that it is a **test of extremeness** - **assuming the null hypothesis is true**, how likely (how extreme) was that particular sample (or more extreme) to be drawn?

* Sample was **likely to be drawn**, then it is likely that the **null is true** (Expected outcome)
* Sample was **unlikely to be drawn** but yet it was drawn, is it due to **variance** or because the sampling distribution is misspecified because the **null not being true**? (Extreme outcome)

!!! Info

    A common phrase is that hypothesis testing is used to differentiate the Signal (True Misspecification) from the noise (Variance).

The probability of drawing the sample is formally known as the **p-value** ($p$). The **threshold** for how likely a sample to be drawn is known is known as the **Significance Level** ($\alpha$):

* $p < \alpha$: Test is successful; null can be rejected
* $p > \alpha$: Test fails; null CANNOT be rejected

!!! Note

    Most textbooks commonly use either 0.05 or 0.01 as the significance level for the tests.

!!! Warning

    Rejecting the null hypothesis does NOT mean that the alternative is accepted. For instance, rejecting the null hypothesis that a large paw print came from a bear does not mean that the paw print came from big foot.

    Thus, the two hypothesis are usually constructed such that they are **complementary** - such that rejecting null **must necessarily mean that the alternative is true**, which provides more insight:

    * $H_{0}: \theta = x$
    * $H_{1}: \theta \ne x$

    Consequently, hypothesis testing is not able to confirm the true value of a parameter; it is only able to quantify whether there is enough statistical evidence to dispute a hypothesized value of a parameter.

The "tail" of a hypothesis test refers to the **side of the distribution** where the values are considered extreme. It is based on the Alternative Hypothesis:

* **Right Tail**: $H_{1}: \theta \gt x$
* **Left Tail**: $H_{1}: \theta \lt x$
* **Two Tail**: $H_{1}: \theta \ne x$

The key difference is that the critical region is split into two for a two-tail test, requiring the sample to be even more extreme to be reject the null:

* **One Tail**: $\alpha$ on **either** side
* **Two tail**: $\frac{\alpha}{2}$ on **both** sides

<!-- Obtained from Medium -->
![TAIL_TEST](Assets/0.%20Review%20of%20Statistical%20Theory.md/TAIL_TEST.png){.center}

### **Sources of Error**

Given the probabilistic nature of hypothesis testing, there are bound to be errors. In particular, there are two types:

* **Type I Error** ($\alpha$): False Positive (Rejecting the null when it is true)
* **Type II Error** ($\beta$): False Negative (Not rejecting the null when it is false)

!!! Tip

    A real world example of hypothesis testing is in court. Generally speaking, the **defendant is innocent until proven guilty** (similar to assuming the null true).

    * **Type I Error**: Convicting to defendant when they are innocent
    * **Type II Error**: Failing to convict the defenant when they are guilty

<!-- Obtained from Scribbr -->
![ERROR_SOURCES](Assets/0.%20Review%20of%20Statistical%20Theory.md/ERROR_SOURCES.png){.center}

The errors occur as a result of the **overlap** between the sampling distributions under the Null and Alternative:

<!-- Obtained from Scribbr -->
![ERROR_DISTRIBUTION](Assets/0.%20Review%20of%20Statistical%20Theory.md/ERROR_DISTRIBUTION.png){.center}

Consider a rejection of the null under the two possible scenarios:

* Sample is due to **Variance**; results in **Type I Error**
* Sample is due to **Misspecification**; results in **True Positive**

The significance level sets the **threshold for extremeness** which rules out variance, thus the probability of type I errors occuring IS the significance level of the test. In essence, if the sample is **more extreme than it is likely to make this error**, then the test is willing to take the risk to conclude that the null can be rejected.

Type II errors are essentially the opposite scenario - the sample is extreme, but **not considered extreme enough** under our significance level, causing it to fail to reject the null. Since the threshold for extremeness is the type I error, **both errors are at odds with each other**.

!!! Warning

    Unlike confidence levels, the probability of a type I or II error occuring is for that *particular* test.

### **Test Statistics**

In order to determine the p-value, the value of the sample statistic on the sampling distribution (percentile) must be determined. This value is known as the **Test Statistic**.

$$
    \text{Test Statistic} = \frac{\hat{\theta} - \theta_{H_{0}}}{\text{SE}(\hat{\theta})}
$$

!!! Note

    For instance, if the sampling distribution is normally distributed, this requires normalizing or studentizing the sample statistic.

Rather than compare the probabilities (which requires an additional step to convert from test statisic to probability), the test statistic itself can be compared to the corresponding **percentile of the significance level**, known as the **Critical Value**. The two comparisons are identical:

<center>

|                 Reject Null                 |              Do not Reject Null              |
| :-----------------------------------------: | :------------------------------------------: |
|        p-value smaller than $\alpha$        |        p-value smaller than $\alpha$         |
| Test statistic *larger* than critical value | Test-statistic *smaller* than critical value |

</center>

!!! Note

    The area bounded by the critical value (area of the significance level) is known as the **Critical Region**.

<!-- Obtained from Analyst Prep -->
![HYPOTHESIS_TESTING](Assets/0.%20Review%20of%20Statistical%20Theory.md/HYPOTHESIS_TESTING.png){.center}

### **Link to Confidence Intervals**

Confidence Intervals measure the **range of values** that the true parameter could lie. Hypothesis tests determine the **reasonability** of a hypothesized value.

If both are constructed based on the same significance level and the hypothesized value lies **WITHIN the confidence interval**, then the **null hypothesised CANNOT be rejected**.

Intuitively, this is because being within the confidence interval itself indicates that the **value is reasonable**, thus automatically failing the test. Mathematically, this is because any value within the confidence interval will **always have a p-value greater than the significance level**.

## **Normal Sampling Distribution**

Many commonly studied sampling distributions can be shown to be **normally distributed**. However, it can also be **transformed to a variety of other distributions** that are commonly used in Statistical Theory.

### **Standard Normal Distribution**

Due to the shifting and scaling properties of the normal distribution, any normal distribution can be converted to a **Standard Normal Distribution** via a process known as **Standardization**:

$$
    Z = \frac{\hat{\theta} - E(\hat{\theta})}{\text{SE}(\hat{\theta})}   
$$

!!! Info
 
    It is recommended to Standardize for two reasons:
    
    1. It removes scale from the picture, **facilitating comparison** across different distributions
    2. Probabilities for the standard normal are well documented, **no need to compute** for each unique normal distribution 

### **t-Distribution**

The SE of the sampling distribution is typically a **function of the underlying** population standard deviation. However, in many cases, it is an **unknown value**. Thus, it can be **estimated** using the sample standard deviation instead:

$$
\begin{aligned}
    \text{SE}(\hat{\theta}) &\approx \hat{\text{SE}}(\hat{\theta}) \\
    f(\sigma_{\theta}) &\approx f(\hat{\sigma}_{\theta})   
\end{aligned}
$$

!!! Warning

    The sample SD is not estimating the SE, it is the **function of the sample SD** that is the estimating the SE.

!!! Tip

    There are now four layers to the estimation:

    1. **Population** - Standard Deviation $\sigma_{\theta}$
    2. **Estimate of the Population** - Sample Standard Deviation $\hat{\sigma}_{\theta}$
    3. **Sampling Distribution** - Standard Error $\sigma_{\hat{\theta}}$
    4. **Estimate of the Sampling Distribution** - Estimator of the Standard Error $\hat{\sigma}_{\hat{\theta}}$

If the standard error was estimated this way, then the resulting distribution now follows a **t-distribution**. It is essentially a standard normal distribution with a **lower peak and fatter tails**, which accounts for the **increased uncertainty** from using an estimate instead of the actual value in specifying the distribution.

The t-distribution has only one parameter - the **degrees of freedom** of the statistic of the sampling distribution. As the degrees of freedom (sample size) increases, the t-distribution **tends towards the standard normal distribution**:

<!-- Obtained from Scribbr -->
![T_Distribution](Assets/0.%20Review%20of%20Statistical%20Theory.md/T_DISTRIBUTION.png){.center}

The process of converting a normal distributed quantity into a t-distributed one is known as **Studentization**:

$$
\begin{aligned}
    t_{n-1}
    &= \frac{\hat{\theta} - E(\hat{\theta})}{\hat{\text{SE}}(\hat{\theta})} \\
    &= \frac{\hat{\theta} - E(\hat{\theta})}{\hat{\sigma}_{\theta}}
\end{aligned}
$$

!!! Info

    Standardization is known as such because it is converting the distribution to a **Standard** normal distribution.

    Studentization is known as such because the t-distribution is more formally known as the **Student's** t-distribution.

### **Chi-Squared Distribution**

A well-known sampling distribution is the **Chi-Square Distribution**. It is equivalent to a **Squared Standard Normal Distribution**:

$$
    \chi^{2}_{1} = Z^{2}
$$

Similar to the t-distribution, it only has one parameter - the **degrees of freedom** of the underlying statistic. The sum of $n$ **independent** squared normal variables results in a Chi-Squared Distribution with **$n$ degrees of freedom**:

$$
    \chi^{2}_{n} = Z^{2}_{1} + Z^{2}_{2} + Z^{2}_{3} + \dots + Z^{2}_{n}
$$

!!! Note

    As the degrees of freedom increases (as more standard normals are summed), the chi-square distribution will tend towards a normal distribution due to the **Central Limit Theorem**.

The shape of the chi-squared distribution is quite different compared to the Standard Normal, reflecting two key properties:

1. Squared variables are always **non-negative**
2. Square of 0 (standard normal peak) is still 0, thus the **peak of the chi-square is still 0**, which **declines thereafter**
3. Squared variables result in larger variance, thus has a **right skew**

<!-- Obtained from Live Boost -->
![CHI_SQUARED](Assets/0.%20Review%20of%20Statistical%20Theory.md/CHI_SQUARED.png){.center}

!!! Note

    Very few real world phenomena follow a chi-squared distribution; it is mainly used for hypothesis testing.

#### **Sample Variance**

By definition, a chi-square variable can be written as:

$$
\begin{aligned}
    \chi^{2}_{n}
    &= \sum^{n}_{1} Z^{2} \\
    &= \sum^{n}_{1} \left(\frac{\theta_{i} - \mu}{\sigma} \right)^{2}    
\end{aligned}
$$

It can be shown via **Cochran's Theorem** (out of scope for this exam) that the following is true:

$$
    \sum^{n}_{1} (\theta_{i} - \bar{\theta}) \sim \sigma^{2} \cdot \chi^{2}_{\nu}   
$$  

Thus, the sample variance (following Bessel's Correction) can be shown to be have a **scaled chi-square distribution**:

$$
\begin{aligned}
    \sum^{n}_{1} (\theta_{i} - \bar{\theta}) &\sim \sigma^{2} \cdot \chi^{2}_{\nu} \\
    \frac{\sum^{n}_{1} (\theta_{i} - \bar{\theta})}{n-1} &\sim \frac{\sigma^{2} \cdot \chi^{2}_{\nu}}{\nu} \\
    \frac{s^{2}}{\sigma^{2}} &\sim \frac{\chi^{2}_{\nu}}{\nu}
\end{aligned}
$$

!!! Note

    A scaled chi-square distribution refers to a one that has been **divided by its degrees of freedom**; average of the squared normals.

#### **Reformulated t**

Using the above, it is possible to express the t-distribution in terms of a chi-squared one:

$$
\begin{aligned}
    t_{\nu}
    &= \frac{\theta - E(\hat{\theta})}{s} \\
    &= \frac{\theta - E(\hat{\theta})}{\sqrt{\mathrm{Var}(\hat{\theta})}} \cdot \sqrt{\frac{\mathrm{Var}(\hat{\theta})}{s^{2}}} \\
    &= \frac{\theta - E(\hat{\theta})}{\sqrt{\mathrm{Var}(\hat{\theta})}} \div \sqrt{\frac{s^{2}}{\mathrm{Var}(\hat{\theta})}} \\
    &= \frac{Z}{\sqrt{\frac{\chi^{2}_{\nu}}{\nu}}}
\end{aligned}
$$

Thus, a t-distribution is the ratio of standard normal variable to the squareroot of a chi-squared distribution divided by its degrees of freedom.

!!! Tip

    As the **degrees of freedom tends to infinity**, the scaled chi-squared distribution will tend to 1. This is why the t-distribution tends towards a standard normal distribution:

    $$
    \begin{aligned}
        \lim_{\nu \to \infty} \frac{\chi_{\nu}}{\nu} \to 1 \\
        \therefore \lim_{n \to \infty} t_{\nu} \to Z
    \end{aligned}
    $$

    The limit of the scaled chi-square variable is a result of **Slutsky's theorem**, which is out of scope for the purposes of this exam.

#### **Goodness of fit**

A Chi-Square test is used to determine whether the population **follows a specified distribution**; how good the population fits the distribution. It can only be used for **Discrete Variables**.

Discrete Variables generally follow a **Multinomial distribution** (out of scope for this exam), which is a generalization of the Binomial distribution:

* **Bi-Nomial**: Two outcomes (EG. Heads or Tails)
* **Multi-Nomial**: Multiple Outcomes (EG. Roll of a six sided die)

Multinomial distributions can typically be expressed in the form of a **Contingency Table**, which contains the **count/frequency** of each observation:

<!-- Obtained from Research Gate -->
![CONTINGENCY_TABLE](Assets/0.%20Review%20of%20Statistical%20Theory.md/CONTINGENCY_TABLE.png){.center}

!!! Tip

    The distribution of majors among for each level of expertise.

The Chi-Square test is to determine whether the **Observed distribution** ($O$) follows the **Expected Distribution** ($E$):

<!-- Obtained from JMP -->
![GOODNESS_OF_FIT](Assets/0.%20Review%20of%20Statistical%20Theory.md/GOODNESS_OF_FIT.png){.center}

Consider a variable with $k$ possible outcomes. The resulting Chi-Squared test-statistic is:

$$
    \chi^{2}_{\text{test}} = \frac{\sum^{k}_{i} (O_{i} - E_{i})^{2}}{E_{i}}
$$

The above can be shown to follow a Chi-Square distribution:

$$
\begin{aligned}
    \chi^{2}_{\text{test}}
    &= \frac{\sum^{k}_{i} (O_{i} - E_{i})^{2}}{E_{i}} \\
    &= \left(\frac{\sum^{k}_{i} (O_{i} - E_{i})}{\sqrt{E_{i}}} \right)^{2} \\
    \\
    \frac{(O_{i} - E_{i})}{\sqrt{E_{i}}} &\sim Z \\
    \\
    \therefore \chi^{2}_{\text{test}} &\sim \chi^{2}_{k-1}
\end{aligned}
$$

!!! Warning

    Despite being the sum of $k$ standard normals, the test-statistic loses one degree of freedom since there is a constraint the total number of observations is fixed.

    If we know the frequencies for $k-1$ categories, then the last observation MUST be a certain number such that the total number of observations tally.

!!! Tip

    The expression within the statistic is standard normal for two reasons:

    1. For large sample sizes, due to CLT, the observations follow a **normal distribution**
    2. For large sample sizes, the variance is **approximately equal to the mean**
    3. Thus, the expression is simply a result of **standardization of this distribution**

    The properties of a multinomial distribution are extremely similar to a binomial one:

    * Expectation of *each* category = $n \cdot p_{i}$
    * Variance of *each* category = $n \cdot p_{i} \cdot (1 - p_{i})$

    When $n$ is large, $(1 - p_{i}) \approx 1$ thus the variance is approximately equal to the mean.

For non-categorical data, an alternative (though less rigorous) method of determining goodness of fit is using a **Quantile-Quantile** plot (Q-Q Plot):

1. Quantiles of the Sample and Theoretical distribution are plotted against each other
2. A reference identity line of $y=x$ is drawn
3. If the quantiles lie mostly on the line (Sample Quantile = Theoretical Quantile), then the sample can be concluded to follow the theoretical distribution 

<!-- Obtained from Analyst Prep -->
![QQ_PLOT](Assets/0.%20Review%20of%20Statistical%20Theory.md/QQ_PLOT.png){.center}

!!! Warning

    Quantiles are "cut-points" in a distribution which **equally splits** the resulting intervals. For instance, the two statistics covered earlier are examples of commonly used Quantiles:

    * **Percentile**: Splits 100 ways, each with 1% density
    * **Quartile**: Splits 4 ways, each with 25% density

    Do not confuse them with Quartiles!

### **F-Distribution**

The **ratio** of two **independent** scaled chi-square distributions results in a **F-distribution** with BOTH the degrees of freedom as parameters:

$$
    F_{m,n} = \frac{\frac{\chi_{m}}{m}}{\frac{\chi_{n}}{n}}
$$

The F-distribution can be reformulated in terms of the previous two distributions:

$$
\begin{aligned}
    (t_{\nu})^{2}
    &= \left(\frac{Z}{\sqrt{\frac{\chi^{2}_{\nu}}{\nu}}} \right)^{2} \\
    &= \frac{Z^{2}}{\frac{\chi^{2}_{\nu}}{\nu}} \\
    &= \frac{\frac{\chi^{2}_{1}}{1}}{\frac{\chi^{2}_{\nu}}{\nu}} \\
    &= F_{1,\nu} \\
    \therefore t^{2}_{\nu} &\sim F_{1, \nu} \\
    \therefore t^{-2}_{\nu} &\sim F_{\nu, 1} \\
    \\
    \lim_{\nu \to \infty} \frac{\chi_{\nu}}{\nu} &\to 1 \\
    \lim_{\nu \to \infty} F_{m, \nu} &\to \frac{\chi_{\nu}}{\nu} \\
    \therefore F_{m, \infty} &\sim \frac{\chi_{\nu}}{\nu}
\end{aligned}
$$

#### **Equality of Variance**

An F-test is typically used to compare if the Variance of two populations are equal:

* $H_{0}: \sigma^{2}_{A} = \sigma^{2}_{B}$
* $H_{1}: \sigma^{2}_{A} \ne \sigma^{2}_{B}$

The corresponding **F-statistic** is the ratio of the two sample variances being tested:

$$
    F_{\text{Statistic}} = \frac{s^{2}_{A}}{s^{2}_{B}}
$$

Under the null, the statistic can be shown to be following a F-distribution:

$$
\begin{aligned}
    F_{\text{Statistic}}
    &= \frac{\frac{\chi_{n_{A}-1}}{n_{A}-1}}{\frac{\chi_{n_{B}-1}}{n_{B}-1}} \\
    &= \frac{\frac{s^{2}_{A}}{\sigma^{2}_{A}}}{\frac{s^{2}_{B}}{\sigma^{2}_{B}}} \\
    &= \frac{s^{2}_{A}}{\sigma^{2}_{A}} \cdot \frac{\sigma^{2}_{B}}{s^{2}_{B}} \\
    &= \frac{s^{2}_{A}}{s^{2}_{B}} \cdot \frac{\sigma^{2}_{B}}{\sigma^{2}_{A}} \\
    &= \frac{s^{2}_{A}}{s^{2}_{B}} \\
    \\
    F_{\text{test}} &\sim F_{n_{A}-1, n_{B}-1}
\end{aligned}
$$

Thus, if the two variances are not equal, the statistic would not follow a F-distribution and hence result in extreme values (relative to the F-distribution).

#### **ANOVA**

Although F-tests compare variances, they can be used to make inferences about the mean as well. This is done through a process known as **Analysis of Variance** (ANOVA).

Under ANOVA, the F-test can be restated as:

$$
\begin{aligned}
    F_{\text{Statistic}}
    &= \frac{\text{Variance Between Groups}}{\text{Variance Within Groups}} \\
    &= \frac{\text{Explained Variance}}{\text{Unexplained Variance}}
\end{aligned}
$$

By the **Law of Total Variance**, variance can be decomposed into two components:

$$
\begin{aligned}
    \text{Var(X)}
    &= E_{Y}[\text{Var}(X \mid Y)] + \text{Var}_{Y}[E_{Y}(X \mid Y)] \\
    &= \text{Variance Within Groups} + \text{Variance Between Groups} \\
    &= \text{Unexplained Variance} + \text{Explained Variance}
\end{aligned}
$$

* **Variance Within Groups**: Variance within the distribution of $X \mid Y$ for a given $Y$ (**Noise**)
* **Variance Between Groups**: Variance between the means of $X \mid Y$ for different $Y$ (**Signal**)

The key is to identify if any differences between groups is **due to a true difference between groups, or if it is due to random variation**. ANOVA uses the variance within groups as a benchmark - if the variance between groups is significantly larger, then it is likely that there is a **true difference** in the two groups.

## **Maximum Likelihood Estimation**

If the **population distribution is known**, there is an **alternative method of estimating** the parameters apart from calculating the corresponding sample statistics, known as **Maximum Likelihood Estimation** (MLE).

There are an infinite number of **variations of the distribution** that could have resulted in the sample, each with **different parameters**.

Technically speaking, any set of parameters could have resulted in the sample. However, the goal of MLE is to find the set of **parameters that are most likely to result in the sample**; in other words, the **probability of obtaining this sample is the highest** with this set of paramters than any other set.

The probability of obtaining the sample is known as its **Likelihood**:

$$
     L(\theta \mid x) = P_{\theta} (X = x)
$$

!!! Warning

    Likelihood functions and PMF/PDFs are often confused with one another as they involve the same expression.
    
    The key is **understanding what is given and what is random**, which results in the subtle but differing notation:

    * **PMF/PDF**: Given parameters, outcomes are random; $P_{X}(x)$
    * **Likelihood**: Given outcomes, parameters are random; $P_{\theta}(x)$

Assuming that the sample is iid, the likehood for the entire sample is the **product of the likelihood for each observation**, known as the **Likelihood Function**:

$$
    L(\theta) = \prod P_{\theta}(X = x_i)
$$

!!! Note

    For discrete distributions, $X = 0$ is a **valid observation** and thus should be considered as well.

The goal is to find the parameters that **maximizes** the likelihood function through calculus:

$$
    \frac{d}{d\theta} L(\theta) = 0
$$

In practice, especially when dealing with multiple parameters, the likelihood function is complicated to work with. Thus, a **log transformation** is often applied to simplify it, turning the **product into a summation**.

This is known as the **Log-Likelihood Function**. Since the logarithm transform is monotonic, both the likelihood and log-likelihood functions share the **same maximum**.

$$
\begin{aligned}
    \ell (\theta) &= \ln L(\theta) \\
    \therefore \frac{d}{d\theta} \ell (\theta) &= 0
\end{aligned}
$$

### **Practical Tips**

Some distributions have complicated PMF/PDFs that make working with them more complicated. Most questions will usually have **some method to simplify the likelihood function**.

The first tip is to understand that since the likelihood function will be logarithm transformed and then differentiated, **factors that contains ONLY constants can be dropped** since they will inevitably be removed later:

$$
\begin{aligned}
    L(\theta) &= a * x \\
    \ell (\theta) &= \ln a + \ln x \\
    \ell' (\theta) &= \frac{1}{x} \\
    \\
    \therefore L(\theta) \propto x
\end{aligned}
$$

The next tip is that if the parameters are **embedded in the power** of some constant, they should be combined together:

$$
\begin{aligned}
    L(\theta)
    &= a^{\theta} \cdot b^{\theta} \cdot c^{\theta} \\
    &= (a \cdot b \cdot c)^{\theta}
\end{aligned}
$$

The reverse also applies...divide power, may not be in the same term, could come from another term

However, if the above terms for some reason are **added instead of multiplied**, then a **substituition** method would be better:

$$
\begin{aligned}
    L(\theta) &= e^{-\frac{k}{100}} - \left(e^{-\frac{k}{100}} \right)^2 \\
    L(\theta) &= p - p^2
\end{aligned}
$$

!!! Note

    If multiple parameters are being estimated, then the likelihood function is **partially differentiated** to each variable instead
    
    The MLE parameters are then the **combination of parameters** that maximizes the function. 

### **Method of Moments**

An alternative method for estimating population parameters is the **Method of Moments** (MOM).

It is based on the **Law of Large Numbers**, which states that the sample mean converges to the population mean (first raw moment), given a **sufficiently large sample size**.

Thus, by **equating the sample raw moments to the population raw moments**, up to the number of parameters to estimate, we can solve for an estimate of the parameters.

!!! Note

    This process can be repeated for as many parameters there are:

    * **One Parameter**: First moments equated
    * **Two Parameters**: First & Second moment equated

    Most MLE questions will only require single parameter estimation. If two parameters are given, then there is **usually some way to simplify it**.

$$
\begin{aligned}
    E(X^k)
    &= \bar{x} \\
    &= \frac{\sum x^k_i}{n}
\end{aligned}
$$

!!! Note

    Needless to say, $\bar{x}$ represents the **average of the quantity being modelled**, NOT the number of observations.

    If there are 10 observations of 2 claims, then we must compute the number of claims as $10 \cdot 2 = 20$.

The main advantage of this method is that it is **computationally simpler** than MLE. For certain known distributions, the MOM estimate and MLE estimate are the same, thus MOM can be used as a **shortcut for MLE**.
