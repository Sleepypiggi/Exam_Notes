# **Model Selection**

## **Overview**

As previously covered, there are **goodness of fit** measures that can be used to determine the best model from a given spread. However, the number of possible models tend to **increase exponentially** with the number of possible predictors as **each combination of predictors** is considered a different model.

Thus, it is **impractical** to fit every possible model. There are well-accepted algorithms that can be used to find the determine the best model in an efficient manner.

<!-- Obtained from Kaggle -->
![MODEL_SELECTION](Assets/5.%20Model%20Selection.md/MODEL_SELECTION.png){.center}

!!! Tip

    In general, model selection helps to pyroduce a **simpler model** that is easier to interpret with lower variance predictions.

## **Selection Algorithm**

### **Best Subset Selection**

The **Best Subset Selection** algorithm is the ideal model selection algorithm, assuming there are no resource constraints:

1. For every possible number of predictors ($p$), fit every combination of available predictors ($g$)
2. Among the models with the same number of predictors, select the model with the highest $R^{2}$ as the best model for that number of predictors ($M_{p}$)
3. Among the $M_{p}$, select the best overall model using a **Selection Criteria** of choice

!!! Tip

    The process is akin to using a double loop:

    ```
    FOR p = 0 TO p THEN
        FOR g = 1 TO g THEN
    ```
!!! Warning

    The number of predictors allowed in the model starts at 0 to reflect that the null model is also a possible model.

Best Subset Selection considers EVERY possible model, which is **impractical** for large $g$. However, it serves as a theoretical benchmark for the "best" possible model:

$$
    \text{Models Considered} = 2^{g}
$$

### **Stepwise Selection**

**Forward Stepwise** starts with 1 predictor and **iteratively adds** one predictor at a time:

1. Fit all possible models for $p=1$
2. Among the fitted models, select the model with the highest $R^{2}$ as $M_{p}$
3. Fit a new model by **ADDING a predictor** to the previous model; consider all possibilities
4. Repeat step (2) - (3) for all $p$ to determine all $M_{p}$
5. Among the $M_{p}$, select the best overall model using a **Selection Criteria** of choice

<!-- Obtained from Quantifying Health -->
![FORWARD_STEPWISE](Assets/5.%20Model%20Selection.md/FORWARD_STEPWISE.png){.center}

**Backward Stepwise** starts with the maximum and **iteratively drops** one predictor at a time:

1. Fit all possible models for $p=p$
2. Among the fitted models, select the model with the highest $R^{2}$ as $M_{p}$
3. Fit a new model by **DROPPING a predictor** from the previous model; consider all possibilities
4. Repeat step (2) - (3) for all $p$ to determine all $M_{p}$
5. Among the $M_{p}$, select the best overall model using a **Selection Criteria** of choice

<!-- Obtained from Quantifying Health -->
![BACKWARD_STEPWISE](Assets/5.%20Model%20Selection.md/BACKWARD_STEPWISE.png){.center}

Using either of the approaches, the **number of models fitted reduces**, making it an appealing choice compared to best subset selection:

$$
    \text{Number of Stepwise} = 1 + \frac{g(g+1)}{2}
$$

However, stepwise selection is a **greedy algorithm** as it picks the best available model at that moment, which may cause it to **miss the overall best model**, since it does not allow for retrospective changes.

!!! Note

    The variable to add or drop is typically determined using **hypothesis testing** to determine the significance of each variable. 
    
    At each step, there might be multiple variables that can be added or dropped. It is important to only add or drop variables **ONE at a time** (most or least significant), as there might **second order impact** which changes its significance to the model.

    The above is applicable even outside of a stepwise model selection context.

### **Comparison**

As its name suggests the best subset approach will always pick the model with the **lowest RSS** (highest $R^{2}$) as it considers all possible models. The other two algorithms have no guarantee to pick such a model given its greedy approach.

!!! Warning

    Neither models have any guarantee to pick the model with the lowest test error, as they are not evaluated using test error.

The number of models considered for every possible $p$ and in totality across the three approaches are summarized below:

<center>

| **Number of Predictors** |   **Best**    |      **Forward**       |      **Backward**      |
| :----------------------: | :-----------: | :--------------------: | :--------------------: |
|            0             |       1       |           1            |           1            |
|            k             | $g \choose k$ |        $p-k+1$         |          k+1           |
|            p             |       1       |           1            |           1            |
|          Total           |    $2^{g}$    | $1 + \frac{g(g+1)}{2}$ | $1 + \frac{g(g+1)}{2}$ |

</center>

!!! Tip

    For $p=0$ and $p=p$, all three algorithms will result in the **SAME best model** for that number of predictors $M_{p}$. This is because there is only **ONE possible model for each** - a model with no predictors and one with all predictors, respectively.

## **Selection Criteria**

Selection Criteria is the metric used to choose the final model of all the shortlisted models with different number of predictors.

During the shortlisting process, the models being compared had the same number of predictors, thus $R^{2}$ provided a fair comparison. However, since each model now has a different number of predictors, $R^{2}$ is **no longer appropriate** as it is affected by the number of predictors. Thus, the following metrics can be used instead:

1. Adjusted $R^{2}$
2. Mallow's $C_{p}$
3. Information Criteria

The above metrics are essentially trying to **indirectly estimate the test error**, by applying an adjustment on the training error based on the number of predictors (flexibility). An alternative approach is to **directly estimate** the test error using **Validation Set** approaches.

### **Training Error Approach**

There are three new metrics to be covered, all based off the same intuition:

* Mallow's $C_p$
* Aikake’s Information Criterion (AIC)
* Bayesian Information Criterion (BIC)

$$
\begin{aligned}
    C_{p} &= \frac{\text{RSS}}{n} + \frac{2 \cdot \text{MSE}}{n} \cdot p \\
    \text{AIC} &= \frac{\text{RSS}}{n \cdot \text{MSE}} + \frac{2}{n} \cdot p \\
    \text{BIC} &= \frac{\text{RSS}}{n \cdot \text{MSE}} + \frac{\ln n}{n} \cdot p
\end{aligned}
$$

!!! Warning

    There are **slightly different formulations** for AIC and BIC; the above is what is used in the source text. Regardless, they should arrive at similar conclusions.

!!! Tip

    If the MSE is unbiased, then the resulting measures are **unbiased** estimates of the test error as well.

The metrics not meant to be used to evaluate a *particular* model’s effectiveness. They are used to compare the **relative effectiveness** of models; as a model selection criteria. The lower the metrics, the better the model in comparison to others.

Notice that the first term is a measure of the **training error**, while the second term is the **adjustment** for the number of predictors. The lower the metric the better, thus the adjustment **penalizes models with more predictors**.

!!! Tip

    Mallows $C_{p}$ and AIC will always produce the same relative model selection as the only difference between them is that AIC is further scaled by the MSE. However, since MSE is positive, the relative strength of models is unaffected.

    Additionally notice that AIC and BIC are extremely similar, with the only difference being on the penalty - where AIC uses a flat 2 while BIC uses $\ln n$. Both will produce the **same selection** for small sample sizes, however for large sample sizes (above 7), BIC penalizes the larger models more.

    $$
        \ln 7 \approx 2
    $$

### **Validation Approach**

As mentioned in the beginning, a model's effectiveness should NOT be evaluated using data that was used to train it. Thus, for model selection, the selection of the final model should be based on data that was not used to train it; using **Validation Data**.

The MSE determined using the Validation data is known as the **Validation Error**, which is an **estimate of the Test Error** (since both are based on data that was not used to train the model).

The model with the **lowest Validation Error** is chosen as the final model. Once determined, **re-train the final model** using ALL non-test observations (Training + Validation data).

!!! Note

    The final model must be retrained as it should be **using as much as information as possible**.

    The model that was selected was trained only using the validation data. When retrained with the all data available (training + validation), a slightly different model will be produced.

<!-- Obtained from Galaxy Inferno -->
![VALIDATION_SET](Assets/5.%20Model%20Selection.md/VALIDATION_SET.png){.center}

#### **Cross Validation**

A more systematic method is to use **Cross Validation** instead:

* Split the model into **$k$ groups** of roughly equal size; each group is known as a **Fold**
* For each fold, train the model on the data **outside the fold** and calculate the validation error against the fold
* Determine the **Cross Validation Error** as the average of the errors from each fold

<!-- Obtained from Statology -->
![CROSS_VALIDATION](Assets/5.%20Model%20Selection.md/CROSS_VALIDATION.png){.center}

An extreme version of Cross Validation is known as the **Leave One Out Cross Validation** (LOOCV), where the number of folds is equal to the number of observations; **each fold contains only one observation**.

!!! Tip

    If there are $k$ folds, then $k$ models will be fitted. Thus, for LOOCV, as many models will be fitted **as there are observations**, making it extremely **computationally intensive**.

#### **Comparisons**

Recall that the purpose of the validation error is to **estimate the test MSE**. Thus, it can be evaluated based on the typical **estimator metrics**:

<center>

|  **Metric**  | **VS** | **CV** | **LOOCV** |
| :----------: | :----: | :----: | :-------: |
|   **Bias**   |  High  | Medium |    Low    |
| **Variance** |  Low   | Medium |   High    |

</center>

!!! Warning

    Note that the above comparisons are RELATIVE to one another.

The validation set approach has the **highest bias** as it only uses a **subset** of the data while **LOOCV** uses all but one, making its bias substantially lower.

For variance, since training set in the LOOCV only differ by one observation, the training sets are highly correlated, making the results **extremely correlated**, resulting in a high variance. Om the flipside, the validation set approach only has one training set, thus the result has limited variance.

!!! Warning

    LOOCV having high variance maybe **untuitive** as the correlated results should result in **clustered predictions**.

    However, consider the possibility that consecutive predictions are close, but there is a trend in the predictions. The resulting predictions will follow the trend, resulting in **high variance**.

!!! Tip

    Cross Validation with $k=5$ provides a good balance, which also takes into account computational efficiency.

### **Complete Process**

1. Choose a model selection method (Best, Stepwise)
2. Perform the model selection method in conjunction with a validation method to determine the **optimal number** of predictors
3. Perform the model selection method again with the **whole dataset** (Training + Validation)
4. Select the model with the number of predictors determined in (2) as the final model

!!! Tip

    The reason for needing to re-run the process is because the model in step (2) did not use the entire available dataset. The algorithm could have reached a different conclusion if the entire dataset was used.

### **Hyperparameter Tuning**

Hyperparameters are inputs decided by the modelled that affects *how* the model should learn.

Rather than arbitrarily picking a value, cross validation can be used to test a range of values and choose the best one.
