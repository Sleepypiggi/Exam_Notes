# **Model Selection**

## **Overview**

As previously covered, there are goodness of fit measures that can be used to determine the best model from a given spread.

However, the number of possible models tend to **increase exponentially** with the number of possible predictors as **each combination of predictors** is considered a different model.

$$
    \text{Number of Possible Models} = 2^{g}
$$

!!! Warning

    The above does NOT consider the possibility of interaction terms or polynomial predictors; the amount is **understated**.

    Similarly, the above is the number of models for a single machine learning type. In reality, **multiple methods could be considered** leading to an even higher number!

Thus, it is **impractical** to fit every possible model. There are well-accepted algorithms that can be used to find the determine the best model in an efficient manner.

<!-- Obtained from Kaggle -->
![MODEL_SELECTION](Assets/5.%20Model%20Selection.md/MODEL_SELECTION.png){.center}

## **Selection Algorithm**

### **Best Subset Selection**

The **Best Subset Selection** algorithm is the ideal model selection algorithm, assuming there are no resource constraints.

There are three key considerations:

1. **How many predictors** should the model use?
2. **Which of the predictors** should be used?
3. What **criteria** should be used to determine the 'best' model?

Thus, the algorithm tracks the following variables:

* Predictors allowed in the model $(p)$
* Predictors being used in the model $(g)$
* Best combination of predictors for a given number of allowed predictors $(M_{p})$
* Best combination of predictors for the best number of allowed predictors; the **final model** $(M_{f})$

``` 
    FOR p = 0 TO p_max THEN
        FOR g = 1 TO g_max THEN

            1. Fit all possible combinations of g into the p allowed predictors
            2. Compute the selection criteria for the fitted model

        NEXT g
                
        1. Set M_p to be the model with the best fit among all g
            
    NEXT p

    1. Set M_f to be the model with the best selection score among all p
```

As alluded to, this method is impractical given the number of models that need to be fitted. However, it is important to understand what is the theoretically best possible method to select models as a benchmark.

!!! Warning

    The number of predictors allowed in the model starts at 0 to reflect that the null model is also a possible model.

### **Stepwise Selection**

Stepwise Selection using an iterative algorithm that 'locks' in the 

**Forward Stepwise** starts with 1 predictor and iteratively adds one predictor at a time:

1. Fit all possible models for $p = 1$
2. Determine the best model based on the selection criteria
3. Fit a new model by **ADDING ONE predictor** from the remaining predictors to the previous model; consider all possibilities
4. Repeat (2) - (3) for all until $p = p_{\text{Max}}$

<!-- Obtained from Quantifying Health -->
![FORWARD_STEPWISE](Assets/5.%20Model%20Selection.md/FORWARD_STEPWISE.png){.center}

**Backward Stepwise** starts with the maximum and iteratively drops one predictor at a time:

1. Fit all possible models for $p = p_{\text{Max}}$
2. Fit a new model by **DROPPING ONE predictor** from the current predictors from the previous model; consider all possibilities
3. Determine the best model based on the selection criteria
4. Repeat (2) - (3) for all until $p = 1$

<!-- Obtained from Quantifying Health -->
![BACKWARD_STEPWISE](Assets/5.%20Model%20Selection.md/BACKWARD_STEPWISE.png){.center}

Using either of the approaches, the **number of models fitted reduces**, especially so for large $g$:

$$
    \text{Number of Stepwise} = 1 + \frac{g(g+1)}{2}
$$

However, stepwise selection is a **greedy algorithm** as it picks the best available model at that moment, which may cause it to **miss the overall best model**, since it does not allow retrospective changes.

## **Selection Criteria**

### **Information Criteria**

Recall how the adjusted R-squared applies a penalty for adding parameters to the model.

Similarly, there are other more general metrics that achieve the same effect:

* Aikake’s Information Criterion (AIC)
* Bayesian Information Criterion (BIC)

$$
\begin{aligned}
\end{aligned}
$$

!!! Warning

    There are **slightly different formulations** for AIC and BIC; the above is what is used in the source text. Regardless, they should arrive at similar conclusions.
    
They are not meant to be used to evaluate a *particular* model’s effectiveness. They are used to compare the **relative effectiveness** of models; as a model selection criteria.

### **Validation Approach**

As mentioned in the beginning, a model's effectiveness should NOT be evaluated using data that was used to train it. Thus, for model selection, the selection of the model should be based on data that was not used to train it; using **Validation Data**.

The MSE determined using the Validation data is known as the **Validation Error**, which is an **estimate of the final Test Error** (since both are based on data that was not used to train the model).

The model with the **lowest Validation Error** is chosen as the final model. Once determined, **re-train the final model** using ALL non-test observations (Training + Validation data).

!!! Note

    The final model must be retrained as it should be **using as much as information as possible**.

<!-- Obtained from Galaxy Inferno -->
![VALIDATION_SET](Assets/5.%20Model%20Selection.md/VALIDATION_SET.png){.center}

#### **Cross Validation**

A more systematic method is to use **Cross Validation** instead:

* Split the model into **$k$ groups** of roughly equal size; each group is known as a **Fold**
* For each fold, train the model on the data **outside the fold** and calculate the validation error against the fold
* Determine the **Cross Validation Error** as the average of the errors from each fold

<!-- Obtained from Statology -->
![CROSS_VALIDATION](Assets/5.%20Model%20Selection.md/CROSS_VALIDATION.png){.center}

An extreme version of Cross Validation is known as the **Leave One Out Cross Validation** (LOOCV), where the number of folds is equal to the number of observations; **each fold contains only one observation**.

!!! Tip

    If there are $k$ folds, then $k$ models will be fitted. Thus, for LOOCV, as many models will be fitted **as there are observations**, making it extremely **computationally intensive**.

#### **Comparisons**

Recall that the purpose of the validation error is to **estimate the test MSE**. Thus, it can be evaluated based on the typical **estimator metrics**:

<center>

|  **Metric**  | **VS** | **CV** | **LOOCV** |
| :----------: | :----: | :----: | :-------: |
|   **Bias**   |  High  | Medium |    Low    |
| **Variance** |  Low   | Medium |   High    |

</center>

!!! Warning

    Note that the above comparisons are RELATIVE to one another.

The validation set approach has the **highest bias** as it only uses a **subset** of the data while **LOOCV** uses all but one, making its bias substantially lower.

For variance, since training set in the LOOCV only differ by one observation, the training sets are highly correlated, making the results **extremely correlated**, resulting in a high variance. Om the flipside, the validation set approach only has one training set, thus the result has limited variance.

!!! Warning

    LOOCV having high variance maybe **untuitive** as the correlated results should result in **clustered predictions**.

    However, consider the possibility that consecutive predictions are close, but there is a trend in the predictions. The resulting predictions will follow the trend, resulting in **high variance**.

!!! Tip

    Cross Validation with $k=5$ provides a good balance, which also takes into account computational efficiency.

### **Hyperparameter Tuning**

