# **Model Selection**

## **Overview**

As previously covered, there are **goodness of fit** measures that can be used to determine the best model from a given spread. However, the number of possible models tend to **increase exponentially** with the number of possible predictors as **each combination of predictors** is considered a different model.

Thus, it is **impractical** to fit every possible model. There are well-accepted algorithms that can be used to find the determine the best model in an efficient manner.

<!-- Obtained from Kaggle -->
![MODEL_SELECTION](Assets/5.%20Model%20Selection.md/MODEL_SELECTION.png){.center}

!!! Tip

    The goal is to achieve a Parsimonious Model (derived from the word Parsimony), which is a model that achieves the desired fit by using the minimum number of variables.
    
    Generally speaking, these simpler models are less flexible which allows them to **perform better on new data**, improving their predictive performance:

    <!— Obtained from Coaching Actuaries —>
    ![SUBSET_PERFORMANCE](Assets/5.%20Model%20Selection.md/SUBSET_PERFORMANCE.png){.center}

## **Selection Algorithm**

### **Best Subset Selection**

The **Best Subset Selection** algorithm is the ideal model selection algorithm, assuming there are no resource constraints:

1. For every possible number of predictors ($p$), fit every combination of available predictors ($g$)
2. Among the models with the same number of predictors, select the model with the highest $R^{2}$ as the best model for that number of predictors ($M_{p}$)
3. Among the $M_{p}$, select the best overall model using a **Selection Criteria** of choice

!!! Tip

    The process is akin to using a double loop:

    ```
    FOR p = 0 TO p THEN
        FOR g = 1 TO g THEN
    ```
!!! Warning

    The number of predictors allowed in the model starts at 0 to reflect that the null model is also a possible model.

Best Subset Selection considers EVERY possible model, which is **impractical** for large $g$. However, it serves as a theoretical benchmark for the "best" possible model:

$$
    \text{Models Considered} = 2^{g}
$$

!!! Warning

    Just because it considers all possible models, it does NOT mean that it is guaranteed to find the best possible model.

    It is able to find the best model from a training error perspective, but the *true* best model typically refers to a **test error** perspective.

    Since there is no way to perfectly estimate the test error, thus there is no guarantee to find the true best model. This is true regardless of whichever method is used.

### **Stepwise Selection**

**Forward Stepwise** starts with 1 predictor and **iteratively adds** one predictor at a time:

1. Fit all possible models for $p=1$
2. Among the fitted models, select the model with the highest $R^{2}$ as $M_{p}$
3. Fit a new model by **ADDING a predictor** to the previous model; consider all possibilities
4. Repeat step (2) - (3) for all $p$ to determine all $M_{p}$
5. Among the $M_{p}$, select the best overall model using a **Selection Criteria** of choice

<!-- Obtained from Quantifying Health -->
![FORWARD_STEPWISE](Assets/5.%20Model%20Selection.md/FORWARD_STEPWISE.png){.center}

**Backward Stepwise** starts with the maximum and **iteratively drops** one predictor at a time:

1. Fit all possible models for $p=p$
2. Among the fitted models, select the model with the highest $R^{2}$ as $M_{p}$
3. Fit a new model by **DROPPING a predictor** from the previous model; consider all possibilities
4. Repeat step (2) - (3) for all $p$ to determine all $M_{p}$
5. Among the $M_{p}$, select the best overall model using a **Selection Criteria** of choice

<!-- Obtained from Quantifying Health -->
![BACKWARD_STEPWISE](Assets/5.%20Model%20Selection.md/BACKWARD_STEPWISE.png){.center}

Using either of the approaches, the **number of models fitted reduces**, making it an appealing choice compared to best subset selection:

$$
    \text{Number of Stepwise} = 1 + \frac{g(g+1)}{2}
$$

However, stepwise selection is a **greedy algorithm** as it picks the best available model at that moment, which may cause it to **miss the overall best model**, since it does not allow for retrospective changes.

### **Stepwise Regression**

A hybrid approach that combines elements of the Forward and Backward method is known as the **Stepwise Regression** method:

1. Start with the null model
2. Consider each possible variable to be added - **add** the variable with the **smallest p-value** from a two-tailed t-test; do NOT add the variable if the p-value is above a **specified threshold**
3. Consider each possible that can be dropped - **drop** the variable with the highest **p-value** from a two-tailed t-test; do NOT drop the variable if the p-value is below a **specified threshold**
4. Repeat until the algorithm no longer adds or drops a variable

The main benefit over regular stepwise approaches is that it covers a **larger number** of models. However, it comes with its own unique issues:

* It only looks at p-values, not any other metric
* It ignores the joint effect of predictors
* It involves a sequence of t-tests, which affects the overall power of the test

!!! Tip
    
    At each step, there might be multiple variables that can be added or dropped. It is important to only add or drop variables **ONE at a time** (most or least significant), as there might **second order impact** which changes its significance to the model.

There are problems inherent to the selection process regardless of the method used:

1. All methods fail to account for any domain knowledge that the user has
2. All methods are **susceptible to data snooping** (repeating the analysis slightly differently to achieve a certain outcome)

### **Comparison**

As its name suggests the best subset approach will always pick the model with the **lowest RSS** (highest $R^{2}$) as it considers all possible models. The other two algorithms have no guarantee to pick such a model given its greedy approach.

!!! Warning

    Neither models have any guarantee to pick the model with the lowest test error, as they are not evaluated using test error.

The number of models considered for every possible $p$ and in totality across the three approaches are summarized below:

<center>

| **Number of Predictors** |   **Best**    |      **Forward**       |      **Backward**      |
| :----------------------: | :-----------: | :--------------------: | :--------------------: |
|            0             |       1       |           1            |           1            |
|            k             | $g \choose k$ |        $p-k+1$         |          k+1           |
|            p             |       1       |           1            |           1            |
|          Total           |    $2^{g}$    | $1 + \frac{g(g+1)}{2}$ | $1 + \frac{g(g+1)}{2}$ |

</center>

!!! Tip

    For $p=0$ and $p=p$, all three algorithms will result in the **SAME best model** for that number of predictors $M_{p}$. This is because there is only **ONE possible model for each** - a model with no predictors and one with all predictors, respectively.

!!! Note

    The number of models looked at for stepwise regression is **not fixed** but is typically larger than Forward/Backward methods.

## **Selection Criteria**

Selection Criteria is the metric used to choose the final model of all the shortlisted models with different number of predictors.

During the shortlisting process, the models being compared had the same number of predictors, thus $R^{2}$ provided a fair comparison. However, since each model now has a different number of predictors, $R^{2}$ is **no longer appropriate** as it is affected by the number of predictors. Thus, the following metrics can be used instead:

1. Adjusted $R^{2}$
2. Mallow's $C_{p}$
3. Information Criteria

The above metrics are essentially trying to **indirectly estimate the test error**, by applying an adjustment on the training error based on the number of predictors (flexibility). An alternative approach is to **directly estimate** the test error using **Validation Set** approaches.

### **Training Error Approach**

There are three new metrics to be covered, all based off the same intuition:

* Mallow's $C_p$
* Aikake’s Information Criterion (AIC)
* Bayesian Information Criterion (BIC)


$$
\begin{aligned}
    \text{AIC}
    &= \frac{1}{n} \cdot (\text{RSS} + 2 \cdot p \cdot \text{MSE}_{\text{Full}}) \\
    \\
    \text{BIC}
    &= \frac{1}{n} \cdot (\text{RSS} + \ln n \cdot p \cdot \text{MSE}_{\text{Full}}) \\
    \\
    C_{p}
    &= \frac{\text{RSS}}{\text{MSE}_{\text{Full}}} - n + 2(p+1) \\
    &\approx \text{AIC}
\end{aligned}
$$

!!! Tip

    The MSE in the above equations represent the MSE for a model with ALL predictors, hence "full".

    If this MSE used is unbiased, then the resulting measures are **unbiased** estimates of the test error as well.

!!! Note

    Adjusted $R^{2}$ is also a valid metric, but the other three are preferred as they have **stronger theoretical justifications** as a measure of model quality.

    Regardless, all metrics are not feasible when there is **high dimensions**. The metrics all rely on the fitted residuals - but if the model has difficulty being fit in the first place, then the metrics cannot be calculated.

The metrics are not meant to be used to evaluate a *particular* model’s effectiveness. They are used to compare the **relative effectiveness** of models; as a model selection criteria. The lower the metrics, the better the model in comparison to others (with respect to test error).

Notice that the first term is a measure of the **training error**, while the second term is the **adjustment** for the number of predictors. The lower the metric the better, thus the adjustment **penalizes models with more predictors**.

!!! Tip

    Notice that AIC and BIC are extremely similar, with the only difference being on the penalty - where AIC uses a flat 2 while BIC uses $\ln n$. Both will produce the **same selection** for small sample sizes, however for large sample sizes (above 7; 8 and above), BIC penalizes the larger models more.

    $$
        \ln 7 \approx 2
    $$

### **Validation Approach**

As mentioned in the beginning, a model's effectiveness should NOT be evaluated using data that was used to train it. Thus, for model selection, the selection of the final model should be based on data that was not used to train it; using **Validation Data**.

The MSE determined using the Validation data is known as the **Validation Error**, which is an **DIRECT estimate of the Test Error** (since both are based on data that was not used to train the model).

The model with the **lowest Validation Error** is chosen as the final model. Once determined, **re-train the final model** using ALL non-test observations (Training + Validation data).

!!! Note

    The final model must be retrained as it should be **using as much as information as possible**.

    The model that was selected was trained only using the validation data. When retrained with the all data available (training + validation), a slightly different model will be produced.

<!-- Obtained from Galaxy Inferno -->
![VALIDATION_SET](Assets/5.%20Model%20Selection.md/VALIDATION_SET.png){.center}

There are two main disadvtanges to using Validation Set:

1. Results tend to be **fickle** as only one iteration is run with only a subset of the data
2. Since only a subset of the data is used, the **true signal might not have been captured**, resulting in **overestimations** of the test error

#### **Cross Validation**

A more *systematic* method is to use **Cross Validation** instead:

* Split the model into **$k$ groups** of roughly equal size; each group is known as a **Fold**
* For each fold, train the model on the data **outside the fold** and calculate the validation error against the fold
* Determine the **Cross Validation Error** as the average of the errors from each fold

<!-- Obtained from Statology -->
![CROSS_VALIDATION](Assets/5.%20Model%20Selection.md/CROSS_VALIDATION.png){.center}

!!! Note

    The systematic approach of CV helps to alleviate the problems of regular validation:

    1. The average of multiple iterations are used that covers all the observations; tends to be less fickle
    2. Majority of the observations are used in each iteration; true signal should be captured which results in more accurate estimations of the test error

#### **LOOCV**

An extreme version of Cross Validation is known as the **Leave One Out Cross Validation** (LOOCV), where the number of folds is equal to the number of observations; **each fold contains only one observation**.

If there are $k$ folds, then $k$ models will be fitted. Thus, for LOOCV, as many models will be fitted **as there are observations**, making it extremely **computationally intensive**. However, one upside is that the LOOCV error is **constant**; it only needs to be run once.

!!! Note

    Other variations have some judgment involved in how to partition the datasets, thus different iterations of the algorithm could lead to different results.

For linear regression specifically, there is a **closed form expression** for LOOCV. The key idea is that due to the **linear nature** of the model, the effect of each predictor on the observation is easily known (Leverage). Thus, the leverage can be used to **remove the effect of a particular observation** on the residuals, achieving the effect of LOOCV:

$$
    \text{LOOCV} = \frac{1}{n} \cdot \sum \frac{y - \hat{y}}{1 - h_{i}}
$$

!!! Tip

    The primary advantage of this is that ONLY ONE model needs to be fitted to obtain the LOOCV error compared to hvaing fit $n$ models when using other approaches.

#### **Comparisons**

Recall that the purpose of the validation error is to **estimate the test MSE**. Thus, it can be evaluated based on the typical **estimator metrics**:

<center>

|  **Metric**  | **VS** | **CV** | **LOOCV** |
| :----------: | :----: | :----: | :-------: |
|   **Bias**   |  High  | Medium |    Low    |
| **Variance** |  Low   | Medium |   High    |

</center>

!!! Warning

    Note that the above comparisons are RELATIVE to one another.

The validation set approach has the **highest bias** as it only uses a **subset** of the data while **LOOCV** uses all but one, making its bias substantially lower.

For variance, since training set in the LOOCV only differ by one observation, the training sets are highly correlated, making the results **extremely correlated**, resulting in a high variance. On the flipside, the validation set approach only has one training set, thus the result has limited variance.

!!! Warning

    LOOCV having high variance maybe **untuitive** as the correlated results should result in **clustered predictions**.

    However, consider the possibility that consecutive predictions are close, but there is a trend in the predictions. The resulting predictions will follow the trend, resulting in **high variance**.

!!! Tip

    Cross Validation with $k=5$ provides a good balance, which also takes into account computational efficiency.

### **Complete Process**

1. Choose a model selection method (Best, Stepwise)
2. Perform the model selection method in conjunction with a validation method to determine the **optimal number** of predictors
3. Perform the model selection method again with the **whole dataset** (Training + Validation)
4. Select the model with the number of predictors determined in (2) as the final model

!!! Tip

    The reason for needing to re-run the process is because the model in step (2) did not use the entire available dataset. The algorithm could have reached a different conclusion if the entire dataset was used.

### **Hyperparameter Tuning**

Hyperparameters are inputs decided by the modelled that affects *how* the model should learn.

It is NOT possible (difficult) to determine the optimal hyper parameter prior to the experiment. Thus, rather than arbitrarily picking a value, cross validation can be used to **test a range of values** and choose the best one.
