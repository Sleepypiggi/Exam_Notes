# **Model Selection**

## **Overview**

As previously covered, there are goodness of fit measures that can be used to determine the best model from a given spread.

However, the number of possible models tend to **increase exponentially** with the number of possible predictors as **each combination of predictors** is considered a different model.

$$
    \text{Number of Possible Models} = 2^{g}
$$

!!! Warning

    The above does NOT consider the possibility of interaction terms or polynomial predictors; the amount is **understated**.

    Similarly, the above is the number of models for a single machine learning type. In reality, multiple methods could be considered leading to an even higher number!

Thus, it is **impractical** to fit every possible model. There are well-accepted algorithms that can be used to find the determine the best model in an efficient manner.

## **Selection Algorithm**

### **Best Subset Selection**

The **Best Subset Selection** algorithm is the ideal model selection algorithm, assuming there are no resource constraints.

There are three key considerations:

1. **How many predictors** should the model use?
2. **Which of the predictors** should be used?
3. What **criteria** should be used to determine the 'best' model?

Thus, the algorithm tracks the following variables:

* Predictors allowed in the model $(p)$
* Predictors being used in the model $(g)$
* Best combination of predictors for a given number of allowed predictors $(M_{p})$
* Best combination of predictors for the best number of allowed predictors; the **final model** $(M_{f})$

``` 
    FOR p = 0 TO p_max THEN
        FOR g = 1 TO g_max THEN

            1. Fit all possible combinations of g into the p allowed predictors
            2. Compute the selection criteria for the fitted model

        NEXT g
                
        1. Set M_p to be the model with the best fit among all g
            
    NEXT p

    1. Set M_f to be the model with the best selection score among all p
```

As alluded to, this method is impractical given the number of models that need to be fitted. However, it is important to understand what is the theoretically best possible method to select models as a benchmark.

!!! Warning

    The number of predictors allowed in the model starts at 0 to reflect that the null model is also a possible model.

### **Stepwise Selection**

Stepwise Selection using an iterative algorithm that 'locks' in the 

**Forward Stepwise** starts with 1 predictor and iteratively adds one predictor at a time:

1. Fit all possible models for $p = 1$
2. Determine the best model based on the selection criteria
3. Fit a new model by **ADDING ONE predictor** from the remaining predictors to the previous model; consider all possibilities
4. Repeat (2) - (3) for all until $p = p_{\text{Max}}$

<!-- Obtained from Quantifying Health -->
![FORWARD_STEPWISE](Assets/5.%20Model%20Selection.md/FORWARD_STEPWISE.png){.center}

**Backward Stepwise** starts with the maximum and iteratively drops one predictor at a time:

1. Fit all possible models for $p = p_{\text{Max}}$
2. Fit a new model by **DROPPING ONE predictor** from the current predictors from the previous model; consider all possibilities
3. Determine the best model based on the selection criteria
4. Repeat (2) - (3) for all until $p = 1$

<!-- Obtained from Quantifying Health -->
![BACKWARD_STEPWISE](Assets/5.%20Model%20Selection.md/BACKWARD_STEPWISE.png){.center}

Using either of the approaches, the **number of models fitted reduces**, especially so for large $g$:

$$
    \text{Number of Stepwise} = 1 + \frac{g(g+1)}{2}
$$

However, stepwise selection is a **greedy algorithm** as it picks the best available model at that moment, which may cause it to **miss the overall best model**, since it does not allow retrospective changes.

## **Selection Criteria**

### **Information Criteria**

Recall how the adjusted R-squared applies a penalty for adding parameters to the model.

Similarly, there are other more general metrics that achieve the same effect:

* Aikake’s Information Criterion (AIC)
* Bayesian Information Criterion (BIC)

$$
\begin{aligned}
\end{aligned}
$$

!!! Warning

    There are **slightly different formulations** for AIC and BIC; the above is what is used in the source text. Regardless, they should arrive at similar conclusions.
    
They are not meant to be used to evaluate a *particular* model’s effectiveness. They are used to compare the **relative effectiveness** of models; as a model selection criteria.

### **Cross Validation**

As alluded to in the beginning, training data should not be used to evaluate a model’s predictive power; the test MSE should be used.


