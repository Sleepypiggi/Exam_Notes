# **Principal Component Analysis**

## **Overview**

**Principal Component Analaysis** (PCA) is an unsupervised statistical learning method that transforms high dimensional data into a **lower dimensional space** by creating **new key variables** ("Principal Components") that are **linear combinations** of the other variables.

!!! Warning

    There are alternative methods of covering PCA via **Linear Algebra** (Eigenvector, Eigenvalues). For the purposes of this exam, it is sufficient to know the below terminology.

    For simplicity, the examples in this section will use two principal components. However, it is possible to have more than two in the exam and in practice.

!!! Note

    For the purposes of this exam, it is assumed that PCA can only be performed with **Quantitative Variables**.

## **Principal Components**

Each principal component is intended to explain **most of the REMAINING variation** in the dataset after accounting for previous PCs. Naturally, the first principal component will **explain the most variation** and each subsequent one will explain lesser and lesser.

However, subsequent PCs should NOT have any overlap in the variation explained; thus should be **uncorrelated** with the previous PCs. Due to certain mathematical properties assumed, this is equivalent to the PCs being **Orthogonal** (perpendicular) to one another.

<!-- Obtained from Learnche -->
![PCA_TWO_COMPONENTS](Assets/8.%20Principal%20Components.md/PCA_TWO_COMPONENTS.png){.center}

!!! Tip

    Recall that being Orthogonal also means that the **Dot Products** of the principal component **LOADINGS** are 0:

    $$
        \sum \varphi_{i,j} \cdot \varphi_{i,k} = 0
    $$

    Note that only the dot product of PAIRS of principal components are 0, not for all principal components.

Due to the linear algebra required, the data must first be **Centered** and **Scaled**. Each PC then **passes through the mean** of the data (which is now the origin post centering):

<!-- Obtained from Learnche -->
![PCA_CENTER_SCALE](Assets/8.%20Principal%20Components.md/PCA_CENTER_SCALE.png){.center}

!!! Note

    The purpose of scaling is to ensure that features with different units are using a **common unit** for comparison. However, if the features are already using the **same units**, there is **no need to scale** the data.

!!! Tip

    The first PCs is essentially the **line of best fit** that passes through the data.
    
    Note that this best fit line is DIFFERENT from an otherwise equivalent linear regression line - the LR line aims to minimizes the residual with respect to the **TARGET variable**, while PCA is with respect to **ALL variables**.

## **Loading and Scores**

The **direction** of the PC in the multi-dimensional space is known as its **Loading**. Mathematically, it is the **contribution of each variable** on the PC. Naturally, the higher the loading, the more that variable contributes to the variance explained for that PC.

!!! Note

    It is typically assumed that the dataset is "Normalized"

Consider the perpendicular projection of each observation onto the PC; the straight line distance between an observation and the PC. The **distance** from the origin to this point on the PC is known as the **Score of the observation**.

<!-- Obtained from LearnChe -->
![PCA_SCORE](Assets/8.%20Principal%20Components.md/PCA_SCORE.png){.center}
 
!!! Note

    Each observation has a score for EACH PC; not just the nearest one.

Mathematically, it can be represented as the following:

* i: Observation Index
* j: PC Index
* k: Feature Index

$$
    z_{i,j} = \sum_{k} \varphi_{j,k} \cdot x_{i,k}
$$

!!! Note

    If the data is scaled, then the **sum of squared loadings** for each PC is 1:

    $$
        \sum \varphi^{2}_{1, k} = 1
    $$

    This is another constraint during the fitting process, apart from being Orthogonal.

Geometrically, the PCs also form a **plane** in the multi-dimensional space. The Score of each observation with respect to the respective PCs is the coordinate of the observations within the coordinate space of the plane. Thus, observations with high scores are "outliers" in the PC space.

<!-- Obtained from Learnche -->
![PCA_PLANE](Assets/8.%20Principal%20Components.md/PCA_PLANE.png){.center}

!!! Info

    For scenarios with more than two PCs, the resulting shape will be a Cube, Tesseract or other equivalent higher dimensional shape.

!!! Tip

    In this sense, PC is said to find the "shape" that minimizes the sum of perpendicular distances from each observation; the "shape" that is closest to the observations

Naturally, the resulting coordinates in the PC system is **different from the original**. This is better appreciated by changing the perspective of a plot to use the PCs as the vertical and horizontal axis respectively:

<!-- Obtained from Coaching Actuaries -->
![PCA_COORDINATE](Assets/8.%20Principal%20Components.md/PCA_COORDINATE.png){.center}

!!! Note

    The first PC is always the horizontal, followed by the vertical, so and so forth. They are more generally referred to as the PC1 direction, PC2 direction etc.

A **bi-plot** combines the Loadings and Scores into a **single plot** (hence "bi") and uses the principal components as the main coordinate space:

* **Primary Axis**: Score
* **Secondary Axis**: Loading
* Horizontal Vertical split remains the same as before

<!-- Obtained from Coaching Actuaries -->
![BI_PLOT](Assets/8.%20Principal%20Components.md/BI_PLOT.png){.center}

The key addition is the loading vectors for each feature; it allows us to obtain key information at a glance:

* **Vector Length**: Magnitude of loadings; identify relative influence each feature has on respective PCs
* **Vector Angle**: Relationship between features on the respective PCs:
    * **Near 0**: Positively correlated
    * **Near 90**: Uncorrelated
    * **Above 90, below 180**: Negatively correlated

!!! Tip

    The inference on vectors is intuitive. If the angle between vectors is small, they are **pointing in the same directions** and hence are positively correlated with the respective PCs; vice-versa.

!!! Warning

    Do not mistakenly read the primary axis for the loading vectors. The above plot colour codes them, but it is unlikely to be the case in the actual exam.

    ![BI_PLOT_SECONDARY](Assets/8.%20Principal%20Components.md/BI_PLOT_SECONDARY.png)

## **Variance**

Recall that the PCs are created such that the total variation explained is maximized. Note that it refers to the variation in the **underling features**. This is in contrast to linear regression where the variation is with respect to the target variable.

The variation of features explained by each principle component is the variation of that particular principal component:

$$
\begin{aligned}
    s_{z_{j}}
    &= \frac{1}{n-1} \cdot \sum (z_{i,j} - \bar{z}_{j})^{2} \\
    &= \frac{1}{n-1} \cdot \sum (z_{i,j})^{2} \\
    &= \frac{1}{n-1} \cdot \sum (\sum_{k} \varphi_{j,k} \cdot x_{i,k})^{2}
\end{aligned}
$$

!!! Note

    The key is noticing that since the data was centered, the **mean of the PC is always 0**. The only way for this to occur is if the **sum of scores** for each principal component is 0:

    $$
        \sum_{i} z_{i,j} = 0
    $$

!!! Tip

    It is possible to infer the relative variance from a Biplot, ONLY if the data was NOT scaled. If unscaled, then **larger loadings would lead to larger variances**, all else equal. Thus, the feature with the largest variance would be the feature with the largest PC1 loading (as PC1 explains the largest amount of variance).

    This cannot be used for scaled data as the variance for all features would be 1.

A **Scree Plot** summarizes the above information by plotting the proportion of variance explained by each successive PC. As alluded to previously, the plot will always be **decreasing** as the variance explained should decrease per PC:

$$
    \text{Prop Var Explained by PC} = \frac{\text{Var Explained by PC}}{\text{Sample Variance}}
$$

<!-- Obtained from Coaching Actuaries -->
![SCREE_PLOT](Assets/8.%20Principal%20Components.md/SCREE_PLOT.png){.center}

!!! Tip

    The number of PCs to include can be chosen by deciding what **proportion of the data** we want explained by the principal components.

    Alternatively, it is also common to choose the point where the subsequent variance explained by future PCs **drops off significantly**. Either way, it is a somewhat subjective process.

## **Other**

### **Collinearity**

In essence, PCA "compresses" the variables together into fewer PCs. **Highly correlated** variables have common signals that can be compressed together, thus **greatly benefitting** from PCA.

!!! Note

    Conversely, when a dataset is uncorrelated, there is **no room to compress the variables** together, hence rendering PCA ineffective. In fact, using PCA would actually reduce the interpretability!

An extreme case would be when the variables are **Perfectly Collinear** (variables can be expressed as an exact linear combination of another). In this case, PCA compresses all features into **ONE MAIN principal component** which is always the **FIRST PC** that explains **ALL** the variation.

This manifests itself on the Biplot where all observations have a **score of 0** for PC2; the observations are **entirely explainable by PC1**: 

<!-- Obtained from Coaching Actuaries -->
![BIPLOT_COLLINEAR](Assets/8.%20Principal%20Components.md/BIPLOT_COLLINEAR.png){.center}

### **Regression**

It is possible to perform Linear Regression using the principal component space. In this case, it is known as a **Principal Component Regression**:

$$
    E(Y \mid X) = \theta_{0} + \theta_{1} \cdot Z_{1} + \theta_{2} \cdot Z_{2} + \dots
$$

!!! Note

    To check with CA if the residuals are maximized with respect to PC

Note that since PC uses **ALL variables**, it is akin to performing linear regression on all variables. Thus, if an equivalent MLR model uses all available predictors, the two will provide the **same prediction**.

!!! Warning

    Although they will lead to the same result, the coefficients will naturally be different since the **baseline is different** (actual observation vs PC).

    However, since the PCs are simply linear combinations of the original features, it is possible to re-calculate the **original regression coefficients** by **decomposing** the PCs back into the features and **regrouping** them:

    $$
        \beta_{j}
        = \theta_{1} \cdot \varphi_{j, 1} + \theta_{2} \cdot \varphi_{j, 2} + \dots
    $$

The number of PCs used is a **flexibility measure**. Including more PCs results in a lower bias but increases the variance (of the prediction). In this sense, the number of PCs can be treated as a hyper-parameter and hence the **optimal number** of PCs can be found via cross-validation.

!!! Warning

    PCR is NOT a variable selection method, as PCs use ALL variables in the dataset to construct the PCs.

!!! Note

    Since each PC is **uncorrelated** with the others, adding or removing PCs does NOT affect any of the coefficients.

Generally speaking, any other statistical learning method can be applied to the reduced PC dataset, not just linear regression. The resulting model should produce **less noisy results** as the signal would be captured in the first few PCs.
