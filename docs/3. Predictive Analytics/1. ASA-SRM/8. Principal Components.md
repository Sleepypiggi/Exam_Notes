# **Principal Component Analysis**

## **Overview**

**Principal Component Analaysis** (PCA) is an unsupervised statistical learning method that transforms high dimensional data into a **lower dimensional space** by creating **new key variables** ("Principal Components") that are **linear combinations** of the other variables.

!!! Warning

    There are alternative methods of covering PCA via **Linear Algebra** (Eigenvector, Eigenvalues). For the purposes of this exam, it is sufficient to know the below terminology.

    For simplicity, the examples in this section will use two principal components. However, it is possible to have more than two in the exam and in practice.

## **Principal Components**

Each principal component is intended to explain **most of the REMAINING variation** in the dataset after accounting for previous PCs. Naturally, the first principal component will **explain the most variation** and each subsequent one will explain lesser and lesser.

However, subsequent PCs should NOT have any overlap in the variation explained; thus should be **uncorrelated** with the previous PCs. Due to certain mathematical properties assumed, this is equivalent to the PCs being **Orthogonal** (perpendicular) to one another.

<!-- Obtained from Learnche -->
![PCA_TWO_COMPONENTS](Assets/8.%20Principal%20Components.md/PCA_TWO_COMPONENTS.png){.center}

!!! Tip

    Recall that being Orthogonal also means that the **Dot Products** of the principal components are 0.

    Note that only the dot product of PAIRS of principal components are 0, not for all principal components.

Due to the linear algebra required, the data must first be **Centered** and **Scaled**. Each PC then **passes through the mean** of the data (which is now the origin post centering):

<!-- Obtained from Learnche -->
![PCA_CENTER_SCALE](Assets/8.%20Principal%20Components.md/PCA_CENTER_SCALE.png){.center}

!!! Tip

    The first PCs is essentially the **line of best fit** that passes through the data.
    
    Note that this best fit line is DIFFERENT from an otherwise equivalent linear regression line - the LR line aims to minimizes the residual with respect to the **TARGET variable**, while PCA is with respect to **ALL variables**.

## **Loading and Scores**

The **direction** of the PC in the multi-dimensional space is known as its **Loading**. Mathematically, it is the **contribution of each variable** on the PC. Naturally, the higher the loading, the more that variable contributes to the variance explained for that PC.

!!! Note

    It is typically assumed that the dataset is "Normalized"

Consider the perpendicular projection of each observation onto the PC; the straight line distance between an observation and the PC. The **distance** from the origin to this point on the PC is known as the **Score of the observation**.

<!-- Obtained from LearnChe -->
![PCA_SCORE](Assets/8.%20Principal%20Components.md/PCA_SCORE.png){.center}
 
!!! Note

    Each observation has a score for EACH PC; not just the nearest one.

Mathematically, it can be represented as the following:

* i: Observation Index
* j: PC Index
* k: Feature Index

$$
    z_{i,j} = \sum_{k} \varphi_{j,k} \cdot x_{i,k}
$$

!!! Note

    The loadings themselves do not 

Geometrically, the PCs also form a **plane** in the multi-dimensional space. The Score of each observation with respect to the respective PCs is the coordinate of the observations within the coordinate space of the plane. Thus, observations with high scores are "outliers" in the PC space.

<!-- Obtained from Learnche -->
![PCA_PLANE](Assets/8.%20Principal%20Components.md/PCA_PLANE.png){.center}

!!! Info

    For scenarios with more than two PCs, the resulting shape will be a Cube, Tesseract or other equivalent higher dimensional shape.

!!! Tip

    In this sense, PC is said to find the "shape" that minimizes the sum of perpendicular distances from each observation; the "shape" that is closest to the observations

Naturally, the resulting coordinates in the PC system is **different from the original**. This is better appreciated by changing the perspective of a plot to use the PCs as the vertical and horizontal axis respectively:

<!-- Obtained from Coaching Actuaries -->
![PCA_COORDINATE](Assets/8.%20Principal%20Components.md/PCA_COORDINATE.png){.center}

!!! Note

    The first PC is always the horizontal, followed by the vertical, so and so forth. They are more generally referred to as the PC1 direction, PC2 direction etc.

A **bi-plot** combines the Loadings and Scores into a **single plot** (hence "bi") and uses the principal components as the main coordinate space:

* **Primary Axis**: Score
* **Secondary Axis**: Loading
* Horizontal Vertical split remains the same as before

<!-- Obtained from Coaching Actuaries -->
![BI_PLOT](Assets/8.%20Principal%20Components.md/BI_PLOT.png){.center}

The key addition is the loading vectors for each feature; it allows us to obtain key information at a glance:

* **Vector Length**: Magnitude of loadings; identify relative influence each feature has on respective PCs
* **Vector Angle**: Relationship between features on the respective PCs:
    * **Near 0**: Positively correlated
    * **Near 90**: Uncorrelated
    * **Above 90, below 180**: Negatively correlated

!!! Tip

    The inference on vectors is intuitive. If the angle between vectors is small, they are **pointing in the same directions** and hence are positively correlated with the respective PCs; vice-versa.

!!! Warning

    Do not mistakenly read the primary axis for the loading vectors. The above plot colour codes them, but it is unlikely to be the case in the actual exam.

    ![BI_PLOT_SECONDARY](Assets/8.%20Principal%20Components.md/BI_PLOT_SECONDARY.png)

## **Variance**

Recall that the PCs are created such that the total variation explained is maximized. Note that it refers to the variation in the **underling features**. This is in contrast to linear regression where the variation is with respect to the target variable.

The variation of features explained by each principle component is the variation of that particular principal component:

$$
\begin{aligned}
    s_{z_{j}}
    &= \frac{1}{n-1} \cdot \sum (z_{i,j} - \bar{z}_{j})^{2} \\
    &= \frac{1}{n-1} \cdot \sum (z_{i,j})^{2} \\
    &= \frac{1}{n-1} \cdot \sum (\sum_{k} \varphi_{j,k} \cdot x_{i,k})^{2}
\end{aligned}
$$

!!! Tip

    The key is noticing that since the data was centered, the **mean of the PC is always 0**.

A **Scree Plot** summarizes the above information by plotting the proportion of variance explained by each successive PC. As alluded to previously, the plot will always be **decreasing** as the variance explained should decrease per PC.

<!-- Obtained from Coaching Actuaries -->
![SCREE_PLOT](Assets/8.%20Principal%20Components.md/SCREE_PLOT.png){.center}

!!! Tip

    The "optimal" number of PCs to include is **subjective**; the most common method is to choose the point where the subsequent variance explained by future PCs **significantly drop off**.

## **Other**

### **Regression**

It is possible to perform Linear Regression using the principal component space. In this case, it is known as a **Principal Component Regression**:

$$
    E(Y \mid X) = \theta_{0} + \theta_{1} \cdot Z_{1} + \theta_{2} \cdot Z_{2} + \dots
$$

!!! Note

    To check with CA if the residuals are maximized with respect to PC

Note that since PC uses **ALL variables**, it is akin to performing linear regression on all variables. Thus, if an equivalent MLR model uses all available predictors, the two will provide the **same prediction**.

!!! Warning

    Although they will lead to the same result, the coefficients will naturally be different since the **baseline is different** (actual observation vs PC).

    However, since the PCs are simply linear combinations of the original features, it is possible to re-calculate the **original regression coefficients** by **decomposing** the PCs back into the features and **regrouping** them:

    $$
        \beta_{j}
        = \theta_{1} \cdot \varphi_{j, 1} + \theta_{2} \cdot \varphi_{j, 2} + \dots
    $$

The number of PCs used is a **flexibility measure**. Including more PCs results in a lower bias but increases the variance (of the prediction). In this sense, the number of PCs can be treated as a hyper-parameter and hence the **optimal number** of PCs can be found via cross-validation.

!!! Warning

    PCR is NOT a variable selection method, as PCs use ALL variables in the dataset to construct the PCs.

!!! Note

    Since each PC is **uncorrelated** with the others, adding or removing PCs does NOT affect any of the coefficients.

Generally speaking, any other statistical learning method can be applied to the reduced PC dataset, not just linear regression. The resulting model should produce **less noisy results** as the signal would be captured in the first few PCs.

### **Collinearity**

In essence, PCA "compresses" the variables together into fewer PCs. **Highly correlated** variables have common signals that can be compressed together, thus **greatly benefitting** from PCA.

!!! Note

    Conversely, when a dataset is uncorrelated, there is **no room to compress the variables** together, hence rendering PCA ineffective. In fact, using PCA would actually reduce the interpretability!

An extreme case would be when the variables are **Perfectly Collinear** (variables can be expressed as an exact linear combination of another). In this case, PCA compresses all features into **ONE MAIN principal component** which is always the **FIRST PC** that explains **ALL** the variation.

This manifests itself on the Biplot where all observations have a **score of 0** for PC2; the observations are **entirely explainable by PC1**: 

<!-- Obtained from Coaching Actuaries -->
![BIPLOT_COLLINEAR](Assets/8.%20Principal%20Components.md/BIPLOT_COLLINEAR.png){.center}




Must be used with quantitative variables
Variance must sum to 1
Unscaled vs Scaled

No need to scale if not in the same units
Larger variance have larger loading


Standardized plot cant tell variance because everything 1

lARGEST VARIANCE would be the largest PC1 loading
Because PC1 explains the largest variance
Largest component in PC1 would have the highest variance

Orthogonal is for the main scatterplot (score), not the biplot