# Gauss Markov Theorem

Using OLS, the estimated regression parameters will always be **unbiased** under certain assumptions. The **Gauss Markov Theorem** extends this, which states that under certain assumptions, the OLS estimators will have the **lowest variance** among all possible linear unbiased estimators.

In a statistics context, they are said to be the **most efficient** among all other linear unbiased estimators. In a regression context, the OLS estimators are said to be the **Best Linear Unbiased Estimators** (BLUE).

This section will go over the various **assumptions** for both OLS and Gauss Markov Theorem. It will also cover the **diagnostics** to determine if the assumptions have been violated.

The assumptions needed for OLS and the Gauss Markov theorem are often **mixed up with each other** as the assumptions needed for OLS are also needed for the theorem. This set of notes makes a clear distinction between the two.

<!-- Obtained from Stack Exchange User DVL -->
![Regression Assumptions](Assets/4.%20Gauss%20Markov%20Theorem.md/Regression%20Assumptions.png)

## **OLS Assumptions**

### **#1: Linearity**

Linear regression is a model where the **relationship** between the DV and IVs are linear. Thus, the **regression parameters must be linear**, but NOT the DV or IV.

This means that the model is still considered a "Linear Regression" even after a transformation of the DV and/or IV.

$$
\begin{aligned}
    y_i &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 \\
    y_i &= \beta_0 + \beta_1 x^2 + \beta_2 x^3 \\
    \ln y_i &= \beta_0 + \beta_1 x_1 + \beta_2 x_2
\end{aligned}
$$

### **#2: Exogenity**

Exogenity refers to how a variable comes from **outside** the model and is thus **independent of any other variables within the model**.

In a regression context, this comes in the form of the errors having a **conditional mean of 0**, ensuring that the errors are random and thus not related to the IVs.

$$
    E(\varepsilon_i | x_{ij}) = 0 \\
$$

> "Endo" and "Exo" in Greek means "In" and "Out" respectively, which is how the meaning of the words were derived.

There are two key implications of Exogenity:

1. By the Law of Total Expectations, the *unconditional* expectation of the error is also 0.
2. By the Linearity of Conditional Expectations, the expectation of the product of the Error and IVs is 0.

$$
\begin{aligned}
    E(\varepsilon_i) = 0 \\
    E(\varepsilon_i x_{ij}) = 0
\end{aligned}
$$

Following these two implications, it can be shown that the Covariance between the Error and IVs are also 0, which is another *consequence* of independence (NOT the other way around).

$$
\begin{aligned}
    Cov (\varepsilon_i, x_{ij})
    &= E(\varepsilon_i x_{ij}) - E(\varepsilon_i) * E(x_{ij}) \\
    &= 0 - 0 * E(x_{ij}) \\
    &= 0
\end{aligned}
$$

Without exogenity, the regression parameters would reflect the **effect of both the IV and the unmodelled variable** within the error term. This causes the OLS **estimate to be biased**, known as the **Omitted Variable Bias**. Since the unmodelled variable confounds the results of the regression, it is known as a **Confounding Variable**.

<!-- Obtained from Statology -->
![Confounding Variables](Assets/4.%20Gauss%20Markov%20Theorem.md/Confounding%20Variable.png){.center}

#### **Residual Analysis**

Since the errors are unobservable, the residuals are used to estimate the errors.

If the fitted model is adequate - all relavent IVs are included in the right form, then the residuals should closely **resemble the errors** and therefore be **structureless** (random). However, if there are **patterns in the residuals**, it indicates that there is additional information that can be used to improve the model and thus should be included.

Due to the OLS, the correlation between residuals and existing IVs will **always be 0** indicating **no linear relationship**. To check for unmodelled **non-linear relationships**, a **Residual Plot** of the IVs against the Residuals can be used.

For instance, if the residual plot shows a quadractic pattern (curve), then a quadractic IV should be added into the model. Mathematically, it can be expressed as a function of the existing estimates:

$$
\begin{aligned}
    \hat{y_i} &= \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\varepsilon}_i \\
    \hat{\varepsilon_i} &= \hat{\gamma}_0 + \hat{\gamma}_1 x + \hat{\gamma}_2 x^2 \text{ (From residual plot)} \\
    \hat{y_i} &= \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\gamma}_0 + \hat{\gamma}_1 x + \hat{\gamma}_2 x^2 \\
    \hat{y_i} &= (\hat{\beta}_0 + \hat{\gamma}_0) + (\hat{\beta}_1 + \hat{\gamma}_1)x + \hat{\gamma}_2 x^2 \\
    \hat{y_i} &= \hat{\beta}_0' + \hat{\beta}_1'x + \hat{\beta}_2' x^2
\end{aligned}
$$

<!-- Obtained from ACTEX Manual -->
![Residual Plot](Assets/4.%20Gauss%20Markov%20Theorem.md/Residual%20Plot.png)

<!-- Added variable plots? -->

### **#3: No Perfect Collinearity**

Collinearity refers to when an IV can be expressed as a **linear combination of one or more other IVs**. Perfect collinearity is an extreme case where an IV can be *perfectly* expressed as a combination of another.

> Technically speaking, Collinearity refers to one to one variable relationship, while Multicollinearity refers to one to many variable relationship, hence "Multi".

1. Variable is a **multiple** of another: $x_1 = cx_2$
2. Variable **differs by a constant** from another: $x_1 = x_2 \pm c$
3. Variable is an **affine transformation** of another:
4. Sum of several variables is fixed: **Dummy Variable Trap**

The issue with perfect collinearity is that it affects the linear algebra used to solve for the coefficients (EG. Two equations to solve for three unknowns). There will be **no unique solutions** - many different values for the coefficients could work equally well.

#### **Imperfect Collinearity**

Imperfect Collinearity is a less extreme case where an IV is highly (but not perfectly) correlated with one or more other IVs.

> Recall that correlation refers to the extent of a *Linear* relationship. Non-linear relationships between variables are fine (EG. Polynomial Regression).

This means that including the IV does not bring much additional predctive power into the model as its effects are already captured through related predictors and thus can be removed from the model.

Unlike in the perfect case, high collinearity does not prevent OLS from finding a solution. However, the intepretation of the variables become complicated:

1. The original intepretation of coefficients "holding other variables constant" is **no longer true** as highly correlated variables tend to move together. Thus, it is **hard to seperate the effects of an individual variable**.
2. Consequently, OLS has difficulty estimating these coefficients, which could result in **weird meaningless estimates**; EG. Large positive coefficient but large negative for its correlated counterpart.
3. This also results in **higher standard errors** for the coefficients of correlated variables. This **reduces the magnitude of t-statistic**, which results in **more false negatives**, failing to reject the null when it should. This results in important variables being omitted from the regression.

Technically speaking, there is *nothing wrong* with collinearity if the purpose of the model is solely for prediction. However, if the purpose of the model was to establish causality, then collinearity poses a problem as it interferes with statistical inference.

#### **Detecting Collinearity**

The simplest way to detect collinearity is through a **Scatterplot or Correlation Matrix**, which shows the correlations between **pairs of variables**. A **correlation of 0.8 and higher** is typically considered high enough where the collinearity becomes problematic.

<!-- Obtained from ACTEX Manual -->
![Correlation Matrix](Assets/4.%20Gauss%20Markov%20Theorem.md/Correlation%20Matrix.png){.center }

However, the issue is that this method can only detect collinearity between pairs of variable at a time. In order to detect collinearity among three or more variables (*multicollinearity*), then the **Variance Inflation Factor (VIF)** should be used instead.

$$
    VIF = \frac{1}{1-R^2_j}
$$

The VIF is derived from the variance of the regression coefficient. As mentioned previously, under the presence of collinearity, the standard error and hence **variance of the coefficient increases** ("inflated"). The *extent* of the increase is known as the VIF.

$$
    \hat{Var}(\hat{\beta_1}) = \frac{MS_{RSS}}{(n-1) s^2} * \frac{1}{1-R^2_j}
$$

The $R^2_j$ in the VIF is the coefficient of determination of a model where the **jth IV is regressed against all other IVs**. A high $R^2_j$ means that the **IV is well explained by the other IVs** (high correlation), which indicates the presence of collinearity. Generally, a $VIF > 10$ is deemed to have *severe collinearity*.

### **#4: No Extreme Outliers**

Outliers are observations with **unusual values of the DV** relative to fitted regression model.

The last OLS assumption is that there are **no extreme outliers** in the dataset used to create the regression model. Generally, as long as the DV and IVs have a **positive and finite Kurtosis**, then the probability of such observations occuring are low.

Outliers are problematic as OLS is **sensitive to outliers**. Extreme outliers have large residuals which **receive more weight** in the optimization process, which causes the resulting model to accomodate it (when it should not), causing the resulting coefficients to be biased.

<!-- Obatained from Research Gate -->
![Outliers](Assets/4.%20Gauss%20Markov%20Theorem.md/Outliers.png){.center}

#### **Identifying Outliers**

By definition, Outliers have **unusually large residuals**. In order to gauge what is considered a "large residual", the residuals are standardized for comparison.

Standardization requires knowledge of the **sampling distribution of the residuals**. Given that the errors have a constant variance of $\sigma^2$, the variance of the residuals can be shown to be:

$$
    var(\hat{\varepsilon}_i) = \sigma^2 (1-h_{ii})
$$

> $h_ii$ is known as the Leverage of the observation, which will be covered in the following section.

The sampling distribution can then be determined:

$$
    \hat{\varepsilon}_i ~ N(0, \sigma^2 (1-h_{ii}))
$$

Thus, the standardized residuals are the raw residuals scaled by their standard error:

$$
    \hat{\varepsilon}_i^{\text{standardized}} = \frac{\hat{\varepsilon}_i}{\sqrt{\sigma^2 (1-h_{ii})}}
$$

> In practice, the variance of the errors are unknown, thus it is estimated using the Sample Variance of the residuals instead.

Residuals with standardized values of **larger than 2 or 3** are considered large and thus can be considered as an outlier.

#### **High Leverage Points**

While outliers are unusual points of the DV, **High Leverage observations** have unusual values of the IV relative to the majority of the values.

It is easy to identify high leverage points when there is only one IV through a scatterplot - simply find the observation that is away from the rest.

It becomes much more complicated when there are multiple IVs. The observation's values for each of the IVs could be in the common range of each IV, but **unusual when taken collectively**:

<!-- Obtained from ACTEX Manual -->
![High Leverage Points](Assets/4.%20Gauss%20Markov%20Theorem.md/High%20Leverage.png){.center}

The leverage of the observation can be determined from the MLR:

$$
\begin{aligned}
    \hat{\boldsymbol{y}}
    &= \boldsymbol{X}\boldsymbol{\beta} \\
    &= \boldsymbol{X}[(X'X)^{-1}y] \\
    &= \boldsymbol{H}\boldsymbol{y}
\end{aligned}
$$

$\boldsymbol{H}$ is known as the **Hat Matrix** as it puts a hat on y in the notation. The leverage of the $ith$ observation is the **$ith$ diagonal element** of the matrix, $h_{ii}$.

An observation is considered to have *high* leverage if its leverage is greater than three times the average leverage:

$$
    h_{ii} \gt 3 \left(\frac{p+1}{n}\right)
$$

#### **Influential Points**

The effect of Outliers and High leverage points can both be summarized into a concept known as **Influence**. An observation is influential if the **exclusion of the observation from the regression** leads to significantly differently results.

In general the process involves three steps:

1. Fit the original model with all $n$ observations; determine the $j-th$ fitted value $\hat{y}_j$
2. Fit an adjusted model with omitting the $i-th$ observation, determine the $j-th$ fitted value $\hat{y}_{j(i)}$
3. Calculate the **change in the $j-th$ fitted value**

This process has to be repeated for **all fitted values for all observations**.

**Cook's Distance** summarizes the effect of the $i-th$ observation on the whole model:

$$
    D_i = \frac{\sum^n_{j=1} (\hat{y}_j - \hat{y}_{j(i)})^2}{(p+1) \cdot MS_{Residuals}}
$$

This method of computation requires $n+1$ datasets - 1 dataset with all the observation and $n$ datasets with the $i-th$ observation omitted. It is also extremely time consuming to have to fit a model to each dataset.

An alternative method of determining Cook's Distance is to make use of both the Outliers and Leverage:

$$
    D_i = \frac{1}{p+1} \cdot (e^{\text{standardized}})^2 \cdot \frac{h_{ii}}{1-h_{ii}}
$$

Thus, an observation must be **unusual in BOTH the DV and IV** in order to be considered influential. Outliers and High leverage points are *necessary but not sufficient* conditions to be influential.

## **Gauss Markov Assumptions**

### **#1: Conditional Homoscedasticity**

Homoscedasticity refers to the error terms having **constant variance** while Heteroscedasticity refers to having non-constant variance.

$$
    var(\varepsilon_i | x_i) = \sigma^2
$$

Under homoscedasticity, the sampling distribution of the estimates are easily derived and thus it can be shown that they are the most efficient estimators. The same cannot be proven under heteroscedasticity.

> Note that that the OLS estimates are still unbiased; they are just not the most efficient.

#### **Identifying Heteroscedasticity**

Similar to before, if the model is adequate, then the residuals should resemble the errors and have **constant variance**. Thus, this can be easily determined through a **residual plot** of the Residuals against the fitted values.

If the points are **equally spread out** about the mean (0) and show **no pattern**, then homoscedasticity is present. However, if the points show an increasing or decreasing variance (typically in the **shape of a funnel**), then heteroscedasticity is present.

<!-- Obtained from Corporate Finance Institute -->
![Homoscedastacity](Assets/4.%20Gauss%20Markov%20Theorem.md/Homoscedasticity.png){.center}

Alternatively, a hypothesis test can be conducted to determine if heteroscedasticity is present, known as the **Bresuch Pagan Test**.

$$
\begin{aligned}
    H_0 &: \sigma^2 \\
    H_1 &: \sigma^2 + \boldsymbol{Z\gamma}
\end{aligned}
$$

The test-statistic is computed as follows:

1. Compute the squared standardized residuals from the original model
2. Regress them onto the variables in Z (LOL need to change this part)
3. Compute the RegSS of the new regression

$$
    T = \frac{RegSS}{2}
$$

The test-statistic follows a **chi-square distribution** with $q$ degrees of freedom, where $q$ is the **number of variables** in $\boldsymbol{Z}$.

$$
    T \sim \chi^2_q
$$

#### **Dealing with Heteroscedasticity**

If prior information is known about the structure of the data, then the most intuitive method would be to incorporate that information into the data.

<!-- Buff up this section -->

$$
    Var(\varepsilon_i) = \frac{\sigma^2}{w_i}
$$

If no prior information is known, then the Heteroscedasticity can be reduced by using a **Variance Stabilizing Transformation**, such as the **Log** or **Squareroot**. Note that since they require positive data, a **constant can be added** to each term before the transformation to ensure that the values are positive.

> It is out of the scope for this set of notes to show why these help to stabilize variance.

Alternatively, if there is only mild heteroscedasticity in the data, then OLS can be used but with an **adjustment to the standard errors** of the coefficients, known as **heteroscedastic-robust standard errors**.

Due to complexity of the computations, it will not be covered in this set of notes. However, the general idea is that an **weighted estimate** of the variance covariance matrix is computed and the standard errors are computed from there.

### **#2: No Serial Correlation**

If errors are correlated with one another, it is known as **Serial Correlation** or **Autocorrelation**.

It implies that there are other unmodelled factors that can be used for prediction, which would imply the current model specification to be wrong.

Thus, for the SLR model to be true, the errors **must be independent of one another**.

$$
    Cov(\varepsilon_i,\varepsilon_j) = 0
$$

Confidence Intervals and PI are narrower than it should be > 95% PI is actually < 95%>
P values lower > Appear statisticlaly significant when they shld not be

Time series tends to have errors that are positively correlated, which is why it has its own dedicated section
No Serial Correlation > Outcome of zero conditional mean, but most likely in time series data

## **Error Distribution**

Although not needed for OLS estimation or Guass Markov, the errors of the regression are usually assumed to be **normally distributed**.

<!-- To check on specific normal distributions for Y > Does it have the same variance as E? -->
$$
    \varepsilon \sim N(0, \sigma^2)
$$

If the errors are normally distributed, then it follows that **$\beta$ is normally distributed** as well since they are linear and additive. This greatly eases the computation needed to determine the **sampling distribution** for statistical inference.

### **Q-Q Plots**

Since the errors are normally distributed, the residuals should be normally distributed as well. This can be verified using a **Quantile-Quantile Plot (QQ Plot)**, which **compares the quantiles** of two distributions.

The first distribution is plotted on the x-axis while the second on the y-axis. If the quantiles are the same (same distribution), then the points should lie on $y = x$, the 45 degree line.

<!-- Obtained from STHDA -->
![QQ Plot](Assets/4.%20Gauss%20Markov%20Theorem.md/QQ%20Plot.png)

<!-- Simple random sample refer to Econometrics bible -->