# **Multiple Linear Regression**

## **Overview**

The natural extension of the SLR model is to include **more than one independent variable**, which thus results in a **Multiple Linear Regression** (MLR) model.

$$
\begin{aligned}
    f &= \beta_{0} + \beta_{1} \cdot X_{1} + \beta_{2} \cdot X_{2} + ... + \beta_{p} \cdot X_{p} \\
    \\
    \therefore E(Y \mid X_{1} = x_{i, 1}, ... X_{i, p} = x_{i,p})
    &= \beta_{0} + \beta_{1} \cdot x_{i, 1} + \beta_{2} \cdot x_{i, 2} + \dots
    \\ 
    \therefore y_{i}
    &= \beta_{0} + \beta_{1} \cdot x_{i, 1} + \beta_{2} \cdot x_{i, 2} + \dots + \varepsilon_{i}
\end{aligned}
$$

Many of the concepts from the SLR model translate into a MLR setting. Thus, this section mainly focuses on the **differences or additional considerations** needed in a MLR setting.

## **Interpretation**

MLR models study how the independent variables **operate together** to influence the dependent variable, thus the regression coefficients have a slightly different interpretation:

* **Intercept Parameter** ($\beta_{0}$): **Expected value** of $Y$ when $X_{1} = X_{2} = \dots = 0$
* **Slope Parameter** ($\beta_{p}$): **Change** in **Expected Value** of $Y$ given a **one unit increase** in $X_{p}$, assuming all others are constant 

## **Matrix Notation**

Given that the MLR model can have infinitely many independent variables, expressing the model in regular algebra format can become tedious. Thus, it is commonly expressed in **Matrix Notation** instead:

$$
\begin{aligned}
    \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix} &=
    \begin{pmatrix}
        1 & x_{11} & x_{12} & ... & x_{1p} \\
        1  & x_{21} & x_{22} & ... & x_{2p} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1  & x_{n1} & x_{n2} & ... & x_{np}
    \end{pmatrix}
    \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{pmatrix} +
    \begin{pmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n \end{pmatrix} \\
    \boldsymbol{y} &= \boldsymbol{X\beta + \varepsilon}
\end{aligned}
$$

$\boldsymbol{X}$ is known as the **Design Matrix**, which contains the independent variables used for the regression. Note that the first column is all '1' to account for the intercept term.

!!! Note

    Recall the key properties of matrices:

    * They are defined using a **"(Row x Column)"** notation
    * They are differentiated from scalar quantities with a **Bolded** symbol
    * **Addition and subtraction** of matrices simply add or subtract the corresponding elements
    * **Multiplication** of matrices involves an **inverted 7** (See image below)

    The **order of multiplication** is important as the resulting matrix will retain the Rows of the LHS and columns of the RHS (which is intuitive from the inverted 7):

    $$
        (m * n) \cdot (n * k) = (m * k)
    $$

    <!-- Obtained from Resourceaholic -->
    ![MATRIX_MULTIPLICATION](Assets/3.%20Multiple%20Linear%20Regression.md/MATRIX_MULTIPLICATION.png){.center}

!!! Tip

    For the purposes of this exam, it is not necessary to know how to evaluate or manipulate matrices. It is however, necessary to recognize and intepret them.

## **Model Fitting**

Similar to the SLR model, the regression parameters can be found by minimizing the sum of squared residuals:

$$
\begin{aligned}
    \text{RSS} 
    &= \sum (y_{i} - \hat{y}_{i})^{2} \\
    &= \sum [y_{i} - (\hat{\beta}_{0} + \hat{\beta}_{1} \cdot x_{i,1}
            + ... + \hat{\beta}_{p} \cdot x_{i, p})]^{2}   
\end{aligned}
$$

Given that there are $p$ independent variables, there are thus $p+1$ minimization problems to solve ($p$ slopes and 1 intercept). It is necessary to use linear algebra to solve the system of equations, resulting in the following **matrix solution**:

$$
    \hat{\boldsymbol{\beta}}
    = \begin{pmatrix} \hat{\beta_0} \\ \hat{\beta_1} \\ \vdots \\ \hat{\beta_p} \end{pmatrix}
    = (\boldsymbol{X^{T} X})^{-1} \boldsymbol{X^{T} y}
$$

!!! Note

    The operation $X^{T}$ means to **Transpose** the matrix - swapping the Rows and Columns of the Matrix. It has several useful properties which is why it is beneficial to leave in this form:

    <!-- Obtained from Cuemath -->
    ![TRANSPOSE_MATRIX](Assets/3.%20Multiple%20Linear%20Regression.md/TRANSPOSE_MATRIX.png){.center}

The resulting fitted model is thus a **regression plane of best fit**, reflecting the multi-dimensional nature of the MLR:

$$
    \hat{y}
    = \hat{\beta}_{0} + \hat{\beta}_{1} \cdot x_{i ,1} + ... + \hat{\beta}_{p} \cdot x_{i,p}
$$

<!-- Obtained from ISLR -->
![REGRESSION_PLANE](Assets/3.%20Multiple%20Linear%20Regression.md/REGRESSION_PLANE.png){.center}

The actual observations can be linked to the fitted regression plane via a **Hat Matrix** $\boldsymbol{H}$:

$$
    \hat{\boldsymbol{y}} = \boldsymbol{H} \boldsymbol{y}
$$

!!! Info

    It is referred to as a "Hat" matrix as it puts a "hat" onto $y$, which is an easy way to remember the expression.

Consequently, this allows the residuals to be expressed using the Hat matrix, sometimes referred to as the **Residual Maker Matrix**:

$$
\begin{aligned}
    \hat{\varepsilon}_{i}
    &= \boldsymbol{y} - \hat{\boldsymbol{y}} \\
    &= \boldsymbol{y} - \boldsymbol{H} \boldsymbol{y} \\
    &= (\boldsymbol{I} - \boldsymbol{H})\boldsymbol{y}
\end{aligned}
$$

!!! Note

    $\boldsymbol{I}$ is known as the **Identity Matrix**, which is a Square Matrix **consisting of 1's in the diagonal and 0's in every other** position. Again, it has several useful properties which makes it desirable to be expressed in this form.

    <!-- Obtained from Geek for Geeks -->
    ![IDENTITY_MATRIX](Assets/3.%20Multiple%20Linear%20Regression.md/IDENTITY_MATRIX.png){.center}

## **ANOVA**

### **Goodness of Fit**

Similar to SLR, ANOVA can be used to decompose the variance into RegSS & RSS, and R-squared can be computed with the **same interpretation**:

$$
    R^{2} = \frac{\text{RegSS}}{\text{TSS}}
$$

!!! Warning

    Unlike SLR, the following expression does NOT hold true as there are multiple independent variables:

    $$
        R^{2} \ne r^{2}_{x_{p}, y}
    $$
    

Recall that for parametric learning methods, increasing the number of parameters (all else equal) tends to **increase the flexibility** of the model; ability to fit the data (RegSS). Thus, in a MLR setting, R-squared can be **artifically inflated** by adding more parameters, even when they are nonsensical!

Thus, to account for this, the **Adjusted R-squared** is considered by dividing each term by their degrees of freedom:

$$
\begin{aligned}
    R^{2}_{\text{Adj}}
    &= 1 - \frac{\frac{\text{RSS}}{n-(p+1)}}{\frac{\text{TSS}}{n-1}} \\
    &= 1 - \frac{\text{RSS}}{\text{TSS}} \cdot \frac{n-1}{n-(p+1)} \\
    &= 1 - (1 - R^{2}) \cdot \frac{n-1}{n-(p+1)}
\end{aligned}
$$


The key to understanding lies in the numerator. If the expression decreases, the overall adjusted R-squared will increase:

$$
    \frac{\text{RSS}}{n-(p+1)}
$$

* If the predictor **does not** significantly reduce RSS, then the **adjusted R-squared increases**
* If the predictor **does** sufficiently reduce RSS, then the **adjusted R-squared decreases**
* This **penalizes the addition of redundant predictors**; avoiding the issue of inflated R-squared

!!! Warning

    The adjusted R-squared has **NO INTERPRETATION**. It is more of a measure to **compare the goodness of fit** between different MLR models.

    R-squared does NOT have to be between 0 and 1, thus it CANNOT be interpreted as a proportion of any sorts.

### **F-Test**

Similar to SLR, we can conduct an F-test using the same components as before, adjusted for the **new degrees of freedom**:

<center>

|   **Source**   | **Sum of Squares** |  **df**   |              **Variance/Mean Squared**               |
| :------------: | :----------------: | :-------: | :--------------------------------------------------: |
| **Regression** |       RegSS        |    $p$    | $\sigma^{2}_{\text{RegSS}} = \frac{\text{RegSS}}{p}$ |
| **Residuals**  |        RSS         | $n-(p+1)$ | $\sigma^{2}_\text{RSS} = \frac{\text{RSS}}{n-(p+1)}$ |
|   **Total**    |        TSS         |   $n-1$   |   $\sigma^{2}_\text{TSS} = \frac{\text{TSS}}{n-1}$   |

</center>

!!! Note

    The df for RegSS is $p$ reflecting that there are now $p$ independent variables used in the regression. Similarly, the df for RSS reflects that there are now $p+1$ estimated parameters.

However, rather than testing if an individual independent variable is useful, it tests if the **entire model** (all the independent variables collectively) is useful in explaining the dependent variable:

* $H_{0}: \beta_{1} = \beta_{2} = ... = \beta_{p} = 0$
* $H_{1}$: At least one $\beta_{p} \ne 0$

$$
    \text{F}_{\text{Statistic}}
    = \frac{\sigma^{2}_{\text{RegSS}}}{\sigma^{2}_\text{RSS}}
$$

Thus, rejecting the null allows us to conclude that **at least one** of the independent variables are useful, but does not provide insight on *which* of them are useful.

!!! Tip

    Similar to the adjusted R-squared, the F-test measures whether the reduction in RSS is "worth" the loss of the degrees of freedom to estimate the new parameters.

!!! Tip

    The F-statistic can also be re-expressed as a function of $R^{2}$:

    $$
        \text{F}_{\text{Statistic}} = \frac{R^{2}}{1 - R^{2}} \cdot \frac{n-p-1}{p}
    $$

    The above applies to SLR as well.

In order to determine which variables are useful, a **Partial F-test** can be used, which compares two models:

* **Full Model**: Model with all $p$ independent variables
* **Reduced Model**: Model with lesser than $q$ variables (where $q \lt p$); **subset** of full model

The partial F-test determines whether the additional variables are **jointly useful** in explaining the dependent. The intuition is that if the variables are useful, then there should be a **decrease in RSS** when moving from the reduced to full, known as the **Extra Sum of Squares** (ExtraSS):

$$
    \text{ExtraSS} = \text{RSS}_{\text{Reduced}} - \text{RSS}_{\text{Full}} 
$$

!!! Note

    The most intuitive way to quantify the degrees of freedom for the ExtraSS is to take the difference in degrees of freedom of the two models:

    $$
        \text{df}_{\text{ExtraSS}} = \text{df}_{\text{Full}} - \text{df}_{\text{Reduced}}
    $$

Formally, the partial F-test can be expressed as the following:

* $H_{0}: \beta_{q+1} = ... = \beta_{p} = 0$
* $H_{1}$: At least one $\beta_{q+n} \ne 0$

$$
    \text{F}_{\text{Partial}}
    = \frac{\sigma^{2}_{\text{ExtraSS}}}{\sigma^{2}_\text{RSS}}
$$

## **Statistical Inference**

### **Sampling Distributions**

Similar to SLR, the regression parameters are normally distributed as well. However, since there are multiple regression parameters, they collectively follow a **multivariate normal distribution**:

$$
    \hat{\beta} \sim N_{p+1} \left(\beta, \sigma^{2} (\boldsymbol{X^{T} X})^{-1} \right)
$$

The matrix in the variance term is known as the **Variance Covariance Matrix**, which provides the covariance between every possible pair of regression parameters, INCLUDING the intercept:

$$
Var(\hat{\beta}) =
\begin{pmatrix}
    Var(\hat{\beta}_0) & Cov(\hat{\beta}_0, \hat{\beta}_1) & ... & Cov(\hat{\beta}_0, \hat{\beta}_p) \\
    Cov(\hat{\beta}_1, \hat{\beta}_0) & Var(\hat{\beta}_1) & ... & Cov(\hat{\beta}_1, \hat{\beta}_p) \\
    \vdots & \vdots & \ddots & \vdots \\
    Cov(\hat{\beta}_p, \hat{\beta}_0) & Cov(\hat{\beta}_1, \hat{\beta}_p) & ... & Var(\hat{\beta}_p)
\end{pmatrix}
$$

!!! Tip

    The Covariance of the same independent variable is its Variance. Thus, the **diagonals of the matrix** represent the variances of the estimators. The matrix is also **symmetrical about the diagonal**, since $\text{Cov}(X,Y) = \text{Cov}(Y,X)$.

!!! Warning

    The matrix always starts with the intercept parameter, thus do not mistakenly take the wrong value.

### **Hypothesis Testing**

Similar to SLR, a t-test can be used to test the usefulness of a single predictor. However, the interpretation is slightly different - it tests the usefulness of the variable while **holding the other variables constant**.

* $H_{0}: \beta_{p} = 0$
* $H_{1}: \beta_{p} \ne 0$

$$
    t = \frac{\hat{\beta_{p}} - \beta_{j}}{\text{SE}({\hat{\beta_{p}}})}
$$

!!! Note

    However, this could lead to several odd situations:

    * Individual variables are significant, but **NOT significant** when considered in totality (Pass t-test, fail F-test)
    * Individual variables are **NOT significant**, but significant when considered in totality (Fail t-test, pass F-test)

    The likely reason for such situations will be covered in a later section.

There are now two possible ways to test for statistical significance:

* **Single F-test** for the entire model
* **Multiple t-test** for each variable

Conducting multiple t-tests simultaneously could lead to the **Multiple Comparisons Problem**. Recall that hypothesis tests assume an $(1-\alpha)%$ chance of correctly rejecting the null. If each test is conducted on independent samples, then the **probability of correctly rejecting the null for ALL tests** drops to $(1-\alpha)^{p}$. The resulting type I error rate is much higher than the intended $\alpha$:

$$
    \text{Type I Error Rate across all tests} = 1 - (1-\alpha)^{p} \gt \alpha
$$

!!! Note

    There are methods to manage this problem through the use of several corrections such as the **Bonferroni Correction** which adjusts the $\alpha$ for each test such that on whole, the desired type I error rate is achieved. However, it has the consequence of **increasing the probability of type II errors**. The details are beyond the scope of the exam.

This is why the **F-test is generally preferred** for MLR, as it has the advantage of naturally controlling the error rate regardless of the number of predictors along with considering the joint effect of variables.

!!! Warning

    The relationship between t and F tests only holds true in a SINGLE variable context. For multiple variables, the two can result in DIFFERENT conclusions!

    * t-tests: Including a single variable is better than excluding it
    * F-tests: Including multiple variables is better than excluding them

    The difference lies in the fact that a variable alone might be strong, but in conjunction with others is not.

### **Confidence & Prediction Intervals**

Similar to SLR, the confidence and prediction intervals can be constructed using the usual expression:

$$
    \text{Interval} = \text{Estimate} \pm \text{Percentile} \cdot \text{SE of Estimate}
$$

It is mathematically intensive to show derive the variance of the prediction error. It is sufficient to know the result:

$$
    \text{Var}(y_{i} - \hat{y}_{i}) = \sigma^{2} \left(1 + X^{T}_{i} \cdot (\boldsymbol{X^{T} X})^{-1} \cdot x_{i} \right)
$$

The key intepretations remain the same.

## **Special Variations**

### **Polynomial Variables**

So far, we have implicitly assumed that all independent variables are first order only. However, it is possible to to use **Higher Order** ones, resulting in a **Polynomial Model**:

$$
    E(Y \mid X) = \beta_{0} + \beta_{1} \cdot x^{1}_{p} + \beta_{2} \cdot x^{2}_{p} + \beta_{3} \cdot x^{3}_{p} + \dots
$$

!!! Note

    This is still considered a **Linear Model** because the dependent variable is a **Linear Combination** of the independent variables; the independent variables themselves can be non-linear.

    The model can still include other independent variables, regardless of their powers; mix of polynomial and non-polynomials are possible.

!!! Note

    Typically, if there is a known relationship between $Y$ and the k-th order of the independent variable, **all orders from $1$ up to $k$** are added for the variable.

    The reason is because we are interested in the way that $Y$ relates to $X$ as a whole. Keeping all powers allows the relationship to better expressed and hence have a better fit.

Although the relationship is better quantified, the regression coefficients become **hard to intepret**. Polynomials have a **non-constant slope**, thus they cannot be intepreted as the change in the dependent given a one unit increase anymore.

<!-- Obtained from Medium -->
![POLYNOMIAL_REGRESSION](Assets/3.%20Multiple%20Linear%20Regression.md/POLYNOMIAL_REGRESSION.png){.center}

### **Dummy Variables**

So far, we have implicitly assumed that all independent variables are quantitative. However, it is possible to to use **Qualitative** ones as well. They are typically represented using a **Dummy Variables**, which are can only take on **1 (Yes) or 0 (No)**.

If there are $n$ possible categories, then $n-1$ dummy variables are needed to fully represent all possible categories. For instance,

$$
    E(Y \mid X)
    = \beta_{0} + \beta_{\text{North}} \cdot x_{\text{North}} + \beta_{\text{West}} \cdot x_{\text{West}} + \beta_{\text{South}} \cdot x_{\text{South}}
$$

!!! Warning

    Only $n-1$ are needed because the final level can be deduced from the other variables. If the final level is included, then the variables will become **Perfectly Collinear**, causing problems which will be discussed in the next section. This is known as the **Dummy Variable Trap**.

The **sum of all dummy variables must equal to 1**. Consider the following two cases:

* **North Chosen**: $x_{\text{North}} = 1, x_{\text{West}} = 0, x_{\text{South}} = 0$
* **East Chosen**: $x_{\text{North}} = 0, x_{\text{West}} = 0, x_{\text{South}} = 0$

The last category that does not have its own variable is known as the **Baseline** of the model (known as the **Reference Category**), which is the default category when **all other variables are 0**. Thus, the intepretation of the coefficients is as follows:

* $\beta_{0}$: Value of $E(Y \mid X)$ at the Baseline
* $\beta_{1}$: Change in value of $Y$ when **moving** from the reference category to the current category

!!! Warning

    Given the above, the choice of reference category matters as it will result in a different model altogether.

Dummy variables are usually used in conjunction with quantitatve ones. This creates a **seperate but parallel regression line** for each of the levels:

$$
\begin{aligned}
    E(Y \mid X)
    &= \beta_{0} + \beta_{1} \cdot x_{1} + \beta_{2} \cdot x_{\text{Dummy}} \\
    &=
    \begin{cases}
        (\beta_{0} + \beta_{2}) + \beta_{1} \cdot x_{1}, & x_{\text{Dummy}} = 1 \\
        \beta_{0} + \beta_{1} \cdot x_{1}, & x_{\text{Dummy}} = 0
    \end{cases}
\end{aligned}
$$

<!-- Obtained from ACTEX Manual -->
![DUMMY_VARIABLE](Assets/3.%20Multiple%20Linear%20Regression.md/DUMMY_VARIABLE.png){.center}

### **Interaction Variables**

So far, we have assumed the individual predictors do not affect one another. However, it is possible that variables might interact with one another to produce **joint effects**. For instance, the production of a factory may depend on the number of **Machines and Workers**. However, the more machines there are, the **greater the effect of an additional worker**.

This joint effect can be captured via an **Interaction Variable**, which is the product of the two predictors:

$$
\begin{aligned}
    E(Y \mid X)
    &= \beta_{0} + \beta_{1} \cdot x_{1} + \beta_{2} \cdot x_{2} + \beta_{3} \cdot x_{1} \cdot x_{2} \\
    &= \beta_{0} + (\beta_{1} + \beta_{3} \cdot x_{2}) \cdot x_{1} + \beta_{2} \cdot x_{2} \\
    &= \beta_{0} + \beta_{1} \cdot x_{1} + (\beta_{2} + \beta_{3} \cdot x_{1}) \cdot x_{2} \\
\end{aligned}
$$

The previous intepretation about the coefficients no longer hold true - a one unit increase in $x_{1}$ will increase $Y$ by $(\beta_{1} + \beta_{3} \cdot x_{2})$, which DEPENDS on the value of $x_{2}$.

!!! Warning

    This could lead to scenarios where the Interaction Variable is statistically significant but the **underlying variables individually are not**.
    
    Through the **Hierarchical Principle**, it is common practice that to include the individual variables as well regardless of their significance. Note that this applies to **Polynomial Regression** as well, which is why all lower order terms are retained as well.

If the interaction variable contains a dummy variable, then the interpretation changes from before. The existing coefficient of the dummy remains the same, but there is an additional **coefficient from the interaction** which represents the **difference in slope** from the two lines:

$$
\begin{aligned}
    E(Y \mid X)
    &= \beta_{0} + \beta_{1} \cdot x_{1} + \beta_{2} \cdot x_{\text{Dummy}} + \beta_{3} \cdot x_{1} \cdot x_{\text{Dummy}} \\
    &=
    \begin{cases}
        (\beta_{0} + \beta_{2}) + (\beta_{1} + \beta_{3}) \cdot x_{1}, & x_{\text{Dummy}} = 1 \\
        \beta_{0} + \beta_{1} \cdot x_{1}, & x_{\text{Dummy}} = 0
    \end{cases}
\end{aligned}
$$

<!-- Obtained from ACTEX Manual -->
![DUMMY_INTERACTION](Assets/3.%20Multiple%20Linear%20Regression.md/DUMMY_INTERACTION.png){.center}

!!! Tip

    There are questions which may ask for the impact of a variable on the response given all other variables are held constant.

    If the variable is part of the an interaction term, two extremes must be considered - Maximum and Minimum value for the other variable in the interaction and then determine if there is a clear one directional impact.

### **Indicator Variables**

In some cases, there might be an **change in the behaviour** of the predictor **across different values** it can take. For instance, high net worth individuals have different spending patterns than regular consumers - behaviour changes past a certain Income threshold.

This effect can be achieved through the use of an **Indicator Function**, which is a essentially a **Dummy Variable** that is dependent on the value of other variables:

$$
\begin{aligned}
    I_{x \gt c}
    &=
    \begin{cases}
        0, & x_{1} \lt c \\
        1, & x_{1} \ge c
    \end{cases} \\
\end{aligned}
$$

The resulting regression is known as a **Piecewise Model**. There are two possible variations:

* **Continuous**: **Kink** at the threshold; relationship is equal at the threshold
* **Non-continuous**: **Jump** at the threshold; relationship is NOT the same at the threshold

**Continuous piecewise** models directly consider the relationship after the threshold as a **seperate variable**:

* $\beta_{0}$: Intercept before the threshold
* $\beta_{1}$: Slope before the threshold
* $\beta_{2}$: Change in slope AFTER the threshold

$$
\begin{aligned}
    E(Y \mid X)
    &= \beta_{0} + \beta_{1} \cdot x_{1} + \beta_{2} \cdot (x_{1}-c) \cdot I_{x \gt c} \\
    &=
    \begin{cases}
        \beta_{0} + \beta_{1} \cdot x_{1}, & x_{1} \lt c \\
        (\beta_{0} - \beta_{2} \cdot c) + (\beta_{1} + \beta_{2}) \cdot x_{1}, & x_{1} \ge c
    \end{cases}
\end{aligned}
$$

<!-- Obtained from ACTEX Manual -->
![PIECEWISE_CONTINUOUS](Assets/3.%20Multiple%20Linear%20Regression.md/PIECEWISE_CONTINUOUS.png){.center}

**Non-continuous piecewise** models instead use an interaction variable instead:

* $\beta_{0}$: Intercept before the threshold
* $\beta_{1}$: Slope before the threshold
* $\beta_{2}$: Change in intercept **AT** the threshold; the "Jump"
* $\beta_{3}$: Change in slope **AFTER** the threshold

$$
\begin{aligned}
    E(Y \mid X)
    &= \beta_{0} + \beta_{1} \cdot x_{1} + \beta_{2} \cdot I_{x \gt c} + \beta_{3} \cdot (x_{1}-c) \cdot I_{x \gt c} \\
    &=
    \begin{cases}
        \beta_{0} + \beta_{1} \cdot x_{1}, & x_{1} \lt c \\
        (\beta_{0} + \beta_{2} - \beta_{3} \cdot c) + (\beta_{1} + \beta_{3}) \cdot x_{1}, & x_{1} \ge c
    \end{cases}
\end{aligned}
$$

<!-- Obtained from ACTEX Manual -->
![PIECEWISE_NON_CONTINUOUS](Assets/3.%20Multiple%20Linear%20Regression.md/PIECEWISE_NON_CONTINUOUS.png){.center}

## **Regularization Methods**

A key problem with MLR is that the addition of more variables makes it **more prone to overfitting** (Low Bias, High Variance). Thus, **regularization methods** have been devised to **reduce variance** at the cost of slightly increasing bias.

The mathematics behind the methods is complicated, it is sufficient to understand the intuition.

### **L2: Ridge**

The first method is known as the **Ridge Regression**. It works by adding a **penalty** to the OLS minimization problem:

$$
    \min \left(\text{RSS} + \lambda \cdot \sum \beta^{2}_{p} \right)
$$

!!! Note

    The penalty term is known as the $\ell_{2}$ norm of the vector of predictors. The **intercept is untouched**.

The penalty causes the coefficients to "shrink" towards zero, functionally **reducing the amount of information learnt** from the data and hence combatting overfitting. The **hyperparameter** $\lambda$ controls the **strength of the shrinkage**

* $\lambda = 0$: No Shrinkage; same as OLS (maximum flexibiliy)
* $\lambda = \infty$: Fully shrinked; all estimates are 0 (minimum flexibility)

!!! Tip

    Ridge Regression will **NEVER** fully shrink the predictor to 0 for any finite $\lambda$. The resulting coefficients will be **close but never exactly 0**.

!!! Warning

    Note that the penalty is applied on the SUM of squared predictors; the sum must shrink to 0. Thus, it is possible for most predictors to shrink while a specific predictor increases (offsetting effects).

    <!-- Obtained from Coaching Actuaries -->
    ![RIDGE_SHRINKAGE](Assets/3.%20Multiple%20Linear%20Regression.md/RIDGE_SHRINKAGE.png){.center}

### **L1: LASSO**

The second method is known as **LASSO regression**. Similarly, it works by adding a **L1 penalty** to the OLS minimization problem:

$$
    \min \left(\text{RSS} + \lambda \cdot \sum |\beta_{p}| \right)
$$

!!! Note

    The penalty term is known as the $\ell_{1}$ norm of the vector of predictors. The **intercept is untouched**.

Similarly, $\lambda$ is the hyperparameter of the model and **functions identically** to the ridge model. The key difference is that the coefficients **can be shrunk to 0 for large finite lambda**; the LASSO model **can drop variables**.

!!! Tip

    This it is known as the **Least Absolute Shrinktage Selection Operator** (LASSO). Its ability to drop predictors allows it to be used as a **model selection** tool.

There is no clear advantage between the two; the key difference lies in the **ability to drop predictors**:

<center>

|           **Ridge**           |            **LASSO**            |
| :---------------------------: | :-----------------------------: |
|         Always Retain         |           Always Drop           |
|  Lower Interpretability |   Higher Interpretability    |
| Higher Flexibility | Lower Flexibility |
| Able to capture small effects | Unable to capture small effects |

<center>

<!-- Obtained from Towards Data Science -->
![REGULARIZATION_VISUAL](Assets/3.%20Multiple%20Linear%20Regression.md/REGULARIZATION_VISUAL.png){.center}

!!! Info

    The approach that combines both Ridge and LASSO together is known as the **Elastic Net**.

### **Transformations**

Before using the two methods, the predictors might need to be first **transformed**. This is because each predictor is of a **different scale** (different units), thus they need to be on the **same basis** to ensure proper results, since they are now combined together in the constraint.

!!! Note

    This was not a problem in regular OLS as the residuals were used, which were all following the scale of the dependent.

There are two main methods of transforming predictors:

* **Centering**: Subtracting the sample mean of each predictor
* **Scaling**: Dividing each predictor by its sample standard error

Regardless of which method is used, after expanding the relevant terms, the expression remains the same. The difference lies in the **allocation between the intercept and slope**:

<center>

|   **Centering**   |     **Scaling**     |
| :---------------: | :-----------------: |
| Intercept changed | Intercept unchanged |
|  Slope unchanged  |    Slope changed    |

</center>

$$
\begin{aligned}
    \hat{y}
    &= 50.7909 - 1.6955 \cdot x_{1} - 0.3227 \cdot x_{2} \\
    \\
    \hat{y}_{\text{Centered}}
    &= 34 - 1.6995 \cdot (x_{1} - 8) - 0.3227 \cdot (x_{2} - 10) \\
    &= 34 + 1.6995(8) + 0.3227(10) - 1.6955 \cdot x_{1} - 0.3227 \cdot x_{2} \\
    &= 50.7909 - 1.6955 \cdot x_{1} - 0.3227 \cdot x_{2} \\
    \\
    \hat{y}_{\text{Scaled}}
    &= 34 - 3.909 \cdot \frac{x_{1}}{2} - 1.2909 \cdot \frac{x_{2}}{4} \\
    &= 50.7909 - 1.6955 \cdot x_{1} - 0.3227 \cdot x_{2} \\
\end{aligned}
$$