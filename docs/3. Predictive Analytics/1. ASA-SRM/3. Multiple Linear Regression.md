# **Multiple Linear Regression**

The natural extension of the SLR model is to include **more than one independent variable**, which thus results in the more generalized **Multiple Linear Regression** (MLR) model.

$$
E(Y|X_1, ... X_p) = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_{p}X_{p}
$$

Unlike the SLR which studies how each *individual* IV influences the DV, the goal of the MLR model is to study how all the IVs operate **together** to influence the DV.

<center>

| $\beta_0$  | $\beta_j$ |
| :-: | :-: |
| $E(Y)$ when $X_1 = X_2 = ... = 0$ | **Change** in $E(Y)$ given a one unit increase in $X_j$, **holding all other $X$'s constant** |
| Intercept Parameter | "Slope" Parameter |

</center>

> For avoidance of doubt, the subscript $i$ will be used to denote observations while $j$ will be used to denote independent variables.

Every observation can also be expressed as the sum of the regression and the error term. However, due to the multi-dimensional nature of the model, it is commonly expressed in matrix notation:

$$
\begin{aligned}
    \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix} &=
    \begin{pmatrix}
        1 & x_{11} & x_{12} & ... & x_{1p} \\
        1  & x_{21} & x_{22} & ... & x_{2p} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1  & x_{n1} & x_{n2} & ... & x_{np}
    \end{pmatrix}
    \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{pmatrix} +
    \begin{pmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n \end{pmatrix} \\
    \boldsymbol{y} &= \boldsymbol{X\beta + \varepsilon}
\end{aligned}
$$

## **Ordinary Least Squares**

Similar to the SLR model, the regression parameters can be found by minimizing the sum of squared residuals:

$$
RSS = \sum y_i - \hat{y} =\sum [y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i1 + \hat{\beta}_2 x_i2 + ... + \hat{\beta}_p x_ip)]^2
$$

There are **$p+1$ FOC equations** to solve through the minimization, with the additional one reflecting the intercept parameter. It is difficult to algebraically solve this system of equations, thus there is no closed form tsolution for each individual paramater. Instead, here is a **vector solution** for all of the parameters:

$$
\hat{\boldsymbol{\beta}} = \begin{pmatrix} \hat{\beta_0} \\ \hat{\beta_1} \\ \vdots \\ \hat{\beta_p} \end{pmatrix} = (\boldsymbol{X'X})^{-1}\boldsymbol{X'y}
$$

> Note that since there are $p+1$ equations that must be solved, the model has $n-p+1$ degrees of freedom.
>
> Following the same logic, there must be **at least $p+1$ observations** in order to solve the equations and hence construct the model.

This results in the following fitted regression model, which can be graphically expressed as a **Regression Plane**:

$$
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_i1 + \hat{\beta}_2 x_i2 + ... + \hat{\beta}_P x_ip
$$

<!-- Obtained from ISLR -->
![Fitted Regression Plane](Assets/3.%20Multiple%20Linear%20Regression.md/Fitted%20Regression%20Plane.png){.center}

### **Manual Computation**

The tricky part is that $(\boldsymbol{X'X})^{-1}$ is **hard to compute by manually**, except for the special case where $p=1$ (SLR). Thus, it is likely that the parameters will be provided by the question.

If required to compute them manually, then $(\boldsymbol{X'X})^{-1}$ is *likely* to be provided. The remaining $\boldsymbol{X'y}$ still needs to be computed and put together to obtain the regression parameters.

However, if the model has $p=2$ but **no intercept**, then $(\boldsymbol{X'X})^{-1}$ is a **2 x 2 Matrix** whose inverse can be easily calculated. Similarly, if $(\boldsymbol{X'X})^{-1}$ is a **Diagonal Matrix**, its inverse can be easily calculated as well.

## **Goodness of Fit**

The ANOVA for MLR follows the same intuition as the SLR version, adjusted for the new degrees of freedom:

<center>

| Source | Sum of Squares | Degrees of Freedom | Mean Square | F-Statistic |
| :-: | :-: | :-: | :-: | :-: |
| Regression | RegSS | $p$ | $MS_{RegSS}$ | $\frac{RegSS}{MSE}$ |
| Error | RSS | $n-(p+1)$ | $MSE$ | - |
| Total | TSS | $n-1$ | $s^2$ | - |

</center>

The coefficient of determination still represents the proportion of variance explained by the regression, but has a slightly different formula:

$$
R^2 = \frac{RegSS}{TSS} = r_{y,\hat{y}}^2
$$

The multiple IVs of the model are now captured through $\hat{y}$ instead of $x$ directly.

However, the hypothesis test under the MLR is vastly different from the SLR version. Instead of testing if an individual IV is useful, it tests if all the IVs are **collectively useful** in helping to explain the DV.

$$
\begin{aligned}
H_0 &: \beta_1 = \beta_2 = ... = \beta_p = 0 \\
H_1 &: \text{At least one } \beta_j \text{ is non-zero}
\end{aligned}
$$

Thus, rejecting the null hypothesis implies that **at least one** of the IVs used is useful, but does not provide much insight into *which* of them are useful.

### **Partial F Test**

A partial F-test can be used to precisely **determine which** of the IVs are useful in explaining the DV.

A regular F test compares the null model with no IVs to the desired model with all the IVs. If the sum of squares are significantly different, then it implies that the **additional IVs are jointly useful** in explaining $Y$.

The partial F test generalizes this idea. Instead of considering a null model with no IVs, a **Reduced Model** with a limited number IVs ($q$) is considered instead. Consequently, the desired model is known as the **Full Model** with all $p$ IVs, where $q < p$.

$$
\begin{aligned}
H_0: \beta_{p-q+1} = ... = \beta_p = 0 \\
H_1: \beta_{p-q+1} = ... = \beta_p \ne 0
\end{aligned}
$$

> The test is also commonly referred to as the Generalized F test, where the models are referred to as the Restricted and Unrestricted Models.

The **difference in the RSS** between the Full and Reduced Model is known as the **Extra Sum of Squares (ExtraSS)**. It represents the contribution of the missing variables in explaining the variance of $Y$. Under the null hypothesis, there should be no difference between the two RSS, and thus $ExtraSS = 0$.

$$
ExtraSS = RSS_{Reduced} - RSS_{Full}
$$

Thus, the Partial F-statistic is testing for the **equality of variance between the two RSS** - if there is a significant difference in the variance of the two, then the null should be rejected and thus $\beta_{p-q+1} = ... = \beta_p \ne 0$.

$$
\begin{aligned}
F
&= \frac{MS_{ExtraSS}}{MS_{RSS_{Full}}} \\
&= \frac{ExtraSS/q}{RSS/(n-2)} \\
&= \frac{(RSS_{Reduced} - RSS_{Full})/q}{RSS/(n-2)} \\
&= (n-2) * \frac{(1- R_{Reduced}^2) - (1 - R_{Full}^2)}{1-RSS_{Full}^2}, \text{divide both by TSS} \\
&= (n-2) * \frac{R_{Full}^2 - R_{Reduced}^2}{1-RSS_{Full}^2}
\end{aligned}
$$

## **Statistical Inference**

### **Sampling Distributions**

Similar to SLR, the regression parameters are normally distributed as well. However, since there are multiple regression parameters, they collectively follow a **multivariate normal distribution**.

$$
\hat{\beta} \sim N_{p+1}(\beta, \sigma^2 (\boldsymbol{X'X})^{-1})
$$

The variance of the distribution is known as the **Variance Covariance Matrix**, which provides the covariances between every possible pair of regression parameters.

Since the covariance of a variable with itself is its variance, the **diagonals are the respective variances** of the parameters. Note that the first element of the diagonal is the intercept, thus the variance of the jth IV is the **(j+1)th element of the diagonal**.

$$
Var(\hat{\beta}) =
\begin{pmatrix}
    Var(\hat{\beta}_0) & Cov(\hat{\beta}_0, \hat{\beta}_1) & ... & Cov(\hat{\beta}_0, \hat{\beta}_p) \\
    Cov(\hat{\beta}_1, \hat{\beta}_0) & Var(\hat{\beta}_1) & ... & Cov(\hat{\beta}_1, \hat{\beta}_p) \\
    \vdots & \vdots & \ddots & \vdots \\
    Cov(\hat{\beta}_p, \hat{\beta}_0) & Cov(\hat{\beta}_1, \hat{\beta}_p) & ... & Var(\hat{\beta}_p)
\end{pmatrix}
$$

> Note that the covariances are symmetrical about the diagonal - $Cov(\hat{\beta}_0, \hat{\beta}_1) = Cov(\hat{\beta}_1, \hat{\beta}_0)$.

### **Hypothesis Testing**

Similar to SLR, the t-test can be used to test for the significance of an individual IV, but the intepretation of the test is different from the SLR case. It tests the usefulness of an **individual IV** in the *presence of the other predictors*.

$$
\begin{aligned}
H_0: \beta_j = 0 \\
H_1: \beta_j \ne 0
\end{aligned}
$$

$$
t(\beta_j) = \frac{\hat{\beta_j} - \beta_j}{\hat{\sigma}_{\hat{\beta_1}}}
$$

> Recall that the variance of the jth IV si the (j+1)th element of the variance covariance matrix.

$$
t(\beta_j) \sim t_{n-p-1}
$$

However, this leads to several odd results which needs to be accounted for:

1. Predictor is not significant individually but **significant when taken alone**.
2. Predictors are not significant individually but **significant when taken together**.

<!-- Covered more in a later section -->

There are now two possible ways to test for the significance of IVs:

1. Conduct a **single F-test** to test for the joint significance of **all IVs**
2. Conduct **multiple t-tests** to test for the joint significance of **each IV**

The problem with the multiple t-test approach lies with the **type I error** of the tests. For $\alpha = 0.05$, the probability of **correctly rejecting** the null is $0.95$. Assuming that all the tests are independent, the **probability of correctly rejecting all the nulls** is $0.95^p$, which drastically decreases with the number of tests conducted.

Given enough predictors, this means that the probability drops to approximately 0, which means that there is bound to be a wrongly rejected null; a **type I error is guaranteed** even though it was supposed to be limited at a 0.05 chance.

> The **Bonferroni Correction** is a method of adjusting $\alpha$ of each hypothesis test such that the overall type I error is kept at its desired level. However, this has the consequence of increasing the probability of type II errors, which is why it is not popular.

The F-test has the advantage of controlling the type I error **regardless of the number of predictors**, which is why it is preferred for hypothesis testing in the MLR.

### **Confidence Intervals**

Similar to SLR, the confidence intervals can be constructed using the distribution of the test-statistic:

$$
\text{Confidence Interval} = \hat{\beta}_j \pm t_{n-p-1, \frac{\alpha}{2}} * \hat{\sigma}_{\hat{\beta_j}}
$$

### **Prediction Intervals**

Unlike in SLR, it is difficult to determine the distribution of the prediction error. Thus, the final result can be found below:

$$
\begin{aligned}
\text{Prediction Interval}
&= \hat{y}_* \pm t_{n-p-1, \frac{\alpha}{2}} * \hat{\sigma}_{y_* - \hat{y_*}} \\
&= \hat{y}_* \pm t_{n-p-1, \frac{\alpha}{2}} * \sqrt{s^2 [1 + x'_* (\boldsymbol{X'X})^-1 * x_*]}
\end{aligned}
$$

Despite the result looking more complicated, the key takeaway remains the same - the further away $x_*$ is from $\bar{x}$, the greater

## **Variations of MLR**

### **Qualitative IV**

The discussion so far has mostly focused on Quantitative IVs, thus this section will explore Qualitative IVs. They can only take values from a list of *pre-defined* values, known as the **Levels** of the variable.

In a regression context, most qualitative IVs are represented in the form of a **Dummy Variable** which can only take two possible levels - Yes (1) or No (0).

> Note that there are other ways of encoding a dummy variable (-1/0/1 etc), but the principles stay large the same.

$$
\text{Dummy Variable} =
\begin{cases}
    1,  & \text{First Level} \\
    0, & \text{Second Level}
\end{cases}
$$

In general, $n-1$ dummy variables are needed to represent a qualitative variable with $n$ levels. This is because the status of the last level can be **deduced from the other dummy variables**. Thus, including a seperate dummy variable for this last level is **redundant** and will lead to the problem of **Collinearity**, which will be explored in a later section.

For instance, consider four levels (North, East, South & West), represented by the three dummy variables ($N, E , S$). If any of the variables are 1, they represent their respective direction (North, East & South). If all of them are 0, then the direction is the **remaining level** (West).

The last remaining level is often referred to as the **Baseline Level** as it is the default level of the variable when all other dummies are 0. Any level can be used as the baseline, but the **parameters will differ** across models with different baselines.

$$
E(Y|X) = \beta_0 + \beta_{1, North} x_{North} + \beta_{2, East} * x_{East} + \beta_{3, South} * x_{South}
$$

<center>

| $\beta_0$  | $\beta_j$ |
| :-: | :-: |
| $E(Y)$ at the baseline level | Change in $E(Y)$ from the baseline to the chosen level |
| $X_1 = X_2 = ... = X_j = 0$ | $X_1 = X_2 = ... = 0; X_j = 1$ |

</center>

Dummy variables are usually **used in conjunction with quantitative ones**. This essentially creates a "seperate" regression model for each of the levels.

For the simplest case of one quantiative and one dummy, $\beta_2$ is the **difference in the intercept** of the two resulting SLR models.

$$
E(Y|X) = \beta_0 + \beta_1 x_1 + \beta_2 x_2
$$

$$
E(Y|X)  =
\begin{cases}
    (\beta_0 + \beta_2) + \beta_1 x_1, & x_2 = 1 \\
    \beta_0 + \beta_1 x_1, & x_2 = 0
\end{cases}
$$

<!-- Obtained from ACTEX Manual -->
![Dummy Variable](Assets/3.%20Multiple%20Linear%20Regression.md/Dummy%20Variable.png){.center}

### **Interaction Model**

So far, it was assumed that each IV had an independent effect on the DV. However, IVs may **interact** to produce a **joint effect** on the DV, where the effect of one IV depends on the value of another IV.

For instance, the production of a factory may depend on the number of Machines and Workers. However, the more machines there are, the **greater the effect of an additional worker**.

Thus, this interaction effect can be captured through an **Interaction Variable**, which is the product of both IVs:

$$
\begin{aligned}
E(Y|X)
&= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 \\
&= \beta_0 + (\beta_1 + \beta_3 x_2)x_1 + \beta_2 x_2 \\
&= \beta_0 + \beta_1 x_1 + (\beta_2 + \beta_3 x_1) x_2 \\
\end{aligned}
$$

A one unit increase in $x_1$ will increase E(Y) by $\beta_1 + \beta_3 x_2$, which **depends on the value of $x_2$** as well, which is is why they interact with each another. Phrased another way, for every one unit increase in $x_2$, the change in E(Y) from a unit increase in $x_1$ increases by $\beta_3$.

> Something unusual to take note of is that the Interaction Variable tests significant but the constituent variables do not. In this case, it is common practice to **retain both the interaction and the consistuent variables** in the model. This is practice is known as the **Hierarchical Principle**.

Models containing dummy variables can also have an interaction effect. Building off the example from the previous section, $\beta_2$ is still the difference in the intercept but with the new $\beta_3$ being the **difference in slopes** of the two resulting SLR models.

$$
E(Y|X)  =
\begin{cases}
    (\beta_0 + \beta_2) + (\beta_1 + \beta_3)x_1, & x_2 = 1 \\
    \beta_0 + \beta_1 x_1, & x_2 = 0
\end{cases}
$$

<!-- Obtained from ACTEX Manual -->
![Dummy Interaction](Assets/3.%20Multiple%20Linear%20Regression.md/Dummy%20Interaction.png){.center}

### **Piecewise Model**

If the DV has an abrupt change in behaviour over different values of the IVs, it can be accounted for the through the use of a **Piecewise Regression**.

The first method of creating a piecewise regression involves the use of an **Indicator Function**. It is essentially a dummy variable which depends on the value of the other IVs.

$$
z_{\{x>=c\}} =
\begin{cases}
    0, & x < c \\
    1, & x \ge c
\end{cases}
$$

$$
\begin{aligned}
E(Y|X)
&= \beta_0 + \beta_1 x_1 + \beta_2 z(x-c) \\
&=
\begin{cases}
    \beta_0 + \beta_1 x_1, & x < c \\
    (\beta_0 - \beta_2c) + (\beta_1 + \beta_2) x_1, & x \ge c
\end{cases}
\end{aligned}
$$

> Note that $z(x-c)$ is treated as a distinct IV and hence can be equivalently expressed as $x_2$; the full notation is used here for clarity.

$c$ is the value at which the DV abruptly changes in behaviour, known as a **Kink** in the graph, which *continuously* connects the two regression lines.

<!-- Obtained from ACTEX Manual -->
![Piecewise Kink Model](Assets/3.%20Multiple%20Linear%20Regression.md/Piecewise%20Kink%20Model.png){.center}

The other method is to use an **interaction variable** instead. Similar to how the interaction variables resulted in the model to "split", the model now splits at $x = c$, resulting in a *non-continuous* gap.

$$
\begin{aligned}
E(Y|X)
&= \beta_0 + \beta_1 x_1 + \beta_2z + \beta_3 zx \\
&=
\begin{cases}
    \beta_0 + \beta_1 x_1, & x < c \\
    (\beta_0 - \beta_2) + (\beta_1 + \beta_3) x_1, & x \ge c
\end{cases}
\end{aligned}
$$

<!-- Obtained from ACTEX Manual -->
![Piecewise Gap Model](Assets/3.%20Multiple%20Linear%20Regression.md/Piecewise%20Gap%20Model.png){.center}

### **Polynomial Model**

If the relationship between the DV and IV is complex (non-linear), then a **Polynomial Regression** can be used to better model the relationship between the two.

$$
E(Y|X) = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3
$$

> Note that it is the same IV used in the regression, just with additional powers.

Although a polynomial regression may capture the true relationship better, the regression parameters become hard to intepret. The partial derivatives can no longer be intepreted as holding other IVs constant as each IV is dependent on the same quantity, just to different powers.

$$
\frac{\partial E(Y|X)}{\partial x} = \beta_1 + 2\beta_2 x + ... + m \beta_m x^{m-1}
$$

<!-- Obtained from Javatpoint -->
![Polynomial Regression](Assets/3.%20Multiple%20Linear%20Regression.md/Polynomial%20Regression.png){.center}
