# **Tree Models**

## **Overview**

Tree Models are the first **supervised AND non-parametric** statistical learning method covered. As such, the model is mainly **algorithmic** in nature.

Broadly speaking, tree models **partition the sample** into several **distinct regions** based on the values of independent variables. For a prediction, the model determines which region the prediction falls in and sets the prediction equal to the **average of the observations within that region**.

Using the **boundaries** of the partitions, a **tree-like structure** can be formed to visualize the model. This is highly datasets with many independent variables as the sample space itself is not easily visualized.

<!-- Obtained from Research Gate -->
![TREE_PARTITION](Assets/7.%20Tree%20Models.md/TREE_PARTITION.png){.center}

!!! Warning

    Only “vertical” and “horizontal” lines (lines **parallel to the axes** in higher dimensions) can be used to partition the observation space.

    When the lines intersect, each region is considered to be unique, even if the split does not change the prediction - regions are **NOT combined**. Graphically, this means that the predictor space must have always lines that touch both ends of the boundaries.
    
    <!-- Self Made -->
    ![REGION_SPLIT](Assets/7.%20Tree%20Models.md/REGION_SPLIT.png)
    
    The above points are mainly applicable for questions which ask to identify valid predictor spaces, typically when there are only two predictors.
    
!!! Warning   

    The predictor space is split using the **Independent Variable** as the boundaries, NOT the dependent variable.

!!! Warning

    Take note of the following "quirks":

    1. Not all features will be used to build the tree
    2. The same features can be used more than once in the tree
    3. Relatively more **important** features will be used for splits **first**

The different points along the tree are known as **Nodes**:

* **Internal Nodes** (Parent Nodes): Nodes where the tree **splits further**
* **Terminal Nodes** (Child Nodes): Nodes where the tree **no longer splits**

!!! Info

    They can also be interpreted using an actual tree analogy - Roots, Branches & Leaves. In this case, the structure is more of an inverted tree.

!!! Tip

    For the purposes of this exam, assume that **TRUE is on the left** while FALSE is on the right. In practice, either is fine as long as it is clearly defined.
    
!!! Warning

    For a tree with $n$ internal nodes, it will always have $n+1$ terminal nodes.
    
    Internal Nodes are also sometimes referred to as **Split**, given that the tree splits at that point.

<!-- Self Made -->
![TREE_TERMINOLOGY](Assets/7.%20Tree%20Models.md/TREE_TERMINOLOGY.png){.center}

Mathematically, each region of the tree can also be expressed using a **series of conditions**. For large trees, this becomes tedious, which is why the diagram is generally preferred:

$$
\begin{aligned}
    R_{1} = {y \mid x_{1} \lt a, x_{2} \lt b, x_{3} \lt c, \dots} \\
    R_{2} = {y \mid x_{1} \lt a, x_{2} \lt b, x_{3} \gt c, \dots} \\
    \dots
\end{aligned}
$$

!!! Note

    A tree with only a **single internal node** (only one split) is known as a **Stump**. It is a tree with only **one predictor**:

    <!-- Self Made -->
    ![TREE_STUMP](Assets/7.%20Tree%20Models.md/TREE_STUMP.png){.center}

## **Construction**

For the purposes of this exam, there is only one algorithm that is used to construct trees, **Recursive Binary Splitting**:

1. Start with all observations in one region (the entire sample space)
2. Consider **all possible splits** for all predictors
3. For each possibility, compute the **total RSS** from the resulting two regions
4. Select the split with the **lowest total RSS**
5. Repeat steps (2) - (4) for each resulting region

!!! Note

    The method gets its name from the following properties:

    * **Binary**: One region is always split into two; no more
    * **Recursive**: Splits rely on previously made splits

    The possible split boundaries are not arbitrarily chosen. They are always the **midpoint of between two observations**:

    * $(X_{1}, X_{2}) = {(1,6), (2, 7), (3, 8), \dots}$
    * Split $X_{1}$ at the following: ${1.5, 2.5, \dots}$
    * Split $X_{2}$ at the following: ${6.5, 7.5, \dots}$

In general, the algorithm can be said to choose the **split that minimizes the RSS at the time**. Similar to subset selection, it is a **greedy algorithm** as it chooses the local best choice at each step, not the global best.

!!! Tip

    Notice that since the **prediction is the average** of the observations in the node, the RSS is essentially the **variance** of the observations in the node. If given the observations or the relevant summed components, the RSS can be easily calculated.
    
    The total RSS is simply the sum of the RSS for each node, no weighting required.
    
    $$
        \text{Total Tree RSS} = \sum \text{Node RSS}
    $$

## **Pruning**

Recursive binary splitting tends to result in **large complex trees** with many internal nodes, which is prone to **overfitting**. In this case, certain **internal nodes should be dropped**, akin to **Pruning the leaves** of a tree.

!!! Note

    Recall that the complexity of a linear regression model was proportional to the **number of predictors** it had. In a similar fashion, the **number of internal nodes** is proportional to its complexity.

The first type of pruning is known as **Pre-pruning**. The algorithm includes a **stopping condition** which **prevents the tree from becoming too complex** in the first place.

!!! Tip

    The conditions are typically chosen such that they avoid overfitting:

    * Requires some **minimum number of observations** within each region
    * Requires some **minimum reduction of RSS** for each split

!!! Warning

    This type of pruning could **already have been baked into the training algorithm**, which is why it is not what people think of when pruning in is mentioned.

<!-- Obtained from Research Gate -->
![PRE_PRUNING](Assets/7.%20Tree%20Models.md/PRE_PRUNING.png){.center}

The second type of pruning is known as **Post-pruning**. A **penalty** is added to the RSS:

* $\lambda$: Hyperparameter
* T: Number of terminal nodes (Leaves)

$$
    \min (\text{RSS} + \lambda \cdot T)
$$

!!! Note

    Similar to Ridge/Lasso, the RSS in the above is the POST-pruning RSS. This is important to distinguish as questions can provide the values in a format like this:
    
    1. Terminal Nodes - RSS for that node
    2. Internal Nodes - RSS for that node, IF the leaves were pruned at that point

    <!-- Obtained from Coaching Actuaries -->
    ![PRUNING_RSS](Assets/7.%20Tree%20Models.md/PRUNING_RSS.png){.center}

    As stated previously, the RSS of the tree at any point is the sum of all terminal RSS. However, if given the above, it is important to sum the correct ones (while also applying the correct amount of penalty).

For every possible $T$, there exists a **subtree that minimizes** the above function. Among the above shortlisted subtrees, the one with the **lowest penalized RSS** is chosen as the final tree.

!!! Note

    This is why this method is known as **Cost Complexity Pruning**, as it adds a cost to the minimization problem.

    From another perspective, this it is also known as **Weakest Link Pruning**, as it removes the least important ("weakest links") from the tree.

The key idea is that $\lambda$ and $T$ are **inversely related**. The higher the penalty, the smaller the tree must be; hence different $\lambda$ will result in a **different final tree**.

!!! Tip

    Since the T is an **integer** and while the $\lambda$ is **continuous**, a range of $\lambda$ could result in the **same T; same pruned tree**:

    <!-- Obtained from Coaching Actuaries -->
    ![LAMBDA_RANGE](Assets/7.%20Tree%20Models.md/LAMBDA_RANGE.png){.center}

!!! Tip

    When $\lambda=0$, the penalized RSS is the **same as the regular RSS** and hence will result in the original tree. Thus, the trees that minimizes the penalized RSS will **always be a subtree** of the original.

    Intuitively, this is because the tree was fitted using a **forward greedy algorithm** to begin with. Thus, going backwards should result in the same tree.

!!! Warning

    Note that converting a subtree into a leaf is NOT simply taking the average of the two original leaves, as there is a **different weightage** applied to each leaf.

Generally speaking, **Post-Pruning is preferred**. Stunting the growth of the tree early might reduce a small amount of RSS early, but result in a large RSS later on. Thus, it is better to let the tree grow and then prune it back to the right size with a full view.

## **Ensemble Methods**

Ensemble Methods use a collection of statistical learning  techniques in conjunction with one another to produce better results than any of the individual methods.

Ensemble Methods are typically used with basic models known as **Weak Learners** to produce a stronger model.

!!! Tip

    The above implies that a tree on its own is **NOT as good** (in terms of predictive performance) as other statistical learning methods; likely due to overfitting.


!!! Info

    Although Ensemble methods are only discussed in the context of trees, they can be used for other methods as well.

### **Bagging**

Bagging refers to **Bootstrap Aggregation**. Bootstrapping is a process whereby "new" samples are created based on the existing sample. Thus, Bagging refers to constructing **multiple trees using bootstrap samples and aggregating their results** to form the final prediction.

Bootstrapping uses **random sampling with replacement** to create artificial samples of the **same size** from the original sample:

* $n^{n}$ **possible samples** (Different permutations count)
* $2n-1 \choose n-1$ **distinct samples** (Different permutations NOT counted)

!!! Note

    Since every sample has an equal chance of being selected, the probability that an observation is **NOT selected for a draw** is:

    $$
        \text{Probability NOT selected for draw} = 1 - \frac{1}{n}
    $$

    Thus, the probability that an observation is **NOT selected at all across the entire sample** ($n$ draws) is:

    $$
        \text{Probability NOT selected} = \left(1 - \frac{1}{n} \right)^{n}
    $$

    As the sample size approaches infinity, the above converges to the following, showing that **about two-thirds of the observations are used**:

    $$
    \begin{aligned}
        \lim \left(1 - \frac{1}{n} \right)^{n} & \approx \frac{1}{3}
        \\
        \text{Probability selected}
        &= 1 - \frac{1}{3} \\
        &= \frac{2}{3}
    \end{aligned}
    $$

Something unique about Bagging is that since **only two-thirds** of the original observations are used for each tree, it is possible to use the **observations that were NOT drawn** as the **validation set**. The resulting (average) error across all instances is known as the **Out of Bag Error** (OOB).

!!! Tip

    The intuition is similar to cross-validation where the **OOB observations were not used to train that particular bagged tree**, thus serves as a good estimate for the test MSE.

    In fact, for a sufficiently large number of trees, the OOB provides an **estimate of the LOOCV error** with the added advantage of being computationally more efficient:

    1. LOOCV error requires that all but one observation is used to train the model
    2. For a large number of bootstrap samples, it increases the chances that ALL observations from the original dataset are used
    3. Thus, the OOB will calculated based on ALL observations EXCEPT the observation being tested; akin to LOOCV

!!! Warning

    OOB does NOT use every single tree in the Ensemble; it only uses those for which the specific observation was NOT used to train the tree.

The **key advantage** to bagging lies in **Variance**. An estimate coming from multiple samples is always **more stable** than estimate coming from just one sample.

!!! Tip

    Since bagging helps to reduce the variance, the underlying trees are often **not pruned** (pre & post) as the nature of bagging helps to counter the problem of high variance for unpruned trees.

    In general, having **more bootstrapped samples is beneficial**; having too many samples does **NOT cause overfitting**. However, it is **computationally expensive**. Thus, the number of samples should be chosen such that the increase in accuracy is higher than the cost of computing.

However, the **key disadvantage** is that the method is that is **not easily interpreted**. A single tree is easily visualized while a forest is not.

!!! Tip  
    
    One way to add more insight to the model is through the use of **Variable Importance Plots**:

    * **Sensitivity** analysis, removing one variable at a time from the model
    * Increase in MSE is recorded for each variable
    * The variable whose absence results in the **greatest increase in MSE is the most important** variable

    <!-- Obtained from Research Gate -->
    ![VARIABLE_IMPORTANCE](Assets/7.%20Tree%20Models.md/VARIABLE_IMPORTANCE.png){.center}

### **Random Forest**

One of the primary issues with Bagging is **Correlation**. Since the bootstrapped **samples are all similar** to the underlying sample, it is likely that each bootstrapped tree will contain the **same set of predictors** (especially if there especially strong predictors).

This can be overcome by **limiting each bootstrapped tree** to be constructed only based on a **random subset of the predictors available** at EACH SPLIT, reducing the monopoly of certain predictors and hence reducing the similarities. Hence, the method is called "Random Forest".

!!! Tip

    It can be understood that Bagging is a **special case of Random Forests** where all the predictors are allowed; bagging is a **non-random forest**.

    Most of the concepts from Bagging thus carry over.

If there are $p$ available predictors and $m$ are excluded, then the probability that a *particular predictor* is not chosen at a given split is:

$$
\begin{aligned}
    \text{P(Predictor not Chosen)}
    &= \frac{{p-1 \choose m} \cdot {1 \choose 0}}{{p \choose m}} \\
    &= \frac{p-m}{p}   
\end{aligned}
$$

This can be interpreted as the **most important predictor** is not included $\frac{p-m}{p}$ of splits, thus for **reasonably small** $m$, the proportion is low which results in de-correlated trees.

!!! Note

    It is common to use the following values for $m$:

    * **Regression Trees**: $m = \frac{p}{3}$
    * **Classification Trees**: $m = \sqrt{p}$

    However, if many variables are correlated, an even smaller value of $m$ may be preferred to ensure more unique trees in the ensemble.

### **Boosting**

Boosting refers to the process of **sequentially training** a series of **shallow trees** on the **error of the previous tree**. The strength of weak trees are "boosted" by **aggregating** them together to form a stronger model.

The key idea is that the errors of the trees contain the effects of the **predictors not included in the model**. Thus, training each successive model on the errors of the prior ones forces the model to **explained the unexplained**.

<!-- Obtained from Research Gate -->
![BOOSTING](Assets/7.%20Tree%20Models.md/BOOSTING.png){.center}

<!-- Obtained from Affine -->
![BOOSTING_VS_BAGGING](Assets/7.%20Tree%20Models.md/BOOSTING_VS_BAGGING.png){.center}

There are three hyperparameters for Boosting:

* **Number of Trees** (b) - How many trees should be used
* **Depth of Trees** (d) - How many splits each tree should have
* **Learning Rate** ($\lambda$) - How much the trees contributes to the overall prediction 

$$
    \text{Boosted Prediction}
    = \lambda \cdot (\text{Tree 1 Prediction} + \text{Tree 2 Prediction} + \dots)
$$

Boosting as a method will result in low bias. However, if too many trees are used, the model could become **overfitted**.

!!! Note

    The learning rate can also be referred to as the **shrinkage parameter** as it reduces the influence of each tree on the overall prediction.
    
    A **small** shrinkage parameter ($\lambda \lt 1$) is preferred so that the learning process is slowed; requiring many trees. 

!!! Tip

    As alluded to, shallow trees are typically used (EG. $d=1$).

## **Classification Trees**

Decision Trees can also be used for **categorical predictors**, in which case it is known as a **Classification Tree**. The model assigns a category to the prediction based on the **most common** category within that region of the tree.

!!! Note

    Most of the key concepts apply, but there are some notable exceptions due to having a categorical independent variable.

Since residuals are unintuitive for categorical data, classification trees focus on **Node Purity** instead. The algorithm divides the predictor space such that **node impurity is minimized**.
    
Purity is a measure of TBC, which can measured using one of the following metrics:

1. Classification Error Rate
2. Gini Index
3. Cross Entropy

Gini Index and Cross Entropy are better measures of purity as they are **sensitive to the distribution** of categories within each region, thus they are the **preferred purity measure** when **growing** out a tree. However, when **pruning** a tree, the primary goal is improve predictive accuracy, thus the classification error rate is used instead.

!!! Tip

    The lower the above metrics, the higher the purity of the node; more imbalanced.
    
    This can be remembered by the fact that the Gini Index is often a measure of **socio-economic inequality** - it is higher when there is a large imbalance between the rich and the poor (loosely speaking). 
    
### **Purity Measures**

A key quantity that will be used is the **proportion of correct classifications** in a particular region ($p_{c}$).

$$
    p_{c} = \frac{\text{Num Obs in Category & Region}}
            {\text{Total Num Obs in Region}}
$$

Thus, the number of misclassifications in that region ONLY is the **complement of the highest** proportion:

$$
    \text{Classification Error (Region)} = 1 - \max(p_{1}, p_{2}, \dots)
$$

!!! Tip

    Recall that the model will predict a category based on the **most frequent category** (highest $p_{c}$) in that region. Thus, any observations that are NOT part of that category are being **misclassified**.

Thus, the overall misclassification rate for the **entire tree** is the **weighted average** of the misclassifications for each region:

$$
    \text{Classification Error (Total)} = \sum \text{Classification Error (Region)} \cdot \frac{n_{\text{Region}}}{n_{\text{Total}}}
$$

The above concept can be applied to the Gini Index and Cross Entropy as well, thus their total formulas will not be shown:

$$
\begin{aligned}
    \text{Gini Index (Region)} &= \sum_{c} p_{c} (1 - p_{c}) \\
    \text{Cross Entropy (Region)} &= -2 \sum_{c} p_{c} \ln p_{c}
\end{aligned}
$$

!!! Warning

    All measures must be positive, but ONLY entropy can be larger than 1.

    Gini Index and Entropy are similar, but there are two key differences:

    1. Entropy has a multiplier of 2
    2. Entropy uses the same propbability for each set

!!! Tip

    Gini Index and Class Entropy are the preferred purity measures when **growing** out a tree. This is because they are **sensitive to the distribution** of categories, while classification error is not. Consider the following two examples - the classification error rate is the same but the purity is much higher in the second region:

    <!-- Self Made -->
    ![PURITY_DISTRIBUTION](Assets/7.%20Tree%20Models.md/PURITY_DISTRIBUTION.png){.center}

    Classification Error is a more one-dimensional approach that tends to result in simpler trees, which is why is often used as the purity measure for **pruning**, along with the fact that the main goal of pruning is to improve predictive accuracy, which is directly what the classification error measures.
    
    In light of this, it is possible for a split to produce **two nodes with the same predicted category**. This is because the **Gini or Entropy would have increased** with the split, even though classification error remains the same.

!!! Note

    There is technically another less common purity measure known as **Tree Deviance** (not to be confused with GLM deviance):
    
    $$
        \text{Tree Deviance} = 2 \cdot \text{Entropy}
    $$

### **Two Category Model**

The most common scenario to appear on the exam would be a model with **ONLY TWO** possible categories.

In such cases, there is a clearly defined order between the purity measures:

$$
    \text{Entropy} \gt \text{Gini} \ge \text{Class Error}
$$

<!—- Obtained from Coaching Actuaries —>
![RELATIVE_PURITY](Assets/7.%20Tree%20Models.md/RELATIVE_PURITY.png){.center}

!!! Tip

    There is only one situation where Gini and Classification error are the same - when the two categories are **equally distributed**.

!!! Warning

    The above relationship only holds for the measures WITHIN a single node. It CANNOT be used to compare across nodes.

