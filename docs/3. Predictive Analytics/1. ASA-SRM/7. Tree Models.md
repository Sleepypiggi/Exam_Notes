# **Tree Models**

## **Overview**

Tree Models are the first **supervised AND non-parametric** statistical learning method covered. As such, the model is mainly **algorithmic** in nature.

Broadly speaking, tree models **partition the sample** into several **distinct regions** based on the values of independent variables. For a prediction, the model determines which region the prediction falls in and sets the prediction equal to the **average of the observations within that region**.

Using the **boundaries** of the partitions, a **tree-like structure** can be formed to visualize the model. This is highly datasets with many independent variables as the sample space itself is not easily visualized.

<!-- Obtained from Research Gate -->
![TREE_PARTITION](Assets/7.%20Tree%20Models.md/TREE_PARTITION.png){.center}

!!! Warning

    Only “vertical” and “horizontal” lines (lines **parallel to the axes** in higher dimensions) can be used to partition the observation space.

    Additionally, the predictor space is split using the **Independent Variable** as the boundaries, NOT the dependent variable.

!!! Warning

    Take note of the following "quirks":

    1. Not all features will be used to build the tree
    2. The same features can be used more than once in the tree

The different points along the tree are known as **Nodes**:

* **Internal Nodes** (Parent Nodes): Nodes where the tree **splits further**
* **Terminal Nodes** (Child Nodes): Nodes where the tree **no longer splits**

!!! Info

    They can also be interpreted using an actual tree analogy - Roots, Branches & Leaves. In this case, the structure is more of an inverted tree.

!!! Tip

    For the purposes of this exam, assume that **TRUE is on the left** while FALSE is on the right. In practice, either is fine as long as it is clearly defined.
    
!!! Warning

    For a tree with $n$ internal nodes, it will always have $n+1$ terminal nodes.
    
    Internal Nodes are also sometimes referred to as **Split**, given that the tree splits at that point.

<!-- Self Made -->
![TREE_TERMINOLOGY](Assets/7.%20Tree%20Models.md/TREE_TERMINOLOGY.png){.center}

Mathematically, each region of the tree can also be expressed using a **series of conditions**. For large trees, this becomes tedious, which is why the diagram is generally preferred:

$$
\begin{aligned}
    R_{1} = {y \mid x_{1} \lt a, x_{2} \lt b, x_{3} \lt c, \dots} \\
    R_{2} = {y \mid x_{1} \lt a, x_{2} \lt b, x_{3} \gt c, \dots} \\
    \dots
\end{aligned}
$$

!!! Note

    A tree with only a **single internal node** (only one split) is known as a **Stump**. It is a tree with only **one predictor**:

    <!-- Self Made -->
    ![TREE_STUMP](Assets/7.%20Tree%20Models.md/TREE_STUMP.png){.center}

## **Construction**

For the purposes of this exam, there is only one algorithm that is used to construct trees, **Recursive Binary Splitting**:

1. Start with all observations in one region (the entire sample space)
2. Consider **all possible splits** for all predictors
3. For each possibility, compute the **total RSS** from the resulting two regions
4. Select the split with the **lowest total RSS**
5. Repeat steps (2) - (4) for each resulting region

!!! Note

    The method gets its name from the following properties:

    * **Binary**: One region is always split into two; no more
    * **Recursive**: Splits rely on previously made splits

    The possible split boundaries are not arbitrarily chosen. They are always the **midpoint of between two observations**:

    * $(X_{1}, X_{2}) = {(1,6), (2, 7), (3, 8), \dots}$
    * Split $X_{1}$ at the following: ${1.5, 2.5, \dots}$
    * Split $X_{2}$ at the following: ${6.5, 7.5, \dots}$

In general, the algorithm can be said to choose the **split that minimizes the RSS at the time**. Similar to subset selection, it is a **greedy algorithm** as it chooses the local best choice at each step, not the global best.

## **Pruning**

Recursive binary splitting tends to result in **large complex trees** with many internal nodes, which is prone to **overfitting**. In this case, certain **internal nodes should be dropped**, akin to **Pruning the leaves** of a tree.

!!! Note

    Recall that the complexity of a linear regression model was proportional to the **number of predictors** it had. In a similar fashion, the **number of internal nodes** is proportional to its complexity.

The first type of pruning is known as **Pre-pruning**. The algorithm includes a **stopping condition** which **prevents the tree from becoming too complex** in the first place.

!!! Tip

    The conditions are typically chosen such that they avoid overfitting:

    * Requires some **minimum number of observations** within each region
    * Requires some **minimum reduction of RSS** for each split

!!! Warning

    This type of pruning could **already have been baked into the training algorithm**, which is why it is not what people think of when pruning in is mentioned.

<!-- Obtained from Research Gate -->
![PRE_PRUNING](Assets/7.%20Tree%20Models.md/PRE_PRUNING.png){.center}

The second type of pruning is known as **Post-pruning**. A hyperparameter **constraint** is added to the RSS, where T is the **number of leaves** in the tree:

$$
    \min (\text{RSS} + \lambda \cdot T)
$$

For every possible $T$, there exists a **subtree that minimizes** the above function. Among the above shortlisted subtrees, the one with the **lowest constrained RSS** is chosen as the final tree.

!!! Note

    This is why this method is known as **Cost Complexity Pruning**, as it adds a cost to the minimization problem.

    From another perspective, this it is also known as **Weakest Link Pruning**, as it removes the least important ("weakest links") from the tree.

The key idea is that $\lambda$ and $T$ are **inversely related**. The higher the constraint, the smaller the tree must be; hence different $\lambda$ will result in a **different final tree**.

!!! Tip

    Since the T is an **integer** and while the $\lambda$ is **continuous**, a range of $\lambda$ could result in the **same T; same pruned tree**:

    <!-- Obtained from Coaching Actuaries -->
    ![LAMBDA_RANGE](Assets/7.%20Tree%20Models.md/LAMBDA_RANGE.png){.center}

!!! Tip

    When $\lambda=0$, the constrained RSS is the **same as the regular RSS** and hence will result in the original tree. Thus, the trees that minimizes the constrained RSS will **always be a subtree** of the original.

    Intuitively, this is because the tree was fitted using a **forward greedy algorithm** to begin with. Thus, going backwards should result in the same tree.

!!! Warning

    Note that converting a subtree into a leaf is NOT simply taking the average of the two original leaves, as there is a **different weightage** applied to each leaf.

Generally speaking, **Post-Pruning is preferred**. Stunting the growth of the tree early might reduce a small amount of RSS early, but result in a large RSS later on. Thus, it is better to let the tree grow and then prune it back to the right size with a full view.

## **Ensemble Methods**

Ensemble Methods use a collection of statistical learning  techniques in conjunction with one another to produce better results than any of the individual methods.

Ensemble Methods are typically used with basic models known as Weak Learners to produce a stronger model.

!!! Info

    Although Ensemble methods are only discussed in the context of trees, they can be used for other methods as well.

### **Bagging**

Bagging refers to **Bootstrap Aggregation**. Bootstrapping is a process whereby "new" samples are created based on the existing sample. Thus, Bagging refers to constructing **multiple trees using bootstrap samples and aggregating their results** to form the final prediction.

Bootstrapping uses **random sampling with replacement** to create artificial samples of the **same size** from the original sample:

* $n^{n}$ **possible samples** (Different permutations count)
* $2n-1 \choose n-1$ **distinct samples** (Different permutations NOT counted)

!!! Note

    Since every sample has an equal chance of being selected, the probability that an observation is **NOT selected for a draw** is:

    $$
        \text{Probability NOT selected for draw} = 1 - \frac{1}{n}
    $$

    Thus, the probability that an observation is **NOT selected at all across the entire sample** ($n$ draws) is:

    $$
        \text{Probability NOT selected} = \left(1 - \frac{1}{n} \right)^{n}
    $$

    As the sample size approaches infinity, the above converges to the following, showing that **about two-thirds of the observations are used**:

    $$
    \begin{aligned}
        \lim \left(1 - \frac{1}{n} \right)^{n} & \approx \frac{1}{3}
        \\
        \text{Probability selected}
        &= 1 - \frac{1}{3} \\
        &= \frac{2}{3}
    \end{aligned}
    $$

Something unique about Bagging is that since **only two-thirds** of the original observations are used for each tree, it is possible to use the **observations that were NOT drawn** as the **validation set**. The resulting (average) error across all instances is known as the **Out of Bag Error** (OOB).

!!! Tip

    The intuition is similar to cross-validation where the **OOB observations were not used to train that particular bagged tree**, thus serves as a good estimate for the test MSE.

    In fact, for a sufficiently large number of trees, the OOB provides an **estimate of the LOOCV error** with the added advantage of being computationally more efficient:

    1. LOOCV error requires that all but one observation is used to train the model
    2. For a large number of bootstrap samples, it increases the chances that ALL observations from the original dataset are used
    3. Thus, the OOB will calculated based on ALL observations EXCEPT the observation being tested; akin to LOOCV

!!! Warning

    OOB does NOT use every single tree in the Ensemble; it only uses those for which the specific observation was NOT used to train the tree.

The **key advantage** to bagging lies in **Variance**. An estimate coming from multiple samples is always **more stable** than estimate coming from just one sample.

!!! Tip

    Since bagging helps to reduce the variance, the underlying trees are often **not pruned** (pre & post) as the nature of bagging helps to counter the problem of high variance for unpruned trees.

    In general, having **more bootstrapped samples is beneficial**; having too many samples does **NOT cause overfitting**. However, it is **computationally expensive**. Thus, the number of samples should be chosen such that the increase in accuracy is higher than the cost of computing.

However, the **key disadvantage** is that the method is that is **not easily interpreted**. A single tree is easily visualized while a forest is not.

!!! Tip  
    
    One way to add more insight to the model is through the use of **Variable Importance Plots**:

    * **Sensitivity** analysis, removing one variable at a time from the model
    * Increase in MSE is recorded for each variable
    * The variable whose absence results in the **greatest increase in MSE is the most important** variable

    <!-- Obtained from Research Gate -->
    ![VARIABLE_IMPORTANCE](Assets/7.%20Tree%20Models.md/VARIABLE_IMPORTANCE.png){.center}

### **Random Forest**

One of the primary issues with Bagging is **Correlation**. Since the bootstrapped **samples are all similar** to the underlying sample, it is likely that each bootstrapped tree will contain the **same set of predictors** (especially if there especially strong predictors).

This can be overcome by **limiting each bootstrapped tree** to be constructed only based on a **random subset of the predictors available** at EACH SPLIT, reducing the monopoly of certain predictors and hence reducing the similarities. Hence, the method is called "Random Forest".

!!! Tip

    It can be understood that Bagging is a **special case of Random Forests** where all the predictors are allowed; bagging is a **non-random forest**.

    Most of the concepts from Bagging thus carry over.

If there are $p$ available predictors and $m$ are excluded, then the probability that a *particular predictor* is not chosen at a given split is:

$$
\begin{aligned}
    \text{P(Predictor not Chosen)}
    &= \frac{{p-1 \choose m} \cdot {1 \choose 0}}{{p \choose m}} \\
    &= \frac{p-m}{p}   
\end{aligned}
$$

This can be interpreted as the **most important predictor** is not included $\frac{p-m}{p}$ of splits, thus for **reasonably small** $m$, the proportion is low which results in de-correlated trees.

!!! Note

    It is common to use the following values for $m$:

    * **Regression Trees**: $m = \frac{p}{3}$
    * **Classification Trees**: $m = \sqrt{p}$

### **Boosting**

Boosting refers to the process of **sequentially training** a series of **shallow trees** on the **error of the previous tree**. The strength of weak trees are "boosted" by **aggregating** them together to form a stronger model.

The key idea is that the errors of the trees contain the effects of the **predictors not included in the model**. Thus, training each successive model on the errors of the prior ones forces the model to **explained the unexplained**.

<!-- Obtained from Research Gate -->
![BOOSTING](Assets/7.%20Tree%20Models.md/BOOSTING.png){.center}

<!-- Obtained from Affine -->
![BOOSTING_VS_BAGGING](Assets/7.%20Tree%20Models.md/BOOSTING_VS_BAGGING.png){.center}

There are three hyperparameters for Boosting:

* **Number of Trees** (b) - How many trees should be used
* **Depth of Trees** (d) - How many splits each tree should have
* **Learning Rate** ($\lambda$) - How much each tree contributes to the overall prediction 

Boosting as a method will result in low bias. However, if too many trees are used, the model could become **overfitted**.

!!! Note

    The learning rate can also be referred to as the **shrinkage parameter** as it reduces the influence of each tree on the overall prediction.
    
    A **small** shrinkage parameter ($\lambda \lt 1$) is preferred so that the learning process is slowed; requiring many trees. 

## **Classification Trees**

Decision Trees can also be used for **categorical predictors**, in which case it is known as a **Classification Tree**. The model assigns a category to the prediction based on the **most common** category within that region of the tree.

!!! Note

    Most of the key concepts apply, but there are some notable exceptions due to having a categorical independent variable.

Since residuals are unintuitive for categorical data, classification trees focus on **Node Purity** instead. The algorithm divides the predictor space such that **node impurity is minimized**.

Purity is a measure of TBC, which can measured using one of the following metrics:

$$
\begin{aligned}
    \text{Classification Error}
    \text{Gini Index}
    \text{Cross Entropy}
\end{aligned}
$$

Two ndes can have the same classification

Gini Index and Cross Entropy are better measures of purity as they are **sensitive to the distribution** of categories within the region.