# **Tree Models**

## **Overview**

Tree Models are the first **supervised AND non-parametric** statistical learning method covered. As such, the model is mainly **algorithmic** in nature.

Broadly speaking, tree models **partition the sample** into several **distinct regions** based on the values of independent variables. For a prediction, the model determines which region the prediction falls in and sets the prediction equal to the **average of the observations within that region**.

Using the **boundaries** of the partitions, a **tree-like structure** can be formed to visualize the model. This is highly datasets with many independent variables as the sample space itself is not easily visualized.

<!-- Obtained from Research Gate -->
![TREE_PARTITION](Assets/7.%20Tree%20Models.md/TREE_PARTITION.png){.center}

!!! Warning

    NOT ALL independent variables in the data will be used to form the tree, only the relatively more important ones.

The different points along the tree are known as **Nodes**:

* **Internal Nodes** (Parent Nodes): Nodes where the tree **splits further**
* **Terminal Nodes** (Child Nodes): Nodes where the tree **no longer splits**

!!! Note

    They can also be interpreted using an actual tree analogy - Roots, Branches & Leaves. In this case, the structure is more of an inverted tree.

!!! Tip

    For the purposes of this exam, assume that **TRUE is on the left** while FALSE is on the right. In practice, either is fine as long as it is clearly defined.

<!-- Self Made -->
![TREE_TERMINOLOGY](Assets/7.%20Tree%20Models.md/TREE_TERMINOLOGY.png){.center}

Mathematically, each region of the tree can also be expressed using a **series of conditions**. For large trees, this becomes tedious, which is why the diagram is generally preferred:

$$
\begin{aligned}
    R_{1} = {y \mid x_{1} \lt a, x_{2} \lt b, x_{3} \lt c, \dots} \\
    R_{2} = {y \mid x_{1} \lt a, x_{2} \lt b, x_{3} \gt c, \dots} \\
    \dots
\end{aligned}
$$

## **Construction**

For the purposes of this exam, there is only one algorithm that is used to construct trees, **Recursive Binary Splitting**:

1. Start with all observations in one region (the entire sample space)
2. Consider **all possible splits** for all predictors
3. For each possibility, compute the **total RSS** from the resulting two regions
4. Select the split with the **lowest total RSS**
5. Repeat steps (2) - (4) for each resulting region
6. **Stop the algorithm** when any of the conditions are met

!!! Note

    The method gets its name from the following properties:

    * **Binary**: One region is always split into two; no more
    * **Recursive**: Splits rely on previously made splits

    The possible split boundaries are not arbitrarily chosen. They are always the **midpoint of between two observations**:

    * $(X_{1}, X_{2}) = {(1,6), (2, 7), (3, 8), \dots}$
    * Split $X_{1}$ at the following: ${1.5, 2.5, \dots}$
    * Split $X_{2}$ at the following: ${6.5, 7.5, \dots}$

!!! Note

    The stopping conditions of the algorithm are arbitrarily chosen by the developer. For instance:

    * Requires some **minimum number of observations** within each region
    * Requires some **minimum reduction of RSS** for each split

In general, the algorithm can be said to choose the **split that minimizes the RSS at the time**. Similar to subset selection, it is a **greedy algorithm** as it chooses the local best choice at each step, not the global best.

## **Pruning**

Even if the stopping conditions are well defined, recursive binary splitting tends to result in **large complex trees** with many internal nodes, which is prone to **overfitting**. In this case, certain **internal nodes should be dropped**, akin to **Pruning the leaves** of a tree.

!!! Note

    Recall that the complexity of a linear regression model was proportional to the **number of predictors** it had. In a similar fashion, the **number of internal nodes** is proportional to its complexity.

...

Notice that the above is akin to the way Ridge and Lasso applies a constraint to the regression. Thus, in a similar fashion, the hyperparameter $\lambda$ can be determined via **Cross Validation**.

## **Alternative Approaches**

### **Bagging**

Bagging refers to **Bootstrap Aggregation**. Bootstrapping is a process whereby "new" samples are created based on the existing sample. Thus, Bagging refers to constructing **multiple trees using bootstrap samples and aggregating their results** to form the final prediction.

### **Random Forest**

One of the primary issues with Bagging is **Correlation**. Since the bootstrapped **samples are all similar** to the underlying sample, it is likely that each bootstrapped tree will contain the **same set of predictors** (especially if there especially strong predictors).

This can be overcome by **limiting each bootstrapped tree** to be constructed only based on a **random subset of the predictors available**, reducing the monopoly of certain predictors and hence reducing the similarities. Hence, the method is called "Random Forest".

!!! Tip

    It can be understood that Bagging is a **special case of Random Forests** where all the predictors are allowed.

### **Boost**


