# **Other Methods**

This section covers other methods that do not fit well with the theme of the other chapters.

## **K-Nearest Neighbours**

**K-Nearest Neighbours** (KNN) is non-parametric method that can be used in a regression or classification setting:

1. Identify the observation in vector space
2. Identify the $k$ **nearest observations** (based on Euclidean Distance)
3. Take the average of the observations in (2) as the prediction (regression) or the most frequent category among them (classification)

<!-- Obtained from Research Gate -->
![KNN](Assets/11.%20Other%20Methods.md/KNN.png){.center}

!!! Tip

    Given a table of data, it might be helpful to draw the scatterplot (to some reasonable scale) to get a sense of the distance.

    If the data has more than three dimensions, manually calculating the euclidean distance is the only method.

$k$ is a measure of **flexibility** - it should not be too large such that observations that are not closely related are used, but it cannot be too small such that it fails to capture the signal.



!!! Warning

    If $k$ is so large that it encompasses the entire dataset $k=n$, then the same prediction will be used for ALL observations

!!! Warning

    Do not confuse K-means clustering and K-Nearest Neighbours! Both contain $k$ but have very different functions.

## Naive Bayes