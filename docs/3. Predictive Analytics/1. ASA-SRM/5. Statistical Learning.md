# **Statistical Learning**

The regression concepts covered in the previous sections are widely considered to be *traditional applied statistics*. In a contemporary context, regression is just one of the many methods that fall under **Statistical Learning**.

It is a framework of harnessing data to gain an **understanding** of how the data is related to one another and/or how a group of variables can be used to accurately **predict** another.

Similar to regression, the relationship between variables can be expressed as a combination of a **Signal Function** and a **Noise** term:

$$
    y = f(x) + \varepsilon
$$

The two terms originate from Engineering, where **Signal** refers to the *meaningful* component of the data while **Noise** refers to the *random variation* that inteferes with the signal.

Statistical learning models are distinguished based on their signal function. There are two main kinds of models:

<center>

| Parametric Models | Non-Parametric |
| :-: | :-: |
| Assumes DV follows a specific functional form | Does not assume any functional form |
| Function parameters determined from the data | - |
| Does not require a large amount of data | Requires a large amount of data to work well |
| EG. Linear Regression | EG. Clustering |

| May not fit the data well | Fits the data well |

| Described by parameters | No parameters used |
| Simple to implement | Requires large amount of data to implement |
| Risk that assumed function is wrong | Does not make any assumption about data |

</center>

They can also be further split according to:

<center>

| Supervised Learning | Unsupervised Learning |
| :-: | :-: |
| Specified DV to supervise the learning | No specific variable chosen |
| Inference/Prediction with respect to the DV | Inference/Prediction for all variables |
| EG. Linear Regression | EG. Clustering |

</center>

Regression vs classification
classification - classifying the observations to a certian level

## **Model Accuracy**

Since the end goal of the model is to make predictions on new unobserved data, the quality of the model should be evaluated against its performance on unobserved data as well.

The **observed data** used to create the model is known as the **Training Data** as it helps *train* the signal function to identify relationships between variables, while the **unobserved data** used to evaluate the model is known as the **Test Data**.

The quality of the model can then be quantified by the **extent to which the model predictions match the data**. Similar to regression, this quantity is known as the **Error** of the model and is summarized through the **Mean Square Error** (MSE) statistic.

The MSE is the **average** of the sum of squared errors and can be calculated for both the training and test data:

$$
\begin{aligned}
    \text{Training MSE} &= \frac{[y_i - \hat{f}(x_0)]^2}{n_{training}} \\
    \text{Test MSE} &= \frac{[y_0 - \hat{f}(x_0)]^2}{n_{test}}
\end{aligned}
$$

> Note that this is different from the MSE defined in ANOVA, where the MS is divded by its degrees of freedom rather than the number of observations. The MSE defined in this section is a general concept while the ANOVA MSE is a purely regression concept.

The Training MSE reflects the goodness of fit of the model while the Test MSE reflects its prediction accuracy. As alluded to earlier, the goal of statistical learning is to choose the model with the **lowest Test MSE**.

In general, the training MSE should always be **smaller** than the testing MSE, which is why they are not interchangeable. This is because all models are **trained to match the training data** to various extents, thus they should naturally have relatively smaller errors.

On the flipside, test MSE should always be **higher** because the model is likely to have **mistakenly captured some of the noise** in the training data that do not generalize to the test data, resulting in a higher testing error.

The extent of the difference is dependent on how well the model *fits* the training data; the extent to which it learns from it:

* **High Flexibility/Complexity** - Tends to overfit the training data; matches data too much; learns too much
* **Low Flexibility/Complexity** - Tends to underfit the training data; matches data too little; learns too little

<!-- Obtained from ACTEX Manual -->
![Model Fir](Assets/5.%20Statistical%20Learning.md/Model%20Fit.png)

This is not to say that low flexibility models are better. In fact, **some level of flexibility is needed** for the model to **pick up most of the signals** in the training data, but not too much such that the noise is captured as well.

Thus, the test MSE generally decreases with flexibility up till a certain point, following which it increases, forming a **U shaped curve**:

<!-- Obtained from ACTEX Manual -->
![Bias Variance Tradeoff](Assets/5.%20Statistical%20Learning.md/Bias%20Variance%20Tradeoff.png)

### **Bias Variance Tradeoff**

The test MSE can be better understood by decomposing it into its consistuent commponents:

$$
    \text{MSE} = \text{Bias}[\hat{f}(x_0)]^2 + \text{Variance}[\hat{f}(x_0)]
$$

The **Bias** of the model (also known as the **Accuracy**) is the difference in expected value of the estimated signal function and the true signal function.

More complex models are better able to capture the signal in the data, thus tends to have a lower bias.

The **Variance** of the model (also known as **Precision**) is the change in the estimated signal function across different datasets. Ideally, the model should have **low variance** such that it would be relatively stable across different training data.

While more complex models are better able to capture signals, this makes them prone to overfitting and hence **more sensitive to differences in the training data**, leading to higher variance.

> Note that although Precision & Accuracy are both synonyms in English, they have distinct meanings in statistics.

<!-- Variance of the error term is the floor -->

<!-- Obtained from ACTEX Manual -->
![Bias VS Variance](Assets/5.%20Statistical%20Learning.md/Bias%20VS%20Variance.png)

Ideally, a model should have both low bias and low variance. However, as explained above, there is an **inherent tension** between Bias and Variance due to the complexity of the model, known as the **Bias-Variance Tradeoff**.

A relatively **simple model** (underfitted) tends to have a **high bias but low variance**. As the complexity increases, the **bias initially decreases more than the variance increases**, causing the test MSE to fall.

At some point, the model becomes **too complex** (overfitted), where the **increase in variance outweighs the fall in bias**, resulting in the U-shaped curve as seen previously.

Thus, the goal is to find an optimal balance in between Bias and Variance where the test MSE is minimized.

## **Resampling Methods**

### Validation Set

### **LOO Cross Validation**

### **K fold Cross Validation**

## **Model Selection**

## **Feature Selection**

### **Forward Stepwise Selection**

### **Backward Stepwise Selection**

### **Stepwise Selection**

## **Shrinkage Methods**