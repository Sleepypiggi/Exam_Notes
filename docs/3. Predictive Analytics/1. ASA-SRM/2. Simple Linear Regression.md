# **Simple Linear Regression**

## **Overview**

One of the most basic forms of statistical learning is known as **Simple Linear Regression** (SLR). It is a **supervised and parametric** method which assumes that the Dependent and Independent variable have a **linear relationship**:

$$
\begin{aligned}
    f &= \beta_{0} + \beta_{1} \cdot X \\
    \\
    \therefore E(Y \mid X = x_{i}) &= \beta_{0} + \beta_{1} \cdot x_{i} \\
    \therefore y_{i} &= \beta_{0} + \beta_{1} \cdot x_{i} + \varepsilon_{i}
\end{aligned}
$$

!!! Info

    The model is called “Simple” because it only contains a *single* independent variable.

## **Interpretation**

The parameters are collectively referred to as the **Regression Coefficients**:

* **Intercept Parameter** ($\beta_{0}$): **Expected value** of $Y$ when $X = 0$
* **Slope Parameter** ($\beta_{1}$): **Change** in **Expected Value** of $Y$ given a **one unit increase** in $X$ 

!!! Warning

    The intepretation uses EXPECTED to emphasise that the model is predicting $E(Y \mid X)$, NOT $Y$ itself.

    The entire line is the expectation, the intercept is just a single point on that line, hence the definition.

## **Model Fitting**

The model is fitted using the **Ordinary Least Squares** (OLS) method. The parameters are estimated such that the resulting fitted model **minimizes the sum of squared residuals**, also known as the **Residual Sum Squared** (RSS):

$$
\begin{aligned}
    \text{RSS}
    &= \sum (y_{i} - \hat{y})^{2} \\
    &= \sum [y_{i} - (\hat{\beta}_{0} + \hat{\beta}_{1} \cdot x_{i})]^{2}
\end{aligned}
$$

!!! Note

    The SQUARED residuals are taken to avoid offsetting effects between the positive and negative residuals as well as to emphasise larger differences.

The minimization is done through calculus by setting the **partial derivatives** for each parameter to 0:

$$
\begin{aligned}
    \frac{\partial}{\partial \beta_{0}} \text{RSS} &= 0 \\
    -2 \sum [y_{i} - (\hat{\beta}_{0} + \hat{\beta}_{1} \cdot x_{i})] &= 0 \\
    \sum y_{i} - \hat{\beta}_{0} \sum 1  - \hat{\beta}_{1} \sum  x_{i} &= 0 \\
    n \bar{y} - \hat{\beta}_{0} \cdot n - \hat{\beta}_{1} \cdot n \bar{x} &= 0 \\
    \\
    \therefore \hat{\beta}_{0} &= \bar{y} - \hat{\beta}_{1} \cdot \bar{x}
\end{aligned}
$$

!!! Tip

    There are two points to note for the above proof:

    1. $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ are **constants** in the above equation
    2. The sum of all observations can be **equivalently expressed** as the average multiplied by the number of observations; $\sum x_{i} = n \cdot \bar{x}$

!!! Info

    By rearranging the terms, it can be shown that the regression model **passes through the mean of the two variables** $(\bar{x}, \bar{y})$:

    $$
        \bar{y} = \hat{\beta}_{0} + \hat{\beta}_{1} \cdot \bar{x}
    $$

$$
\begin{aligned}
    \frac{\partial}{\partial \beta_{1}} \text{RSS} &= 0 \\
    -2 \sum x_{i} [y_{i} - (\hat{\beta}_{0} + \hat{\beta}_{1} \cdot x_{i})] &= 0 \\
    \sum (y_{i} x_{i}) - \hat{\beta}_{0} \sum x_{i} - \hat{\beta}_{1} \sum x^{2}_{i} &= 0 \\
    \sum (y_{i} x_{i}) - (\bar{y} - \hat{\beta}_{1} \bar{x}) \sum x_{i} - \hat{\beta}_{1} \sum x^{2}_{i} &= 0 \\
    \sum (y_{i} x_{i}) - \bar{y} \sum x_{i} + \hat{\beta}_{1} \bar{x} \sum x_{i} - \hat{\beta}_{1} \sum x^{2}_{i} &= 0 \\
    \sum (y_{i} x_{i}) - \bar{y} \cdot n\bar{x} + \hat{\beta}_{1} \bar{x} \cdot n \bar{x} - \hat{\beta}_{1} \sum x^{2}_{i} &= 0 \\
    \sum (y_{i} x_{i}) - n\bar{x}\bar{y} &= \hat{\beta}_{1} \sum (x^{2}_{i}) - n\bar{x}^{2} \\
    \\
    \therefore \hat{\beta}_{1}
    &= \frac{\sum (x_{i} y_{i}) - n\bar{x}\bar{y}}{\sum (x_{i}^{2}) - n\bar{x}^{2}} \\
    &= \frac{\sum (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum (x_{i} - \bar{x})^{2}} \\
    &= r_{x,y} \cdot \frac{s_{y}}{s_{x}}
\end{aligned}
$$

!!! Tip

    Notice that the denominator can be expressed in a few different ways:

    $$
    \begin{aligned}
        \sum (x_{i} - \bar{x})^{2}
        &= \sum (x^{2}_{i}) - n \bar{x}^{2} \\
        &= \sum (x^{2}_{i}) - n (\frac{\sum x_{i}}{n})^{2} \\
        &= \sum (x^{2}_{i}) - n (\frac{(\sum x_{i})^{2}}{n^{2}}) \\
        &= \sum (x^{2}_{i}) - \frac{(\sum x_{i})^{2}}{n}
    \end{aligned}
    $$

    It is important to know alternate formulations because questions will likely only provide certain components such as $\sum (x^{2}_{i})$.
    
    Naturally, the above shortcut is applicable to any “sum of squares” like quantity.

Graphically, the fitted model will be the **line of best fit** that passes through the sample; the line will pass through the **middle of all observations**, with a **roughly equal residuals above and below** the line:

<!-- Obtained from ISLR -->
![BEST_FIT_LINE](Assets/2.%20Simple%20Linear%20Regression.md/BEST_FIT_LINE.png){.center}

## **ANOVA**

### **Goodness of Fit**

**Goodness of Fit** refers to how well the fitted the model can explain the sample data. Residuals are essentially the **unexplained portion** of the sample - the smaller the residuals, the **better the fit** of the model.

The **RSS** from before is used to quantify the residuals. As stated before, the squared nature makes it ideal as it prevents offsetting effects from positive and negative residuals.

However, the RSS on its own is not very meaningful as there is **no indication on what is a "high" or "low" value**. Thus, we consider a **Total Sum of Squares** (TSS) to use as a benchmark:

$$
    \text{TSS} = \sum (y_{i} - \bar{y})^{2}
$$

!!! Note

    The TSS is the RSS for a special model known as the **Null Model**, which ONLY contains the intercept parameter ($\beta_{0}$). Using OLS, the output of the null model will always be the **sample mean of the dependent variable** $\bar{y}$.

    $$
    \begin{aligned}
        y_{i} &= \hat{\beta_{0}} + \hat{\varepsilon} \\
        \\
        -2 \sum (y_{i} - \hat{\beta_{0}}) &= 0 \\
        n \bar{y} - n \hat{\beta_{0}} &= 0 \\
        \hat{\beta_{0}} &= \bar{y} \\
        \\
        \therefore \hat{y} &= \bar{y}
    \end{aligned}
    $$

    Due to this, the RSS will always be **at least smaller than or equal to** the TSS as it contains additional components to explain the variation.

Intuitively, the TSS represents the **inherent variation** in the sample; notice that it is essentially the numerator of the **sample variance**.

The TSS can be then decomposed into two parts to further aid analysis:

* **TSS**: Total Sum of Squares; **inherent variation** in the sample
* **RSS**: Residual Sum of Squares; **unexplained variation** (Hence residual)
* **RegSS**: Regression Sum of Squares; **explained variation** (Hence regression)

$$
\begin{aligned}
    \text{TSS}
    &= \sum (y_{i} - \bar{y})^{2} \\
    &= \sum[(y_{i} - \hat{y}) + (\hat{y} - \bar{y})]^{2} \\
    &= \sum(y_{i} - \hat{y})^{2} + \sum(\hat{y} - \bar{y})^{2} + 2 \sum \left([y_{i} - \hat{y}][\hat{y}-\bar{y}] \right) \\
    &= \sum(y_{i} - \hat{y})^{2} + \sum(\hat{y} - \bar{y})^{2} + 0 \\
    &= \text{RSS} + \text{RegSS}
\end{aligned}
$$

!!! Info

    The proof for why the last term is equal to 0 is beyond the scope of this exam. It is sufficient to know the result.

The Goodness of Fit can be measured by the **proportion of inherent variation that is explained** by the regression, known as the **Coefficient of Determination**:

$$
    R^{2} = \frac{\text{RegSS}}{\text{TSS}} = 1 - \frac{\text{RSS}}{\text{TSS}}
$$

<!-- Self Made -->
![SUM_OF_SQUARES](Assets/2.%20Simple%20Linear%20Regression.md/SUM_OF_SQUARES.png){.center}

!!! Warning

    Some texts may use different terminology for the above terms:

    <center>

    |      **Current Terminology**      |    **Alternate Terminology**    |
    | :-------------------------------: | :-----------------------------: |
    |    Total Sum of Squares (TSS)     |   Sum of Squared Total (SST)    |
    |   Residual Sum of Squares (RSS)   |   Sum of Squared Errors (SSE)   |
    | Regression sum of Squares (RegSS) | Sum of Squared Regression (SSR) |

    </center>

    There is no universally accepted terminology; both are acceptable. **The key is not to confuse RSS with SSR**. They are using the same letters but are completely different.

!!! Note

    There are two **alternate expressions** for the above concepts:

    * RegSS can be expressed as a function of the Slope Parameter
    * Coefficient of determination can be expressed as a function of the sample correlation

    $$
    \begin{align}
        \text{RegSS}
        &= \sum(\hat{y} - \bar{y})^{2} \\
        &= \sum(\hat{\beta}_{0} + \hat{\beta}_{1} \cdot x_{i} - \bar{y})^{2} \\
        &= \sum[(\bar{y} - \hat{\beta}_{1} \cdot \bar{x}) + \hat{\beta}_{1} \cdot x_{i} - \bar{y}]^{2} \\
        &= \sum[\hat{\beta}_{1} (x_{i} - \bar{x})]^{2} \\
        &= \hat{\beta}^{2}_{1} \sum(x_{i} - \bar{x})^{2} \\
    \end{align}
    $$

    $$
    \begin{aligned}
        R^{2}
        &= \frac{\text{RegSS}}{\text{TSS}} \\
        &= \frac{\hat{\beta}^{2}_{1} \sum(x_{i} - \bar{x})^{2}}{\sum (y_{i} - \bar{y})^{2}} \\
        &= \left(\frac{\sum (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum (x_{i} - \bar{x})^{2}}\right)^{2}
            \cdot \frac{\sum(x_{i} - \bar{x})^{2}}{\sum (y_{i} - \bar{y}) ^{2}} \\
        &= \frac{\sum (x_{i} - \bar{x})^{2} (y_{i} - \bar{y})^{2}}{\sum (x_{i} - \bar{x})^4}
            \cdot \frac{\sum(x_{i} - \bar{x})^{2}}{\sum (y_{i} - \bar{y}) ^{2}} \\
        &= \frac{\sum (x_{i} - \bar{x})^{2} (y_{i} - \bar{y})^{2}}{\sum (x_{i} - \bar{x})^{2} \sum (y_{i} - \bar{y})^{2}} \\
        &= \left(\frac{\sum (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum (x_{i} - \bar{x}) \sum (y_{i} - \bar{y})}\right)^{2} \\
        &= r_{x,y}^{2}
    \end{aligned}
    $$

!!! Info

    The decomposition of total variance (TSS) into explainable (RegSS) and unexplainable (RSS) components is known as an Analysis of Variance (ANOVA) process.
    
!!! Warning

    The relationship between R-squared and the sample correlation ONLY holds true for SLR.
    
    This is because R-squared is the squared correlation of $y$ and $\hat{y}$. **ONLY Under SLR**, $\hat{y}$ and $x$ are **perfectly correlated**, thus the two are **interchangeable**:
    
    $$
        R^{2} = r_{y, \hat{y}}^{2} = r_{y,x}^{2}
    $$

### **F-Test**

Using the above ANOVA components, an F-test can be conducted to determine if there is a true relationship between $X$ and $Y$:

* $H_{0}$: $\beta_{1} = 0$
* $H_{1}$: $\beta_{1} \ne 0$

However, an F-test uses **Variance** while the ANOVA components are **sum of squared deviations**. Thus, they can be coverted into “Mean Squares/Variances" by **dividing by their degrees of freedom**:

<center>

|   **Source**   | **Sum of Squares** | **df** |              **Variance/Mean Squared**               |
| :------------: | :----------------: | :----: | :--------------------------------------------------: |
| **Regression** |       RegSS        |  $1$   | $\sigma^{2}_{\text{RegSS}} = \frac{\text{RegSS}}{1}$ |
| **Residuals**  |        RSS         | $n-2$  |   $\sigma^{2}_\text{RSS} = \frac{\text{RSS}}{n-1}$   |
|   **Total**    |        TSS         | $n-1$  |   $\sigma^{2}_\text{TSS} = \frac{\text{TSS}}{n-1}$   |

</center>

!!! Warning

    Some texts refer to the Mean Square of Residuals as the **Mean Square Error** (MSE). This is because residuals and errors are used interchangeably. However, to avoid confusion within the notes, we avoid that notation.

    Also note that the Mean Square of Total is essentially the unbiased sample variance.

!!! Tip

    The degrees of freedom for the different components can be tricky to understand:

    * **TSS** is the deviations of the **observations from the mean**. All $n$ observations in the sample are used, but one degree of freedom has been **used up by the mean**; $\nu = n - 1$.
    * **RSS** is the deviations of the **observations from the prediction**. All $n$ observations in the sample are used, but two degrees of freedom has been **used up by the Intercept and Slope**; $\nu = n-2$.
    * **RegSS** is the deviations of the **prediction from the sample mean**. Since the prediction only relies on a **single additional input** (non-intercept coefficient), there is only one degree of freedom; $\nu = 1$.

    Alternatively, since $\text{TSS} = \text{RegSS} + \text{RSS}$, the degrees of freedom should follow the same sum. Thus, RegSS must have one degree of freedom.

!!! Info

    The estimation of the two parameters involves a minimization problem which sets their first derivative to 0. It can be shown the following two constraints must hold true:

    $$
    \begin{aligned}
        \sum \hat{\varepsilon_{i}} &= 0 \\
        \sum x_{i} \hat{\varepsilon_{i}} &= 0       
    \end{aligned}
    $$

    The first constraint is that the **residuals must sum to 0**. This is intuitive, as the line of best fit is sort of a "mean" of the datapoints, ensuring that the residuals are equally above and below the line.

    The second constraint is that the **dot product of the independent and residuals sum to 0**. This means that the two are **uncorrelated**; all information from $X$ has been fitted inside the model.

Thus, the F-statistic can be constructed:

$$
\begin{aligned}
    \text{F}_{\text{Statistic}}
    &= \frac{\text{Between-Group-Variance}}{\text{Within-Group-Variance}} \\
    &= \frac{\sigma^{2}_{\text{RegSS}}}{\sigma^{2}_\text{RSS}} \\
    \\
    \text{F}_{\text{Statistic}} &\sim F_{1, n-2}
\end{aligned}
$$

The key is understanding that if the null is true (no relationship), then the RegSS and RSS are both noise and will have similar variances. Thus, the ratio of the two will have an F-distribution.

## **Statistical Inference**

### **Sampling Distributions**

Following traditional statistical inference, it is important to assess the quality of the fitted parameters. Thus, it is necessary to understand their **sampling distribution**. Recall the expression for the SLR parameters:

$$
\begin{aligned}
    \hat{\beta}_{0}
    &= \bar{y} - \hat{\beta}_{1} \cdot \bar{x} \\
    \\
    \hat{\beta}_{1}
    &= \frac{\sum (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum (x_{i} - \bar{x})^{2}} \\   
\end{aligned}
$$

Notice that since $X$ is a constant, the two estimators are a **linear combination of $Y$**. It is typically assumed that $Y$ is normally distributed, thus the **estimators are normally distributed** as well.

!!! Note

    Recall that a linear combination of normal variables will still be normally distributed.

The proof of Expectation and Variance is mathematically intensive and beyond the scope of the exam; it is sufficient to know the results:

$$
\begin{aligned}
    E(\hat{\beta_{0}})
    &= \beta_{0}
    \\
    \text{Var}(\hat{\beta_{0}})
    &= \sigma^{2}_{Y} \cdot \left(\frac{1}{n} + \frac{\bar{x}^{2}}{\sum (x_{i} - \bar{x})^{2}} \right)
    \\
    \hat{\beta_{0}} &\sim N \left(E(\hat{\beta_{0}}), \text{Var}(\hat{\beta_{0}}) \right) \\
    \\
    E(\hat{\beta_{1}})
    &= \beta_{1}
    \\
    \text{Var}(\hat{\beta_{1}})
    &= \sigma^{2}_{Y} \cdot \left(\frac{1}{\sum (x_{i} - \bar{x})^{2}} \right)
    \\
    \hat{\beta_{1}} &\sim N \left(E(\hat{\beta_{1}}), \text{Var}(\hat{\beta_{1}}) \right) \\
    \\
    E(\hat{Y})
    &= E(Y \mid X)
    \\
    \text{Var}(\hat{Y})
    &= \sigma^{2}_{Y} \cdot \left(\frac{1}{n} + \frac{(x - \bar{x})^{2}}{\sum (x_{i} - \bar{x})^{2}} \right)
\end{aligned}
$$

!!! Warning

    Note that the Variance of the prediction, the numerator uses $x$ rather than $x_{i}$. This is to reflect that it should be using the **value of $x$ for that specific prediction**.

!!! Tip

    In practice, the actual variance of the error is unknown. Thus, we estimate it by with the **Mean Square of the Residuals** instead:

    $$
    \begin{aligned}
        \widehat{\text{Var}(\hat{\beta_{0}})}
        &= \sigma^{2}_{\text{RSS}} \cdot \left(\frac{1}{n} + \frac{\bar{x}^{2}}{\sum (x_{i} - \bar{x})^{2}} \right) \\
        \\
        \widehat{\text{Var}(\hat{\beta_{1}})}
        &= \sigma^{2}_{\text{RSS}} \cdot \left(\frac{1}{\sum (x_{i} - \bar{x})^{2}} \right) \\
        \\
        \widehat{\text{Var}(\hat{Y})}
        &= \sigma^{2}_{\text{RSS}} \cdot \left(\frac{1}{n} + \frac{(x - \bar{x})^{2}}{\sum (x_{i} - \bar{x})^{2}} \right)
    \end{aligned}
    $$

### **Hypothesis Testing**

Similar to before, the focus is to determine if there is a **true relationship** between $X$ and $Y$:

* $H_{0}$: $\beta_{1} = 0$
* $H_{1}$: $\beta_{1} \ne 0$

!!! Note

    The focus is typically on the Slope parameter rather than the Intercept. The intercept typically does not address any relevant questions.

Since the $\hat{\beta_{1}}$ is normally distributed and the population variance is unknown, the a **t-test** is used to determine the relationship:

$$
\begin{aligned}
    t_{\text{Statistic}} &= \frac{\hat{\beta}_{1} - 0}{\widehat{\text{SE}}(\hat{\beta_{1}})} \\
    t_{\text{Statistic}} &\sim t_{n-2}
\end{aligned}
$$

!!! Tip

    Recall the the degrees of freedom for a t-distribution arises from the **df from the approximation of the sample variance**. In this case, it follows the mean square of the residuals, which has $n-2$ df.

!!! Note

    Given the relationship between the t and F distributions, the two tests covered above will always lead to the same results.

    $$
        t^{2}_{n-2} \sim F_{1, n-2}
    $$

### **Confidence and Prediction Intervals**

Using the sampling distributions for the above estimators, the confidence interval can be constructed using the general expression:

$$
    \text{CI} = \hat{\theta} + t_{\frac{1-\alpha}{2}, n-2} \cdot \text{SE}(\hat{\theta})
$$

!!! Note

    Confidence Intervals can be computed for all $\hat{\beta}_{0}$, $\hat{\beta}_{1}$ AND $\hat{y}$.

    Recall that $\hat{y}$ itself is an estimator of the mean of the conditional distribution, thus has a confidence interval.

For prediction intervals, we consider the sampling distribution of the prediction error:

$$
\begin{aligned}
    y_{i} - \hat{y}_{i} &\sim N \left(0, \text{Var}(y_{i} - \hat{y}_{i}) \right) \\
    \\
    \text{Var}(y_{i} - \hat{y}_{i})
    &= \text{Var}(y_{i}) + \text{Var}(\hat{y}_{i}) \\
    &= \sigma^{2}_{\text{RSS}} + \sigma^{2}_{\text{RSS}} \cdot \left(\frac{1}{n}
        + \frac{(x - \bar{x})^{2}}{\sum (x_{i} - \bar{x})^{2}} \right) \\
    &= \sigma^{2}_{\text{RSS}} \cdot \left(1 + \frac{1}{n} + \frac{(x - \bar{x})^{2}}{\sum (x_{i} - \bar{x})^{2}} \right)
\end{aligned}
$$

!!! Tip

    Notice that the variance of the prediction error is the Variance of the error PLUS the variance of the prediction.

Thus, the prediction interval can be constructed using the same general expression:

$$
    \text{PI} = \hat{y}_{i} + t_{\frac{1-\alpha}{2}, n-2} \cdot \text{SE}(y_{i} - \hat{y}_{i})
$$
