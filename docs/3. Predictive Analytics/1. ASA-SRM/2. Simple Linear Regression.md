# **Simple Linear Regression**

**Simple Linear Regression** (SLR) assumes a **Linear Relationship** between a **Numeric** DV and *single* continuous quantitative IV. The model is considered to be *simple* because it only contains a *single* independent variable.

$$
E(Y|X) = \beta_0 + \beta_1 X
$$

<center>

| $\beta_0$  | $\beta_1$ |
| :-: | :-: |
| **Expected value** of $Y$ when $X = 0$ | **Change in the expected value** of $Y$ given a one unit increase in $X$ |
| Intercept Parameter | Slope Parameter |

</center>

Each observation can also be expressed as sum of the regression and its error term:

$$
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
$$

## **Ordinary Least Squares**

SLR parameters are estimated using the **Ordinary Least Squares** method, which minimizes the **Sum of Squared Residuals** of the fitted model. It is commonly referred to as the **Residual Sum Squared** (RSS).

$$
RSS = \sum y_i - \hat{y} =\sum [y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i)]^2
$$

The minimization is solved through calculus by setting the partial derivatives of the RSS to 0:

For the intercept parameter,

$$
\begin{aligned}
\frac{\partial}{\partial \beta_0} RSS &= 0 \\
-2 \sum [y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i)] &= 0 \\
\sum y_i - \sum \hat{\beta}_0 - \sum \hat{\beta}_1 x &= 0 \\
n\bar{y} -n\hat{\beta}_0 - n\hat{\beta}_1 \bar{x} &= 0 \\
\end{aligned}
$$

$$\therefore \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$$

For the slope parameter,

$$
\begin{aligned}
\frac{\partial}{\partial \beta_1} RSS &= 0 \\
-2 \sum x_i [y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i)] &= 0 \\
\sum (y_i x_i) - \hat{\beta}_0 \sum x_i - \hat{\beta}_1 \sum x^2_i &= 0 \\
\sum (y_i x_i) - (\bar{y} - \hat{\beta}_1 \bar{x}) \sum x_i - \hat{\beta}_1 \sum x^2_i &= 0 \\
\sum (y_i x_i) - n\bar{x}\bar{y} + n\hat{\beta}_1 \bar{x}^2 - \hat{\beta}_1 \sum x^2_i &= 0 \\
\sum (y_i x_i) - n\bar{x}\bar{y} &= \hat{\beta}_1 \sum (x^2_i) - n\bar{x}^2 \\
\end{aligned}
$$

$$
\therefore \hat{\beta}_1
= \frac{\sum (x_i y_i) - n\bar{x}\bar{y}}{\sum (x_i^2) - n\bar{x}^2}
= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
= r * \frac{s_y}{s_x}
$$

This results in the following fitted regression model, which can be graphically expressed as a **Regression Line**:

$$
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_i
$$

<!-- Obtained from ISLR -->
![Fitted Regression Line](Assets/2.%20Simple%20Linear%20Regression.md/Fitted%20Regression%20Line.png){.center}

> Note that it should $\hat{\varepsilon}$ in the above image, not $e_i$.

### **OLS Properties**

By re-arranging the formula for $\hat{\beta}_0$, we can show that **$(\bar{x}, \bar{y})$ always lies** on the fitted regression model:

$$
\bar{y} = \hat{\beta}_0 + \hat{\beta}_1 \bar{x}
$$

Additionally, since the parameters are estimated through minimization, the resulting model **must always fulfil the two first order conditions**. The model thus has $n-2$ degrees of freedom to reflect these "constraints".

<center>

| $\beta_0$ FOC | $\beta_1$ FOC |
|:-:|:-:|
| $\frac{\partial}{\partial \beta_0} = -2 \sum [y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i)] = 0$ | $\frac{\partial}{\partial \beta_1} = -2 \sum x_i [y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i)] = 0$ |
| $\sum \hat{\varepsilon_i} = 0$ | $\sum x_i \hat{\varepsilon_i} = 0$ |
| Residuals are negatively correlated | Residuals and Independendent variables are uncorrelated |

</center>

Using the above results, we can also show the following that the mean of the regression outputs is equal to the mean of the population:

$$
\begin{aligned}
\hat{\varepsilon_i} &= y_i - \hat{y_i} \\
\sum \hat{\varepsilon_i} &= \sum y_i - \hat{y_i} \\
0 &= n\bar{y} - n\bar{\hat{y}} \\
\bar{\hat{y}} &= \bar{y} \\
\end{aligned}
$$

## **Goodness of Fit**

Ideally, the regression should fit the sample closely, having **as small as residuals as possible**. The size of all the residuals in the model can be summarized through the RSS. The lower the RSS, the better the fit of the model.

Recall that the residuals naturally sum to 0 under OLS - the residuals are thus squared to remove the sign so that they can be summed together.

$$
RSS = \sum (y_i - \hat{y})^2
$$

However, the SSR on its own is hard to intepret as there is no indication of how low or high it actually is. Thus, the **Total Sum of Squares** (TSS) can be used as a **benchmark for the RSS** as it is at least equal to or higher than the RSS.

The TSS represents the RSS for a **Null Regression** - a model with containing **only the intercept parameter**. The output of this regression is **always the sample mean $\bar{y}$**, which is used for the computation of its residuals.

It represents the worst possible model which thus has the **highest possible RSS**. The lower the RSS compared to the TSS, the better the fit of the model.

$$
TSS = \sum (y_i - \bar{y})
$$

### **Null Model**

Consider a regression with only the intercept; $\beta_1 = 0$. It is known as the **Null Model** as there are **no independent variables** used.

$$
y = \beta_0
$$

We can estimate $\hat{\beta_0}$ using OLS, which results in the following result:

$$
\begin{aligned}
-2 \sum (y_i - \hat{\beta_0}) &= 0 \\
n \bar{y} - n \hat{\beta_0} &= 0 \\
\hat{\beta_0} &= \bar{y} \\
\end{aligned}
$$

$$
\therefore \hat{y} = \bar{y}
$$

### **Sum of Squares**

The TSS can be further decomposed into two more parts for analysis:

$$
\begin{aligned}
TSS &= \sum (y_i - \bar{y})^2 \\
TSS &= \sum[(y_i - \hat{y}) + (\hat{y}-\bar{y})]^2 \\
TSS &= \sum(y_i - \hat{y})^2 + \sum(\hat{y}-\bar{y})^2 + 2 \sum((y_i - \hat{y})(\hat{y}-\bar{y})) \\
TSS &= \sum(y_i - \hat{y})^2 + \sum(\hat{y}-\bar{y})^2 + 0 \\
TSS &= RSS + RegSS
\end{aligned}
$$

| Residual SS (RSS) | Regression SS (RegSS) |
|:-:|:-:|
| $\sum(\hat{y}-\bar{y})^2$ | $\sum(y_i - \hat{y})^2$ |
| Variation of the observed values about the regression  | Variation of the regression output about the sample mean |
| Variation explained by the regression | Variation unexplained by the regression |

<!-- Obtained from Vitaflux -->
![Sum of Squares](Assets/2.%20Simple%20Linear%20Regression.md/Sum%20of%20Squares.png){.center}

Note that it can also be expressed in terms of the **Slope Parameter**:

$$
\begin{align}
RegSS
&= \sum(\hat{y}-\bar{y})^2 \\
&= \sum(\hat{\beta}_0 + \hat{\beta}_1 x - \bar{y})^2 \\
&= \sum(\bar{y} - \beta_1 \bar{x} + \hat{\beta}_1 x - \bar{y})^2 \\
&= \sum[\hat{\beta}_1 (x_i - \bar{x})]^2 \\
&= \hat{\beta}^2_1 \sum(x_i - \bar{x})^2 \\
\end{align}
$$

The **Coefficient of Determination** $R^2$ can also be used to demonstrate goodness of fit. It measures the **proportion of variation** explained by the regression model:

$$
R^2 = \frac{RegSS}{TSS} = 1 - \frac{RSS}{TSS}
$$

Building off the above expression, it can also be expressed in terms of the **Sample Correlation**:

$$
\begin{align}
R^2
&= \frac{\hat{\beta}^2_1 \sum(x_i - \bar{x})^2}{\sum (y_i - \bar{y}) ^2} \\
&= \left(\frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}\right)^2 \frac{\sum(x_i - \bar{x})^2}{\sum (y_i - \bar{y}) ^2} \\
&= \frac{\sum (x_i - \bar{x})^2 (y_i - \bar{y})^2}{\sum (x_i - \bar{x})^4} \frac{\sum(x_i - \bar{x})^2}{\sum (y_i - \bar{y}) ^2} \\
&= \frac{\sum (x_i - \bar{x})^2 (y_i - \bar{y})^2}{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2} \\
&= \left(\frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x}) \sum (y_i - \bar{y})}\right)^2 \\
&= r_{y,x}^2
\end{align}
$$

### **Degrees of Freedom**

The TSS is based on the naive model with only the intercept parameter, thus, it is subject to the **single contraint** of all residuals summing to 0. The TSS thus has **$n-1$ degrees of freedom**.

The RSS is based on the SLR with both the intercept and slope parameter, thus it is subject to an **additional constraint** of the sumproduct of all residuals and independent variables being 0. The RSS thus has **$n-2$ degrees of freedom.**

The sum of the RSS and RegSS is equal to the TSS, thus the sum of their degrees of freedom must also be equal to that of the TSS. By working backwards, the RegSS thus has **only $1$ degree of freedom**.

### **Mean Squared**

The division of any Sum of Square (TSS, RSS, RegSS) by its Degrees of Freedom is known as the **Mean Squared** (MS), which is a measure of its **average variance**.

The MS of the TSS is the **Unbiased Estimator for Population Variance**, which is why this process is known as the **Analysis of Variance**, as it decomposes the variance of $Y$ into its constituent components:

$$
s = \frac{TSS}{n-1}
$$

The MS of the RSS is known the **Mean Squared Residuals**, often also referred to as the **Mean Squared Error**, as it is an estimate for the population variance of the error $\sigma^2$:

$$
MS_{\text{Residuals}} = \frac{RSS}{n-2}
$$

The MS of the RegSS is known as the **Mean Squared Regression**, which represents the proportion of variance explained per $X$ used.

$$
MS_{\text{Regression}} = \frac{RegSS}{1}
$$

### **F Statistic**

The ANOVA parameters can be used to conduct a hypothesis test to determine if there is in fact a relationship between $X$ and $Y$:

* $H_0$: $\beta_1 = 0$
* $H_1$: $\beta_1 \ne 0$

Under the null hypothesis, there should be **no difference** between the assumed model and a null model as both only contain the intercept parameter, thus $TSS = RSS$, where $RegSS = 0$.

Thus, the F-statistic is testing for the **equality of variance between the TSS and RSS** - if there is a significant difference in the variance of the two, then the null should be rejected and thus $\beta_1 \ne 0$.

The F-statistic can be constructed using the sum of squares:

$$
\begin{aligned}
F
&= \frac{MS_{RegSS}}{MS_{RSS}} \\
&= \frac{RegSS/1}{RSS/(n-2)} \\
&= \frac{(TSS - RSS)/1}{RSS/(n-2)} \\
&= (n-2) * \frac{R^2}{1-R^2}, \text{divide both by TSS}
\end{aligned}
$$

Similar to the variance test, it can be shown that under the null, this test-statistic follows an F distribution with $1$ and $n-2$ degrees of freedom:

$$
\begin{aligned}
T
&= \frac{MS_{RegSS}}{MS_{RSS}} \\
&= \frac{\sigma^2_{RegSS} \frac{MS_{Reg}}{\sigma^2_{RegSS}}}{\sigma^2_{RSS} \frac{MS_{RSS}}{\sigma^2_{RSS}}} \\
&= \frac{\sigma^2_{RegSS}}{\sigma^2_{RSS}} * \frac{\frac{1 * MS_{Reg}}{\sigma^2_{RegSS}} * \frac{1}{1}}{\frac{(n-2) * MS_{RegSS}}{\sigma^2_{RegSS}}* \frac{1}{n-2}} \\
&= \frac{\sigma^2_{RegSS}}{\sigma^2_{RSS}} * \frac{\chi_1}{\chi_{n-2}} * (n-2) \\
&= 1 * F_{1, n-2} * (n-2) \\
&= F_{1, n-2} * (n-2)
\end{aligned}
$$

$$
\therefore F \sim F_{1, n-2}
$$

<!--As with the sample variance case, the key to understanding the proof is that the ratio of $\sigma^2_{RegSS}$ and $\sigma^2_{RSS}$ is 1 u-->

### **ANOVA Table**

All the above information is then summarized in a table for convenience, known as the **ANOVA Table**:

<center>

| Source | Sum of Squares | Degrees of Freedom | Mean Square | F-Statistic |
| :-: | :-: | :-: | :-: | :-: |
| Regression | RegSS | $1$ | $MS_{RegSS}$ | $\frac{RegSS}{MSE}$ |
| Error | RSS | $n-2$ | $MSE$ | - |
| Total | TSS | $n-1$ | $s^2$ | - |

</center>

## **Statistical Inference**

### **Sampling Distributions**

Since the errors are assumed to be normally distributed, then $Y$ is assumed to be normally distributed as well. Since $Y$ is a linear combination of the regression parameters, then the parameters (& their estimates) are **normally distributed** as well.

Both estimates can be **expressed in another form** that makes it *more convenient* to find their expectation & variances.

$$
\begin{aligned}
\hat{\beta}_1
&= \frac{\sum [(x_i - \bar{x})(y_i - \bar{y})]}{\sum (x_i - \bar{x})^2} \\
&= \frac{\sum (x_i - \bar{x})y_i}{\sum (x^2_i - \bar{x})} - \frac{\bar{y} \sum (x_i - \bar{x})}{\sum (x^2_i - \bar{x})} \\
&= \sum \frac{(x_i - \bar{x})}{(x^2_i - \bar{x})}* y_i - 0 \\
&= \sum w_i * y_i \\
\\
\hat{\beta}_0
&= \bar{y} - \hat{\beta}_1 \bar{x} \\
&= \frac{1}{n} \sum y_i - \bar{x} \sum w_i * y_i \\
&= \sum y_i (\frac{1}{n} - \bar{x}w_i)
\end{aligned}
$$

$w_i$ is a sort of "weight" parameter of the sum of squares. It has three interesting properties that makes it useful:

$$
\begin{aligned}
\sum w_i
&= \frac{\sum (x_i - \bar{x})}{\sum (x_i - \bar{x})^2} \\
&= \frac{n\bar{x}-n\bar{x}}{\sum (x_i - \bar{x})^2} \\
&= \frac{0}{\sum (x^2_i - \bar{x})} \\
&= 0 \\
\\
\sum w_i x_i
&= \frac{\sum x_i(x_i - \bar{x})}{\sum (x_i - \bar{x})^2} \\
&= \frac{\sum (x^2_i - \bar{x} \sum x_i)}{\sum x^2_i - 2\bar{x}\sum x_i + \sum \bar{x}^2} \\
&= \frac{\sum x^2_i - \bar{x}(n \bar{x})}{\sum x^2_i - 2\bar{x}(n\bar{x}) + n\bar{x}^2} \\
&= \frac{\sum x^2_i - n\bar{x}^2}{\sum x^2_i - 2n\bar{x}^2 + n\bar{x}^2} \\
&=\frac{\sum x^2_i - n\bar{x}^2}{\sum x^2_i - n\bar{x}^2} \\
&= 1 \\
\\
\sum w_i^2
&= \frac{\sum (x_i - \bar{x})^2}{\sum (x_i - \bar{x})^4} \\
&= \frac{1}{\sum (x_i - \bar{x})^2} \\
\\
\sum (\frac{1}{n} - \bar{x}w_i)
&= n(\frac{1}{n}) - \bar{x} \sum w_i \\
&= 1 - 0 \\
&= 1 \\
\\
\sum (\frac{1}{n} - \bar{x}w_i)x_i
&= \frac{1}{n} \sum x_i - \bar{x} \sum w_i x_i \\
&= \frac{1}{n} (n\bar{x}) - \bar{x} (1) \\
&= \bar{x} - \bar{x} \\
&= 0
\end{aligned}
$$

Using this, the Expectation & Variance can be determined:

$$
\begin{aligned}
E(\hat{\beta}_1)
&= \sum w_i E(y_i) \\
&= \sum w_i E(\beta_0 + \beta_1 x_i) \\
&= \beta_0 \sum w_i + \beta_1 \sum w_i x_i \\
&= \beta_0 (0) + \beta_1 (1) \\
&= \beta_1\\
\\
Var(\hat{\beta}_1)
&= Var(\sum w_i y_i) \\
&= \sum w_i^2 Var (y_i) \\
&= \frac{1}{\sum (x_i - \bar{x})^2} \\
&= \frac{\sigma^2}{\sum (x_i - \bar{x})^2}
\end{aligned}
$$

$$
\therefore \hat{\beta}_1 \sim N(\beta_1, \frac{\sigma^2}{\sum (x_i - \bar{x})^2})
$$

$$
\begin{aligned}
E(\hat{\beta}_0)
&= \sum (\frac{1}{n} - \bar{x}w_i) E(y_i) \\
&= \sum (\frac{1}{n} - \bar{x}w_i) (\beta_0 + \beta_1 x_i) \\
&= \beta_0 \sum (\frac{1}{n} - \bar{x}w_i) + \beta_1 \sum (\frac{1}{n} - \bar{x}w_i)x_i \\
&= \beta_0 (1) + \beta_1 (0) \\
&= \beta_0 \\
\\
Var(\hat{\beta}_0)
&= Var(\sum (\frac{1}{n} - \bar{x}w_i)y_i) \\
&= \sum (\frac{1}{n} - \bar{x}w_i)^2 Var (y_i) \\
&= \sigma^2 \sum (\frac{1}{n^2} -\frac{2\bar{x}w_i}{n} + \bar{x}^2 w_i^2) \\
&= \sigma^2 (\sum \frac{1}{n^2} - \frac{2\bar{x}}{n} \sum w_i + \bar{x}^2 \sum w_i^2) \\
&= \sigma^2 [n(\frac{1}{n^2}) - \frac{2\bar{x}}{n} (0) + \bar{x}^2 (\frac{1}{\sum (x_i - \bar{x})^2})] \\
&= \sigma^2 (\frac{1}{n} + \frac{\bar{x}^2}{\sum (x_i - \bar{x})^2})
\end{aligned}
$$

$$
\therefore \hat{\beta}_0 \sim N(\beta_0, \sigma^2 (\frac{1}{n} + \frac{\bar{x}^2}{\sum (x_i - \bar{x})^2}))
$$

### **Hypothesis Testing**

Since the regression parameters are normally distributed, a z-statistic can also be used to conduct the tests. However, since the population variance is not known, a **t-statistic** is used instead:

$$
\begin{aligned}
t &= \frac{\hat{\beta_1} - \beta_1}{\hat{\sigma}_{\hat{\beta_1}}}
\end{aligned}
$$

Since the population variance is estimated by the MSE which has $n-2$ degrees of freedom, the corresponding chi-squared and hence t-distribution has $n-2$ degrees of freedom as well.

$$
\begin{aligned}
\hat{Var}(\hat{\beta_1})
&= \frac{MS_{RSS}}{\sum (x_i - \bar{x})^2} \\
&= \frac{MS_{RSS}}{\sum (x_i - \bar{x})^2} * \frac{\frac{1}{n-1}}{\frac{1}{n-1}} \\
&= \frac{MS_{RSS}}{(n-1) s^2}
\end{aligned}
$$

$$
t \sim t_{n-2}
$$

Since the square of the t-statistic is the F-statistic, both are **equivalent** ways of doing so and will always lead to the same conclusions.

$$
t^2 \sim F_{1, n-2}
$$

### **Confidence Intervals**

Since the distribution of t-statistic is known, it can be used to determine the confidence interval of the estimate:

$$
P\left(\text{Margin of Error} < \frac{\hat{\beta_1} - \beta_1}{\hat{\sigma}_{\hat{\beta_1}}} < \text{Margin of Error}\right) = 1 - \alpha
$$

$$
\text{Confidence Interval} = \hat{\beta}_1 \pm t_{n-2, \frac{\alpha}{2}} * \hat{\sigma}_{\hat{\beta_1}}
$$

### **Prediction Intervals**

Consider the Prediction Error of the SLR model:

$$
y_* - \hat{y_*} = \varepsilon_* + [(\beta_0 + \beta_1 x_*) - (\hat{\beta_0} + \hat{\beta_1}x_*)]
$$

Since both $y_*$ and $\hat{y_*}$ are normally distributed, the prediction errors are **normally distributed** as well:

$$
\begin{aligned}
E(y_* - \hat{y_*})
&= E[\varepsilon_* + [(\beta_0 + \beta_1 x_*) - (\hat{\beta_0} + \hat{\beta_1}x_*)]] \\
&= 0 + \beta_0 + \beta_1 E(x_*) - \beta_0 - \beta_1 E(x_*) \\
&= 0 \\
\\
Var(y_* - \hat{y_*})
&= Var(\varepsilon_* + [(\beta_0 + \beta_1 x_*) - (\hat{\beta_0} + \hat{\beta_1}x_*)] \\
&= Var(\varepsilon_*) + Var[(\beta_0 + \beta_1 x_*) - (\hat{\beta_0} + \hat{\beta_1}x_*)] \\
&= ... \\
&= \sigma^2 [1 + \frac{1}{n} + \frac{(x_* - \bar{x})^2}{\sum (x_i - \bar{x})^2}]
\end{aligned}
$$

$$
\therefore y_* - \hat{y_*} \sim N\left(0, \sigma^2 [1 + \frac{1}{n} + \frac{(x_* - \bar{x})^2}{\sum (x_i - \bar{x})^2}]\right)
$$

Similar to before, since the population variance is unknown, it can be approximated using the MSE. Thus, a t-statistic constructed from this sample will have a **t-distribution**, allowing the following prediction interval to be calculated:

$$
\text{Prediction Interval} = \hat{y}_* \pm t_{n-2, \frac{\alpha}{2}} * \hat{\sigma}_{y_* - \hat{y_*}}
$$

Notice that the standard error of the prediction interval increases as $x_*$ moves **further away** $\bar{x}$, indicating that the predictions become **less accurate** for those values.
