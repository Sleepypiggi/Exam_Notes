# **Simple Linear Regression**

## **Overview**

One of the most basic forms of statistical learning is known as **Simple Linear Regression** (SLR). It is a **supervised and parametric** method which assumes that the Dependent and Independent variable have a **linear relationship**:

$$
\begin{aligned}
    f &= \beta_{0} + \beta_{1} \cdot X \\
    \\
    \therefore E(Y \mid X = x_{i}) &= \beta_{0} + \beta_{1} \cdot x_{i} \\
    \therefore y_{i} &= \beta_{0} + \beta_{1} \cdot x_{i} + \varepsilon_{i}
\end{aligned}
$$

* **Intercept Parameter** ($\beta_{0}$): **Expected value** of $Y$ when $X = 0$
* **Slope Parameter** ($\beta_{1}$): **Change** in **Expected Value** of $Y$ given a **one unit increase** in $X$ 

!!! Warning

    The intepretation uses EXPECTED to emphasise that the model is predicting $E(Y \mid X)$, NOT $Y$ itself.

!!! Info

    The model is called "Simple" because it only contains a *single* independent variable.

## **Model Fitting**

The model is fitted using the **Ordinary Least Squares** (OLS) method. The parameters are estimated such that the resulting fitted model **minimizes the sum of squared residuals**, also known as the **Residual Sum Squared** (RSS):

$$
\begin{aligned}
    \text{RSS}
    &= \sum (y_{i} - \hat{y})^{2} \\
    &= \sum [y_{i} - (\hat{\beta}_{0} + \hat{\beta}_{1} \cdot x_{i})]^{2}
\end{aligned}
$$

!!! Note

    The SQUARED residuals are taken to avoid offsetting effects between the positive and negative residuals.

The minimization is done through calculus by setting the **partial derivatives** for each parameter to 0:

$$
\begin{aligned}
    \frac{\partial}{\partial \beta_{0}} \text{RSS} &= 0 \\
    -2 \sum [y_{i} - (\hat{\beta}_{0} + \hat{\beta}_{1} \cdot x_{i})] &= 0 \\
    \sum y_{i} - \hat{\beta}_{0} \sum 1  - \hat{\beta}_{1} \sum  x_{i} &= 0 \\
    n \bar{y} - \hat{\beta}_{0} \cdot n - \hat{\beta}_{1} \cdot n \bar{x} &= 0 \\
    \\
    \therefore \hat{\beta}_{0} &= \bar{y} - \hat{\beta}_{1} \cdot \bar{x}
\end{aligned}
$$

!!! Tip

    There are two points to note for the above proof:

    1. $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ are **constants** in the above equation
    2. The sum of all observations can be **equivalently expressed** as the average multiplied by the number of observations; $\sum x_{i} = n \cdot \bar{x}$

$$
\begin{aligned}
    \frac{\partial}{\partial \beta_{1}} \text{RSS} &= 0 \\
    -2 \sum x_{i} [y_{i} - (\hat{\beta}_{0} + \hat{\beta}_{1} \cdot x_{i})] &= 0 \\
    \sum (y_{i} x_{i}) - \hat{\beta}_{0} \sum x_{i} - \hat{\beta}_{1} \sum x^{2}_{i} &= 0 \\
    \sum (y_{i} x_{i}) - (\bar{y} - \hat{\beta}_{1} \bar{x}) \sum x_{i} - \hat{\beta}_{1} \sum x^{2}_{i} &= 0 \\
    \sum (y_{i} x_{i}) - \bar{y} \sum x_{i} + \hat{\beta}_{1} \bar{x} \sum x_{i} - \hat{\beta}_{1} \sum x^{2}_{i} &= 0 \\
    \sum (y_{i} x_{i}) - \bar{y} \cdot n\bar{x} + \hat{\beta}_{1} \bar{x} \cdot n \bar{x} - \hat{\beta}_{1} \sum x^{2}_{i} &= 0 \\
    \sum (y_{i} x_{i}) - n\bar{x}\bar{y} &= \hat{\beta}_{1} \sum (x^{2}_{i}) - n\bar{x}^{2} \\
    \\
    \therefore \hat{\beta}_{1}
    &= \frac{\sum (x_{i} y_{i}) - n\bar{x}\bar{y}}{\sum (x_{i}^{2}) - n\bar{x}^{2}} \\
    &= \frac{\sum (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum (x_{i} - \bar{x})^{2}} \\
    &= r_{x,y} \cdot \frac{s_{y}}{s_{x}}
\end{aligned}
$$

Graphically, the fitted model will be the **line of best fit** that passes through the sample; the line will pass through the **middle of all observations**, with a **roughly equal residuals above and below** the line:

<!-- Obtained from ISLR -->
![BEST_FIT_LINE](Assets/2.%20Simple%20Linear%20Regression.md/Fitted%20Regression%20Line.png){.center}

## **ANOVA**

### **Goodness of Fit**

**Goodness of Fit** refers to how well the fitted the model can explain the sample data. Residuals are essentially the **unexplained portion** of the sample - the smaller the residuals, the **better the fit** of the model.

The **RSS** from before is used to quantify the residuals. As stated before, the squared nature makes it ideal as it prevents offsetting effects from positive and negative residuals.

However, the RSS on its own is not very meaningful as there is **no indication on what is a "high" or "low" value**. Thus, we consider a **Total Sum of Squares** (TSS) to use as a benchmark:

$$
    \text{TSS} = \sum (y_{i} - \bar{y})^{2}
$$

!!! Note

    The TSS is the RSS for a special model known as the **Null Model**, which ONLY contains the intercept parameter ($\beta_{0}$). Using OLS, the output of the null model will always be the **sample mean of the dependent variable** $\bar{y}$.

    $$
    \begin{aligned}
        y_{i} &= \hat{\beta_{0}} + \hat{\varepsilon} \\
        \\
        -2 \sum (y_{i} - \hat{\beta_{0}}) &= 0 \\
        n \bar{y} - n \hat{\beta_{0}} &= 0 \\
        \hat{\beta_{0}} &= \bar{y} \\
        \\
        \therefore \hat{y} &= \bar{y}
    \end{aligned}
    $$

    Due to this, the RSS will always be **at least smaller than or equal to** the TSS as it contains additional components to explain the variation.

Intuitively, the TSS represents the **inherent variation** in the sample; notice that it is essentially the numerator of the **sample variance**.

The TSS can be then decomposed into two parts to further aid analysis:

* **TSS**: Total Sum of Squares; **inherent variation** in the sample
* **RSS**: Residual Sum of Squares; **unexplained variation** (Hence residual)
* **RegSS**: Regression Sum of Squares; **explained variation** (Hence regression)

$$
\begin{aligned}
    \text{TSS}
    &= \sum (y_{i} - \bar{y})^{2} \\
    &= \sum[(y_{i} - \hat{y}) + (\hat{y} - \bar{y})]^{2} \\
    &= \sum(y_{i} - \hat{y})^{2} + \sum(\hat{y} - \bar{y})^{2} + 2 \sum \left([y_{i} - \hat{y}][\hat{y}-\bar{y}] \right) \\
    &= \sum(y_{i} - \hat{y})^{2} + \sum(\hat{y} - \bar{y})^{2} + 0 \\
    &= \text{RSS} + \text{RegSS}
\end{aligned}
$$

!!! Info

    The proof for why the last term is equal to 0 is beyond the scope of this exam. It is sufficient to know the result.

The Goodness of Fit can be measured by the **proportion of inherent variation that is explained** by the regression, known as the **Coefficient of Variation**:

$$
    R^{2} = \frac{\text{RegSS}}{\text{TSS}} = 1 - \frac{\text{RSS}}{\text{TSS}}
$$

<!-- Self Made -->
![SUM_OF_SQUARES](Assets/2.%20Simple%20Linear%20Regression.md/SUM_OF_SQUARES.png){.center}

!!! Warning

    Some texts may use different terminology for the above terms:

    <center>

    |      **Current Terminology**      |    **Alternate Terminology**    |
    | :-------------------------------: | :-----------------------------: |
    |    Total Sum of Squares (TSS)     |   Sum of Squared Total (SST)    |
    |   Residual Sum of Squares (RSS)   |   Sum of Squared Errors (SSE)   |
    | Regression sum of Squares (RegSS) | Sum of Squared Regression (SSR) |

    </center>

    There is no universally accepted terminology; both are acceptable. **The key is not to confuse RSS with SSR**. They are using the same letters but are completely different.

!!! Note

    There are two **alternate expressions** for the above concepts:

    * RegSS can be expressed as a function of the Slope Parameter
    * Coefficient of determination can be expressed as a function of the sample correlation

    $$
    \begin{align}
        \text{RegSS}
        &= \sum(\hat{y} - \bar{y})^{2} \\
        &= \sum(\hat{\beta}_{0} + \hat{\beta}_{1} \cdot x_{i} - \bar{y})^{2} \\
        &= \sum[(\bar{y} - \hat{\beta}_{1} \cdot \bar{x}) + \hat{\beta}_{1} \cdot x_{i} - \bar{y}]^{2} \\
        &= \sum[\hat{\beta}_{1} (x_{i} - \bar{x})]^{2} \\
        &= \hat{\beta}^{2}_{1} \sum(x_{i} - \bar{x})^{2} \\
    \end{align}
    $$

    $$
    \begin{aligned}
        R^{2}
        &= \frac{\text{RegSS}}{\text{TSS}} \\
        &= \frac{\hat{\beta}^{2}_{1} \sum(x_{i} - \bar{x})^{2}}{\sum (y_{i} - \bar{y})^{2}} \\
        &= \left(\frac{\sum (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum (x_{i} - \bar{x})^{2}}\right)^{2}
            \cdot \frac{\sum(x_{i} - \bar{x})^{2}}{\sum (y_{i} - \bar{y}) ^{2}} \\
        &= \frac{\sum (x_{i} - \bar{x})^{2} (y_{i} - \bar{y})^{2}}{\sum (x_{i} - \bar{x})^4}
            \cdot \frac{\sum(x_{i} - \bar{x})^{2}}{\sum (y_{i} - \bar{y}) ^{2}} \\
        &= \frac{\sum (x_{i} - \bar{x})^{2} (y_{i} - \bar{y})^{2}}{\sum (x_{i} - \bar{x})^{2} \sum (y_{i} - \bar{y})^{2}} \\
        &= \left(\frac{\sum (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum (x_{i} - \bar{x}) \sum (y_{i} - \bar{y})}\right)^{2} \\
        &= r_{x,y}^{2}
    \end{aligned}
    $$

!!! Info

    The decomposition of total variance (TSS) into explainable (RegSS) and unexplainable (RSS) components is known as an ANOVA process.

### **F-Test**

Using the above ANOVA components, an F-test can be conducted to determine if there is a true relationship between $X$ and $Y$:

* $H_{0}$: $\beta_{1} = 0$
* $H_{1}$: $\beta_{1} \ne 0$

However, an F-test uses **Variance** while the ANOVA components are **sum of squared deviations**. Thus, they can be coverted into "Variances" by **dividing by their degrees of freedom**:

<center>

|   **Source**   | **Sum of Squares** | **df** |              **Variance/Mean Squared**               |
| :------------: | :----------------: | :----: | :--------------------------------------------------: |
| **Regression** |       RegSS        |  $1$   | $\sigma^{2}_{\text{RegSS}} = \frac{\text{RegSS}}{1}$ |
| **Residuals**  |        RSS         | $n-2$  |   $\sigma^{2}_\text{RSS} = \frac{\text{RSS}}{n-1}$   |
|   **Total**    |        TSS         | $n-1$  |   $\sigma^{2}_\text{TSS} = \frac{\text{TSS}}{n-1}$   |

</center>

!!! Warning

    Some texts refer to the Mean Square of Residuals as the **Mean Square Error** (MSE). This is because residuals and errors are used interchangeably. However, to avoid confusion within the notes, we avoid that notation.

    Also note that the Mean Square of Total is essentially the unbiased sample variance.

!!! Tip

    The degrees of freedom for the different components can be tricky to understand.

    TSS is the deviations of the **observations from the mean**. All $n$ observations in the sample are used, but one degree of freedom has been **used up by the mean**; $\nu = n - 1$.

    RSS is the deviations of the **observations from the prediction**. All $n$ observations in the sample are used, but two degrees of freedom has been **used up by the Intercept and Slope**; $\nu = n-2$.
    
    RegSS is the deviations of the **prediction from the sample mean**. Since the prediction only relies on a **single input**, there is only one degree of freedom; $\nu = 1$.

    Alternatively, since $\text{v} = \text{RegSS} + \text{RSS}$, the degrees of freedom should follow the same sum. Thus, RegSS must have one degree of freedom.

Thus, the F-statistic can be constructed:

$$
\begin{aligned}
    \text{F}_{\text{Statistic}}
    &= \frac{\text{Between-Group-Variance}}{\text{Within-Group-Variance}} \\
    &= \frac{\sigma^{2}_{\text{RegSS}}}{\sigma^{2}_\text{RSS}} \\
    \\
    \text{F}_{\text{Statistic}} &\sim F_{1, n-2}
\end{aligned}
$$

The key is understanding that if the null is true (no relationship), then the RegSS and RSS are both noise and will have similar variances. Thus, the ratio of the two will have an F-distribution.

!!! Tip

    The F-statistic can also be re-expressed as a function of $R^{2}$:

    $$
    \begin{aligned}
        \text{F}_{\text{Statistic}}
        &= \frac{\sigma^{2}_{\text{RegSS}}}{\sigma^{2}_\text{RSS}} \\
        &= \frac{\frac{\text{RegSS}}{1}}{\frac{\text{RSS}}{n-2}} \\
        &= \frac{\text{RegSS}}{\text{RSS}} \cdot (n-2) \\
        &= \frac{\text{TSS} - \text{RSS}}{\text{RSS}} \cdot (n-2) \\
        &= \frac{\frac{\text{TSS} - \text{RSS}}{\text{TSS}}}{\frac{\text{RSS}}{\text{TSS}}} \cdot (n-2) \\
        &= \frac{R^{2}}{1 - R^{2}} \cdot (n-2)
    \end{aligned}
    $$

## **Statistical Inference**

### **Sampling Distributions**

Since the errors are assumed to be normally distributed, then $Y$ is assumed to be normally distributed as well. Since $Y$ is a linear combination of the regression parameters, then the parameters (& their estimates) are **normally distributed** as well.

Both estimates can be **expressed in another form** that makes it *more convenient* to find their expectation & variances.

$$
\begin{aligned}
\hat{\beta}_{1}
&= \frac{\sum [(x_{i} - \bar{x})(y_{i} - \bar{y})]}{\sum (x_{i} - \bar{x})^{2}} \\
&= \frac{\sum (x_{i} - \bar{x})y_{i}}{\sum (x^{2}_{i} - \bar{x})} - \frac{\bar{y} \sum (x_{i} - \bar{x})}{\sum (x^{2}_{i} - \bar{x})} \\
&= \sum \frac{(x_{i} - \bar{x})}{(x^{2}_{i} - \bar{x})}* y_{i} - 0 \\
&= \sum w_{i} * y_{i} \\
\\
\hat{\beta}_{0}
&= \bar{y} - \hat{\beta}_{1} \bar{x} \\
&= \frac{1}{n} \sum y_{i} - \bar{x} \sum w_{i} * y_{i} \\
&= \sum y_{i} (\frac{1}{n} - \bar{x}w_{i})
\end{aligned}
$$

$w_{i}$ is a sort of "weight" parameter of the sum of squares. It has three interesting properties that makes it useful:

$$
\begin{aligned}
\sum w_{i}
&= \frac{\sum (x_{i} - \bar{x})}{\sum (x_{i} - \bar{x})^{2}} \\
&= \frac{n\bar{x}-n\bar{x}}{\sum (x_{i} - \bar{x})^{2}} \\
&= \frac{0}{\sum (x^{2}_{i} - \bar{x})} \\
&= 0 \\
\\
\sum w_{i} x_{i}
&= \frac{\sum x_{i}(x_{i} - \bar{x})}{\sum (x_{i} - \bar{x})^{2}} \\
&= \frac{\sum (x^{2}_{i} - \bar{x} \sum x_{i})}{\sum x^{2}_{i} - 2\bar{x}\sum x_{i} + \sum \bar{x}^{2}} \\
&= \frac{\sum x^{2}_{i} - \bar{x}(n \bar{x})}{\sum x^{2}_{i} - 2\bar{x}(n\bar{x}) + n\bar{x}^{2}} \\
&= \frac{\sum x^{2}_{i} - n\bar{x}^{2}}{\sum x^{2}_{i} - 2n\bar{x}^{2} + n\bar{x}^{2}} \\
&=\frac{\sum x^{2}_{i} - n\bar{x}^{2}}{\sum x^{2}_{i} - n\bar{x}^{2}} \\
&= 1 \\
\\
\sum w_{i}^{2}
&= \frac{\sum (x_{i} - \bar{x})^{2}}{\sum (x_{i} - \bar{x})^4} \\
&= \frac{1}{\sum (x_{i} - \bar{x})^{2}} \\
\\
\sum (\frac{1}{n} - \bar{x}w_{i})
&= n(\frac{1}{n}) - \bar{x} \sum w_{i} \\
&= 1 - 0 \\
&= 1 \\
\\
\sum (\frac{1}{n} - \bar{x}w_{i})x_{i}
&= \frac{1}{n} \sum x_{i} - \bar{x} \sum w_{i} x_{i} \\
&= \frac{1}{n} (n\bar{x}) - \bar{x} (1) \\
&= \bar{x} - \bar{x} \\
&= 0
\end{aligned}
$$

Using this, the Expectation & Variance can be determined:

$$
\begin{aligned}
E(\hat{\beta}_{1})
&= \sum w_{i} E(y_{i}) \\
&= \sum w_{i} E(\beta_{0} + \beta_{1} x_{i}) \\
&= \beta_{0} \sum w_{i} + \beta_{1} \sum w_{i} x_{i} \\
&= \beta_{0} (0) + \beta_{1} (1) \\
&= \beta_{1}\\
\\
Var(\hat{\beta}_{1})
&= Var(\sum w_{i} y_{i}) \\
&= \sum w_{i}^{2} Var (y_{i}) \\
&= \frac{1}{\sum (x_{i} - \bar{x})^{2}} \\
&= \frac{\sigma^{2}}{\sum (x_{i} - \bar{x})^{2}}
\end{aligned}
$$

$$
\therefore \hat{\beta}_{1} \sim N(\beta_{1}, \frac{\sigma^{2}}{\sum (x_{i} - \bar{x})^{2}})
$$

$$
\begin{aligned}
E(\hat{\beta}_{0})
&= \sum (\frac{1}{n} - \bar{x}w_{i}) E(y_{i}) \\
&= \sum (\frac{1}{n} - \bar{x}w_{i}) (\beta_{0} + \beta_{1} x_{i}) \\
&= \beta_{0} \sum (\frac{1}{n} - \bar{x}w_{i}) + \beta_{1} \sum (\frac{1}{n} - \bar{x}w_{i})x_{i} \\
&= \beta_{0} (1) + \beta_{1} (0) \\
&= \beta_{0} \\
\\
Var(\hat{\beta}_{0})
&= Var(\sum (\frac{1}{n} - \bar{x}w_{i})y_{i}) \\
&= \sum (\frac{1}{n} - \bar{x}w_{i})^{2} Var (y_{i}) \\
&= \sigma^{2} \sum (\frac{1}{n^{2}} -\frac{2\bar{x}w_{i}}{n} + \bar{x}^{2} w_{i}^{2}) \\
&= \sigma^{2} (\sum \frac{1}{n^{2}} - \frac{2\bar{x}}{n} \sum w_{i} + \bar{x}^{2} \sum w_{i}^{2}) \\
&= \sigma^{2} [n(\frac{1}{n^{2}}) - \frac{2\bar{x}}{n} (0) + \bar{x}^{2} (\frac{1}{\sum (x_{i} - \bar{x})^{2}})] \\
&= \sigma^{2} (\frac{1}{n} + \frac{\bar{x}^{2}}{\sum (x_{i} - \bar{x})^{2}})
\end{aligned}
$$

$$
\therefore \hat{\beta}_{0} \sim N(\beta_{0}, \sigma^{2} (\frac{1}{n} + \frac{\bar{x}^{2}}{\sum (x_{i} - \bar{x})^{2}}))
$$

### **Hypothesis Testing**

Since the regression parameters are normally distributed, a z-statistic can also be used to conduct the tests. However, since the population variance is not known, a **t-statistic** is used instead:

$$
\begin{aligned}
t &= \frac{\hat{\beta_{1}} - \beta_{1}}{\hat{\sigma}_{\hat{\beta_{1}}}}
\end{aligned}
$$

Since the population variance is estimated by the MSE which has $n-2$ degrees of freedom, the corresponding chi-squared and hence t-distribution has $n-2$ degrees of freedom as well.

$$
\begin{aligned}
\hat{Var}(\hat{\beta_{1}})
&= \frac{MS_{RSS}}{\sum (x_{i} - \bar{x})^{2}} \\
&= \frac{MS_{RSS}}{\sum (x_{i} - \bar{x})^{2}} * \frac{\frac{1}{n-1}}{\frac{1}{n-1}} \\
&= \frac{MS_{RSS}}{(n-1) s^{2}}
\end{aligned}
$$

$$
t \sim t_{n-2}
$$

Since the square of the t-statistic is the F-statistic, both are **equivalent** ways of doing so and will always lead to the same conclusions.

$$
t^{2} \sim F_{1, n-2}
$$

### **Confidence Intervals**

Since the distribution of t-statistic is known, it can be used to determine the confidence interval of the estimate:

$$
P\left(\text{Margin of Error} < \frac{\hat{\beta_{1}} - \beta_{1}}{\hat{\sigma}_{\hat{\beta_{1}}}} < \text{Margin of Error}\right) = 1 - \alpha
$$

$$
\text{Confidence Interval} = \hat{\beta}_{1} \pm t_{n-2, \frac{\alpha}{2}} * \hat{\sigma}_{\hat{\beta_{1}}}
$$

### **Prediction Intervals**

Consider the Prediction Error of the SLR model:

$$
y_* - \hat{y_*} = \varepsilon_* + [(\beta_{0} + \beta_{1} x_*) - (\hat{\beta_{0}} + \hat{\beta_{1}}x_*)]
$$

Since both $y_*$ and $\hat{y_*}$ are normally distributed, the prediction errors are **normally distributed** as well:

$$
\begin{aligned}
E(y_* - \hat{y_*})
&= E[\varepsilon_* + [(\beta_{0} + \beta_{1} x_*) - (\hat{\beta_{0}} + \hat{\beta_{1}}x_*)]] \\
&= 0 + \beta_{0} + \beta_{1} E(x_*) - \beta_{0} - \beta_{1} E(x_*) \\
&= 0 \\
\\
Var(y_* - \hat{y_*})
&= Var(\varepsilon_* + [(\beta_{0} + \beta_{1} x_*) - (\hat{\beta_{0}} + \hat{\beta_{1}}x_*)] \\
&= Var(\varepsilon_*) + Var[(\beta_{0} + \beta_{1} x_*) - (\hat{\beta_{0}} + \hat{\beta_{1}}x_*)] \\
&= ... \\
&= \sigma^{2} [1 + \frac{1}{n} + \frac{(x_* - \bar{x})^{2}}{\sum (x_{i} - \bar{x})^{2}}]
\end{aligned}
$$

$$
\therefore y_* - \hat{y_*} \sim N\left(0, \sigma^{2} [1 + \frac{1}{n} + \frac{(x_* - \bar{x})^{2}}{\sum (x_{i} - \bar{x})^{2}}]\right)
$$

Similar to before, since the population variance is unknown, it can be approximated using the MSE. Thus, a t-statistic constructed from this sample will have a **t-distribution**, allowing the following prediction interval to be calculated:

$$
\text{Prediction Interval} = \hat{y}_* \pm t_{n-2, \frac{\alpha}{2}} * \hat{\sigma}_{y_* - \hat{y_*}}
$$

Notice that the standard error of the prediction interval increases as $x_*$ moves **further away** $\bar{x}$, indicating that the predictions become **less accurate** for those values.

### **OLS Properties**

By re-arranging the formula for $\hat{\beta}_{0}$, we can show that **$(\bar{x}, \bar{y})$ always lies** on the fitted regression model:

$$
\bar{y} = \hat{\beta}_{0} + \hat{\beta}_{1} \bar{x}
$$

Additionally, since the parameters are estimated through minimization, the resulting model **must always fulfil the two first order conditions**. The model thus has $n-2$ degrees of freedom to reflect these "constraints".

<center>

|                                             $\beta_{0}$ FOC                                             |                                                $\beta_{1}$ FOC                                                |
| :-----------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------------: |
| $\frac{\partial}{\partial \beta_{0}} = -2 \sum [y_{i} - (\hat{\beta}_{0} + \hat{\beta}_{1} x_{i})] = 0$ | $\frac{\partial}{\partial \beta_{1}} = -2 \sum x_{i} [y_{i} - (\hat{\beta}_{0} + \hat{\beta}_{1} x_{i})] = 0$ |
|                                    $\sum \hat{\varepsilon_{i}} = 0$                                     |                                    $\sum x_{i} \hat{\varepsilon_{i}} = 0$                                     |
|                                   Residuals are negatively correlated                                   |                            Residuals and Independendent variables are uncorrelated                            |

</center>

Using the above results, we can also show the following that the mean of the regression outputs is equal to the mean of the population:

$$
\begin{aligned}
\hat{\varepsilon_{i}} &= y_{i} - \hat{y_{i}} \\
\sum \hat{\varepsilon_{i}} &= \sum y_{i} - \hat{y_{i}} \\
0 &= n\bar{y} - n\bar{\hat{y}} \\
\bar{\hat{y}} &= \bar{y} \\
\end{aligned}
$$