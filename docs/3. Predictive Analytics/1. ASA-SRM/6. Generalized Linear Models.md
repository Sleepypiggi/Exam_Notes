# **Generalized Linear Models**

## **Overview**

Linear Regression assumes that the dependent variable **can change constantly and indefinitely** in either direction, which is not appropriate for certain quantities that **changes non-linearly** or for values that can **only take values of a certain range** (EG. Positive only or between 0 and 1).

Another assumption made is that the dependent variable is normally distributed. Despite it being one of the most common distributions in reality, not all quantities follow it.

Thus, a **Generalized Linear Model** (GLM) overcomes the above two constraints by:

1. Allowing the dependent variable to have a distribution from the **Linear Exponential Family** (LEF)
2. Specifying a **link function** that relates the dependent to the linear equation, allowing it to change **non-linearly**

<!-- Obtained from Daily Dose of Data Science -->
![SLR_VS_GLM](Assets/6.%20Generalized%20Linear%20Models.md/SLR_VS_GLM.png){.center}

## **Key Concepts**

### **Linear Exponential Family**

The LEF of distributions encompasses a wide range of distributions, each with their own **unique parameterization**. However, they have common characteristics that allow them them to be **re-parameterized** into a common format:

$$
\begin{aligned}
    \text{Y} &\sim \text{LEF}(\theta, \phi) \\
    \\
    f(Y \mid \theta, \phi) &= \exp \left(\frac{y \theta - b(\theta)}{\phi} + a(y, \phi) \right) \\
    \\
    E(Y) &= b'(\theta) \\
    \text{Var}(Y) &= \phi \cdot b''(\theta)
\end{aligned}
$$

!!! Info

    Apart from providing a common framework, re-paramterizing this way has certain mathematical properties which makes the resulting calculations smoother. It is not necessary to understand the intricate mathematical details.

!!! Tip

    The two paraemters are referred to as the **Canonical ($\theta$) and the Dispersion ($\phi$) Parameters** respectively. They have no clear interpretation, it is sufficient to know that they affect the **mean and variance** respectively.

    Consequently, the first and second derivatives are known as the **Mean and Variance Function** respectively. The key is remembering that the Variance is **not entirely determined by the variance function**, unlike the mean.

    It is typically assumed that $\phi$ is a **known constant**.

<!-- Obtained from Daily Dose of Data Science -->
![LEF](Assets/6.%20Generalized%20Linear%20Models.md/LEF.png){.center}

### **Link Functions**

Under linear regression, the model **directly affects** the dependent variable. Since the model changes linearly, the dependent variable **must change linearly** as well:

$$
    E(Y \mid X)
    = \beta_{0} + \beta_{1} \cdot x_{1} + \beta_{2} \cdot x_{2} + \dots
$$

Thus, this can be overcome by introducing an intermediate **Link Function** between the dependent variable and the model output. Thus, even if the model changes linearly, the link function can cause a **non-linear impact on the dependent variable**:

$$
\begin{aligned}
    E(Y \mid X)
    &= g^{-1}(\beta_{0} + \beta_{1} \cdot x_{1} + \beta_{2} \cdot x_{2} + \dots) \\
    &= g^{-1}(\boldsymbol{X}^{T} \boldsymbol{\beta}) 
\end{aligned}
$$

!!! Note

    By definition, the link functions maps the dependent variable to the linear predictors. Hence, going the other way round, it is known as the **Inverse Link Function**.

The key consideration when choosing a link function is the **domain**:

* Linear Predictor generally has an **unrestricted domain** $(-\infty, \infty)$
* Dependent Variable generally has a **restricted domain** $(a, b)$
* Link function must be able to **map the two domains together**

!!! Note

    Another consideration is **interpretability**. Since link functions directly impact the dependent variable, it is desirable to use a **simpler function** (all else equal) to ease understanding.

<!-- Self Made -->
![LINK_FUNCTION](Assets/6.%20Generalized%20Linear%20Models.md/LINK_FUNCTION.png){.center}

!!! Tip

    The existence of link functions means that the intepretation of regression coefficients is not the same as OLS. The effect on the predictor **must consider the link function** as well.

Within the subset of possible link functions, there exists a **Canonical Link Function** which is the **Inverse Mean Function**. This means that the linear predictor will be equivalent to the **canonical parameter** for the distribution:

$$
\begin{aligned}
    \boldsymbol{X}^{T} \boldsymbol{\beta}
    &= g \left[E(Y \mid X) \right] \\
    &= b'^{-1} \left[E(Y \mid X) \right] \\
    &= b'^{-1} \left[b'(\theta) \right] \\
    &= \theta
\end{aligned}
$$

!!! Tip

    The canonical link has certain mathematical properties that will **ease the parameter estimation process**, which is why it is generally preferred.

<!-- Self Made -->
![CANONICAL_LINK](Assets/6.%20Generalized%20Linear%20Models.md/CANONICAL_LINK.png){.center}

The following table summarizes the key distributions and their corresponding link functions:

<!-- Obtained from Coaching Actuaries -->
![COMMON_EXAMPLES](Assets/6.%20Generalized%20Linear%20Models.md/COMMON_EXAMPLES.png){.center}

!!! Tip

    Any **multiplicative constant** within the canonical link function itself does not make a difference to the result. An expression **with or without** the constant can be deemed as the Canonical Link function.

!!! Note

    Recall from earlier exams that the **Exponential Distribution** is a special case of the Gamma distribution. Thus, their canonical link functions are the same.

!!! Warning

    Although most GLMs will use their canonical link, it is NOT a requirement that they will. Always read the question to check which link is being used!

### **Maximum Likelihood**

The parameters are estimated using **Maximum Likelihood Estimation** (MLE), where the likelihood function can be expressed as the following:

$$
\begin{aligned}
    \ell(\beta)
    &= \ln \prod \exp \left(\frac{y \theta - b(\theta)}{\phi} + a(y, \phi) \right) \\
    &= \sum \ln \exp \left(\frac{y \theta - b(\theta)}{\phi} + a(y, \phi) \right) \\
    &= \sum \left(\frac{y \theta - b(\theta)}{\phi} + a(y, \phi) \right) \\
    \\
    \frac{\partial^{2} \beta}{\partial \beta^{2}} &= 0
\end{aligned}
$$

Since $\beta$ and $\theta$ are linked, solving for $\beta$ provides $\theta$ as well. It is typically asummed that $\phi$ is 1 for most key distributions.

!!! Info

    There are **no closed form solutions** to the above system of equations, they are typically solved through numerical methods. It is not necessary to know the details for this exam.

!!! Note

    Recall that MLE achieves **Asymptotic Normality**. This will be the basis used for GLM-related statistical inference. The actual normal distribution will be used (rather than t-distribution) because in the limit, the variance of the normal distribution is **known**.

    However, the "adequateness" of the asymptotic results are dependent on the **choice of distribution**, but not the distribution of the underlying data:

    * Consider a **continuous** dataset with **equal mean and variance**
    * The key is to choose a distribution with **equal mean and variance as well**, regardless of what the underlying distribution is
    * For instance, if a poisson distribution was used, it is technically "wrong" because it assumes discrete data while the underlying data is continuous, but since it models the mean and variance correctly, it can be used

### **Deviance**

For linear rgeression, the parameters were estimated based on the residuals; thus, the goodness of fit was measured using the residuals via ANOVA. Similarly, since GLMs use MLE, their fit should be **measuring using likelihood**.

Consider an extreme case of GLM where the **number of parameters in the model is exactly equal to the number observations**, causing it to **fit the data perfectly**. This is known as a **Saturated Model**. This saturated model obviously suffers from overfitting (as it perfectly captures the noise in each sample), thus should not be used for prediction.

However, it can be used as a **benchmark** for the total information available in the sample and consequently **how much is lost** through the fitting process. This is measured via the **Deviance** of the model, which is the **difference in the likelihood** of the two models:

$$
    D^{*} = 2 \cdot (\ell_{\text{Saturated}} - \ell_{\text{Fitted}})
$$

!!! Tip

    The above are the **MAXIMIZED log-likelihoods**, which uses the likelihoods with the **MLE estimate**:

    * **Saturated**: Uses actual observations (Since a saturated model fits the data perfectly)
    * **Fitted**: Uses fitted observations

    Recall that since the likelihood is already a sum for each observation, the deviance for a particular observation is known as the **Deviance Residuals**.

    $$
        D^{*} = \sum D^{*}_{i}
    $$

The fitted model should have **sufficiently high likelihood** to ensure that it captures the signals, but be reasonably smaller than the saturated model to prevent overfitting. Thus, models with **smaller (but NOT too small) deviance** are preferred.

!!! Tip

    The subtraction of the two likelihoods would cause the second term in the probability function to cancel, leaving only the difference in the numerators:

    $$
        D^{*} = \frac{D}{\varphi}
    $$

    Thus, the most accurate terminology would be that the difference in the two likelihoods is known as the **Scaled Deviance**, while the differences in the numerator only is the **Deviance**.

    However, since $\varphi$ is **typically assumed to be 1** for most distributions used in SRM, the two are **identical**.

!!! Warning

    Another reason is that under GLM, the total variance **CANNOT be decomposed** into Explained and Unexplained components; there is an addittional component:

    $$
    \begin{aligned}
        \text{TSS}
        &= \sum(y_{i} - \hat{y})^{2} + \sum(\hat{y} - \bar{y})^{2}
            + 2 \sum \left([y_{i} - \hat{y}][\hat{y}-\bar{y}] \right) \\
        &= \text{RSS} + \text{RegSS} + 2 \sum \left([y_{i} - \hat{y}][\hat{y}-\bar{y}] \right)
    \end{aligned}
    $$

    The above additional component would cancel out under OLS, thus was not issue. However, this does not apply to GLMs. Thus, the usual **ANOVA inferences do NOT apply in a GLM context**.

!!! Note

    Consider the Likelihood for the following two distributions - Binomial and Poisson. The key is understanding that whenever the predicted quantity ($\pi$ or $\lambda$) comes up in the equation:

    * **Saturated Likelihood**: Fill in the actual observation (since perfectly predict)
    * **Fitted Likelihood**: Fill in the predicted value
    * Terms with JUST the observed value will naturally cancel out during the deviance process

    $$
    \begin{aligned}
        L_{\text{Binomial}} &= \sum (n \choose y) \pi^{y} (1 - \pi)^{n-y} \\
        \ell_{\text{Binomial}} &= \sum y \ln \pi + (n-y) \ln (1 - \pi) \\
        \ell_{\text{Poisson, Saturated}} &= \sum y_{i} \ln y_{i} + (n - y_{i}) \ln (1 - y_{i}) \\
        \ell_{\text{Poisson, Fitted}} &= \sum y_{i} \ln \hat{y}_{i} + (n - \hat{y}_{i}) \ln (1 - \hat{y}_{i}) \\
        \\
        \therefore D^{*}_{\text{Binomial}}
        &= 2 \cdot (\ell_{\text{Binomial, Saturated}} - \ell_{\text{Binomial, Fitted}}) \\
        &= 2 \cdot \sum \left[ (y_{i} \ln y_{i} - y_{i} \ln \hat{y}_{i}) + ((n - y_{i}) \ln (1 - y_{i}) - (n - \hat{y}_{i}) \ln (1 - \hat{y}_{i})) \right] \\
        &= 2 \cdot \sum \left[ y_{i} \ln \frac{y_{i}}{\hat{y}_{i}} + (n - y_{i}) \ln \frac{1 - y_{i}}{1 - \hat{y}_{i}} \right] \\
        \\
        L_{\text{Poisson}} &= \sum \frac{\lambda^{y} e^{-\lambda}}{y!} \\
        \ell_{\text{Poisson}} &= \sum y \ln \lambda - \lambda - \ln y! \\
        \ell_{\text{Poisson, Saturated}} &= \sum y_{i} \ln y_{i} - y_{i} - \ln y_{i}! \\
        \ell_{\text{Poisson, Fitted}} &= \sum y_{i} \ln \hat{y}_{i} - \hat{y}_{i} - \ln y_{i}! \\
        \\
        \therefore D^{*}_{\text{Poisson}}
        &= 2 \cdot (\ell_{\text{Poisson, Saturated}} - \ell_{\text{Poisson, Fitted}}) \\
        &= 2 \cdot \sum \left[ (y_{i} \ln y_{i} - y_{i} \ln \hat{y}_{i}) - (y_{i} - \hat{y}_{i}) - (\ln y_{i}! - \ln y_{i}!) \right] \\
        &= 2 \cdot \sum \left[ y_{i} \ln \left(\frac{y_{i}}{\hat{y}_{i}} \right) - (y_{i} - \hat{y}_{i}) \right] \\
    \end{aligned}
    $$

The concept of deviance is similar to goodness of fit, thus can be used similarly to R-squared, known as **Pseduo R-squared**:

$$
    R^{2}_{\text{Pse}}
    = \frac{\ell_{\text{fitted}} - \ell_{0}}{\ell_{\text{Sat}} - \ell_{0}}
$$

!!! Note

    $\ell_{0}$ is assumed to the likelihood for the **null model**.

## **Statistical Inference**

### **Likelihood Ratio Test**

Similar to how F-tests made use of the ANOVA results to perform statistical inference, GLMs can make use of the Deviance results in a similar fashion, via a **Likelihood Ratio Test (LRT)**, which compares a pair of **NESTED GLMs**:

* $H_{0}$: $\beta_{q} = 0$
* $H_{1}$: $\beta_{q} \ne 0$

$$
\begin{aligned}
    \chi_{\text{Statistic}}
    &= 2 \cdot (\ell_{\text{Full}} - \ell_{\text{Restricted}}) \\
    &= 2 \cdot (\ell_{\text{Full}} - \ell_{\text{Saturated}} + \ell_{\text{Saturated}} - \ell_{\text{Restricted}}) \\
    &= 2 \cdot (\ell_{\text{Full}} - \ell_{\text{Saturated}}) + 2 \cdot (\ell_{\text{Saturated}} - \ell_{\text{Restricted}}) \\
    &= - 2 \cdot (\ell_{\text{Saturated}} - \ell_{\text{Full}}) + 2 \cdot (\ell_{\text{Saturated}} - \ell_{\text{Restricted}}) \\
    &= 2 \cdot (\ell_{\text{Saturated}} - \ell_{\text{Restricted}}) - 2 \cdot (\ell_{\text{Saturated}} - \ell_{\text{Full}}) \\
    &= D_{\text{Restricted}} - D_{\text{Full}} \\
    \\
    \chi_{\text{Statistic}} &\sim \chi^{2}_{p_{\text{Full}} - p_{\text{Restricted}}}
\end{aligned}
$$

If there is a **significant drop in likelihood** between the two models, then the reduced predictors are significant and thus should not be dropped from the model.

!!! Warning

    Even though Ratio is in the name, the test-statistic is NOT ratio of likelihoods.

!!! Tip

    For the purposes of this exam, it is typically assumed that the restricted model is the null model; similar overall set-up to an F-test.

### **Pearson Residuals**

Pearson Residuals are the raw residuals scaled by the standard deviation:

$$
    e_{\text{Pearson}} = \frac{y - \hat{y}}{\sqrt{\text{Var}(y)}}
$$

!!! Warning

    The standard deviation is from the variance of the underlying distribution, NOT of the coefficients or the sort.
    
    * Binomial: $\text{Var(Y)} = np(1-p)$
    * Poisson: $\text{Var(Y)} = \mu$

If the model fits the data well, the sum of squared pearson residuals should be **roughly equal to its degrees of freedom** ($n-p-1$):

$$
    \sum e^{2}_{\text{Pearson}} \sim n-p-1
$$

!!! Warning

    The sum of squared pearson residuals is also known as the Pearson chi square statistic. Although the names are rightfully similar, do NOT confuse the above with the Pearson Chi-Square Goodness of fit test; they are quantifying DIFFERENT things:

    * **Sum of Pearson Residuals**: Determine if the **fitted GLM** is a good fit for the data
    * **Chi Square Goodness of fit**: Determine if a **chosen distribution** is a good fit for the data

!!! Note

    The reason why "normal" residuals are not useful in GLMs is because each model follows a **different distribution**, thus there are is **no uniform behaviour for raw residuals**.

!!! Tip

    As they are stills residuals, they can be used in a **similar way** - to find outliers or identify misspecification.

### **Overdispersion**

Dispersion is a measure of the **variability of the actual data** compared to the assumed variability in the chosen distribution. If the variability is larger, then it is known as **Overdispersion**, vice versa. This affects the standard error of the estimates:

* **Overdispersion**: Standard Errors understated; False positives
* **Underdispersion**: Standard Errors overstated; False negatives

One method to overcome this is to multiply a the **squared pearson statistic** ($\delta$) to the variance to **re-scale** the variance to an appropriate level

$$
\begin{aligned}
    \delta &= \frac{\sum e^{2}_{\text{Pearson}}}{n-p-1} \\
    \\
    \therefore \text{Var(Y')} &= \delta \cdot \text{Var}(Y)
\end{aligned}
$$

!!! Warning

    The above method only works for distributions which have a **restrictive variance** (related to the mean). If the variance is unrestricted (EG. Normal), the adjustment would not solve the mismatch in dispersion.

!!! Tip

    Note that this means that the pearson chi-squared statistic can also be used to **indicate when large over-dispersion is present**, since the adjustment required is directly related to it.

The above **alters the variance of the estimates** to match the data ($\beta$ is unaffected):

* Affects the standard errors and hence resulting statistical inferences
* Affects interpretation as it cannot be interpreted as the original distribution 

!!! Note

    This also introduces uncertainty into the variance, thus the normally asymptotic distribution should NOT be used; a t-distribution should be used instead.

## **Categorical Models**

### **Binary Response**

A Binary response variable has only two outcomes (0,1), thus a **Bernoulli Distribution** is used to model it. There are **three main link functions** that can map the the range into (0,1):

* **Logit**: $\ln \frac{\mu}{1-\mu}$
* **Probit**: $\Phi^{-1}(\mu)$
* **Complementary Log-Log**: $\ln[- \ln(1-\mu)]$

<!-- Obtained from Coaching Actuaries -->
![BINARY_LINK](Assets/6.%20Generalized%20Linear%20Models.md/BINARY_LINK.png){.center}

!!! Tip

    Notice the similarities between the three:

    * **Negative IV**: Complementary Log-Log & Logit are similar
    * **Positive IV**: Complementary Log-Log & Probit are similar

    Generally speaking, it is **NOT EASY** to distinguish between the three methods graphically.
    
!!! Tip

    Logit and Probit are the most popular methods. However, logit is preferred as it can be expressed in a closed form.
    
!!! Note

    Despite being called logistic *regression*, it is actually a classification method.   

Logit is the canonical link function; the resulting GLM is known as a **Logistic Regression**. It models the **Log-Odds** of an event occuring:

$$
\begin{aligned}
    \ln \frac{1-\mu}{\mu} &= \beta_{0} + \beta_{1} \cdot X_{1} + \dots \\
    \frac{1-\mu}{\mu} &= \exp (\beta_{0} + \beta_{1} \cdot X_{1} + \dots) \\
    \text{Odds} &= \exp (\beta_{0} + \beta_{1} \cdot X_{1} + \dots) \\
    \\
    \therefore \ln \text{Odds} &= Y \\
    \frac{1-\mu}{\mu} &= e^{Y} \\
    \mu &= e^{Y} - \mu \cdot e^{Y} \\
    \mu &= \frac{e^{Y}}{1 + e^{Y}} \\
    \text{P(A)} &= \frac{e^{Y}}{1 + e^{Y}}
\end{aligned}
$$

!!! Note

    Odds is the **ratio of the probability** of an event occuring to it not occuring:

    $$
        \text{Odds} = \frac{\text{P(A)}}{1 - \text{P(A)}}
    $$

    It is an alternative way of representing probabilities, typically used in a gambling context.

It can be shown that the exponent of the regression coefficient is the **multiplicate increase in the odds** given a one unit increae in that variable (continuous) or presence of that variable (categorical):

$$
\begin{aligned}
    \frac{\text{Odds}_{1}}{\text{Odds}_{0}}
    &= \frac{\exp (\beta_{0} + \beta_{1} \cdot (X_{1}+1))}{\exp (\beta_{0} + \beta_{1} \cdot X_{1})} \\
    &= \exp (\beta_{1}) \\
    \\
    \text{Percentage Change} &= \exp (\beta_{1}) - 1
\end{aligned}
$$

!!! Note

    The primary motivation for the Logistic Regression is because a **regular linear model** is unable to effectively model dummy variables due to the following limitations:

    * **Domain Mismatch**: Linear models range from negative to positive infinity; Probabilities can only range from 0 to 1
    * **Heteroscedasticity**: Linear models require homoscedasticity; Bernouilli exhibits heteroscedasticity (variance function linked to mean)
    * **Invalid Residuals**: Linear models require continuous residuals; Residuals for probabilities dont make sense

### **Nominal Response**

For a variable with more than two levels, a **Generalized Logit Model** is used. Rather than odds, it models the log of the **relative probability** of the target category and reference category.

For a variable with $k$ categories, $k-1$ models will be fitted, for each of the **non-reference** categories:

$$
\begin{aligned}
    \ln \left(\frac{P(1)}{P(k)} \right) &= \beta_{1} + \beta_{1, 1} \cdot X \\
    \ln \left(\frac{P(2)}{P(k)} \right) &= \beta_{2} + \beta_{1, 2} \cdot X \\
    \ln \left(\frac{P(3)}{P(k)} \right) &= \beta_{3} + \beta_{1, 3} \cdot X
\end{aligned}
$$

The system of equations above will result in $k-1$ equations (since the last category does not have its own equation) for $k$ unknowns. The final equation is the fact that the unknowns are probabilities, thus all have to **sum to 1**:

$$
    P(1) + P(2) + P(3) + \dots = 1
$$

!!! Note

    If the model has $p$ predictors and $k$ categories, then a total of $p \cdot (k-1)$ coefficients are needed, reflecting the above fact.

### **Ordinal Response**

When there is an Order to the nominal variables, they are distinguished based on their **cumulative probabilities**. In essence, the prediction space is split into **distinct regions** representing each of the categories:

<!-- Obtained from Medium -->
![ORDINAL_TAU_CUTS](Assets/6.%20Generalized%20Linear%20Models.md/ORDINAL_TAU_CUTS.png){.center}

There are will always be **$k-1$ cut points** which are always **non-decreasing**. Thus, a larger prediction which will lead to a **"higher" category**, hence preserving the order of the prediction.

In terms of modelling, a **Cumulative Logit Model** is used, which models the **cumulative odds** of each of the categories except the last. Given the system of equations, the **probabilities for each category** can be determined. The prediction of the model is the category with the **highest probability**:

* $\alpha$: Cut Points
* $\beta_{j}$: Regression coefficient

$$
\begin{aligned}
    \ln \left(\frac{P(1)}{1 - P(1)} \right) &= \alpha_{1} + \beta_{1, 1} \cdot X \\
    \ln \left(\frac{P(1) + P(2)}{1 - (P(1) + P(2))} \right) &= \alpha_{2} + \beta_{1, 2} \cdot X \\
    \ln \left(\frac{P(1) + P(2) + P(3)}{1 - (P(1) + P(2) + P(3))} \right) &= \alpha_{3} + \beta_{1, 3} \cdot X
\end{aligned}
$$

!!! Tip

    Similar to before, the model only provides $k-1$ equations for $k$ probabilities to be solved for. Thus, the same approach can be taken to solve for the final probability:

    $$
        P(1) + P(2) + P(3) + \dots = 1
    $$

    Similarly, the model requires a large number of coefficients to be estimated, one set for each of the modelled $k-1$ categories.

A common variation of is to use the **Proportional Odds** Cumulative Model. It assumes that the **odds ratios are the same across thresholds**, thus allowing each equation to use the **SAME $\beta$**, with only the $\alpha$ changing for each, reducing the number of cofficients needed:

$$
\begin{aligned}
    \ln \left(\frac{P(1)}{1 - P(1)} \right) &= \alpha_{1} + \beta_{1} \cdot X \\
    \ln \left(\frac{P(1) + P(2)}{1 - (P(1) + P(2))} \right) &= \alpha_{2} + \beta_{1} \cdot X \\
    \ln \left(\frac{P(1) + P(2) + P(3)}{1 - (P(1) + P(2) + P(3))} \right) &= \alpha_{3} + \beta_{1} \cdot X
\end{aligned}
$$

!!! Warning

    It is simply that the $\beta$'s are constant across predictions, not that they are not needed.

    In the event that the base model already does not include any predictors (simplest null model) then the proportional odds model does not provide any advantage.

Thus, it can be easily seen that the **ratio of two cumulative odds** is simply the difference in cut points:

$$
\begin{aligned}
    \frac{\frac{P(1)}{1 - P(1)}}{\frac{P(1) + P(2)}{1 - (P(1) + P(2))}}
    &= \frac{\exp (\alpha_{1} + \beta_{1} \cdot X)}{\exp (\alpha_{2} + \beta_{1} \cdot X)} \\
    &= \exp (\alpha_{1} - \alpha_{2})
\end{aligned}
$$

!!! Tip

    This is useful for situations where the same model is used to make predictions for different observations. If the outcome for one observation is known, then the other can be easily solved.

## **Count Models**

### **Poisson Model**

Since counts are non-negative integers, the **Poisson Distribution** can be used. The canonical link is the **natural log**, where the resulting model is known as a **Poisson Count Model**. It models the **log of the mean**, where the exponent of the regression coefficients represent a multiplicative increase in the mean:

$$
    \ln \mu = \beta_{0} + \beta_{1} \cdot X_{1} + \dots
$$

!!! Note

    Count models are sometimes also known as **Frequency** Models.

Recall that the poisson distribution can be interpreted as the **average number of occurences for a given exposure** (w). The average thus be re-expressed as a combination of exposure and occurrence per exposure:

$$
    \mu = w \cdot \lambda
$$

If the underlying data has **different subgroups with different exposures**, it will be more appropriate to model the **rate per exposure** rather than the actual count:

$$
\begin{aligned}
    \ln \mu
    &= \ln w \cdot \lambda \\
    &= \ln w + \ln \lambda \\
    \\
    \ln \lambda &= \beta_{0} + \beta_{1} \cdot X_{1} + \dots
\end{aligned}
$$

!!! Tip

    To obtain the estimated count, substitute the actual exposure (w) together with the model and solve for $\mu$. Due to this, the exposure is also sometimes known as an **Offset**.

!!! Note

    Consider the following example for different exposures:

    <center>

    | **Subject** | **Number of A's (Count)** | **Number of Students (Exposure)** |
    | :---------: | :-----------------------: | :-------------------------------: |
    |    Math     |             5             |                10                 |
    |   Science   |             8             |                20                 |
    |   History   |            10             |                30                 |

    </center>

    It is not fair to directly compare the number of A's for each subject and conclude which is doing best - we should consider the **underlying exposure** as well to consider the A-rate.

The poisson coefficients have a very similar intepretation with the logit model - it represents the **multiplicative increase** in the count given a one unit increase or in the presence of another variable.

### **Overdispersion**

Although the poisson distribution is suited to model counts, it has a **restrictive** property where the mean and variance must be equal. This makes it inadequate where the underlying data exhibits **overdispersion**. Thus, a different distribution should be used.

A generic method of dealing with overdispersion is to use the **Negative Binomial Distribution** which is **more flexible** as its variance is larger than its mean.

!!! Info

    Note that the poisson is actually a limiting case of the negative binomial where the parameters go to infinity, thus the change in distribution is not too drastic.

Alternatively, it is possible to create a new distribution using a **mixture** of distributions to achieve the desired property, where a parameter of the primary distribution follows a secondary distribution. For instance:

* Poisson & Gamma Mixture (results in Negative Binomial)
* Poisson & Lognormal Mixture

!!! Note

    There are two **general names** given to such models:

    1. **Latent Class Models**: Discrete Mixtures
    2. **Hetereogeneity Models**: Continuous Mixtures

<!-- Obtained from Coaching Actuaries -->
![OVERDISPERSION_MODELS](Assets/6.%20Generalized%20Linear%20Models.md/OVERDISPERSION_MODELS.png){.center}

### **Excess Zeroes**

A common reason for Over-dispersion is due to an **excess of zeroes** in reality compared to our expectation, known as **Zero Inflation**. In these cases, the original distribution must be **adjusted to account for probability of zeroes** occuring.

A **Zero Inflated Model** is appropriate when the underlying **event occurred, but no count was made**. For instance, a policyholder might choose not to make a claim despite incurring a loss due to considering premium rate increases.

Thus, it is necessary to account for and **distinguish between the types** of zeroes. This can be done via a **Logistic Regression** which is set to identify **False Zeroes**:

* False Zeroes (EG. Policyholders who incurred a loss but did not report)
* True Zeroes (EG. Policyholders who did not incur a loss)

Thus, mixing the Poisson and the Logistic Regression output, the PMF of the **mixture** can be constructed:

* **Logistic** - Identify observations with false zeroes
* **Poisson** - Underlying data generating process (assuming no false)

$$
\begin{aligned}
    \text{Mixture PMF}
    &= \begin{cases}
        \pi + (1 - \pi) \cdot P(Y = 0) ,&Y = 0 \\
        (1 - \pi) \cdot P(Y \gt 0) ,&Y \gt 0
    \end{cases}
\end{aligned}
$$

!!! Tip

    The key idea is that the original probabilities are adjusted to account for the possibility of being False Zeroes.

A **Hurdle Model** is appropriate when the zeroes occur because a certain **hurdle or threshold was not crossed**. For instance, when projecting the number of laps swam based on the number of people who visited the swimming pool, it should account for individuals who **never entered** the pool (Hurdle) (EG. Parents).

In this case, there is a clear reason for the zeroes, thus the probability for the zero count should be **modelled seperate** for the non-zero values:

* **Logistic** - Identify observations that cross the threshold ($\pi$)
* **Poisson** - Underlying data generating process (assuming all cross)

$$
\begin{aligned}
    \text{Mixture PMF}
    &= \begin{cases}
        \pi ,&Y = 0 \\
        (1 - \pi) \cdot \frac{P(Y \gt 0)}{1 - P(Y =0)} ,&Y \gt 0
    \end{cases}
\end{aligned}
$$

!!! Tip

    The resulting distribution should use a **zero-truncated** probability to reflect that the original distribution should not be used for zero counts.
