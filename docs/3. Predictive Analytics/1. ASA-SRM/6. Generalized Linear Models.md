# **Generalized Linear Models**

## **Overview**

Linear Regression assumes that the dependent variable **can change constantly and indefinitely** in either direction, which is not appropriate for certain quantities that **changes non-linearly** or for values that can **only take values of a certain range** (EG. Positive only or between 0 and 1).

Another assumption made is that the dependent variable is normally distributed. Despite it being one of the most common distributions in reality, not all quantities follow it.

Thus, a **Generalized Linear Model** (GLM) overcomes the above two constraints by allowing the dependent to follow a distribution

1. Allowing the dependent variable to have a distribution from the **Linear Exponential Family** (LEF)
2. Specifying a **link function** that allows the dependent variable to change **non-linearly**

<!-- Obtained from Daily Dose of Data Science -->
![SLR_VS_GLM](Assets/6.%20Generalized%20Linear%20Models.md/SLR_VS_GLM.png){.center}

## **Key Concepts**

### **Linear Exponential Family**

The LEF of distributions encompasses a wide range of distributions, each with their own **unique parameterization**. However, they have common characteristics that allow them them to be **re-parameterized** into a common format:

$$
\begin{aligned}
    \text{Y} &\sim \text{LEF}(\theta, \phi) \\
    \\
    f(Y \mid \theta, \phi) &= \exp \left(\frac{y \theta - b(\theta)}{\phi} + a(y, \phi) \right) \\
    \\
    E(Y) &= b'(\theta) \\
    \text{Var}(Y) &= \phi \cdot b''(\theta)
\end{aligned}
$$

!!! Info

    Apart from providing a common framework, re-paramterizing this way has certain mathematical properties which makes the resulting calculations smoother. It is not necessary to understand the intricate mathematical details.

!!! Tip

    The two paraemters are referred to as the **Canonical ($\theta$) and the Dispersion ($\phi$) Parameters** respectively. They have no clear interpretation, it is sufficient to know that they affect the **mean and variance** respectively.

    Consequently, the first and second derivatives are known as the **Mean and Variance Function** respectively. The key is remembering that the Variance function is **not entirely determined by the variance function**, unlike the mean.

    It is typically assumed that $\phi$ is a **known constant**.

<!-- Obtained from Daily Dose of Data Science -->
![LEF](Assets/6.%20Generalized%20Linear%20Models.md/LEF.png){.center}

### **Link Functions**

Under linear regression, the model **directly affects** the dependent variable. Since the model changes linearly, the dependent variable **must change linearly** as well:

$$
    E(Y \mid X)
    = \beta_{0} + \beta_{1} \cdot x_{1} + \beta_{2} \cdot x_{2} + \dots
$$

Thus, this can be overcome by introducing an intermediate **Link Function** between the dependent variable and the model output. Thus, even if the model changes linearly, the link function can cause a **non-linear impact on the dependent variable**:

$$
\begin{aligned}
    E(Y \mid X)
    &= g^{-1}(\beta_{0} + \beta_{1} \cdot x_{1} + \beta_{2} \cdot x_{2} + \dots) \\
    &= g^{-1}(\boldsymbol{X}^{T} \boldsymbol{\beta}) 
\end{aligned}
$$

!!! Note

    By definition, the link functions maps the dependent variable to the linear predictors. Hence, going the other way round, it is known as the **Inverse Link Function**.

The key consideration when choosing a link function is the **domain**:

* Linear Predictor generally has an **unrestricted domain** $(-\infty, \infty)$
* Dependent Variable generally has a **restricted domain** $(a, b)$
* Link function must be able to **map the two domains together**

!!! Note

    Another consideration is **interpretability**. Since link functions directly impact the dependent variable, it is desirable to use a **simpler function** (all else equal) to ease understanding.

<!-- Self Made -->
![LINK_FUNCTION](Assets/6.%20Generalized%20Linear%20Models.md/LINK_FUNCTION.png){.center}

!!! Tip

    The existence of link functions means that the intepretation of regression coefficients is not the same as OLS. The effect on the predictor **must consider the link function** as well.

Within the subset of possible link functions, there exists a **Canonical Link Function** which is the **Inverse Mean Function**. This means that the linear predictor will be equivalent to the **canonical parameter** for the distribution:

$$
\begin{aligned}
    \boldsymbol{X}^{T} \boldsymbol{\beta}
    &= g \left[E(Y \mid X) \right] \\
    &= b'^{-1} \left[E(Y \mid X) \right] \\
    &= b'^{-1} \left[b'(\theta) \right] \\
    &= \theta
\end{aligned}
$$

!!! Tip

    The canonical link has certain mathematical properties that will **ease the parameter estimation process**, which is why it is generally preferred.

<!-- Self Made -->
![CANONICAL_LINK](Assets/6.%20Generalized%20Linear%20Models.md/CANONICAL_LINK.png){.center}

The following table summarizes the key distributions and their corresponding link functions:

<!-- Obtained from Coaching Actuaries -->
![COMMON_EXAMPLES](Assets/6.%20Generalized%20Linear%20Models.md/COMMON_EXAMPLES.png){.center}

!!! Tip

    Any **multiplicative constant** within the canonical link function itself does not make a difference to the result. An expression **with or without** the constant can be deemed as the Canonical Link function.

### **Maximum Likelihood**

The parameters are estimated using **Maximum Likelihood Estimation** (MLE), where the likelihood function can be expressed as the following:

$$
\begin{aligned}
    \ell(\beta)
    &= \ln \prod \exp \left(\frac{y \theta - b(\theta)}{\phi} + a(y, \phi) \right) \\
    &= \sum \ln \exp \left(\frac{y \theta - b(\theta)}{\phi} + a(y, \phi) \right) \\
    &= \sum \left(\frac{y \theta - b(\theta)}{\phi} + a(y, \phi) \right) \\
    \\
    \frac{\partial^{2} \beta}{\partial \beta^{2}} &= 0
\end{aligned}
$$

Since $\beta$ and $\theta$ are linked, solving for $\beta$ provides $\theta$ as well. It is typically asummed that $\phi$ is 1 for most key distributions.

!!! Info

    There are **no closed form solutions** to the above system of equations, they are typically solved through numerical methods. It is not necessary to know the details for this exam.

!!! Note

    Recall that MLE achieves **Asymptotic Normality**. This will be the basis used for GLM-related statistical inference. The actual normal distribution will be used (rather than t-distribution) because in the limit, the variance of the normal distribution is **known**.

    However, the "adequateness" of the asymptotic results are dependent on the **choice of distribution**, but not the distribution of the underlying data:

    * Consider a **continuous** dataset with **equal mean and variance**
    * The key is to choose a distribution with **equal mean and variance as well**, regardless of what the underlying distribution is
    * For instance, if a poisson distribution was used, it is technically "wrong" because it assumes discrete data while the underlying data is continuous, but since it models the mean and variance correctly, it can be used

### **Deviance**

For linear rgeression, the parameters were estimated based on the residuals; thus, the goodness of fit was measured using the residuals via ANOVA. Similarly, since GLMs use MLE, their fit should be **measuring using likelihood**.

Consider an extreme case of GLM where the **number of parameters in the model is exactly equal to the number observations**, causing it to **fit the data perfectly**. This is known as a **Saturated Model**. This saturated model obviously suffers from overfitting (as it perfectly captures the noise in each sample), thus should not be used for prediction.

However, it can be used as a **benchmark** for the total information available in the sample and consequently **how much is lost** through the fitting process. This is measured via the **Deviance** of the model, which is the **difference in the likelihood** of the two models:

$$
    D^{*} = 2 \cdot (\ell_{\text{Saturated}} - \ell_{\text{Fitted}})
$$

!!! Tip

    The above are the **MAXIMIZED log-likelihoods**, which uses the likelihoods with the **MLE estimate**:

    * **Saturated**: Uses actual observations (Since a saturated model fits the data perfectly)
    * **Fitted**: Uses fitted observations

    Recall that since the likelihood is already a sum for each observation, the deviance for a particular observation is known as the **Deviance Residuals**.

    $$
        D^{*} = \sum D^{*}_{i}
    $$

The fitted model should have **sufficiently high likelihood** to ensure that it captures the signals, but be reasonably smaller than the saturated model to prevent overfitting. Thus, models with **smaller (but NOT too small) deviance** are preferred.

!!! Tip

    The subtraction of the two likelihoods would cause the second term in the probability function to cancel, leaving only the difference in the numerators:

    $$
        D^{*} = \frac{D}{\varphi}
    $$

    Thus, the most accurate terminology would be that the difference in the two likelihoods is known as the **Scaled Deviance**, while the differences in the numerator only is the **Deviance**.

    However, since $\varphi$ is **typically assumed to be 1** for most distributions used in SRM, the two are **identical**.

!!! Warning

    Another reason is that under GLM, the total variance **CANNOT be decomposed** into Explained and Unexplained components; there is an addittional component:

    $$
    \begin{aligned}
        \text{TSS}
        &= \sum(y_{i} - \hat{y})^{2} + \sum(\hat{y} - \bar{y})^{2}
            + 2 \sum \left([y_{i} - \hat{y}][\hat{y}-\bar{y}] \right) \\
        &= \text{RSS} + \text{RegSS} + 2 \sum \left([y_{i} - \hat{y}][\hat{y}-\bar{y}] \right)
    \end{aligned}
    $$

    The above additional component would cancel out under OLS, thus was not issue. However, this does not apply to GLMs.

    It can also be shown that for linear regression, the Deviance is **equivalent to the RSS**.
    
!!! Warning

    Deviance can only be used to compare NESTED models. 

## **Statistical Inference**

### **Likelihood Ratio Test**

Similar to how F-tests made use of the ANOVA results to perform statistical inference, GLMs can make use of the Deviance results in a similar fashion, via a **Likelihood Ratio Test (LRT)**, which compares a pair of **nested GLMs**:

* $H_{0}$: Reduced model is adq
* $H_{1}$:

$$
\begin{aligned}
    \chi_{\text{Statistic}} = 2 \cdot (\ell_{\text{Full}} - \ell_{\text{Restricted}}) \\
    \\
    \chi_{\text{Statistic}} &\sim \chi^{2}_{p_{\text{Full}} - p_{\text{Restricted}}}
\end{aligned}
$$

If there is a **significant drop in likelihood** between the two models, then the reduced predictors are significant and thus should not be dropped from the model.

### **Pearson Residuals**

Pearson Residuals are the raw residuals scaled by the standard deviation:

$$
    e_{\text{Pearson}} = \frac{y - \hat{y}}{\sqrt{\text{Var}(y)}}
$$

If the model fits the data well, the sum of squared pearson residuals should be **roughly equal to its degrees of freedom** ($n-p-1$):

$$
    \sum e^{2}_{\text{Pearson}} \sim n-p-1
$$

!!! Warning

    The sum of squared pearson residuals is also known as the Pearson chi square statistic. Although the names are rightfully similar, do NOT confuse the above with the Pearson Chi-Square Goodness of fit test; they are quantifying DIFFERENT things:

    * **Sum of Pearson Residuals**: Determine if the **fitted GLM** is a good fit for the data
    * **Chi Square Goodness of fit**: Determine if a **chosen distribution** is a good fit for the data

!!! Note

    The reason why "normal" residuals are not useful in GLMs is because each model follows a **different distribution**, thus there are is **no uniform behaviour for raw residuals**.

### **Overdispersion**

Dispersion is a measure of the **variability of the data** compared to the assumed variability in the chosen distribution. If the variability is larger, then it is known as **Overdispersion**, vice versa.

Naturally, this affects the standard errors which affects typical statistical inference procedures - hypothesis testing and confidence intervals.

One method to overcome this is to multiply a the **squared pearson statistic** ($\delta$) to the variance to **re-scale** the variance to an appropriate level

$$
\begin{aligned}
    \delta &= \frac{\sum e^{2}_{\text{Pearson}}}{n-p-1} \\
    \\
    \therefore \text{Var(Y')} &= \delta \cdot \text{Var}(Y)
\end{aligned}
$$

!!! Warning

    The above method only works for distributions which have a **restrictive variance** (related to the mean). If the variance is unrestricted (EG. Normal), the adjustment would not solve the mismatch in dispersion.

!!! Tip

    Note that this means that the pearson chi-squared statistic can also be used to **indicate when large over-dispersion is present**, since the adjustment required is directly related to it.

The above **alters the variance of the estimates** to match the data ($\beta$ is unaffected):

* Affects the standard errors and hence resulting statistical inferences
* Affects interpretation as it cannot be interpreted as the original distribution 

!!! Note

    This also introduces uncertainty into the variance, thus the normally asymptotic distribution should NOT be used; a t-distribution should be used instead.

## **Categorical Models**

### **Binary Response**

A Binary response variable has only two outcomes (0,1), thus a **Bernoulli Distribution** is used to model it. There are **three main link functions** that can map the the range into (0,1):

* **Logit**: $\ln \frac{\mu}{1-\mu}$
* **Probit**: $\Phi^{-1}(\mu)$
* **Complementary Log-Log**: $\ln[- \ln(1-\mu)]$

<!-- Obtained from Coaching Actuaries -->
![BINARY_LINK](Assets/6.%20Generalized%20Linear%20Models.md/BINARY_LINK.png){.center}

!!! Tip

    Notice the similarities between the three:

    * **Negative IV**: Complementary Log-Log & Logit are similar
    * **Positive IV**: Complementary Log-Log & Probit are similar

    Generally speaking, it is NOT EASY to distinguish between the three methods graphically.
    
!!! Tip

    Logit and Probit are the most popular methods. However, logit is preferred as it can be expressed in a closed form. 

Logit is the canonical link function; the resulting GLM is known as a **Logistic Regression**. It models the **Log-Odds** of an event occuring:

$$
\begin{aligned}
    \ln \frac{1-\mu}{\mu} &= \beta_{0} + \beta_{1} \cdot X_{1} + \dots \\
    \frac{1-\mu}{\mu} &= \exp (\beta_{0} + \beta_{1} \cdot X_{1} + \dots) \\
    \text{Odds} &= \exp (\beta_{0} + \beta_{1} \cdot X_{1} + \dots) \\
    \\
    \therefore \ln \text{Odds} &= Y \\
    \frac{1-\mu}{\mu} &= e^{Y} \\
    \mu &= e^{Y} - \mu \cdot e^{Y} \\
    \mu &= \frac{e^{Y}}{1 + e^{Y}} \\
    \text{P(A)} &= \frac{e^{Y}}{1 + e^{Y}}
\end{aligned}
$$

!!! Note

    Odds is the **ratio of the probability** of an event occuring to it not occuring:

    $$
        \text{Odds} = \frac{\text{P(A)}}{1 - \text{P(A)}}
    $$

    It is an alternative way of representing probabilities, typically used in a gambling context.

The **exponent of the regression coefficients** can be intepreted as the **multiplicative increase in odds** given a one unit increase or in the presence of that variable:

$$
\begin{aligned}
    \exp(\beta_{1})
    &= \frac{\exp (\beta_{0} + \beta_{1} \cdot (X_{1}+1))}{\exp (\beta_{0} + \beta_{1} \cdot X_{1})} \\
    &= \frac{\text{Odds}_{1}}{\text{Odds}_{0}}
\end{aligned}
$$

### **Nominal Response**

For a variable with more than two levels, a **Generalized Logit Model** is used. Rather than odds, it models the log of the **relative probability** of the target category and reference category.

For a variable with $k$ categories, $k-1$ models will be fitted, for each of the non-reference categories

$$
\begin{aligned}
    \ln \left(\frac{P(1)}{P(k)} \right) &= \beta_{1} + \beta_{1, 1} \cdot X \\
    \ln \left(\frac{P(2)}{P(k)} \right) &= \beta_{2} + \beta_{1, 2} \cdot X \\
    \ln \left(\frac{P(3)}{P(k)} \right) &= \beta_{3} + \beta_{1, 3} \cdot X
\end{aligned}
$$

!!! Note

    If the model has $n$ variables and $k$ categories, then a total of $n \cdot (k-1)$ coefficients are needed.

The system of equations above will result in $k-1$ equations for $k$ unknowns. The final equation is the fact that the unknowns are probabilities, thus all have to **sum to 1**:

$$
    P(1) + P(2) + P(3) + \dots = 1
$$

### **Ordinal Response**

When there is an Order to the nominal variables, they are distinguished based on their **cumulative probabilities**. In essence, the prediction space is split into **distinct regions** representing each of the categories:

<!-- Obtained from Medium -->
![ORDINAL_TAU_CUTS](Assets/6.%20Generalized%20Linear%20Models.md/ORDINAL_TAU_CUTS.png){.center}

There are will always be **$k-1$ cut points** which are always **non-decreasing**. Thus, a larger prediction which will lead to a **"higher" category**, hence preserving the order of the prediction.

In terms of modelling, a **Cumulative Logit Model** is used, which models the **cumulative odds** of each of the categories except the last. Given the system of equations, the **probabilities for each category** can be determined. The prediction of the model is the category with the **highest probability**:

* $\alpha$: Cut Points
* $\beta_{j}$: Regression coefficient

$$
\begin{aligned}
    \ln \left(\frac{P(1)}{P(2) + P(3) + P(4) + \dots} \right) &= \alpha_{1} + \beta_{1, 1} \cdot X \\
    \ln \left(\frac{P(1) + P(2)}{P(3) + P(4) + \dots} \right) &= \alpha_{2} + \beta_{1, 2} \cdot X \\
    \ln \left(\frac{P(1) + P(2) + P(3)}{P(4) + \dots} \right) &= \alpha_{3} + \beta_{1, 3} \cdot X
\end{aligned}
$$

The system of equations above will result in $k-1$ equations for $k$ unknowns. The final equation is the fact that the unknowns are probabilities, thus all have to **sum to 1**:

$$
    P(1) + P(2) + P(3) + \dots = 1
$$

!!! Tip

    Similar to before, if the model has $n$ variables and $k$ categories, then a total of $n \cdot (k-1)$ coefficients are needed.

    However, a common variation of the above is to use a **Proportional Odds Cumulative Model**. It assumes that the **odds ratios are the same across thresholds**, thus allowing each equation to use the **same $\beta$**, with only the $\alpha$ changing for each, reducing the number of cofficients needed.

    An interesting property is that the **ratio of two cumulative odds are independent** of the predictors and coefficients, which might have some **niche use cases** where the prediction under different set of X's are provided and tasked to solve for an unknown:

    $$
        \frac{\Pi_{1} \div (1 - \Pi_{1})}{\Pi_{2} \div (1 - \Pi_{2})} = \exp (\alpha_{1} - \alpha_{2})
    $$


## **Count Models**

### **Poisson Model**

Since counts are non-negative integers, the **Poisson Distribution** can be used. The canonical link is the **natural log**, where the resulting model is known as a **Poisson Count Model**. It models the **log of the mean**, where the exponent of the regression coefficients represent a multiplicative increase in the mean:

$$
    \ln \mu = \beta_{0} + \beta_{1} \cdot X_{1} + \dots
$$

Exposures

Also known as frequency

### **Alternative Models**