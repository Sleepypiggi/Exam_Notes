# **Generalized Linear Models**

## **Overview**

Linear Regression assumes that the dependent variable **can change constantly and indefinitely** in either direction, which is not appropriate for certain quantities that **changes non-linearly** or for values that can **only take values of a certain range** (EG. Positive only or between 0 and 1).

Another assumption made is that the dependent variable is normally distributed. Despite it being one of the most common distributions in reality, not all quantities follow it.

Thus, a **Generalized Linear Model** (GLM) overcomes the above two constraints by allowing the dependent to follow a distribution

1. Allowing the dependent variable to have a distribution from the **Linear Exponential Family** (LEF)
2. Specifying a **link function** that allows the dependent variable to change **non-linearly**

<!-- Obtained from Daily Dose of Data Science -->
![SLR_VS_GLM](Assets/6.%20Generalized%20Linear%20Models.md/SLR_VS_GLM.png){.center}

## **Key Concepts**

### **Linear Exponential Family**

The LEF of distributions encompasses a wide range of distributions, each with their own **unique parameterization**. However, they have common characteristics that allow them them to be **re-parameterized** into a common format:

$$
\begin{aligned}
    \text{Y} &\sim \text{LEF}(\theta, \phi) \\
    \\
    f(Y \mid \theta, \phi) &= \exp \left(\frac{y \theta - b(\theta)}{\phi} + a(y, \phi) \right) \\
    \\
    E(Y) &= b'(\theta) \\
    \text{Var}(Y) &= \phi \cdot b''(\theta)
\end{aligned}
$$

!!! Info

    Apart from providing a common framework, re-paramterizing this way has certain mathematical properties which makes the resulting calculations smoother. It is not necessary to understand the intricate mathematical details.

!!! Tip

    The two paraemters are referred to as the **Canonical ($\theta$) and the Dispersion ($\phi$) Parameters** respectively. They have no clear interpretation, it is sufficient to know that they affect the **mean and variance** respectively.

    Consequently, the first and second derivatives are known as the **Mean and Variance Function** respectively. The key is remembering that the Variance function is **not entirely determined by the variance function**, unlike the mean.

    It is typically assumed that $\phi$ is a **known constant**.

<!-- Obtained from Daily Dose of Data Science -->
![LEF](Assets/6.%20Generalized%20Linear%20Models.md/LEF.png){.center}

### **Link Functions**

Under linear regression, the model **directly affects** the dependent variable. Since the model changes linearly, the dependent variable **must change linearly** as well:

$$
    E(Y \mid X)
    = \beta_{0} + \beta_{1} \cdot x_{1} + \beta_{2} \cdot x_{2} + \dots
$$

Thus, this can be overcome by introducing an intermediate **Link Function** between the dependent variable and the model output. Thus, even if the model changes linearly, the link function can cause a **non-linear impact on the dependent variable**:

$$
\begin{aligned}
    E(Y \mid X)
    &= g^{-1}(\beta_{0} + \beta_{1} \cdot x_{1} + \beta_{2} \cdot x_{2} + \dots) \\
    &= g^{-1}(\boldsymbol{X}^{T} \boldsymbol{\beta}) 
\end{aligned}
$$

!!! Note

    By definition, the link functions maps the dependent variable to the linear predictors. Hence, going the other way round, it is known as the **Inverse Link Function**.

The key consideration when choosing a link function is the **domain**:

* Linear Predictor generally has an **unrestricted domain** $(-\infty, \infty)$
* Dependent Variable generally has a **restricted domain** $(a, b)$
* Link function must be able to **map the two domains together**

!!! Note

    Another consideration is **interpretability**. Since link functions directly impact the dependent variable, it is desirable to use a **simpler function** (all else equal) to ease understanding.

<!-- Self Made -->
![LINK_FUNCTION](Assets/6.%20Generalized%20Linear%20Models.md/LINK_FUNCTION.png){.center}

Within the subset of possible link functions, there exists a **Canonical Link Function** which is the **Inverse Mean Function**. This means that the linear predictor will be equivalent to the **canonical parameter** for the distribution:

$$
\begin{aligned}
    \boldsymbol{X}^{T} \boldsymbol{\beta}
    &= g \left[E(Y \mid X) \right] \\
    &= b'^{-1} \left[E(Y \mid X) \right] \\
    &= b'^{-1} \left[b'(\theta) \right] \\
    &= \theta
\end{aligned}
$$

!!! Tip

    The canonical link has certain mathematical properties that will **ease the parameter estimation process**, which is why it is generally preferred.

<!-- Self Made -->
![CANONICAL_LINK](Assets/6.%20Generalized%20Linear%20Models.md/CANONICAL_LINK.png){.center}

### **Common Examples**

The following table summarizes the key distributions and their corresponding link functions:

<!-- Obtained from Coaching Actuaries -->
![COMMON_EXAMPLES](Assets/6.%20Generalized%20Linear%20Models.md/COMMON_EXAMPLES.png){.center}

!!! Tip

    Any **multiplicative constant** within the canonical link function itself does not make a difference to the result. An expression **with or without** the constant can be deemed as the Canonical Link function.

## **Model Fitting**

The parameters are estimated using **Maximum Likelihood Estimation** (MLE).

$$
    TBC
$$

!!! Info

    There are **no closed form solutions** to the above system of equations, they are typically solved through numerical methods. It is not necessary to know the details for this exam.

## **Goodness of Fit**

### **Deviance**

For linear rgeression, the parameters were estimated based on the residuals; thus, the goodness of fit was measured using the residuals via ANOVA. Similarly, since GLMs use MLE, their fit should be **measuring using likelihood**.

Consider an extreme case of GLM where the **number of parameters in the model is exactly equal to the number observations**, causing it to **fit the data perfectly**. This is known as a **Saturated Model**. This saturated model obviously suffers from overfitting (as it perfectly captures the noise in each sample), thus should not be used for prediction.

However, it can be used as a **benchmark** for the total information available in the sample and consequently **how much is lost** through the fitting process. This is measured via the **Deviance** of the model, which is the **difference in the likelihood** of the two models:

$$
    D = 2 \cdot (\ell_{\text{Saturated} - \ell_{Fitted}})
$$

The fitted model should have sufficiently high likelihood to ensure that it captures the signals, but be reasonably smaller than the saturated model to prevent overfitting. Thus, models with **smaller (but NOT too small) deviance** are preferred.

!!! Warning

    Another reason is that under GLM, the total variance **CANNOT be decomposed** into Explained and Unexplained components; there is an addittional component:

    $$
    \begin{aligned}
        \text{TSS}
        &= \sum(y_{i} - \hat{y})^{2} + \sum(\hat{y} - \bar{y})^{2}
            + 2 \sum \left([y_{i} - \hat{y}][\hat{y}-\bar{y}] \right) \\
        &= \text{RSS} + \text{RegSS} + 2 \sum \left([y_{i} - \hat{y}][\hat{y}-\bar{y}] \right)
    \end{aligned}
    $$

    The above additional component would cancel out under OLS, thus was not issue. However, this does not apply to GLMs.

!!! Note

    It can be shown that for linear regression, the Deviance is **equivalent to the RSS**.

### Other

## **Statistical Inference**

### **Likelihood Ratio Test**

Similar to how F-tests made use of the ANOVA results to perform statistical inference, GLMs can make use of the Deviance results in a similar fashion, via a **Likelihood Ratio Test (LRT)**.

The key intuition is the same. A par of nested models are considered (Full & Restricted). If there is a **significant drop in likelihood** between the two models, then the reduced predictors are significant.


Consider two nested models - Full & Restricted:

* $H_{0}$:
* $H_{1}$

Under the null hypothesis, the test statistic asymptotically follows a Chi Squared Distribution. Thus, there must be a sizeable difference in the two for the test to reject the null.

!!! Note

    This is because Chi-squared distributions are positive only, having higher density in smaller values. Thus, only truly large differences in likelihood will be flagged out. 

### **Goodness of Fit Test**

## **Specific Forms of GLM**

### **Logit Regression**

### **Probit Regression**

### **Poisson Rgeression**