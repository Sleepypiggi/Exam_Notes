# **Introduction to Statistical Learning**

## **Estimation VS Prediction**

The primary focus of traditional statistics lies in **Estimation**. However, the focus of statistical learning is **Prediction**. Both are **approximation methods** and although they are used interhchangeably in layman terms, there is a **technical difference** between the two:

<center>

|                 **Estimation**                  |                  **Prediction**                   |
| :---------------------------------------------: | :-----------------------------------------------: |
| Approximating a **parameter** of a distribution | Approximating a **realization** of a distribution |
|                 Non-stochastic                  |                    Stochastic                     |

</center>

For instance, given that $X \sim \text{Dist}(\theta)$:

* $X$ is the distribution
* $\theta$ is a parameter of the distribution (to be estimated)
* $x_{i}$ is a realization of the distribution (to be predicted)

## **Overview of Key Elements**

There are two main components to statistical learning:

<center>

|  **Dependent Variable**  | **Independent Variable** |
| :----------------------: | :----------------------: |
| Variable to be predicted | Variable used to predict |
|          Only 1          |    May have multiple     |
|     Output Variable      |      Input Variable      |
|         $y_{i}$          |     $x_{i}, x_{i,j}$     |

</center>

!!! Note

    Observations typically record the two variables together ($y_{i}$, $x_{i}$). In the case where there are multiple independent variables ($x_{i,j}$), $i$ represents the observation while $j$ represents the independent variable.

Within the above, they are further split by their **nature**:

<center>

| **Quantitative Variable** |   **Qualitative Variable**    |
| :-----------------------: | :---------------------------: |
|     Continuous Values     |      Discrete Categories      |
|      EG. 3.14, 1.849      | EG. North, South, East & West |
|  **Regression** Problem   |  **Classification** Problem   |

</center>

!!! Note

    Categorical variables are more generally represented by a **discrete number**. For instance:

    * True (1), False (0)
    * North (0), East (1), South (2), West (4)

    If there the **order of the categories** are important, then they are known as **Ordinal variables**. If not, they are known as **Nominal Variables**.

There are two broad categories of **how the "learning" occurs**:

<center>

|           **Supervised**            |         **Unsupervised**         |
| :---------------------------------: | :------------------------------: |
| Has a specified Indpendent Variable |     No Independent Variable      |
|   To identify what influences $y$   | To identify patterns in the data |
|        EG. Linear Regression        | EG. Principal Component Analysis |

</center>

!!! Note

    The key focus of the majority of the exam will be on **Supervised learning**, unless otherwise stated. A relatively small portion on Unsupervised learning will be covered towards the end.

Within **Supervised learning** methods, it can be further broken down into two forms:

<center>

|                   **Parametric**                   |            **Non-parametric**             |
| :------------------------------------------------: | :---------------------------------------: |
| Assumes a **functional form** for the relationship |   No assumption made about relationship   |
|    **Equation with parameters** to be estimated    | Algorithmic method with **no parameters** |
|   May not fit the data well (wrongly specified)    |            Fits the data well             |
|        Does not require as large a dataset         |  Requires **large dataset** to work well  |
|               EG. Linear Regression                |            EG. Decision Trees             |

</center>

Lastly, there are two main goals for statistical learning:

* **Prediction** - What is the **value** of the dependent variable given a set of independent variables
* **Inference** - What is the **relationship** between the dependent and independent variables

Prediction is largely related to the **Flexibility** (or **Complexity**) of the model; how well the model tracks the sample. As will be shown later, a moderate amount of flexibility is ideal with regards to prediction.

Generally speaking, there is an **inverse relationship** between flexibility and intepretability. Highly flexible models that tend to have **complex mathematical components** that allow it to match the data, at the cost of being **unable to inuitively explain** the relationship.

!!! Tip

    Consider the following two models:

    * **LHS**: Less Flexible but easier to intepret (Linear relationship)
    * **RHS**: More flexible but harder to intepret (Unknown relationship)

    <!-- Obtained from Coaching Actuaries -->
    ![FLEXIBILITY_INFERENCE](Assets/1.%20Statistical%20Learning.md/FLEXIBILITY_INFERENCE.png){.center}

## **Regression**

### **Specification**

There exists a **true relationship** between the Dependent and Independent Variables:

$$
    Y = f(X_{1}, X_{2}, \dots, X_{n}) + \epsilon 
$$

The above can be understood as the combination of:

* **Signal**: **Systematic** relationship between $Y$ and $X$'s
* **Noise**: **Unpredictable** random variations (also known as the **Error**)

In the above, both $Y$ and $X$ are random variables. However, for statistical learning, we are interested in knowing what the value of $Y$ is **GIVEN a set of realized $X$'s**. Thus, we are focused on the **Conditional Distribution** of $Y \mid X$. The prediction is the **expected value** of this conditional distribution:

$$
\begin{aligned}
    Y \mid X=x &\sim f(x_{1}, x_{2}, \dots, x_{n}) \\
    E(Y \mid X=x) &= E \left[f(x_{1}, x_{2}, \dots, x_{n}) \right] + E(\epsilon \mid X) \\
    E(Y \mid X=x) &= f(x_{1}, x_{2}, \dots, x_{n})
\end{aligned}
$$

The signal function is known what is commonly referred to as the **Regression Model**.

!!! Warning

    There is a **need to differentiate** the Conditional and Unconditional expectations. Recall the law of total expectation:

    $$
        E(Y) = \sum E(Y \mid X = x_{i}) \cdot P(X = x_{i}) 
    $$

    There is no need for the unconditional expectation in a regression setting as we already have a realized value of $X$ that we are interested in using; there is no probability involved.

!!! Note

    There are two points to take note of:

    1. The systematic relationship results in constant, thus the expectation is itself
    2. The mean of the error is 0, reflecting that random events are typically symmetrical

<!-- Obtained from Colorado Uni -->
![CONDITIONAL_EXPECTATION](Assets/1.%20Statistical%20Learning.md/CONDITIONAL_EXPECTATION.png){.center}

It is important to understand that the regression model is the **MEAN, not the actual observations** of $Y$:

<!-- Obtained from Cloudera -->
![ERROR_TERM](Assets/1.%20Statistical%20Learning.md/ERROR_TERM.png){.center}

The true specification for the **Population Regression Model** ($f$) is unknown. Different statistical learning methods assume a **different form for $f$**, which is then estimated based on the sample to obtain a **Sample Regression Model** $\hat{f}$.  

!!! Note

    The process of estimating $\hat{f}$ is also known as "Training" the model. It also sometimes referred to as "Fitting" the model to the data.

    It can also be intepreted that $\hat{f}$ is the **estimator of the mean of the conditional distribution**.

### **Evaluation**

The main goal of the model is to be **used for prediction**. Thus, the strength of the model should be evaluated based on its ability to make **accurate predictions**.

The key consideration lies in **Overfitting**. If the model matches the underlying sample too closely, it will **mistakenly capture the noise present as a signal**. This impedes its ability to make accurate predictions. Thus, the model must be tested on data that was not used to train it:

* **Training Data**: Observations that were used to train model
* **Test Data**: Obervations that were NOT used to train the model

!!! Info

    Operationally, this implies that the sample should be partitioned at the start of the process.

The above concerns can be quantified using the **Mean Squared Error** (MSE):

$$
\begin{aligned}
    \text{MSE}
    &= E(Y - \hat{Y}) \\
    &= \frac{\sum (y_{i} - \hat{y}_{i})^{2}}{n}    
\end{aligned}
$$

<center>

|      **Training MSE**      |      **Test MSE**       |
| :------------------------: | :---------------------: |
|   Based on Training Set    |    Based on Test Set    |
| Measure of **Flexibility** | Measure of **Accuracy** |
|     Moderate is better     |     Lower is better     |

</center>

The two share the following relationships:

* **Training MSE is always lower** than test MSE since the model was fit based on the training data
* **Training MSE decreases with flexibility** since more flexible models are better able to match the data
* **Test MSE has a U-Shape** reflecting that models which are too inflexible **may not capture the signal** while those that are too flexible may **capture the wrong signal**

<!-- Obtained from Coaching Actuaries -->
![TRAINING_TEST_MSE](Assets/1.%20Statistical%20Learning.md/TRAINING_TEST_MSE.png){.center}

Recall that the MSE can be decomposed into a combination of Bias and Variance. In a regression context, there is an **additional component** representing the error term as well:

$$
    \text{MSE}
    = \underbrace{(\text{Bias}[f(x_{1}, x_{2}, \dots, x_{n})])^{2} + \mathrm{Var}[f(x_{1}, x_{2}, \dots, x_{n})]}_{\text{Reducible}}
    + \underbrace{\mathrm{Var}(\epsilon)}_{\text{Irreducible}}
$$

* **Reducible**: Error stemming from the **choice** of $\hat{f}$
* **Irreducible**: Error stemming from **inherent variability**

The U-shape of the test MSE can be explained due to the above decomposition. A low test MSE would imply a model with low bias and variance - but such a model is not possible because of the **inverse relationship** between the two:

* **Bias**: Too little flexibility fails to capture the signals in the data (**Underfitting**)
* **Variance**: Too much flexibility results in mistakenly capturing noise in the data as signals (**Overfitting**)

The reduction in Bias from choosing a more complex model is greater than the increased variance, causing test MSE to fall. However at some point, the increase in variance outweighs the falling bias, causing the test MSE to rise. It is **not possible to minimize both**; there is a **Bias-Variance Tradeoff**.

<!-- Obtained from Geek for Geeks -->
![BIAS_VARIANCE_TRADEOFF](Assets/1.%20Statistical%20Learning.md/BIAS_VARIANCE_TRADEOFF.png){.center}

!!! Warning

    GENERALLY speaking, the flexibility of the model is often **proportional to the number of free parameters** available (for a supervised learning method).

!!! Note

    The proof for the decomposition of MSE is not necessary for the exam, but provided below:

    $$
    \begin{aligned}
        \text{MSE}
    \end{aligned}
    $$

### **Confidence & Prediction Intervals**

Recall that **Confidence Intervals** are a range of values that the **true parameter** will fall within for a given level of confidence. In a regression setting, recall that the model is estimating the **mean of the conditional distribution**; a parameter. Thus, the confidence interval for a model measures the **range of values for $E(Y \mid X)$**.

!!! Note

    In the case of Parametric learning methods, the estimate of each parameter that goes into $\hat{f}$ will also have an associated confidence interval.

On the other hand, **Prediction Intervals** are the range of values that a **realization of a random variable** will fall within for a given level of confidence. In a regression setting, this refers to the **actual dependent variable itself**, not the mean.

Note that since the actual dependent variable is a combination of the mean and the error term, the resulting prediction interval will **always be wider than the confidence interval**, reflecting the additional variance from the error term:

<center>

| **Prediction Interval** | **Confidence Interval** |
|:-:|:-:|
| Range of Random Variable | Range of Parameter |
| Mean & Error Term | Mean only |

</center>

<!-- Obtained from Datacamp -->
![PREDICTION_CONFIDENCE](Assets/1.%20Statistical%20Learning.md/PREDICTION_CONFIDENCE.png){.center}

!!! Note

    Intuitively, the confidence interval represents the uncertainty arising from the **reducible variance** while the prediction interval arises as a result of the **irreducible variance**.

    In an extreme example, the confidence interval will **shrink to 0** as the sample size increases to the population, but the prediction interval will always **at least be equal to the variance of the error**.

## **Classification Problems**
