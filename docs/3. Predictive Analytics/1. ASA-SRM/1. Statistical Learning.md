# **Introduction to Statistical Learning**

## **Estimation VS Prediction**

The primary focus of traditional statistics lies in **Estimation**. However, the focus of statistical learning is **Prediction**. Both are **approximation methods** and although they are used interhchangeably in layman terms, there is a **technical difference** between the two:

<center>

|                 **Estimation**                  |                  **Prediction**                   |
| :---------------------------------------------: | :-----------------------------------------------: |
| Approximating a **parameter** of a distribution | Approximating a **realization** of a distribution |
|                 Non-stochastic                  |                    Stochastic                     |

</center>

For instance, given that $X \sim \text{Dist}(\theta)$:

* $X$ is the distribution
* $\theta$ is a parameter of the distribution (to be estimated)
* $x_{i}$ is a realization of the distribution (to be predicted)

## **Overview of Key Elements**

There are two main components to statistical learning:

<center>

|  **Dependent Variable**  | **Independent Variable** |
| :----------------------: | :----------------------: |
| Variable to be predicted | Variable used to predict |
|          Only 1          |    May have multiple     |
|     Output Variable      |      Input Variable      |
|         $y_{i}$          |     $x_{i}, x_{i,j}$     |

</center>

!!! Note

    The following notation is common:
    
    * $i$: Observation Index
    * $j$: Variable Index
    * Most subscripts follow (Observation, Variable)

Within the above, they are further split by their **nature**:

<center>

| **Quantitative Variable** |   **Qualitative Variable**    |
| :-----------------------: | :---------------------------: |
|     Continuous Values     |      Discrete Categories      |
|      EG. 3.14, 1.849      | EG. North, South, East & West |
|  **Regression** Problem   |  **Classification** Problem   |

</center>

!!! Note

    Categorical variables are more generally represented by a **discrete number**. For instance:

    * True (1), False (0)
    * North (0), East (1), South (2), West (4)

    If there the **order of the categories** are important, then they are known as **Ordinal variables**. If not, they are known as **Nominal Variables**.

There are two broad categories of **how the "learning" occurs**:

<center>

|           **Supervised**            |         **Unsupervised**         |
| :---------------------------------: | :------------------------------: |
| Has a specified Indpendent Variable |     No Independent Variable      |
|   To identify what influences $y$   | To identify patterns in the data |
|        EG. Linear Regression        | EG. Principal Component Analysis |

</center>

!!! Note

    The key focus of the majority of the exam will be on **Supervised learning**, unless otherwise stated. A relatively small portion on Unsupervised learning will be covered towards the end.

    There technically exists a special class of learning known as **Semi-Supervised**, which typically occurs due to a lack of complete data. It is out of scope for the purposes of this exam.

Within **Supervised learning** methods, it can be further broken down into two forms:

<center>

|                   **Parametric**                   |              **Non-parametric**              |
| :------------------------------------------------: | :------------------------------------------: |
| Assumes a **functional form** for the relationship |    No assumption made about relationship     |
|    **Equation with parameters** to be estimated    |  Algorithmic method with **no parameters**   |
|    More Robust (Less sensitive) to data changes    | Less Robust (More sensitive) to data changes |
|        Does not require as large a dataset         |   Requires **large dataset** to work well    |
|               EG. Linear Regression                |              EG. Decision Trees              |

</center>

!!! Tip

    Non-Parametric models will outperform parametric ones when the assumed functional form is different from what was specified.
    
    This tends to occur when the underlying data generating process is **complicated**, as most specified functional forms are not that complicated (EG. *Linear* regression).

!!! Note

    Parametric tends to be more robust likely because they have an underlying functional form to anchor them; non-parametric methods are so to speak “at the mercy of the data”.

<!-- Obtained from Coaching Actuaries -->
![METHOD_OVERVIEW](Assets/1.%20Statistical%20Learning.md/METHOD_OVERVIEW.png){.center}

Lastly, there are two main goals for statistical learning:

* **Prediction** - What is the **value** of the dependent variable given a set of independent variables
* **Inference** - What is the **relationship** between the dependent and independent variables

Prediction is largely related to the **Flexibility** (or **Complexity**) of the model. For a parametric method, it is directly related to the **number of parameters** in the model (more accurately, the **degrees of freedom** available).

!!! Tip

    Consider the following example:

    * **Prediction**: How much will a home be worth given its characteristics?
    * **Inference**: How much *more* will a home be worth if it has a view of the river?

    Inference is largely about determining the impact of one or more variables on the dependent.

!!! Note

    If prediction is the only focus, then understanding *how* the model arrives at the prediction is less irrelevant; as long as the prediction is reliable.
    
    This is especially so for complex models, which are typically referred to as “Black Boxes” where the users do not have “visibility” on the inner workings of the model.

Generally speaking, there is an **inverse relationship** between flexibility and intepretability. Highly flexible models that tend to have **complex mathematical components** that allow it to match the data, at the cost of being **unable to inuitively explain** the relationship.

!!! Tip

    Consider the following two models:

    * **LHS**: Less Flexible but easier to intepret (Linear relationship)
    * **RHS**: More flexible but harder to intepret (Complex relationship)

    <!-- Obtained from Coaching Actuaries -->
    ![FLEXIBILITY_INFERENCE](Assets/1.%20Statistical%20Learning.md/FLEXIBILITY_INFERENCE.png){.center}
    
    Flexibility is about **how well the model matches the data**. Less flexible models are **smoother** because they do NOT try to match the idiosyncrasies of the data.

<!-- Obtained from ISLR -->
![METHOD_OVERVIEW_2](Assets/1.%20Statistical%20Learning.md/METHOD_OVERVIEW_2.png){.center}

!!! Warning

    Despite linear regression being more interpretable than trees, the ISLR author has also noted that **Trees are "easier to explain" than linear regression**.

    Although both intepretability and ease of explanation usually refer to the same thing, for the purposes of this exam treat them as **seperate**.

    The author notes that the **interpretability is in the eye of the beholder** - some prefer the coefficients from linear regression while others prefer to the tree visual - one or the other can be easier to explain to different parties.

## **Regression**

### **Specification**

There exists a **true relationship** between the Dependent and Independent Variables:

$$
    Y = f(X_{i}) + \varepsilon 
$$

!!! Info

    In essence, statistical learning refers to a set of approaches for estimating $f$.

The above can be understood as the combination of:

* **Signal**: **Systematic** relationship between $Y$ and $X$'s
* **Noise**: **Unpredictable** random variations (also known as the **Error**)

In the above, both $Y$ and $X$ are random variables. However, for statistical learning, we are interested in knowing what the value of $Y$ is **GIVEN a set of realized $X$'s**. Thus, we are focused on the **Conditional Distribution** of $Y \mid X$. The prediction is the **expected value** of this conditional distribution:

$$
\begin{aligned}
    Y \mid X = x_{i} &\sim f(x_{i}) \\
    E(Y \mid X = x_{i}) &= E \left[f(x_{i}) \right] + E(\varepsilon \mid X = x_{i}) \\
    E(Y \mid X = x_{i}) &= f(x_{i})
\end{aligned}
$$

The signal function is known what is commonly referred to as the **Regression Model**.

!!! Warning

    There is a **need to differentiate** the Conditional and Unconditional expectations. Recall the law of total expectation:

    $$
        E(Y) = \sum E(Y \mid X = x_{i}) \cdot P(X = x_{i}) 
    $$

    There is no need for the unconditional expectation in a regression setting as we already have a realized value of $X$ that we are interested in using; there is no probability involved.

!!! Note

    There are two points to take note of:

    1. The systematic relationship results in constant, thus the expectation is itself
    2. The mean of the error is 0, reflecting that random events are typically symmetrical

It is important to understand that the regression model is the **MEAN, not the actual observations** of $Y$:

<!-- Obtained from Colorado Uni -->
![CONDITIONAL_EXPECTATION](Assets/1.%20Statistical%20Learning.md/CONDITIONAL_EXPECTATION.png){.center}

<!-- Obtained from Cloudera -->
![ERROR_TERM](Assets/1.%20Statistical%20Learning.md/ERROR_TERM.png){.center}

The true specification for the **Population Regression Model** ($f$) is unknown. Different statistical learning methods assume a **different form for $f$**, which is then estimated based on the sample to obtain a **Sample Regression Model** $\hat{f}$.  

!!! Note

    The process of estimating $\hat{f}$ is also known as "Training" the model. It also sometimes referred to as "Fitting" the model to the data.

The fitted $\hat{f}$ is the **estimator of the mean of the conditional distribution**. Consequently, the difference between the observation and the fitted model is known as the **Residual**, which is the **estimator of the error**:

$$
\begin{aligned}
    y_{i}
    &= \hat{f} + \hat{\varepsilon} \\
    &= \widehat{E(Y \mid X)} + \hat{\varepsilon} \\
    &= \hat{y} + \hat{\varepsilon}
\end{aligned}
$$

!!! Warning

    For simplicity, the estimate of the mean of the conditional distribution is denoted as $\hat{y}$. **DO NOT** confuse this as the estimate of the actual observation!

    The residuals are also sometimes noted as $e_{i}$ for convenience.

<!-- Self Made -->
![ERROR_VS_RESIDUALS](Assets/1.%20Statistical%20Learning.md/ERROR_VS_RESIDUALS.png){.center}

If $\hat{f}$ was correctly specified and fitted, the resulting residuals should exhibit the **same characteristics** as the underlying errors. This is important, as many statistical learning methods make **assumptions about the nature of the Errors**. Thus, the residuals can be used to **check the validity of the assumptions** or make other inferences.

!!! Info

    For simplicity, the above diagrams were illustrated using Simple Linear Regression. However, the concepts apply more generally to all statistical learning methods.

### **Evaluation**

The main goal of the model is to be **used for prediction**. Thus, the strength of the model should be evaluated based on its ability to make **accurate predictions**.

The key consideration lies in **Overfitting**. If the model matches the underlying sample too closely, it will **mistakenly capture the noise present as a signal**. This impedes its ability to make accurate predictions. Thus, the model must be tested on data that was not used to train it:

* **Training Data**: Observations that were used to train model
* **Test Data**: Obervations that were NOT used to train the model

!!! Info

    Operationally, this implies that the sample should be partitioned at the start of the process.

The above concerns can be quantified using the **Mean Squared Error** (MSE):

$$
\begin{aligned}
    \text{MSE}_{\text{Training}}
    &= \frac{\sum (y_{i, \text{Training}} - \hat{y}_{i})^{2}}{n_{\text{Training}}} \\
    \\
    \text{MSE}_{\text{Test}}
    &= \frac{\sum (y_{i, \text{Test}} - \hat{y}_{i})^{2}}{n_{\text{Test}}}    
\end{aligned}
$$

<center>

|      **Training MSE**      |      **Test MSE**       |
| :------------------------: | :---------------------: |
|   Based on Training Set    |    Based on Test Set    |
| Measure of **Flexibility** | Measure of **Accuracy** |
|     Moderate is better     |     Lower is better     |

</center>

The two share the following relationships:

* **Training MSE is always lower** than test MSE since the model was fit based on the training data
* **Training MSE decreases with flexibility** since more flexible models are better able to match the data
* **Test MSE has a U-Shape** reflecting that models which are too inflexible **may not capture the signal** while those that are too flexible may **capture the wrong signal**

<!-- Obtained from Coaching Actuaries -->
![TRAINING_TEST_MSE](Assets/1.%20Statistical%20Learning.md/TRAINING_TEST_MSE.png){.center}

!!! Warning

    There is **NO direct relationship** between the training and test error; the two are independent.

Recall that the MSE can be decomposed into a combination of **Bias and Variance**. In a regression context, there is an **additional component** representing the error term as well:

* **Reducible**: Error stemming from the **choice** of $\hat{f}$
* **Irreducible**: Error stemming from **inherent variability** (cannot be predicted by $x$)

$$
\begin{aligned}
    \text{MSE}
    &= \underbrace{(\text{Bias}[f(x_{i})])^{2} + \mathrm{Var}[f(x_{i})]}_{\text{Reducible}}
    + \underbrace{\mathrm{Var}(\varepsilon)}_{\text{Irreducible}}    
\end{aligned}
$$

!!! Tip

    Thus, the primary goal of statistical learning is to estimate $\hat{f}$ that the reducible error is minimized.

!!! Warning

    Some texts may refer to the above as:
    
    * **Accuracy** (Bias) - Error from by **approximating** $f$ (real world, complicated) with a simpler model ($\hat{f}$)
    * **Precision** (Variance) - Error from **changes** in $\hat{f}$ across different samples  
   
    Although they are used interchangeably in day to day conversations, they have a **specific meaning** in a statistical context.

It is **not possible to minimize both**; there is an inverse relationship known as the **Bias-Variance Tradeoff** which results in the U-shaped curve:

* Increasing complexity **decreases bias** as it allows the model to **capture the signal**
* However, it also **increases variance** as the model will **mistakenly pick up noise** in the data as signal
* Initially, the reduction in bias is higher as the model **rapidly improves**
* However, once majority of the signal has been captured, **more noise will be increasingly captured**, causing the increase in variance to be higher
* Thus, the goal is to choose a model with **moderate amount of flexibility** to minimize the MSE

<!-- Obtained from Geek for Geeks -->
![BIAS_VARIANCE_TRADEOFF](Assets/1.%20Statistical%20Learning.md/BIAS_VARIANCE_TRADEOFF.png){.center}

!!! Note

    The MSE curve is floored at the line representing the irreducible variance.

!!! Tip

    The inversion of test MSE depends largely on the underlying data generating process:

    * **LHS**: Underlying process is of moderate complexity (Typical case described above) 
    * **Center**: Underlying process is simple, thus **low flexibility** methods are sufficient (**Benefits less** from Bias reduction)
    * **RHS**: Underlying process is complicated, thus **high flexibility** methods needed (**Benefits more** from Bias reduction)

    The inversion of the curve thus depends on the underlying functional form - whenever the marginal benefit from Bias reduction falls off.

    <!-- Obtained from ISLR -->
    ![BIAS_MSE_CASES](Assets/1.%20Statistical%20Learning.md/BIAS_MSE_CASES.png){.center}

    Thus, even simpler methods can have high predictive accuracy. There is NO universally best learning method across all datasets; it is about choosing the best method for the data.

<center>

|      **Under-fitting**      |      **Over-fitting**       |
| :-------------------------: | :-------------------------: |
|   High Bias, Low Variance   |   Low Bias, High Variance   |
| High training, Low test MSE | Low training, High test MSE |
|  Fail to capture the noise  |  Mistakenly captures noise  |

</center>

<!-- Obtained from ACTEX Manual -->
![UNDER_OVER_FITTING](Assets/1.%20Statistical%20Learning.md/UNDER_OVER_FITTING.png){.center}

### **Confidence & Prediction Intervals**

Recall that **Confidence Intervals** are a range of values that the **true parameter** will fall within for a given level of confidence. In a regression setting, recall that the model is estimating the **mean of the conditional distribution**; a parameter. Thus, the confidence interval for a model measures the **range of values for $E(Y \mid X)$**.

!!! Note

    In the case of Parametric learning methods, the estimate of each parameter that goes into $\hat{f}$ will also have an associated confidence interval.

On the other hand, **Prediction Intervals** are the range of values that a **REALIZATION of a random variable** will fall within for a given level of confidence. In a regression setting, this refers to the **ACTUAL dependent variable itself**, not the mean.

Note that since the actual dependent variable is a combination of the mean and the error term, the resulting prediction interval will **always be wider than the confidence interval**, reflecting the additional variance from the error term:

<center>

| **Prediction Interval**  | **Confidence Interval** |
| :----------------------: | :---------------------: |
| Range of Random Variable |   Range of Parameters   |
|   Range for $Y \mid X$   | Range of $E(Y \mid X)$  |
|    Mean & Error Term     |        Mean only        |

</center>

Both Confidence and Prediction Intervals are expressed using the **same general formula**:

$$
    \text{Interval} = \text{Estimate} \pm \text{Percentile} \cdot \text{SE of Estimate}
$$

!!! Warning

    For a prediction interval, the prediction is technically not an estimate; the term above is used for simplicity.

<!-- Obtained from Datacamp -->
![PREDICTION_CONFIDENCE](Assets/1.%20Statistical%20Learning.md/PREDICTION_CONFIDENCE.png){.center}

!!! Note

    Intuitively, the confidence interval represents the uncertainty arising from the **reducible variance** while the prediction interval additionally includes the **irreducible variance** from inherent randomness.

    In an extreme example, the confidence interval will **shrink to 0** as the sample size increases to the population, but the prediction interval will always **at least be equal to the variance of the error**.
    
    $$
    \begin{aligned}
        \lim_{n \to \infty} (\text{CI}) &= 0 \\
        \lim_{n \to \infty} (\text{PI}) &= \text{Var}(\varepsilon)
    \end{aligned}
    $$

## **Classification Problems**

Almost all of the key concepts from a Regression context applies in a classification context as well. The key difference being instead of measuring the absolute error, the **error rate** is used instead:

$$
    \text{Error Rate} = \frac{\text{Number of Correct Classifications}}{n}
$$

The **Bayes Error Rate** is known as the **lowest possible error rate**, akin to the irreducible error that arises due to the noise in the data itself.

!!! Note

    The Bayes Error Rate is the error rate that is achieved when using a **Bayes Classifier**, which is a **conceptual model** that assigns observations to the category with the **highest probability**, given the predictor values. It is conceptual because in reality it is NOT possible to know the conditional probabilities for each class.

    Graphically, the above can be described using a **Decision Boundary**:

    * LHS of boundary: Classified as Orange
    * RHS of boundary: Classified as Blue

    <!-- Obtained from ISLR -->
    ![BAYES_DECISION_BOUNDARY](Assets/1.%20Statistical%20Learning.md/BAYES_DECISION_BOUNDARY.png){.center}

Assume that there are two categories (A, B) and that **population probabilities are known** (since all available information is used). Based on this, such a model would assign categories based on the probability:

* The category with the **highest probability** for that observation will be assigned the prediction
* However, there is **no guarantee** that the actual observation will be that category
* Thus, the probability that it is **NOT the highest probability category** is the error rate

The **Bayes Error Rate** is determined using the above for all combinations of the independent variables:

$$
\begin{aligned}
    \text{Bayes Error Rate}
    &= 1 - E \left(\max P \left(Y = j \mid X \right) \right) \\
    &= 1 - \frac{\sum \max P \left(Y = j \mid X \right)}{n}
\end{aligned}
$$

<!-- Obtained from Coaching Actuaries -->
![BAYES_PROBABILITY](Assets/1.%20Statistical%20Learning.md/BAYES_PROBABILITY.png){.center}