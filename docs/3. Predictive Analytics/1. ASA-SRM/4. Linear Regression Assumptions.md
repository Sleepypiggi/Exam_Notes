# **Linear Regression Asssumptions**

## **Overview**

There are **5 key assumptions** that have been implicitly made thus far for Linear Regression, each allowing us to make key conclusions about the model:

<!-- Obtained from Stack Exchange User DVL -->
![REGRESSION_ASSUMPTIONS](Assets/4.%20Linear%20Regression%20Assumptions.md/REGRESSION_ASSUMPTIONS.png){.center}

!!! Note

    The colours of the arrows indicate which assumptions are cumulatively needed:

    * Red + Blue = Purple
    * Green + Purple = Black

The purpose of this section is to examine WHY we need the respective assumptions, CONSEQUENCES on the results if they are violated and HOW to verify the assumptions.

If the first two blocks of assumptions are fulfilled, then the resulting regression esimate is known as the **Best Linear Unbiased Estimator** (BLUE), which has the **lowest variance** (most efficient) among all other linear unbiased estimates. This is known as the **Guass Markov Theorem**.

!!! Warning

    The last block of assumptions on the distribution of the errors being normal and iid are NOT needed for the estimates to be BLUE. They are however needed for **statistical inference**.

!!! Warning

    Remember that being unbiased means that the **EXPECTED VALUE** of the estimates are equal to the true value. Thus, for a particular sample, the estimated coefficients might be different.

## **Assumption 1: Linearity**

This assumes that the **true relationship** between the dependent and independent variables are Linear, but not the dependent or independent variables themselves. This allows for **transformations** of the respective variables:

$$
\begin{aligned}
    Y &= \beta_{0} + \beta_{1} \cdot X_{1} + \beta_{2} \cdot X_{2} \\
    Y &= \beta_{0} + \beta_{1} \cdot X_{1} + \beta_{2} \cdot X^{2}_{1} \\
    \ln Y &= \beta_{0} + \beta_{1} \cdot X_{1} + \beta_{2} \cdot X_{2}
\end{aligned}
$$

If the relationship between the two is not linear, a linear regression model will not be able to accurately capture the relationship between the two, resulting in **Bias estimates**, which consequently impacts the validity of BLUE and the statistical inferences.

Linearity can be verified using either of the following:

* **Scatterplot of the dependent and independent**: Plot should show a linear relationship (intuitive)
* **Scatterplot of the residuals and the fitted model** (Residual Plot): Plot should be centered around 0

!!! Tip

    In reality, Errors are unobservable, thus the best estimate of them are the residuals.

    IF the fitted model has **fully captured the relationship** between the dependent and independent, then the resulting residuals should be **random** - with an average of 0.

    Thus, if we observe the residuals to be centered around 0, it implied the fitted linear model has fully captured the relationship, implying that the **true relationship is indeed linear**.

<!-- Obtained from Lumen Learning -->
![SCATTER_RESIDUAL_PLOT](Assets/4.%20Linear%20Regression%20Assumptions.md/SCATTER_RESIDUAL_PLOT.png){.center}

If it is found that the relationship is non-linear, **transformations** could be done on the dependent and/or independent variable to make the resulting relationship linear.

## **Assumption 2: Perfect Collinearity**

Collinearity (or Multi-collinearity) refers to when an independent variable can be expressed as a **linear combination of one or more other independent variables**.

Perfect collinearity refers to when it can be **perfectly expressed** as another variable:

* Multiple of another variable: $x_{1} = c \cdot x_{2}$
* Differs by a constant: $x_{1} = x_{2} \pm c$
* Dummy variable trap
* Affine transformations

If there are perfectly collinear variables, then there will be **insufficient information** to solve for the parameter estimates (EG. Two equations to solve for three unknowns). This results in **non-unique solutions**.

!!! Warning

    It is NOT a necessity for the predictors to be independent; zero collinearity. It is generally assumed that collinearity is low.

### **Imperfect Collinearity**

Imperfect collinearity occurs when one variable is **highly (but NOT perfectly) correlated** with one or more other variables. Intuitively, this means that the variable does not add much predictive power as its effects would have **already been captured via the related predictors**, hence can be removed from the model with little loss.

Imperfect collinearity does NOT prevent OLS from finding a solution and does NOT affect the overall predictive power of the model. It affects the inferences of the model:

* Usual intepretation of "holding other variables constant" no longer holds true as the variables move together; **hard to seperate the effects of individual variables**
* Consequently, OLS has difficulty estimating the estimates as it cannot clearly distinguish them; resulting in **unstable estimates** which can change drastically over different samples
* Consequently, this **increases the variance of the estimates**, which increases p-values, potentially leading to **more false negatives** and hence important variables being omitted from the regression

!!! Warning

    Collinearity ONLY affects t-test results, overall predictive power (measured by F-tests), are unaffected.

!!! Tip

    There is a special type of Variable known as a **Suppressor Variable**, which by itself is not strongly correlated with the dependent, but when combined with another variable, leads to a significantly stronger model.

    Collinearity is likely (but not guaranteed) to be present when supressor variables are present.

### **Detecting Collinearity**

The simplest way to detect collinearity is through a **Scatterplot or Correlation Matrix**, which shows the correlations between **pairs of variables**. A **correlation of 0.8 and higher** is typically considered high enough where the collinearity becomes problematic.

<!-- Obtained from ACTEX Manual -->
![CORRELATION_MATRIX](Assets/4.%20Linear%20Regression%20Assumptions.md/CORRELATION_MATRIX.png){.center}

HOWEVER, the above method can only detect collinearity between pairs of variables at a time. To check for collinearity between **3 or more** variables, the **Variance Inflation Factor** (VIF) can be used instead.

Recall that in the presence of collinearity, the variance of estimates increases; the VIF is a **measure of the increase in variance**. 

$$
    \text{VIF}_{p} = \frac{1}{1 - R^{2}_{p}}
$$

!!! Tip

    Different texts have **different thresholds** for high collinearity. For this exam, assume the **threshold to be 10**.
    
    The reciprocal of the VIF is known as the Tolerance.

$R^{2}_{p}$ is the R-squared for a model where the p-th predictor is regressed against ALL OTHER predictors. If this R-squared is high, it implies that this predictor is well explained by other predictors (high correlation, high collinearity).

!!! Warning

    The VIF needs to be calculated for EACH variable to conclude whether collinearity is present in the model:
    
    * Variable 1 against Variable 2,3
    * Variable 2 against Variable 1,3
    * Variable 3 against Variable 1,2
    
    Collinearity is present even if just one combination of variables cross the threshold.

Naturally, the solution is to **drop highly collinear variables**; keep only one of the bunch.

!!! Tip

    The VIF for uncorrelated predictors is 1, reflecting that the variance remains **unaffected** by the other variables.

## **Assumption 3: Exogenity**

**Exogenity** refers to a variable that is **outside** the model and thus must be **independent of the other variables inside the model**. In the context of linear regression, the **errors should be independent of any of the predictors**:

$$
\begin{aligned}
    E(\varepsilon \mid X) &= 0 \\
    \\
    E(\varepsilon)
    &= E[E(\varepsilon \mid X)] \\
    &= E[0] \\
    &= 0 \\
    \\
    E(X \cdot \varepsilon)
    &= E[X \cdot E(\varepsilon \mid X)] \\
    &= E(X \cdot 0) \\
    &= 0 \\
    \\
    \therefore \text{Cov}(\varepsilon, X)
    &= E(\varepsilon, X) - E(X) \cdot E(\varepsilon) \\
    &= 0 - E(X) \cdot 0 \\
    &= 0
\end{aligned}
$$

!!! Info

    "Exo" and "Endo" in greek means "In" and "Out" respectively. Thus, another way of referring to the assumptions is whether or not Endogenity is present.

!!! Warning

    Recall that Zero Covariance is a result of independence, NOT the other way around.

If Exogenity is violated, there is a relationship between the predictors and the error (reflecting unmodelled variables). This means that the estimated coefficients would reflect the **effect of BOTH the predictor and the unmodelled variable**, resulting in a **biased estimate**, which consequently impacts the validity of BLUE and the statistical inferences.

<!-- Obtained from Statology -->
![OMITTED_VARIABLE_BIAS](Assets/4.%20Linear%20Regression%20Assumptions.md/OMITTED_VARIABLE_BIAS.png){.center}

!!! Info

    The ommitted correlated variable is said to Confound (mix up) the regression results. Thus, it is sometimes referred to as the **Confounding Variable**. This process in general is known as the **Omitted Variable Bias**.

Similar to before, a **residual plot of the residuals against the predictors** can be used to check for Exogenity; there should be no visible trend in the plots:

<!-- Obtained from ACTEX Manual -->
![Residual Plot](Assets/4.%20Gauss%20Markov%20Theorem.md/Residual%20Plot.png)

!!! Note

    The above plot are the residuals against a single predictor. Plotting against the fitted values would imply plotting against all predictors at once.

There are two possible solutions if Exogenity is violated:

1. Include the Confounding Variable
2. Introduce an Instrumental Variable

<!-- Obtained From Shakrim Blog -->
![INSTRUMENTAL_VARIABLE](Assets/4.%20Linear%20Regression%20Assumptions.md/INSTRUMENTAL_VARIABLE.png){.center}

## **Assumption 4: Homoscedasticity**

**Homoscedasticity** refers to the error terms having **constant variance** while **Heteroscedasticity** refers to having non-constant variance.

$$
    \text{Var}(\varepsilon \mid X) = \sigma^{2}
$$

!!! Info

    Similarly, they come from the terms "Homo" and "Hetero" which means "Same" and "Different".

If Homoscedastacity is not present, the **sampling distribution of the estimates are not easily derived** and hence cannot be proven that they are the most efficient estimators. Consequently, statistical inference results might not be reliable as well.

Similar to before, a residual plot can be used to identify patterns. If the **spread of the plots are changing** (typically in the shape of a funnel for increasing variance), then Heteroscedasticity is present:

<!-- Obtained from Corporate Finance Institute -->
![HOMOSCEDASTICITY](Assets/4.%20Linear%20Regression%20Assumptions.md/HOMOSCEDASTICITY.png){.center}

!!! Tip

    A model can be both Unbiased and Heteroscedastic, they have no influence on one another:

    <!-- Obtained from Condor -->
    ![HETEROSCEDASTIC_SCENARIOS](Assets/4.%20Linear%20Regression%20Assumptions.md/HETEROSCEDASTIC_SCENARIOS.png){.center}

The most direct method to deal with Heteroscedasticity is to perform a **transformation** on the variables that will stabilize its variance, such the **Logarithm or Squareroot**. These transformations typically **compress larger values while expanding smaller values**, helping to shrink the variance.

!!! Tip

    The transformations listed above require positive-only data, thus a **positive constant** can be added to all variables to ensure that they are positive - EG. $\ln(1+Y)$

!!! Warning

    The **direction of heteroscedasticity** matters. Concave transformations like log or squareroot work for datasets with **increasing variance**, which is usually the case. For decreasing variance or more complex patterns, other methods should be used.

### **Weighted Least Squares**

Alternatively, if the source of heteroscedasticity is known, it is possible to use a **Weighted Least Squares** approach to account for it. Weights are assigned to **each observation**, such that the weight is the amount needed to **scale the the variance to a constant amount**:

$$
\begin{aligned}
    \text{Assumed Constant Variance} &= \sigma^{2} \\
    \text{Current Variance} &= \text{Var}(\varepsilon_{i}) \\
    \text{Var}(\varepsilon_{i}) &= \frac{\sigma^{2}}{w_{i}}
\end{aligned}
$$

!!! Info

    In practice, the weights are completely arbitrary and up to the modeller. They must be positive and do not need to sum to 1.

A new funcitonal form is assumed, where the **resulting variance is constant**:

$$
\begin{aligned}
    y_{i} &= \beta_{0} + \beta_{1} \cdot X_{1} + \varepsilon \\
    \sqrt{w_{i}} \cdot y_{i} &= \sqrt{w_{i}} \cdot \left(\beta_{0} + \beta_{1} \cdot x_{1} + \varepsilon_{i} \right) \\
    \\
    \text{Var}(\sqrt{w_{i}} \cdot \varepsilon_{i})
    &= \left(\sqrt{w_{i}} \right)^{2} \cdot \text{Var}(\varepsilon_{i}) \\
    &= w_{i} \cdot \frac{\sigma^{2}}{w_{i}} \\
    &= \sigma^{2}
\end{aligned}
$$

!!! Tip

    The above doesnt "fix" the heteroscedasticity, it simply accounts for it by using an alternative method.

    In another way, heteroscedasticity is not inherently problematic; the problem is applying a method that assumes Homoscedastacity (linear regression) to a dataset with heteroscedasticity.

## **Assumption 5: No Serial Correlation**

This assumption states that the error terms should **NOT be correlated** with one another:

$$
\begin{aligned}
    \text{Cov}(\varepsilon_{i}, \varepsilon_{j})
    &= E(\varepsilon_{i}, \varepsilon_{j}) - E(\varepsilon_{i}) \cdot E(\varepsilon_{j}) \\
    &= E(\varepsilon_{i}, \varepsilon_{j}) - 0 \\
    \\
    E(\varepsilon_{i}) &= E(\varepsilon_{j}) = 0, \text{(Assumption 3)} \\
    \\
    \therefore \text{Cov}(\varepsilon_{i}, \varepsilon_{j}) &= 0 \rightarrow E(\varepsilon_{i}, \varepsilon_{j}) = 0
\end{aligned}
$$

!!! Info

    This is also sometimes referred to as the **No Autocorrelation** assumption.

Similarly, this can be validated using a residual plot against other residuals **over different predictions**. The resulting plot should not have any exhibit any clear trend or pattern:

<!-- Obtained from Stack Exchange -->
![AUTOCORRELATION](Assets/4.%20Linear%20Regression%20Assumptions.md/AUTOCORRELATION_PLOT.png)

If violated, it implies that knowing one error can **systematically provide information about another**; meaning that there are additional **unmodelled factors** to consider. The residuals are no longer accurate, which consequently impacts the validity of BLUE and the statistical inferences.

## **Assumption 6: Normality**

The last assumption is assuming that the error terms follow a **Normal Distribution**:

$$
    \varepsilon \sim N(0, \sigma^{2})
$$

!!! Tip

    This assumption also implies that the dependent is normally distributed, as a linear function involving a normal variable is also normally distributed.

It can verified by using a **QQ plot** or other equivalent distribution tests (EG. Kolmogorov Test).

Violating the normality assumption does not impact the validity of the estimates or predictive power; it only impacts the **validity of statistical inference**. For large samples, CLT states that the distribution of the estimators are **approximately normal**, thus inferences are still valid. However, this **does NOT apply to prediction intervals** as they directly contain the variance of the error term, which follows an unknown distribution.

## **Other Considerations**

### **Leverage**

Observations with unusually high values in the **Independent Variable** are known as **High Leverage Points**. The easiest way to identify high leverage points would be through a scatterplot of independent variables:

<!-- Obtained from ACTEX Manual -->
![High Leverage Points](Assets/4.%20Gauss%20Markov%20Theorem.md/High%20Leverage.png){.center}

When there are multiple variables, another consideration is that the observation might seem normal within each variable, but **unusual when taken collectively**. This is hard to visualize in multiple dimensions, thus leverage can also be quantified via the following:

$$
    h_{i} = \boldsymbol{X}_{i} \cdot (\boldsymbol{X}^{T} \cdot \boldsymbol{X})^{-1} \cdot \boldsymbol{X}_{i}
$$

It can be mathematically shown that the leverage is equivalent to the **element on the diagonal** in the Hat matrix for that particular observation:

$$
    h_{i} = \boldsymbol{H}_{ii}
$$

<!-- Obtained from Stack Exchange -->
![LEVERAGE_HAT_MATRIX](Assets/4.%20Linear%20Regression%20Assumptions.md/LEVERAGE_HAT_MATRIX.png){.centee}

An observation is considered to have *high* leverage if its leverage is greater than three times the **average leverage**:

$$
    h_{ii} \gt 3 \left(\frac{p+1}{n}\right)
$$

!!! Tip

    The leverage for each observations lies within the following range:

    $$
        \frac{1}{n} \lt h_{i} \lt 1
    $$

    It can be shown that the sum of diagonal of the Hat matrix (the leverage) is $p+1$:

    $$
        \sum H_{ii} = p+1
    $$

    Thus, the average leverage per observation is:

    $$
        \text{Average Leverage} = \frac{p+1}{n}
    $$ 

### **Outliers**

Observations with unusually high values in the **dependent variable** are known as **Outliers**. Naturally, these observations have **large residuals**. However, residuals contain the units the of the dependent variable, which affects its scale. Thus, the residuals must be transformed into a **unitless form** to determine a universal benchmark.

Outliers are observations with **unusually large values of the dependent variable**, relative to the fitted regression model. In other words, they have **large residuals**, relative to other observations:

<!-- Obtained from Coaching Actuaries -->
![RESIDUAL_OUTLIER](Assets/4.%20Linear%20Regression%20Assumptions.md/RESIDUAL_OUTLIER.png){.center}

!!! Tip

    One consequence of having many outliers is that the calculated MSE is **inflated** because the outliers artifically increase the RSS. This created a **TENDENCY** for statistical tests to fail to reject the null.

    In reality, if there are outliers and the statistical test are able to reject the null, there is no reason to doubt its validity. This is because even under "adverse" situations, the test can still reject the null, thus under normal circumstances it would continue to reject the null.

    However, if the test currently fails to reject the null, it could be due to the "adverse" situation; under normal circumstances it might reject the null thus there is a reason to doubt the results.

It can be shown that the variance of the residual is a function of its Leverage:

$$
    \text{Var}(\hat{\varepsilon}_{i}) = \sigma^{2} \cdot (1 - h_{i})
$$

!!! Warning

    The above the is the variance of the RESIDUAL, NOT the error term!

Following the OLS assumptions made earlier, it can be shown to have the following sampling distribution:

$$
    \hat{\varepsilon}_{i} \sim N(0, \sigma^{2} \cdot (1 - h_{i}))
$$

Thus, the residual can be **standardized** via division of its standard error. However, the true variance is likely unknown in practice. Thus, it is replaced with the mean squared of the residual:

$$
\begin{aligned}
    \hat{\varepsilon}_{i, \text{Standardized}}
    &= \frac{\hat{\varepsilon}_{i}}{\sqrt{\sigma^{2} (1 - h_{ii})}} \\
    &= \frac{\hat{\varepsilon}_{i}}{\sqrt{\sigma^{2}_{\text{RSS}} (1 - h_{ii})}}
\end{aligned}
$$

It is generally accepted that observations with Standardized Residuals **larger than 2** are considered to be Outliers:

$$
    \hat{\varepsilon}_{i, \text{Standardized}} \ge 2
$$

### **Influence**

The effect of Leverage and Residual Size can be combined into a single measure known as **Influence**. The influence of an observation is quantified via **Cook's Distance**, which is a measure of **how the fitted model changes** if the observation were removed:

$$
    D_{i} = \frac{\sum (\hat{y}_{j} - \hat{y}_{j(i)})^{2}}{(p+1) \cdot \text{MS}_{\text{RSS}}}
$$

However, this method is **computationally expensive** as it requires $n+1$ datasets and models:

* 1 dataset and fitted model with all observations
* n datasets and fitted models, each with the i-th observation removed

Thus, it can expressed as a combination of the **Leverage and Residual Size**:

$$
\begin{aligned}
    \sum (\hat{y}_{j} - \hat{y}_{j(i)})^{2} &= \frac{e^{2}_{i} \cdot h_{i}}{(1 - h_{i})^{2}} \\
    \\
    \therefore D_{i}
    &= \frac{\sum (\hat{y}_{j} - \hat{y}_{j(i)})^{2}}{(p+1) \cdot \text{MS}_{\text{RSS}}} \\
    &= \frac{\frac{e^{2}_{i} \cdot h_{i}}{(1 - h_{i})^{2}}}{(p+1) \cdot \text{MS}_{\text{RSS}}} \\
    &= \frac{e^{2}_{i} \cdot h_{i}}{(p+1) \cdot \text{MS}_{\text{RSS}}(1 - h_{i})^{2}} \\
    &= \frac{e^{2}_{i}}{\text{MS}_{\text{RSS}}(1 - h_{i})} \cdot \frac{h_{i}}{(p+1)(1 - h_{i})} \\
    &= \left(\frac{e_{i}}{\sqrt{\text{MS}_{\text{RSS}}(1 - h_{i})}} \right)^{2} \cdot \frac{h_{i}}{(p+1)(1 - h_{i})} \\
    &= (\hat{\varepsilon}_{i, \text{Standardized}})^{2} \cdot \frac{h_{i}}{(p+1)(1 - h_{i})}
\end{aligned}
$$

The issue with highly influential points is that the fitted model would be **vastly different without them**, causing **instability** in the estimates:  

<!-- Obtained from Quantitative Research Methods for Political Science -->
![INFLUENCE](Assets/4.%20Linear%20Regression%20Assumptions.md/INFLUENCE.png){.center}

!!! Tip

    In general, influential points should be removed automatically **ONLY IF** it was due to an **error in the data collection** process. Otherwise, other options should be considered:

    1. Retain the observations but adding a note
    2. Create a Binary variable to account for the observations

### **High Dimensionality**

As alluded to previously, increasing the number of parameters in the model does not necessarily benefit the model. This phenomenon is known as the **Curse of Dimensionality**.

A sample of 100 observations is a lot of information for a model with 2 predictors, but is sparse for a model with 50 predictors. Thus, **increasing the number of predictors relative to the sample size** tends to have detrimental effects:

* $p \gt n$: **Insufficient information** to fit the model
* $p = n$: Model fits data *perfectly*, resulting in **overfitting**
* $p \lt n$, but limited degrees of freedom: Esimates become **unstable**, resulting in higher standard errors and hence **incorrect statistical inferences**

!!! Note

    It is also a possibility that increasing the number of predictors results in choosing two or more highly correlated variables, resulting in Multicollinearity.

The natural solution would be to **limit the number of predictors** by choosing only truly relevant variables and/or to **increase the sample size**.

#### **Partial Least Squares**

Another method is to use **Partial Least Squares** (PLS). It is very similar to PCA - it seeks to create new variables that are linear combinations of existing variables (known as **Directions**) which are then used to perform the regression instead. Each subsequent direction uses the residuals of the previous direction.

The key difference when compared to PCA is that PLS finds new variables with the goal of explaining the dependent variable. Consequently, cross validation can be used to determine the optimal number of directions.

!!! Tip

    ALL features are used to create the directions; there is no variable selection (dropping of variables) involved, only involvement to different extents.
