# **Linear Regression Asssumptions**

## **Overview**

There are **5 key assumptions** that have been implicitly made thus far for Linear Regression, each allowing us to make key conclusions about the model:


<!-- Obtained from Stack Exchange User DVL -->
![REGRESSION_ASSUMPTIONS](Assets/4.%20Linear%20Regression%20Assumptions.md/REGRESSION_ASSUMPTIONS.png){.center}

!!! Note

    The colours of the arrows indicate which assumptions are cumulatively needed:

    * Red + Blue = Purple
    * Green + Purple = Black

The purpose of this section is to examine WHY we need the respective assumptions, CONSEQUENCES on the results if they are violated and HOW to verify the assumptions.

If the first two blocks of assumptions are fulfilled, then the resulting regression esimate is known as the **Best Linear Unbiased Estimator** (BLUE), which has the **lowest variance** (most efficient) among all other linear unbiased estimates. This is known as the **Guass Markov Theorem**.

!!! Warning

    The last block of assumptions on the distribution of the errors being normal and iid are NOT needed for the estimates to be BLUE. They are however needed for **statistical inference**.

## **Assumption 1: Linearity**

This assumes that the **true relationship** between the dependent and independent variables are Linear, but not the dependent or independent variables themselves. This allows for **transformations** of the respective variables:

$$
\begin{aligned}
    Y &= \beta_{0} + \beta_{1} \cdot X_{1} + \beta_{2} \cdot X_{2} \\
    Y &= \beta_{0} + \beta_{1} \cdot X_{1} + \beta_{2} \cdot X^{2}_{1} \\
    \ln Y &= \beta_{0} + \beta_{1} \cdot X_{1} + \beta_{2} \cdot X_{2}
\end{aligned}
$$

If the relationship between the two is not linear, a linear regression model will not be able to accurately capture the relationship between the two, resulting in **Bias estimates**, which consequently impacts the validity of BLUE and the statistical inferences.

Linearity can be verified using either of the following:

* **Scatterplot of the dependent and independent**: Plot should show a linear relationship (intuitive)
* **Scatterplot of the residuals and the fitted model** (Residual Plot): Plot should be centered around 0

!!! Tip

    In reality, Errors are unobservable, thus the best estimate of them are the residuals.

    IF the fitted model has **fully captured the relationship** between the dependent and independent, then the resulting residuals should be **random** - with an average of 0.

    Thus, if we observe the residuals to be centered around 0, it implied the fitted linear model has fully captured the relationship, implying that the **true relationship is indeed linear**.

<!-- Obtained from Lumen Learning -->
![SCATTER_RESIDUAL_PLOT](Assets/4.%20Linear%20Regression%20Assumptions.md/SCATTER_RESIDUAL_PLOT.png){.center}

If it is found that the relationship is non-linear, **transformations** could be done on the dependent and/or independent variable to make the resulting relationship linear.

## **Assumption 2: Perfect Collinearity**

Collinearity (or Multi-collinearity) refers to when an independent variable can be expressed as a **linear combination of one or more other independent variables**.

Perfect collinearity refers to when it can be **perfectly expressed** as another variable:

* Multiple of another variable: $x_{1} = c \cdot x_{2}$
* Differs by a constant: $x_{1} = x_{2} \pm c$
* Dummy variable trap
* Affine transformations

If there are perfectly collinear variables, then there will be **insufficient information** to solve for the parameter estimates (EG. Two equations to solve for three unknowns). This results in **non-unique solutions**.

### **Imperfect Collinearity**

Imperfect collinearity occurs when one variable is **highly (but NOT perfectly) correlated** with one or more other variables. Intuitively, this means that the variable does not add much predictive power as its effects would have **already been captured via the related predictors**, hence can be removed from the model with little loss.

Imperfect collinearity does NOT prevent OLS from finding a solution and does NOT affect the overall predictive power of the model. It affects the inferences of the model:

* Usual intepretation of "holding other variables constant" no longer holds true as the variables move together; **hard to seperate the effects of individual variables**
* Consequently, OLS has difficulty estimating the estimates as it cannot clearly distinguish them; resulting in **unstable estimates** which can change drastically over different samples
* Consequently, this **increases the variance of the estimates**, which increases p-values, potentially leading to **more false negatives** and hence important variables being omitted from the regression

!!! Warning

    Collinearity only affects t-test results, overall predictive power (measured by F-tests), are unaffected.

### **Detecting Collinearity**

The simplest way to detect collinearity is through a **Scatterplot or Correlation Matrix**, which shows the correlations between **pairs of variables**. A **correlation of 0.8 and higher** is typically considered high enough where the collinearity becomes problematic.

<!-- Obtained from ACTEX Manual -->
![CORRELATION_MATRIX](Assets/4.%20Linear%20Regression%20Assumptions.md/CORRELATION_MATRIX.png){.center}

However, the above method can only detect collinearity between pairs of variables at a time. To check for collinearity between 3 or more variables, the **Variance Inflation Factor** (VIF) can be used instead.

Recall that in the presence of collinearity, the variance of estimates increases; the VIF is a **measure of the increase in variance**. Different texts have **different thresholds** for high collinearity. For this exam, assume the **threshold to be 10**.

$$
    \text{VIF}_{p} = \frac{1}{1 - R^{2}_{p}}
$$

$R^{2}_{p}$ is the R-squared for a model where the p-th predictor is regressed against ALL OTHER predictors. If this R-squared is high, it implies that this predictor is well explained by other predictors (high correlation, high collinearity).

Naturally, the solution is to **drop highly collinear variables**; keep only one of the bunch.

## **Assumption 3: Exogenity**

**Exogenity** refers to a variable that is **outside** the model and thus must be **independent of the other variables inside the model**. In the context of linear regression, the **errors should be independent of any of the predictors**:

$$
\begin{aligned}
    E(\varepsilon \mid X) &= 0 \\
    \\
    E(\varepsilon)
    &= E[E(\varepsilon \mid X)] \\
    &= E[0] \\
    &= 0 \\
    \\
    E(X \cdot \varepsilon)
    &= E[X \cdot E(\varepsilon \mid X)] \\
    &= E(X \cdot 0) \\
    &= 0 \\
    \\
    \therefore \text{Cov}(\varepsilon, X)
    &= E(\varepsilon, X) - E(X) \cdot E(\varepsilon) \\
    &= 0 - E(X) \cdot 0 \\
    &= 0
\end{aligned}
$$

!!! Info

    "Exo" and "Endo" in greek means "In" and "Out" respectively. Thus, another way of referring to the assumptions is whether or not Endogenity is present.

!!! Warning

    Recall that Zero Covariance is a result of independence, NOT the other way around.

If Exogenity is violated, there is a relationship between the predictors and the error (reflecting unmodelled variables). This means that the estimated coefficients would reflect the **effect of BOTH the predictor and the unmodelled variable**, resulting in a **biased estimate**, which consequently impacts the validity of BLUE and the statistical inferences.

<!-- Obtained from Statology -->
![OMITTED_VARIABLE_BIAS](Assets/4.%20Linear%20Regression%20Assumptions.md/OMITTED_VARIABLE_BIAS.png){.center}

!!! Info

    The ommitted correlated variable is said to Confound (mix up) the regression results. Thus, it is sometimes referred to as the **Confounding Variable**. This process in general is known as the **Omitted Variable Bias**.

Similar to before, a **residual plot of the residuals against the predictors** can be used to check for Exogenity; there should be no visible trend in the plots:

<!-- Obtained from ACTEX Manual -->
![Residual Plot](Assets/4.%20Gauss%20Markov%20Theorem.md/Residual%20Plot.png)

!!! Note

    The above plot are the residuals against a single predictor. Plotting against the fitted values would imply plotting against all predictors at once.

There are two possible solutions if Exogenity is violated:

1. Include the Confounding Variable
2. Introduce an Instrumental Variable

<!-- Obtained From Shakrim Blog -->
![INSTRUMENTAL_VARIABLE](Assets/4.%20Linear%20Regression%20Assumptions.md/INSTRUMENTAL_VARIABLE.png){.center}

## **Assumption 4: Homoscedasticity**

**Homoscedasticity** refers to the error terms having **constant variance** while **Heteroscedasticity** refers to having non-constant variance.

$$
    \text{Var}(\varepsilon \mid X) = \sigma^{2}
$$

!!! Info

    Similarly, they come from the terms "Homo" and "Hetero" which means "Same" and "Different".

If Homoscedastacity is not present, the **sampling distribution of the estimates are not easily derived** and hence cannot be proven that they are the most efficient estimators. Consequently, statistical inference results might not be reliable as well.

Similar to before, a residual plot can be used to identify patterns. If the **spread of the plots are changing** (typically in the shape of a funnel for increasing variance), then Heteroscedasticity is present:

<!-- Obtained from Corporate Finance Institute -->
![HOMOSCEDASTICITY](Assets/4.%20Linear%20Regression%20Assumptions.md/HOMOSCEDASTICITY.png){.center}

!!! Tip

    A model can be both Unbiased and Heteroscedastic, they have no influence on one another:

    <!-- Obtained from Condor -->
    ![HETEROSCEDASTIC_SCENARIOS](Assets/4.%20Linear%20Regression%20Assumptions.md/HETEROSCEDASTIC_SCENARIOS.png){.center}

The most direct method to deal with Heteroscedasticity is to perform a **transformation** on the variables that will stabilize its variance, such the **Logarithm or Squareroot**. These transformations typically compress larger values while expanding smaller values, helping to shrink the variance.

!!! Tip

    The transformations listed above require positive-only data, thus a constant can be added to all variables to ensure that they are positive.

Alternatively, if the source of heteroscedasticity is known, it is possible to use a **Weighted Least Squares** approach to account for it. Weights are assigned to **each observation**, such that the weight is the amount needed to **scale the the variance to a constant amount**:

$$
\begin{aligned}
    \text{Assumed Constant Variance} &= \sigma^{2} \\
    \text{Current Variance} &= \text{Var}(\varepsilon_{i}) \\
    \text{Var}(\varepsilon_{i}) &= \frac{\sigma^{2}}{w_{i}}
\end{aligned}
$$

!!! Info

    In practice, the weights are completely arbitrary and up to the modeller. They must be positive and do not need to sum to 1.

A new funcitonal form is assumed, where the **resulting variance is constant**:

$$
\begin{aligned}
    y_{i} &= \beta_{0} + \beta_{1} \cdot X_{1} + \varepsilon \\
    \sqrt{w_{i}} \cdot y_{i} &= \sqrt{w_{i}} \cdot \left(\beta_{0} + \beta_{1} \cdot x_{1} + \varepsilon_{i} \right) \\
    \\
    \text{Var}(\sqrt{w_{i}} \cdot \varepsilon_{i})
    &= \left(\sqrt{w_{i}} \right)^{2} \cdot \text{Var}(\varepsilon_{i}) \\
    &= w_{i} \cdot \frac{\sigma^{2}}{w_{i}} \\
    &= \sigma^{2}
\end{aligned}
$$

!!! Tip

    The above doesnt "fix" the heteroscedasticity, it simply accounts for it by using an alternative method.

    In another way, heteroscedasticity is not inherently problematic; the problem is applying a method that assumes Homoscedastacity (linear regression) to a dataset with heteroscedasticity.

## **Assumption 5: No Serial Correlation**

This assumption states that the error terms should **NOT be correlated** with one another:

$$
\begin{aligned}
    \text{Cov}(\varepsilon_{i}, \varepsilon_{j})
    &= E(\varepsilon_{i}, \varepsilon_{j}) - E(\varepsilon_{i}) \cdot E(\varepsilon_{j}) \\
    &= E(\varepsilon_{i}, \varepsilon_{j}) - 0 \\
    \\
    E(\varepsilon_{i}) &= E(\varepsilon_{j}) = 0, \text{(Assumption 3)} \\
    \\
    \therefore \text{Cov}(\varepsilon_{i}, \varepsilon_{j}) &= 0 \rightarrow E(\varepsilon_{i}, \varepsilon_{j}) = 0
\end{aligned}
$$

!!! Info

    This is also sometimes referred to as the **No Autocorrelation** assumption.

Similarly, this can be validated using a residual plot against other residuals **over different predictions**. The resulting plot should not have any exhibit any clear trend or pattern:

<!-- Obtained from Stack Exchange -->
![AUTOCORRELATION](Assets/4.%20Linear%20Regression%20Assumptions.md/AUTOCORRELATION_PLOT.png)

If violated, it implies that knowing one error can **systematically provide information about another**; meaning that there are additional **unmodelled factors** to consider. The residuals are no longer accurate, which consequently impacts the validity of BLUE and the statistical inferences.

## **Assumption 6: Normality**

The last assumption is assuming that the error terms follow a **Normal Distribution**:

$$
    \varepsilon \sim N(0, \sigma^{2})
$$

It can verified by using a **QQ plot** or other equivalent distribution tests (EG. Kolmogorov Test).

Violating the normality assumption does not impact the validity of the estimates or predictive power; it only impacts the validity of statistical inference.

## **Other Assumptions**

### **No Extreme Outliers**

Outliers are observations with **unusual values of the DV** relative to fitted regression model.

The last OLS assumption is that there are **no extreme outliers** in the dataset used to create the regression model. Generally, as long as the DV and IVs have a **positive and finite Kurtosis**, then the probability of such observations occuring are low.

Outliers are problematic as OLS is **sensitive to outliers**. Extreme outliers have large residuals which **receive more weight** in the optimization process, which causes the resulting model to accomodate it (when it should not), causing the resulting coefficients to be biased.

<!-- Obatained from Research Gate -->
![Outliers](Assets/4.%20Gauss%20Markov%20Theorem.md/Outliers.png){.center}

#### **Identifying Outliers**

By definition, Outliers have **unusually large residuals**. In order to gauge what is considered a "large residual", the residuals are standardized for comparison.

Standardization requires knowledge of the **sampling distribution of the residuals**. Given that the errors have a constant variance of $\sigma^2$, the variance of the residuals can be shown to be:

$$
    var(\hat{\varepsilon}_i) = \sigma^2 (1-h_{ii})
$$

> $h_ii$ is known as the Leverage of the observation, which will be covered in the following section.

The sampling distribution can then be determined:

$$
    \hat{\varepsilon}_i ~ N(0, \sigma^2 (1-h_{ii}))
$$

Thus, the standardized residuals are the raw residuals scaled by their standard error:

$$
    \hat{\varepsilon}_i^{\text{standardized}} = \frac{\hat{\varepsilon}_i}{\sqrt{\sigma^2 (1-h_{ii})}}
$$

> In practice, the variance of the errors are unknown, thus it is estimated using the Sample Variance of the residuals instead.

Residuals with standardized values of **larger than 2 or 3** are considered large and thus can be considered as an outlier.

#### **High Leverage Points**

While outliers are unusual points of the DV, **High Leverage observations** have unusual values of the IV relative to the majority of the values.

It is easy to identify high leverage points when there is only one IV through a scatterplot - simply find the observation that is away from the rest.

It becomes much more complicated when there are multiple IVs. The observation's values for each of the IVs could be in the common range of each IV, but **unusual when taken collectively**:

<!-- Obtained from ACTEX Manual -->
![High Leverage Points](Assets/4.%20Gauss%20Markov%20Theorem.md/High%20Leverage.png){.center}

The leverage of the observation can be determined from the MLR:

$$
\begin{aligned}
    \hat{\boldsymbol{y}}
    &= \boldsymbol{X}\boldsymbol{\beta} \\
    &= \boldsymbol{X}[(X'X)^{-1}y] \\
    &= \boldsymbol{H}\boldsymbol{y}
\end{aligned}
$$

$\boldsymbol{H}$ is known as the **Hat Matrix** as it puts a hat on y in the notation. The leverage of the $ith$ observation is the **$ith$ diagonal element** of the matrix, $h_{ii}$.

An observation is considered to have *high* leverage if its leverage is greater than three times the average leverage:

$$
    h_{ii} \gt 3 \left(\frac{p+1}{n}\right)
$$

#### **Influential Points**

The effect of Outliers and High leverage points can both be summarized into a concept known as **Influence**. An observation is influential if the **exclusion of the observation from the regression** leads to significantly differently results.

In general the process involves three steps:

1. Fit the original model with all $n$ observations; determine the $j-th$ fitted value $\hat{y}_j$
2. Fit an adjusted model with omitting the $i-th$ observation, determine the $j-th$ fitted value $\hat{y}_{j(i)}$
3. Calculate the **change in the $j-th$ fitted value**

This process has to be repeated for **all fitted values for all observations**.

**Cook's Distance** summarizes the effect of the $i-th$ observation on the whole model:

$$
    D_i = \frac{\sum^n_{j=1} (\hat{y}_j - \hat{y}_{j(i)})^2}{(p+1) \cdot MS_{Residuals}}
$$

This method of computation requires $n+1$ datasets - 1 dataset with all the observation and $n$ datasets with the $i-th$ observation omitted. It is also extremely time consuming to have to fit a model to each dataset.

An alternative method of determining Cook's Distance is to make use of both the Outliers and Leverage:

$$
    D_i = \frac{1}{p+1} \cdot (e^{\text{standardized}})^2 \cdot \frac{h_{ii}}{1-h_{ii}}
$$

Thus, an observation must be **unusual in BOTH the DV and IV** in order to be considered influential. Outliers and High leverage points are *necessary but not sufficient* conditions to be influential.

## High dimension

df