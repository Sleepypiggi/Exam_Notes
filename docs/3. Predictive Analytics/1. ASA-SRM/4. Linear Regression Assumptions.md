# **Linear Regression Asssumptions**

## **Overview**

There are **5 key assumptions** that have been implicitly made thus far for Linear Regression, each allowing us to make key conclusions about the model:

<!-- Obtained from Stack Exchange User DVL -->
![REGRESSION_ASSUMPTIONS](Assets/4.%20Linear%20Regression%20Assumptions.md/REGRESSION_ASSUMPTIONS.png){.center}

!!! Note

    The colours of the arrows indicate which assumptions are cumulatively needed:

    * Red + Blue = Purple
    * Green + Purple = Black

The purpose of this section is to examine WHY we need the respective assumptions, CONSEQUENCES on the results if they are violated and HOW to verify the assumptions.

If the first two blocks of assumptions are fulfilled, then the resulting regression esimate is known as the **Best Linear Unbiased Estimator** (BLUE), which has the **lowest variance** (most efficient) among all other linear unbiased estimates. This is known as the **Guass Markov Theorem**.

!!! Warning

    The last block of assumptions on the distribution of the errors being normal and iid are NOT needed for the estimates to be BLUE. They are however needed for **statistical inference**.

!!! Warning

    Remember that being unbiased means that the **EXPECTED VALUE** of the estimates are equal to the true value. Thus, for a particular sample, the estimated coefficients might be different.

## **Assumption 1: Linearity**

This assumes that the **true relationship** between the dependent and independent variables are Linear, but not the dependent or independent variables themselves. This allows for **transformations** of the respective variables:

$$
\begin{aligned}
    Y &= \beta_{0} + \beta_{1} \cdot X_{1} + \beta_{2} \cdot X_{2} \\
    Y &= \beta_{0} + \beta_{1} \cdot X_{1} + \beta_{2} \cdot X^{2}_{1} \\
    \ln Y &= \beta_{0} + \beta_{1} \cdot X_{1} + \beta_{2} \cdot X_{2}
\end{aligned}
$$

If the relationship between the two is not linear, a linear regression model will not be able to accurately capture the relationship between the two, resulting in **Bias estimates**, which consequently impacts the validity of BLUE and the statistical inferences.

Linearity can be verified using either of the following:

* **Scatterplot of the dependent and independent**: Plot should show a linear relationship (intuitive)
* **Scatterplot of the residuals and the fitted model** (Residual Plot): Plot should be centered around 0

!!! Tip

    In reality, Errors are unobservable, thus the best estimate of them are the residuals.

    IF the fitted model has **fully captured the relationship** between the dependent and independent, then the resulting residuals should be **random** - with an average of 0.

    Thus, if we observe the residuals to be centered around 0, it implied the fitted linear model has fully captured the relationship, implying that the **true relationship is indeed linear**.

<!-- Obtained from Lumen Learning -->
![SCATTER_RESIDUAL_PLOT](Assets/4.%20Linear%20Regression%20Assumptions.md/SCATTER_RESIDUAL_PLOT.png){.center}

If it is found that the relationship is non-linear, **transformations** could be done on the dependent and/or independent variable to make the resulting relationship linear.

## **Assumption 2: Perfect Collinearity**

Collinearity (or Multi-collinearity) refers to when an independent variable can be expressed as a **linear combination of one or more other independent variables**.

Perfect collinearity refers to when it can be **perfectly expressed** as another variable:

* Multiple of another variable: $x_{1} = c \cdot x_{2}$
* Differs by a constant: $x_{1} = x_{2} \pm c$
* Dummy variable trap
* Affine transformations

If there are perfectly collinear variables, then there will be **insufficient information** to solve for the parameter estimates (EG. Two equations to solve for three unknowns). This results in **non-unique solutions**.

!!! Warning

    It is NOT a necessity for the predictors to be independent; zero collinearity. It is generally assumed that collinearity is low.

If variables are perfectly collinear, the only solution is to **drop the highly collinear variables** and keep only one of the bunch.

### **Imperfect Collinearity**

Imperfect collinearity occurs when one variable is **highly (but NOT perfectly) correlated** with one or more other variables. Intuitively, this means that the variable does not add much predictive power as its effects would have **already been captured via the related predictors**, hence can be removed from the model with little loss.

Imperfect collinearity does NOT prevent OLS from finding a solution and does NOT affect the overall predictive power of the model. It affects the inferences of the model:

* Usual intepretation of "holding other variables constant" no longer holds true as the variables move together; **hard to seperate the effects of individual variables**
* Consequently, OLS has difficulty estimating the estimates as it cannot clearly distinguish them; resulting in **unstable estimates** which can change drastically over different samples
* Consequently, this **increases the variance of the estimates**, which increases p-values, potentially leading to **more false negatives** and hence important variables being omitted from the regression

!!! Warning

    Collinearity ONLY affects t-test results, overall predictive power (measured by F-tests), are unaffected.

!!! Tip

    There is a special type of Variable known as a **Suppressor Variable**, which by itself is not strongly correlated with the dependent, but when combined with another variable, leads to a significantly stronger model.

    Collinearity is typically acceptable in the presence of a suppressor variable.

If imperfect collinearity is present, regularization methods (Ridge/LASSO) can be used to deal with them:

* Coefficient estimates are unstable as there are many different ways to distribute the coefficients to achieve the same overall impact
* Regularization imposes a **size constraint that biases them towards 0**, forcing the estimates to hover around that range, reducing variance
* EG. Without regularization, there could be a large positive and large negative coefficient offsetting each other. Regularization eliminates such a case from occuring as they are not allowed to vary so much

### **Detecting Collinearity**

The simplest way to detect collinearity is through a **Scatterplot or Correlation Matrix**, which shows the correlations between **pairs of variables**. A **correlation of 0.8 and higher** is typically considered high enough where the collinearity becomes problematic.

<!-- Obtained from ACTEX Manual -->
![CORRELATION_MATRIX](Assets/4.%20Linear%20Regression%20Assumptions.md/CORRELATION_MATRIX.png){.center}

HOWEVER, the above method can only detect collinearity between pairs of variables at a time. To check for collinearity between **3 or more** variables, the **Variance Inflation Factor** (VIF) can be used instead.

Recall that in the presence of collinearity, the variance of estimates increases; the VIF is a **measure of the increase in variance**. 

$$
    \text{VIF}_{p} = \frac{1}{1 - R^{2}_{p}}
$$

!!! Tip

    Different texts have **different thresholds** for high collinearity. For this exam, assume the **threshold to be 10**.
    
    The reciprocal of the VIF is known as the Tolerance.

$R^{2}_{p}$ is the R-squared for a model where the p-th predictor is regressed against ALL OTHER predictors. If this R-squared is high, it implies that this predictor is **well explained** by other predictors (high correlation, high collinearity).

!!! Warning

    The VIF needs to be calculated for EACH variable to conclude whether collinearity is present in the model. Collinearity is present even if just *one* combination of variables cross the threshold:
    
    * Variable 1 against Variable 2,3
    * Variable 2 against Variable 1,3
    * Variable 3 against Variable 1,2
    
    The VIF for uncorrelated predictors is 1, reflecting that the variance remains **unaffected** by the other variables.

The VIF directly impacts the standard error of the regression coefficients:

$$
\begin{aligned}
    \text{SE}'(\beta_{p})
    &= \sqrt{\text{VIF}} \cdot \text{SE}(\beta_{p}) \\
    &= \sqrt{\text{VIF}} \cdot \sigma^{2}_{\text{RSS}} \cdot \frac{1}{\sum (x - \bar{x})^{2}}
\end{aligned}
$$

!!! Tip

    Thus, given the final standard errors, this can be used to backsolve for the VIF as well.

## **Assumption 3: Exogenity**

**Exogenity** refers to a variable that is **outside** the model and thus must be **independent of the other variables inside the model**. In the context of linear regression, the **errors should be independent of any of the predictors**:

$$
\begin{aligned}
    E(\varepsilon \mid X) &= 0 \\
    \\
    E(\varepsilon)
    &= E[E(\varepsilon \mid X)] \\
    &= E[0] \\
    &= 0 \\
    \\
    E(X \cdot \varepsilon)
    &= E[X \cdot E(\varepsilon \mid X)] \\
    &= E(X \cdot 0) \\
    &= 0 \\
    \\
    \therefore \text{Cov}(\varepsilon, X)
    &= E(\varepsilon, X) - E(X) \cdot E(\varepsilon) \\
    &= 0 - E(X) \cdot 0 \\
    &= 0
\end{aligned}
$$

!!! Info

    "Exo" and "Endo" in greek means "In" and "Out" respectively. Thus, another way of referring to the assumptions is whether or not Endogenity is present.

!!! Warning

    Recall that Zero Covariance is a result of independence, NOT the other way around.

If Exogenity is violated, there is a relationship between the predictors and the error (reflecting unmodelled variables). This means that the estimated coefficients would reflect the **effect of BOTH the predictor and the unmodelled variable**, resulting in a **biased estimate**, which consequently impacts the validity of BLUE and the statistical inferences.

<!-- Obtained from Statology -->
![OMITTED_VARIABLE_BIAS](Assets/4.%20Linear%20Regression%20Assumptions.md/OMITTED_VARIABLE_BIAS.png){.center}

!!! Info

    The ommitted correlated variable is said to Confound (mix up) the regression results. Thus, it is sometimes referred to as the **Confounding Variable**. This process in general is known as the **Omitted Variable Bias**.

Similar to before, a **residual plot of the residuals against the predictors** can be used to check for Exogenity; there should be no visible trend in the plots:

<!-- Obtained from Penn State University -->
![SCATTER_RESIDUAL_PREDICTOR](Assets/4.%20Linear%20Regression%20Assumptions.md/SCATTER_RESIDUAL_PREDICTOR.png)

!!! Note

    The above plot are the residuals against a single predictor. Plotting against the fitted values would imply plotting against all predictors at once.

There are two possible solutions if Exogenity is violated:

1. Include the Confounding Variable
2. Introduce an Instrumental Variable

<!-- Obtained From Shakrim Blog -->
![INSTRUMENTAL_VARIABLE](Assets/4.%20Linear%20Regression%20Assumptions.md/INSTRUMENTAL_VARIABLE.png){.center}

## **Assumption 4: Homoscedasticity**

**Homoscedasticity** refers to the error terms having **constant variance** while **Heteroscedasticity** refers to having non-constant variance.

$$
    \text{Var}(\varepsilon \mid X) = \sigma^{2}
$$

!!! Info

    Similarly, they come from the terms "Homo" and "Hetero" which means "Same" and "Different".

If Homoscedastacity is not present, the **sampling distribution of the estimates are not easily derived** and hence cannot be proven that they are the most efficient estimators. Consequently, statistical inference results might not be reliable as well.

Similar to before, a residual plot can be used to identify patterns. If the **spread of the plots are changing** (typically in the shape of a funnel for increasing variance), then Heteroscedasticity is present:

<!-- Obtained from Corporate Finance Institute -->
![HOMOSCEDASTICITY](Assets/4.%20Linear%20Regression%20Assumptions.md/HOMOSCEDASTICITY.png){.center}

!!! Tip

    A model can be both Unbiased and Heteroscedastic, they have no influence on one another:

    <!-- Obtained from Condor -->
    ![HETEROSCEDASTIC_SCENARIOS](Assets/4.%20Linear%20Regression%20Assumptions.md/HETEROSCEDASTIC_SCENARIOS.png){.center}

The most direct method to deal with Heteroscedasticity is to perform a **transformation** on the variables that will stabilize its variance, such the **Logarithm or Squareroot**. These transformations typically **compress larger values while expanding smaller values**, helping to shrink the variance.

!!! Tip

    The transformations listed above require positive-only data, thus a **positive constant** can be added to all variables to ensure that they are positive - EG. $\ln(1+Y)$

!!! Warning

    The **direction of heteroscedasticity** matters. Concave transformations like log or squareroot work for datasets with **increasing variance**, which is usually the case. For decreasing variance or more complex patterns, other methods should be used.

The inherent problem with Heteroscedasticity is its effect on Standard Errors; the current formulation assumes a homoscedastic one. Thus, it is possible to use a standard error that can account for it - known as a **Heteroscedastic Consistent Standard Error** or **Robust Standard Error**.

!!! Note

    The mathematics for robust standard errors are beyond the scope of this course. It is sufficient to know that they can be used.

### **Breusch Pagan Test**

Heteroskedasticity implies that the size of the residuals **changes with the predictors**. Thus, this can be formally tested using the **Breusch Pagan Test** where the **residuals are regressed against the predictors**. If there is found to be a relationship between the two, then heteroskedasticity is likely present.

* $H_{0}$: $\beta^{*}_{p} \eq 0$
* * $H_{1}$: $\beta^{*}_{p} \ne 0$

$$
\begin{aligned}
    \text{Test Statistic} &= \frac{\text{RSS}^{*}}{2} \\
    \text{Test Statistic} &\sim \chi^{2}_{p}
\end{aligned}
$$

### **Weighted Least Squares**

If the source of heteroscedasticity is known, it is possible to use a **Weighted Least Squares** approach to account for it. Weights are assigned to **each observation**, such that the weight is the amount needed to **scale the the variance to a constant amount**:

$$
\begin{aligned}
    \text{Assumed Constant Variance} &= \sigma^{2} \\
    \text{Current Variance} &= \text{Var}(\varepsilon_{i}) \\
    \text{Var}(\varepsilon_{i}) &= \frac{\sigma^{2}}{w_{i}}
\end{aligned}
$$

!!! Info

    In practice, the weights are completely arbitrary and up to the modeller. They must be positive and do not need to sum to 1.

A new funcitonal form is assumed, where the **resulting variance is constant**:

$$
\begin{aligned}
    y_{i} &= \beta_{0} + \beta_{1} \cdot X_{1} + \varepsilon \\
    \sqrt{w_{i}} \cdot y_{i} &= \sqrt{w_{i}} \cdot \left(\beta_{0} + \beta_{1} \cdot x_{1} + \varepsilon_{i} \right) \\
    \\
    \text{Var}(\sqrt{w_{i}} \cdot \varepsilon_{i})
    &= \left(\sqrt{w_{i}} \right)^{2} \cdot \text{Var}(\varepsilon_{i}) \\
    &= w_{i} \cdot \frac{\sigma^{2}}{w_{i}} \\
    &= \sigma^{2}
\end{aligned}
$$

!!! Tip

    The above doesnt "fix" the heteroscedasticity, it simply accounts for it by using an alternative method.

    In another way, heteroscedasticity is not inherently problematic; the problem is applying a method that assumes Homoscedastacity (linear regression) to a dataset with heteroscedasticity.

## **Assumption 5: No Serial Correlation**

This assumption states that the error terms should **NOT be correlated** with one another:

$$
\begin{aligned}
    \text{Cov}(\varepsilon_{i}, \varepsilon_{j})
    &= E(\varepsilon_{i}, \varepsilon_{j}) - E(\varepsilon_{i}) \cdot E(\varepsilon_{j}) \\
    &= E(\varepsilon_{i}, \varepsilon_{j}) - 0 \\
    \\
    E(\varepsilon_{i}) &= E(\varepsilon_{j}) = 0, \text{(Assumption 3)} \\
    \\
    \therefore \text{Cov}(\varepsilon_{i}, \varepsilon_{j}) &= 0 \rightarrow E(\varepsilon_{i}, \varepsilon_{j}) = 0
\end{aligned}
$$

!!! Info

    This is also sometimes referred to as the **No Autocorrelation** assumption.

Similarly, this can be validated using a residual plot against other residuals **over different predictions**. The resulting plot should not have any exhibit any clear trend or pattern:

<!-- Obtained from Stack Exchange -->
![AUTOCORRELATION](Assets/4.%20Linear%20Regression%20Assumptions.md/AUTOCORRELATION_PLOT.png)

If violated, it implies that knowing one error can **systematically provide information about another**; meaning that there are additional **unmodelled factors** to consider. The residuals are no longer accurate, which consequently impacts the validity of BLUE and the statistical inferences.

## **Assumption 6: Normality**

The last assumption is assuming that the error terms follow a **Normal Distribution**:

$$
    \varepsilon \sim N(0, \sigma^{2})
$$

!!! Tip

    This assumption also implies that the dependent is normally distributed, as a linear function involving a normal variable is also normally distributed.

It can verified by using a **QQ plot** or other equivalent distribution tests (EG. Kolmogorov Test).

Violating the normality assumption does not impact the validity of the estimates or predictive power; it only impacts the **validity of statistical inference**. For large samples, CLT states that the distribution of the estimators are **approximately normal**, thus inferences are still valid. However, this **does NOT apply to prediction intervals** as they directly contain the variance of the error term, which follows an unknown distribution.

## **Other Considerations**

### **Influence**

An **Influential Observation** is an observation, that when excluding from the model, leads to a **significant impact** on the model's estimation and results. It is typically a result a consequence of either or both of the following effects:

1. Unusual Values in X (known as **High Leverage** observations)
2. Unusual Values in Y (known as **Outliers**)

!!! Warning

    In other texts, outliers refer to unusual values in both X and Y; in the more general sense of the word. For the purposes of this exam, assume that they only refer to Y values.

<!-- Obtained from Cambridge -->
![INFLUENCE_EXAMPLE](Assets/4.%20Linear%20Regression%20Assumptions.md/INFLUENCE_EXAMPLE.png){.center}

!!! Warning

    Observations with high leverage or that is an outlier merely have the *potential* to be Influential. They are not guaranteed to be influential; it depends on other component.

#### **Leverage**

The **Leverage** for an observation is the element for that observation on the **diagonal of the Hat matrix**:

$$
\begin{aligned}
    h_{i}
    &= \boldsymbol{H}_{ii} \\
    &= \boldsymbol{X}^{T}_{i} (\boldsymbol{X}^{T} \cdot \boldsymbol{X})^{-1} \boldsymbol{X}_{i} \\
    &= \frac{SE(\hat{y})}{\sigma^{2}}
\end{aligned}
$$

<!-- Obtained from Stack Exchange -->
![LEVERAGE_HAT_MATRIX](Assets/4.%20Linear%20Regression%20Assumptions.md/LEVERAGE_HAT_MATRIX.png){.center}

!!! Tip

    The diagonal of the hat matrix has two key interpretations:

    1. It quantifies the impact that the observed value of $y$ has on its predicted value $\hat{y}$ (Hence the name "Leverage")
    2. It quantifies the distance between the observed $x$ value from the mean of the $x$ value

    Note that despite the interpretation (1), the impact is entirely dependent on $x$ values, as seen in the formula. The more unusual the $x$ value, the higher leverage it has.

!!! Note

    Intepretation (1) translated into mathematical terms is that it is the partial derivative of the prediction with respect to the observed value.

    Thus for **SLR** only, it can be shown that the leverage reduces to the following:

    $$
        h_{i} = \frac{1}{n} + \frac{(x_{i} - \bar{x})^{2}}{\sum (x - \bar{x})^{2}}
    $$

Leverage MUST be a **positive value** and the **sum of all leverages** is equivalent to the **number of predictors AND the intercept**. It is generally accepted that a high leverage observation must have be **at least 3 times** the average leverage:

$$
\begin{aligned}
    0 &\lt h_{ii} \lt 1 \\
    \\
    \bar{h}
    &= \frac{\sum h}{n} \\
    &= \frac{p+1}{n} \\
    \\
    \therefore h_{ii} &\gt 3 \cdot \bar{h}
\end{aligned}
$$

#### **Outliers**

The size of the residual can be used to determine the presence of Outliers. However, since the size of the residual is affected by its underlying units, we typically **standardize the residuals** before analyzing them. It is generally accepted that observations with a **standardized residual larger than 2** are considered outliers:

$$
\begin{aligned}
    \hat{\varepsilon}_{\text{Sta}}
    &= \frac{\hat{\varepsilon}^{2}}{\sqrt{\sigma^{2}_{\text{RSS}} (1 - h_{i})}} \\
    \\
    \therefore \hat{\varepsilon}_{\text{Sta}} &\gt 2
\end{aligned}
$$

!!! Tip

    It can be shown the distribution of the **residuals** are:
   
    $$
    \begin{aligned}
        \hat{\varepsilon}_{i} &\sim N(0, \sigma^{2} \cdot (1 - h_{i})) \\
        \\
        \text{Var}(\hat{\varepsilon})
        &= \text{Var}(\boldsymbol{y} - \hat{\boldsymbol{y}}) \\
        &= \text{Var}((\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{y}) \\
        &= (\boldsymbol{I} - \boldsymbol{H})^{2} \cdot \text{Var}(\boldsymbol{y}) \\
        &= (\boldsymbol{I} - \boldsymbol{H}) \cdot \text{Var}(\boldsymbol{y}), \text{Idempotent} \\
        &= (1 - h_{i}) \cdot \sigma^{2}
    \end{aligned}
    $$

    Note that the above is for the residuals, NOT the actual errors themselves.

!!! Tip

    One consequence of having many outliers is that the calculated MSE is **inflated** because the outliers artifically increase the RSS. This created a **TENDENCY** for statistical tests to fail to reject the null.

    In reality, if there are outliers and the statistical test are able to reject the null, there is no reason to doubt its validity. This is because even under "adverse" situations, the test can still reject the null, thus under normal circumstances it would continue to reject the null.

    However, if the test currently fails to reject the null, it could be due to the "adverse" situation; under normal circumstances it might reject the null thus there is a reason to doubt the results.

However, one issue with the above analysis is that the fitted model has already been influenced by the observation, thus the residuals will not flag out as being large. This can be overcome by fitting a **new model WITHOUT the observation** and computing the residual based on it, known as a **Deleted Residual**. Standardizing them results in **Studentized Residuals**.

$$
\begin{aligned}
    d_{i}
    &= y_{i} - \hat{y}_{(i)} \\
    \\
    \hat{\varepsilon}_{\text{Stu}}
    &= \frac{d_{i}}{\text{SE}(d_{i})} \\
    &= \frac{\hat{\varepsilon}_{t}}{\sqrt{\sigma^{2}_{(i)} (1 - h_{i})}} \\
    &= \hat{\varepsilon}_{\text{Sta}} \cdot \left(\frac{n-k-2}{n-k-1 - \hat{\varepsilon}^{2}_{\text{Sta}}} \right)^{0.5}
\end{aligned}
$$

!!! Tip

    $\hat{y}_{(i)}$ is the prediction for a model with i-th observation removed; similar for the MSE.

!!! Note

    As its name suggests, the above residuals follow the **student's t-distribution**.

#### **Cooks Distance**

The effect of Leverage and Residual Size can be combined into a single measure known as **Cook's Distance**, which is a measure of **how the fitted model changes** if the observation were removed:

$$
\begin{aligned}
    D_{i}
    &= \frac{d_{i}^{2}}{\sigma^{2} (p+1)} \\
    &= d_{i}^{2} \cdot \frac{1}{\sigma^{2} (p+1)} \\
    &= \hat{\varepsilon}^{2} \cdot \frac{h_{i}}{(1 - h_{i})^{2}} \cdot \frac{1}{\sigma^{2} (p+1)} \\
    &= \frac{\hat{\varepsilon}^{2}}{\sigma^{2} (p+1)} \cdot \frac{h_{i}}{(1 - h_{i})^{2}} \\
    &= \frac{\hat{\varepsilon}^{2}}{\sigma^{2} (1 - h_{i})} \cdot \frac{h_{i}}{(1 - h_{i})(p+1)} \\
    &= \frac{\hat{\varepsilon}^{2}}{(\sqrt{\sigma^{2} (1 - h_{i})})^{2}} \cdot \frac{h_{i}}{(1 - h_{i})(p+1)} \\
    &= \left(\frac{\hat{\varepsilon}}{\sqrt{\sigma^{2} (1 - h_{i})}} \right)^{2} \cdot \frac{h_{i}}{(1 - h_{i})(p+1)} \\
    &= (\hat{\varepsilon}_{\text{Sta}})^{2} \cdot \frac{h_{i}}{(1 - h_{i})(p+1)}
\end{aligned}
$$

!!! Tip

    It is computationally expensive to compute the Cooks Distance because a unique model has to be fit for every observation. However, since the leverage of an observation already tells us how much an observation impacts the predicted value, there is **no need to run a seperate model** each time; the original residuals can be scaled accordingly to determine its impact.

    Thus, the final result can be reduced to a combination of the **Standardized Residual** (Outlier) and **Leverage**.

    <!-- Obtained from rpubs -->
    ![LEVERAGE_INFLUENCE](Assets/4.%20Linear%20Regression%20Assumptions.md/LEVERAGE_INFLUENCE.png){.center}

!!! Note

    A typical value for Cooks Distance is $\frac{1}{n}$. It can be understood as each observation having equal importance.

If the influential point is a result of a **data collection** error, then the observation should be removed. Otherwise, we need a good reason to delete the observation. Observations **should NOT be deleted just because they do not fit our pre-conceived model**; we should aim to reflect reality as closely as possible. Thus, some common methods would be to:

1. Make a note about the observation
2. Create a binary variable to account for the observation (ie have results with and without it)

### **High Dimensionality**

As alluded to previously, increasing the number of parameters in the model does not necessarily benefit the model. This phenomenon is known as the **Curse of Dimensionality**.

A sample of 100 observations is a lot of information for a model with 2 predictors, but is sparse for a model with 50 predictors. Thus, **increasing the number of predictors relative to the sample size** tends to have detrimental effects:

* $p \gt n$: **Insufficient information** to fit the model
* $p = n$: Model fits data *perfectly*, resulting in **overfitting**
* $p \lt n$, but limited degrees of freedom: Esimates become **unstable**, resulting in higher standard errors and hence **incorrect statistical inferences**

!!! Note

    It is also a possibility that increasing the number of predictors results in choosing two or more highly correlated variables, resulting in Multicollinearity.

The natural solution would be to **limit the number of predictors** by choosing only truly relevant variables and/or to **increase the sample size**.

#### **Partial Least Squares**

Another method is to use **Partial Least Squares** (PLS). It is very similar to PCA - it seeks to create new variables that are linear combinations of existing variables (known as **Directions**) which are then used to perform the regression instead. Each subsequent direction uses the residuals of the previous direction.

The key difference when compared to PCA is that PLS finds new variables with the goal of explaining the dependent variable. Consequently, cross validation can be used to determine the optimal number of directions.

!!! Tip

    ALL features are used to create the directions; there is no variable selection (dropping of variables) involved, only involvement to different extents.
