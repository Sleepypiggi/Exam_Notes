# **Linear Regression Asssumptions**

## **Overview**

There are **5 key assumptions** that have been implicitly made thus far for Linear Regression, each allowing us to make key conclusions about the model:


<!-- Obtained from Stack Exchange User DVL -->
![REGRESSION_ASSUMPTIONS](Assets/4.%20Linear%20Regression%20Assumptions.md/REGRESSION_ASSUMPTIONS.png){.center}

!!! Note

    The colours of the arrows indicate which assumptions are cumulatively needed:

    * Red + Blue = Purple
    * Green + Purple = Black

The purpose of this section is to examine WHY we need the respective assumptions, CONSEQUENCES on the results if they are violated and HOW to verify the assumptions.

If the first two blocks of assumptions are fulfilled, then the resulting regression esimate is known as the **Best Linear Unbiased Estimator** (BLUE), which has the **lowest variance** (most efficient) among all other linear unbiased estimates. This is known as the **Guass Markov Theorem**.

!!! Warning

    The last block of assumptions on the distribution of the errors being normal and iid are NOT needed for the estimates to be BLUE. They are however needed for **statistical inference**.

## **Assumption 1: Linearity**

This assumes that the **true relationship** between the dependent and independent variables are Linear, but not the dependent or independent variables themselves. This allows for **transformations** of the respective variables:

$$
\begin{aligned}
    Y &= \beta_{0} + \beta_{1} \cdot X_{1} + \beta_{2} \cdot X_{2} \\
    Y &= \beta_{0} + \beta_{1} \cdot X_{1} + \beta_{2} \cdot X^{2}_{1} \\
    \ln Y &= \beta_{0} + \beta_{1} \cdot X_{1} + \beta_{2} \cdot X_{2}
\end{aligned}
$$

If the relationship between the two is not linear, a linear regression model will not be able to accurately capture the relationship between the two, resulting in **Bias estimates**, which consequently impacts the validity of BLUE and the statistical inferences.

Linearity can be verified using either of the following:

* **Scatterplot of the dependent and independent**: Plot should show a linear relationship (intuitive)
* **Scatterplot of the residuals and the fitted model** (Residual Plot): Plot should be centered around 0

!!! Tip

    The intuition behind Residual Plots is that IF the fitted model has **fully captured the relationship** between the dependent and independent, then the resulting residuals should be **random** - with an average of 0.

    Thus, if we observe the residuals to be centered around 0, it implied the fitted linear model has fully captured the relationship, implying that the **true relationship is indeed linear**.

<!-- Obtained from Lumen Learning -->
![SCATTER_RESIDUAL_PLOT](Assets/4.%20Linear%20Regression%20Assumptions.md/SCATTER_RESIDUAL_PLOT.png){.center}

If it is found that the relationship is non-linear, **transformations** could be done on the dependent and/or independent variable to make the resulting relationship linear.

## **Assumption 2: Perfect Collinearity**

Collinearity (or Multi-collinearity) refers to when an independent variable can be expressed as a **linear combination of one or more other independent variables**.

Perfect collinearity refers to when it can be **perfectly expressed** as another variable:

* Multiple of another variable: $x_{1} = c \cdot x_{2}$
* Differs by a constant: $x_{1} = x_{2} \pm c$
* Dummy variable trap
* Affine transformations

If there are perfectly collinear variables, then there will be **insufficient information** to solve for the parameter estimates (EG. Two equations to solve for three unknowns). This results in **non-unique solutions**.

### **Imperfect Collinearity**

Imperfect collinearity occurs when one variable is **highly (but NOT perfectly) correlated** with one or more other variables. Intuitively, this means that the variable does not add much predictive power as its effects would have **already been captured via the related predictors**, hence can be removed from the model with little loss.

Imperfect collinearity does NOT prevent OLS from finding a solution and does NOT affect the overall predictive power of the model. It affects the inferences of the model:

* Usual intepretation of "holding other variables constant" no longer holds true as the variables move together; **hard to seperate the effects of individual variables**
* Consequently, OLS has difficulty estimating the estimates as it cannot clearly distinguish them; resulting in **unstable estimates** which can change drastically over different samples
* Consequently, this **increases the variance of the estimates**, which increases p-values, potentially leading to **more false negatives** and hence important variables being omitted from the regression

!!! Warning

    Collinearity only affects t-test results, overall predictive power (measured by F-tests), are unaffected.

### **Detecting Collinearity**

The simplest way to detect collinearity is through a **Scatterplot or Correlation Matrix**, which shows the correlations between **pairs of variables**. A **correlation of 0.8 and higher** is typically considered high enough where the collinearity becomes problematic.

<!-- Obtained from ACTEX Manual -->
![CORRELATION_MATRIX](Assets/4.%20Linear%20Regression%20Assumptions.md/CORRELATION_MATRIX.png){.center}

However, the above method can only detect collinearity between pairs of variables at a time. To check for collinearity between 3 or more variables, the **Variance Inflation Factor** (VIF) can be used instead.

Recall that in the presence of collinearity, the variance of estimates increases; the VIF is a **measure of the increase in variance**. Different texts have **different thresholds** for high collinearity. For this exam, assume the **threshold to be 10**.

$$
    \text{VIF}_{p} = \frac{1}{1 - R^{2}_{p}}
$$

$R^{2}_{p}$ is the R-squared for a model where the p-th predictor is regressed against ALL OTHER predictors. If this R-squared is high, it implies that this predictor is well explained by other predictors (high correlation, high collinearity).

Naturally, the solution is to **drop highly collinear variables**; keep only one of the bunch.

## **Assumption 3: Exogenity**

Exogenity refers to how a variable comes from **outside** the model and is thus **independent of any other variables within the model**.

In a regression context, this comes in the form of the errors having a **conditional mean of 0**, ensuring that the errors are random and thus not related to the IVs.

$$
    E(\varepsilon_i | x_{ij}) = 0 \\
$$

> "Endo" and "Exo" in Greek means "In" and "Out" respectively, which is how the meaning of the words were derived.

There are two key implications of Exogenity:

1. By the Law of Total Expectations, the *unconditional* expectation of the error is also 0.
2. By the Linearity of Conditional Expectations, the expectation of the product of the Error and IVs is 0.

$$
\begin{aligned}
    E(\varepsilon_i) = 0 \\
    E(\varepsilon_i x_{ij}) = 0
\end{aligned}
$$

Following these two implications, it can be shown that the Covariance between the Error and IVs are also 0, which is another *consequence* of independence (NOT the other way around).

$$
\begin{aligned}
    Cov (\varepsilon_i, x_{ij})
    &= E(\varepsilon_i x_{ij}) - E(\varepsilon_i) * E(x_{ij}) \\
    &= 0 - 0 * E(x_{ij}) \\
    &= 0
\end{aligned}
$$

Without exogenity, the regression parameters would reflect the **effect of both the IV and the unmodelled variable** within the error term. This causes the OLS **estimate to be biased**, known as the **Omitted Variable Bias**. Since the unmodelled variable confounds the results of the regression, it is known as a **Confounding Variable**.

<!-- Obtained from Statology -->
![Confounding Variables](Assets/4.%20Gauss%20Markov%20Theorem.md/Confounding%20Variable.png){.center}

#### **Residual Analysis**

Since the errors are unobservable, the residuals are used to estimate the errors.

If the fitted model is adequate - all relavent IVs are included in the right form, then the residuals should closely **resemble the errors** and therefore be **structureless** (random). However, if there are **patterns in the residuals**, it indicates that there is additional information that can be used to improve the model and thus should be included.

Due to the OLS, the correlation between residuals and existing IVs will **always be 0** indicating **no linear relationship**. To check for unmodelled **non-linear relationships**, a **Residual Plot** of the IVs against the Residuals can be used.

For instance, if the residual plot shows a quadractic pattern (curve), then a quadractic IV should be added into the model. Mathematically, it can be expressed as a function of the existing estimates:

$$
\begin{aligned}
    \hat{y_i} &= \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\varepsilon}_i \\
    \hat{\varepsilon_i} &= \hat{\gamma}_0 + \hat{\gamma}_1 x + \hat{\gamma}_2 x^2 \text{ (From residual plot)} \\
    \hat{y_i} &= \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\gamma}_0 + \hat{\gamma}_1 x + \hat{\gamma}_2 x^2 \\
    \hat{y_i} &= (\hat{\beta}_0 + \hat{\gamma}_0) + (\hat{\beta}_1 + \hat{\gamma}_1)x + \hat{\gamma}_2 x^2 \\
    \hat{y_i} &= \hat{\beta}_0' + \hat{\beta}_1'x + \hat{\beta}_2' x^2
\end{aligned}
$$

<!-- Obtained from ACTEX Manual -->
![Residual Plot](Assets/4.%20Gauss%20Markov%20Theorem.md/Residual%20Plot.png)

<!-- Added variable plots? -->

## **Assumption 4: Homoscedasticity**

Homoscedasticity refers to the error terms having **constant variance** while Heteroscedasticity refers to having non-constant variance.

$$
    var(\varepsilon_i | x_i) = \sigma^2
$$

Under homoscedasticity, the sampling distribution of the estimates are easily derived and thus it can be shown that they are the most efficient estimators. The same cannot be proven under heteroscedasticity.

> Note that that the OLS estimates are still unbiased; they are just not the most efficient.

#### **Identifying Heteroscedasticity**

Similar to before, if the model is adequate, then the residuals should resemble the errors and have **constant variance**. Thus, this can be easily determined through a **residual plot** of the Residuals against the fitted values.

If the points are **equally spread out** about the mean (0) and show **no pattern**, then homoscedasticity is present. However, if the points show an increasing or decreasing variance (typically in the **shape of a funnel**), then heteroscedasticity is present.

<!-- Obtained from Corporate Finance Institute -->
![Homoscedastacity](Assets/4.%20Gauss%20Markov%20Theorem.md/Homoscedasticity.png){.center}

Alternatively, a hypothesis test can be conducted to determine if heteroscedasticity is present, known as the **Bresuch Pagan Test**.

$$
\begin{aligned}
    H_0 &: \sigma^2 \\
    H_1 &: \sigma^2 + \boldsymbol{Z\gamma}
\end{aligned}
$$

The test-statistic is computed as follows:

1. Compute the squared standardized residuals from the original model
2. Regress them onto the variables in Z (LOL need to change this part)
3. Compute the RegSS of the new regression

$$
    T = \frac{RegSS}{2}
$$

The test-statistic follows a **chi-square distribution** with $q$ degrees of freedom, where $q$ is the **number of variables** in $\boldsymbol{Z}$.

$$
    T \sim \chi^2_q
$$

#### **Dealing with Heteroscedasticity**

If prior information is known about the structure of the data, then the most intuitive method would be to incorporate that information into the data.

<!-- Buff up this section -->

$$
    Var(\varepsilon_i) = \frac{\sigma^2}{w_i}
$$

If no prior information is known, then the Heteroscedasticity can be reduced by using a **Variance Stabilizing Transformation**, such as the **Log** or **Squareroot**. Note that since they require positive data, a **constant can be added** to each term before the transformation to ensure that the values are positive.

> It is out of the scope for this set of notes to show why these help to stabilize variance.

Alternatively, if there is only mild heteroscedasticity in the data, then OLS can be used but with an **adjustment to the standard errors** of the coefficients, known as **heteroscedastic-robust standard errors**.

Due to complexity of the computations, it will not be covered in this set of notes. However, the general idea is that an **weighted estimate** of the variance covariance matrix is computed and the standard errors are computed from there.

### **Assumption 5: Serial Correlation**

If errors are correlated with one another, it is known as **Serial Correlation** or **Autocorrelation**.

It implies that there are other unmodelled factors that can be used for prediction, which would imply the current model specification to be wrong.

Thus, for the SLR model to be true, the errors **must be independent of one another**.

$$
    Cov(\varepsilon_i,\varepsilon_j) = 0
$$

Confidence Intervals and PI are narrower than it should be > 95% PI is actually < 95%>
P values lower > Appear statisticlaly significant when they shld not be

Time series tends to have errors that are positively correlated, which is why it has its own dedicated section
No Serial Correlation > Outcome of zero conditional mean, but most likely in time series data

## **Assumption 6: Normality**

Although not needed for OLS estimation or Guass Markov, the errors of the regression are usually assumed to be **normally distributed**.

<!-- To check on specific normal distributions for Y > Does it have the same variance as E? -->
$$
    \varepsilon \sim N(0, \sigma^2)
$$

If the errors are normally distributed, then it follows that **$\beta$ is normally distributed** as well since they are linear and additive. This greatly eases the computation needed to determine the **sampling distribution** for statistical inference.

### **Q-Q Plots**

Since the errors are normally distributed, the residuals should be normally distributed as well. This can be verified using a **Quantile-Quantile Plot (QQ Plot)**, which **compares the quantiles** of two distributions.

The first distribution is plotted on the x-axis while the second on the y-axis. If the quantiles are the same (same distribution), then the points should lie on $y = x$, the 45 degree line.

<!-- Obtained from STHDA -->
![QQ Plot](Assets/4.%20Gauss%20Markov%20Theorem.md/QQ%20Plot.png)

<!-- Simple random sample refer to Econometrics bible -->


## **Other Assumptions**

### **No Extreme Outliers**

Outliers are observations with **unusual values of the DV** relative to fitted regression model.

The last OLS assumption is that there are **no extreme outliers** in the dataset used to create the regression model. Generally, as long as the DV and IVs have a **positive and finite Kurtosis**, then the probability of such observations occuring are low.

Outliers are problematic as OLS is **sensitive to outliers**. Extreme outliers have large residuals which **receive more weight** in the optimization process, which causes the resulting model to accomodate it (when it should not), causing the resulting coefficients to be biased.

<!-- Obatained from Research Gate -->
![Outliers](Assets/4.%20Gauss%20Markov%20Theorem.md/Outliers.png){.center}

#### **Identifying Outliers**

By definition, Outliers have **unusually large residuals**. In order to gauge what is considered a "large residual", the residuals are standardized for comparison.

Standardization requires knowledge of the **sampling distribution of the residuals**. Given that the errors have a constant variance of $\sigma^2$, the variance of the residuals can be shown to be:

$$
    var(\hat{\varepsilon}_i) = \sigma^2 (1-h_{ii})
$$

> $h_ii$ is known as the Leverage of the observation, which will be covered in the following section.

The sampling distribution can then be determined:

$$
    \hat{\varepsilon}_i ~ N(0, \sigma^2 (1-h_{ii}))
$$

Thus, the standardized residuals are the raw residuals scaled by their standard error:

$$
    \hat{\varepsilon}_i^{\text{standardized}} = \frac{\hat{\varepsilon}_i}{\sqrt{\sigma^2 (1-h_{ii})}}
$$

> In practice, the variance of the errors are unknown, thus it is estimated using the Sample Variance of the residuals instead.

Residuals with standardized values of **larger than 2 or 3** are considered large and thus can be considered as an outlier.

#### **High Leverage Points**

While outliers are unusual points of the DV, **High Leverage observations** have unusual values of the IV relative to the majority of the values.

It is easy to identify high leverage points when there is only one IV through a scatterplot - simply find the observation that is away from the rest.

It becomes much more complicated when there are multiple IVs. The observation's values for each of the IVs could be in the common range of each IV, but **unusual when taken collectively**:

<!-- Obtained from ACTEX Manual -->
![High Leverage Points](Assets/4.%20Gauss%20Markov%20Theorem.md/High%20Leverage.png){.center}

The leverage of the observation can be determined from the MLR:

$$
\begin{aligned}
    \hat{\boldsymbol{y}}
    &= \boldsymbol{X}\boldsymbol{\beta} \\
    &= \boldsymbol{X}[(X'X)^{-1}y] \\
    &= \boldsymbol{H}\boldsymbol{y}
\end{aligned}
$$

$\boldsymbol{H}$ is known as the **Hat Matrix** as it puts a hat on y in the notation. The leverage of the $ith$ observation is the **$ith$ diagonal element** of the matrix, $h_{ii}$.

An observation is considered to have *high* leverage if its leverage is greater than three times the average leverage:

$$
    h_{ii} \gt 3 \left(\frac{p+1}{n}\right)
$$

#### **Influential Points**

The effect of Outliers and High leverage points can both be summarized into a concept known as **Influence**. An observation is influential if the **exclusion of the observation from the regression** leads to significantly differently results.

In general the process involves three steps:

1. Fit the original model with all $n$ observations; determine the $j-th$ fitted value $\hat{y}_j$
2. Fit an adjusted model with omitting the $i-th$ observation, determine the $j-th$ fitted value $\hat{y}_{j(i)}$
3. Calculate the **change in the $j-th$ fitted value**

This process has to be repeated for **all fitted values for all observations**.

**Cook's Distance** summarizes the effect of the $i-th$ observation on the whole model:

$$
    D_i = \frac{\sum^n_{j=1} (\hat{y}_j - \hat{y}_{j(i)})^2}{(p+1) \cdot MS_{Residuals}}
$$

This method of computation requires $n+1$ datasets - 1 dataset with all the observation and $n$ datasets with the $i-th$ observation omitted. It is also extremely time consuming to have to fit a model to each dataset.

An alternative method of determining Cook's Distance is to make use of both the Outliers and Leverage:

$$
    D_i = \frac{1}{p+1} \cdot (e^{\text{standardized}})^2 \cdot \frac{h_{ii}}{1-h_{ii}}
$$

Thus, an observation must be **unusual in BOTH the DV and IV** in order to be considered influential. Outliers and High leverage points are *necessary but not sufficient* conditions to be influential.

## High dimension

df