# **Linear Regression**

## **Population Regression Model**

Regression is a statistical model that relates a **Dependent Variable** (DV) to one or more **Independent Variables** (IV). The dependent variable is *regressed on to* the independent variable.

They are fundamentally a function of the independent variables and several **Regression Parameters**, $\beta$. The functional form of the regression is based on the relationship between the variables. The goal is to use a model that **best captures the relationship** between the variables.

$$
Y = f(X, \beta)
$$

<center>

| Independent Variable(s) | Dependent Variables |
| :-: | :-: |
| Variable used to make predictions | Variable being predicted |
| *Free* to change the value | *Depends* on the value of indepenent variable |
| Deterministic | Random Variable |
| Denoted as $X$ | Denoted as $Y$ |

</center>

To be precise, for every set of IVs, the DV has a **Conditional Distribution** dependent on the given IV.

For instance, the the $Y$ could take any possible value (Marginal Distribution), but *given* these set of $X$, the possible values can be **narrowed down to a certain range** (conditional distribution).

<!-- Narrowed down or have different pribability -->

$$
\displaylines{
Y \sim Distribution \\
Y|X \sim Conditional~Distribution}
$$

Thus, the output of the regression model is actually the **Expected Value** of the conditional distribution, $E(Y|X)$, for every possible $X$.

$$
E(Y|X) = f(X, \beta)
$$

<!-- Obtained from Colorado Uni -->
![Dependent Expectation](Assets/1.%20Regression%20Overview.md/Regression%20Expectation.png){ .center}

The actual observations are unlikely to be *exactly equal* to its expectation, thus there is a **difference** between an observation and the corresponding regression output. It known as the **Random Error Term** which accounts for **all other factors** that affect the DV that are not captured in the regression.

This means that the relationship between the $Y$ and $X$ is only **approximate**, as the true relationship is probably different due to the possibility of unaccounted variables.

Note that the **sign of the errors** are significant - positive implies the actual value lies above the regression output while negative implies it lies below.

$$
\varepsilon_i = y_i - f(x_i, \beta)
$$

<!-- Obtained from Cloudera -->
![Regression Errors](Assets/1.%20Regression%20Overview.md/Regression%20Errors.png){ .center}

This means that $Y$ (not its expectation!) can be expressed as a sum of the regression model and the error terms:

$$
y_i = f(x_i,\beta) + \varepsilon_i
$$

* The Regression is known as the **Systematic** component as it is *shared* among all observations
* The Error is known as the **Non-Systematic** component as it is *unique* to each observation

## **Sample Regression Model**

In practice, the population is unobservable hence it is impossible to construct the population regression model. Instead, a regression model is constructed from a sample instead, which aims to **estimate** the population model.

$$
\hat{y} = f(X,\hat{\beta})
$$

Similarly, the output of this model can be compared to the actual observations. However, the resulting difference is known as the **Residual** of the model, which like all the other components, is an estimate for the Error term.

$$
\hat{\varepsilon_i} = y_i - \hat{y_i}
$$

There are several different methods to estimate the regression parameters, but they usually involve **minimizing the residuals** of the model, such that the resulting model **best fits** the given sample, which is why it is also known as the **Fitted Regression Model**.

## **Hypothesis Testing**

Once a regression model has been fit, the next step is to determine if the relationship found in the sample is indicative of a relationship in the population.

This can be determined through the following **two-sided hypothesis test**:

* Null Hypothesis: $\beta_1 = 0$
* Alternative Hypothesis: $\beta_1 \ne 0$

Under the null, the regression parameters are assumed to be 0, implying that there is **no relationship** between $Y$ and $X$. The test should **reject the null**, proving that there IS a relationship between the DV and IVs.

## **Prediction**

Once the *best* model has been determined, it can be used to make **Predictions** about future unobserved values. Let these future values be denoted by the subscript $*$.

<!-- Obtained from ACTEX Manual -->
![Regression Errors](Assets/1.%20Regression%20Overview.md/Prediction%20Interval.png){ .center}

These unobserved DVs come from the population, thus can be expressed as a function of the population model:

$$
y_* = f(x_*, \beta) + \varepsilon_*
$$

The corresponding values from the sample regression model is an **estimate** for this unobserved value:

$$
\hat{y}_* = f(x_*, \hat{\beta})
$$

Like before, the predicted value is unlikely to be *exactly equal* to the actual value. Thus, the difference between both values can be measured as the **Prediction Error**:

$$
y_* - \hat{y_*} = \varepsilon_* + [f(x_*, \beta) - f(x_*, \hat{\beta})]
$$

The prediction error is thus made up of **two components**:

1. Inherent error present in the DV ($\varepsilon_*$)
2. Error in estimating the population model ($f(x_*, \beta) - f(x_*, \hat{\beta})$)

Based on the distribution of the prediction error, a **Prediction Interval** at a given confidence level can be calculated to accompany the regression estimate, which is essentially a **confidence interval for the predicted value**.

Note that the prediction intervals will always be **wider than confidence intervals**. This is because CIs only takes into the account the error in estimating the population model/parameters while PIs take into account the inherent error of the DV as well.

<!-- Obtained from Towards Data Science -->
![Confidence Prediction Interval](Assets/1.%20Regression%20Overview.md/Confidence%20Prediction%20Interval.png)
