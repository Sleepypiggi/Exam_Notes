# **Clustering**

## **Overview**

Clustering is an **Unsupervised** statistical learning method that is used to find **Clusters** within the dataset. They are a **subgroup of observations** that are **similar** to one another.

!!! Note

    An observation can only be part of ONE cluster.

## **Euclidean Distance**

Similarity between observations is quantified via **Euclidean Distance**. It is essentially the **shortest possible distance** between two observations in a multi-dimensional space.

$$
    D = \sqrt{(x_{1} - x_{2})^{2} + (y_{1} - y_{2})^{2} + (z_{1} - z_{2})^{2}}
$$

!!! Tip

    The order of the subtraction (1-2 or 2-1) is **irrelevant** as the difference is squared.

    For higher dimensions, simply **add on another squared difference** for each additional dimension. Squareroot is used REGARDLESS of the dimension.

!!! Tip

    In a two-dimensional context, the Euclidean Distance is essentially the straight line distance between two points:

The Euclidean Distance between the two

## **K-Means Clustering**

The first type of clustering is known as **K-means clustering**. It works by clustering the data into a **pre-specified number of clusters** ($k$), such that the **total within cluster variation is minimized**.

1. **Randomly assign each observation** to one of the $k$ clusters
2. Calculate the **Centroid** of the clusters
3. Re-assign observations to the nearest cluster, based on their Euclidean Distance from the Centroids in step (2)
4. Repeat steps (2) and step (3) until there are no more changes in clusters

!!! Warning

    K-means clustering is a **greedy algorithm** as it finds the local best fit rather than the global best.

    Additionally, since the **initial cluster assignment is random**, the algorithm will produce **different clusters** each time it is run. This is contrast to subset selection where the starting point is always the same, thus will produce the same results.

The choice of $k$ is **arbitrary**, thus it might not yield the best results. It is thus recommended to **repeat the algorithm** several times for different $k$ to have a better understanding of the data.

### **Cluster Variation**

## **Hierarchical Clustering**