# **Clustering**

## **Overview**

Clustering is an **Unsupervised** statistical learning method that is used to find **Clusters** within the dataset. They are a **subgroup of homogenous observations** that are **similar** to one another; different clusters naturally have different characteristics.

!!! Note

    An observation can only be part of ONE cluster.

!!! Note

    Generally speaking there are two types of clustering - **Observation vs Feature** Clustering:

    * **Observation Clustering**: Grouping Customers
    * **Feature Clustering**: Grouping Customer attributes (EG. Age, Income)

## **Euclidean Distance**

Similarity between observations is quantified via **Euclidean Distance**. It is essentially the **shortest possible distance** between two observations in a multi-dimensional space.

$$
    D = \sqrt{(x_{1} - x_{2})^{2} + (y_{1} - y_{2})^{2} + (z_{1} - z_{2})^{2}}
$$

!!! Tip

    The order of the subtraction (1-2 or 2-1) is **irrelevant** as the difference is squared.

    For higher dimensions, simply **add on another squared difference** for each additional dimension. Squareroot is used REGARDLESS of the dimension.

!!! Tip

    In a two-dimensional context, the Euclidean Distance is essentially the straight line distance between two points.

!!! Tip

    By default, the calculator will display the exact form; (EG. $\sqrt{2}$ instead of $1.4 \dots$). This can make it hard to compare quantities, thus change the 'Mode' to 'Classic' to force all calculated values to be displayed in decimal form.

### **Cluster Variation**

The variation within a cluster is the sum of all **squared euclidean distances** for ALL PAIRS of observations within that cluster, **divided by the number of observations**:

$$
\begin{aligned}
    \text{Cluster Variation} &= \frac{\sum \text{Pairwise Distance}}{n_{\text{Cluster}}} \\
    \text{Total Cluster Variation} &= \sum \text{Cluster Variation}
\end{aligned}
$$

This is best visualized through a table:

* Each cell contains the squared euclidean distance between the two observations
* The sum of all cells is the total squared euclidean distances for the cluster

<!-- Self Made -->
![VARIATION_TABLE](Assets/9.%20Clustering.md/VARIATION_TABLE.png){.center}

!!! Warning

    The **diagonal is always 0** because the euclidean distance between a point and itself is 0.

    The table considers **ALL pairs of observations**, not just unique pairs. In other words, 1-2 and 2-1 are both considered, even though they result in the same euclidean distance. This is why the values are **symmetrical about the diagonal**.

    Cluster Variation requires ALL values in the table to be summed. However, since the table is symmetrical, it is equivalent to TWO TIMES the sum of the **triangle of values**.

### **Centroid**

Another related concept is the **Centroid of the cluster**. As its name suggests, it is the **centre of the cluster** in multi-dimensional space, equivalent to the average of each individual dimension.

<!-- Self Made -->
![CLUSTER_CENTROID](Assets/9.%20Clustering.md/CLUSTER_CENTROID.png){.center}

Similar to Variance, the centroid can be used to determine the cluster variation as well:

$$
    \text{Cluster Variation} = 2 \cdot \text{Sum of Squared ED from Centroid}
$$

!!! Tip

    The two reflects the idea from before that the variation should consider all pairs of observations (symmetry).

!!! Warning

    Unlike the previous formula, there is NO need to divide by the number of observations.

## **K-Means Clustering**

The first type of clustering is known as **K-means clustering**. It works by clustering the data into a **pre-specified number of clusters** ($k$), such that the **total within cluster variation is minimized**:

1. **Randomly assign each observation** to one of the $k$ clusters (No observations are excluded)
2. Calculate the **Centroid** of the clusters
3. Re-assign observations to the nearest cluster, based on their Euclidean Distance from the Centroids in step (2)
4. Repeat steps (2) and step (3) until there are no more changes in clusters

!!! Warning

    K-means clustering is a **greedy algorithm** as it finds the local best fit rather than the global best.

    Additionally, since the **initial cluster assignment is random**, the algorithm will produce **different clusters each time** it is run. This is contrast to subset selection where the starting point is always the same, thus will produce the same results.

!!! Note

    Until the final cluster assignments, each step is guaranteed to reduce the squared euclidean distances.

The choice of $k$ is **arbitrary**, thus it might not yield the best results. It is thus recommended to **repeat the algorithm** several times for different $k$ to have a better understanding of the data.

!!! Tip

    Euclidean Distance does not exist for categorical data. Thus, Clustering can only be applied to quantitative variables.

## **Hierarchical Clustering**

The second type is known as **Hierarchical Clustering**. It works by iteratively merging or splitting data points based on their **similarity**, till all observations are either in one giant cluster or in individual clusters.

<!-- Obtained from Medium -->
![DENDROGRAM](Assets/9.%20Clustering.md/DENDROGRAM.png){.center}

Due to the iterative nature of the method, a *hierarchy* of clusters are formed, where clusters are **nested subsets of one another**. The clustering can be **"sliced"** to determine the final number of clusters. It is thus best visualized in a tree-like structure known as a **Dendrogram**:

<!-- Obtained from Medium -->
![SLICED_DENDROGRAM](Assets/9.%20Clustering.md/SLICED_DENDROGRAM.png){.center}

!!! Tip

    The Dendrogram can be sliced at any possible level, resulting in a different number of clusters:

    <!-- Obtained from Medium -->
    ![SLICED_DENDROGRAM_ALT](Assets/9.%20Clustering.md/SLICED_DENDROGRAM_ALT.png){.center}

    Note that not all dendrogram extend the branches all the way down, which may lead to incorrectly counting the number of clusters.

!!! Note

    The act of slicing the dendrogram is akin to choosing $k$ in $k$ means clustering. Thus, some domain knowledge is important to ensure that the number of clusters produced are reasonable.

Generally speaking, there are two approaches to hierarchical clustering:

<center>

|           **Bottom-Up**            |              **Top-Down**               |
| :--------------------------------: | :-------------------------------------: |
|   Each starts in its own cluster   |      All start in one big cluster       |
| Iteratively merge until all in one | Iteratively split until each in its own |
|           Agglomerative            |                Divisive                 |
|             SRM focus              |              Out of Scope               |

</center>

1. Assign each observation to its own cluster
2. Compute the distance between each pair of clusters
3. Merge the two clusters with the least dissimilarity (most similar)
4. Repeat step (2) and (3) until there is one cluster left

If there two or more pairs of clusters have the same dissimilarity, then the pair of clusters which contain the **lowest indexed observation** will be merged.

!!! Tip

    At each step, if there $n$ clusters before merging, then $n \choose 2$ **unique pairs** of clusters are considered. The number naturally decreases per iteration of the algorithm:

    <!-- Self Made -->
    ![PAIRWISE_CLUSTERS](Assets/9.%20Clustering.md/PAIRWISE_CLUSTERS.png){.center}

    Another fact is that if there are a total of $n$ clusters, the algorithm will result in $n-1$ merges.

### **Similarity Measure**

The basis for merging a cluster is based on how similar the two clusters are, known as **Dissimilarity Measure**. For the purposes of this exam, assume that **Euclidean Distance** is used unless explicitly stated otherwise.

!!! Note

    Another type of measure is known as the **Peason Correlation Distance**. Type of items vs number of items?

    However, one caveat is that it requires the dataset to have **at least three features** for it to be used.

The key difference is that dissimilarity is measuring the Euclidean Distance **between two clusters** rather than two observations. There are four main ways to perform this, known as the choice of **Linkage**:

* **Complete**: Compute all pairwise distances, use the **highest**
* **Single**: Compute all pairwise distances, use the **lowest**
* **Average**: Compute all pairwise distances, use the **average**
* **Centroid**: Use the distance **between centroids**

!!! Tip

    For a cluster with **only one observation**; starting cluster - the choice of linkage **does not matter** as they will all lead to the same result.

The choice of linkage does affect how the **shape** of the resulting Dendrogram. In general, Complete and Average linkage is preferred:

* **Single**: Tends to merge single observation clusters first, resulting in **extended and trailing** diagrams
* **Centroid**: May result in **Inversions** where the clusters are merged at a lower level; **unintuitive**
* **Complete & Average**: Tend to produce more **balanced trees**

### **Dendrogram Analysis**

The vertical axis on the Dendrogram is the **dissimilarity measure**. Thus, we are able to tell the **exact dissimilarity** when two clusters merge. However, they do not provide direct insight into the relationship between sub-clusters; that **must be inferred**.

Consider the following simple Dendrogram:

Firstly, the **order of fusion** matters - Clusters that **merge earlier are more similar** than clusters that merge later - $(B,C)$ is more similar than $(A,B)$ or $(A,C)$!

Next, the **choice of linkage matters**, as it allows us to draw simple relationships about the distance:

* **Complete**: $\max [(A,B), (A,C)] = 10$; pairs cant be larger than 10
* **Single**: $\min [(A,B), (A,C)] = 10$; pairs cant be smaller than 10

Questions of this nature will generally be simple in nature.

## Other

Standardize to ensure all same scale, equal importance
Non robust -> Sensitive to changes
HARD TO CHECK

## **K-Nearest Neighbours**

**K-Nearest Neighbours** (KNN) is non-parametric method that can be used in a regression or classification setting:

1. Identify the observation in vector space
2. Identify the $k$ **nearest observations** (based on Euclidean Distance)
3. Take the average of the observations in (2) as the prediction (regression) or the most frequent category among them (classification)

<!-- Obtained from Research Gate -->
![KNN](Assets/9.%20Clustering.md/KNN.png){.center}

!!! Warning

    Do not confuse K-means clustering and K-Nearest Neighbours! Both contain $k$ but have very different functions. This section is placed here as no other section suits it.

!!! Tip

    Given a table of data, it might be helpful to draw the scatterplot (to some reasonable scale) to get a sense of the distance.

    If the data has more than three dimensions, manually calculating the euclidean distance is the only method.

The choice of $k$ is critical as it is a measure of **flexibility**:

* $k=1$: Each observation used as its own prediction; perfectly fits the data (High Variance, Low Bias)
* $k=n$: All observations used for each prediction; constant prediction (Low Variance, High bias)  

!!! Warning

    The test error follows the usual U-shape pattern, given that the boundary values for $k$ both have high MSE. However, the training error is the other way around.

    Typically, the training error is proportional with the flexibility measure. However, $k=1$ has the lowest training error in this case - it is **inversely proportional**. Another way of thinking about it is that the flexibility measure is $\frac{1}{k}$ instead.

!!! Tip

    Despite being a rather simplisitc method, KNN (as a classifier) performs quite well; similar to the Bayes Classifier, given that the right $K$ is chosen.
