# **Time Series**

## **Overview**

Generally speaking, there are **three main types** of data:

<center>

| **Cross Sectional**  |     **Time Series**     |          **Panel**           |
| :------------------: | :---------------------: | :--------------------------: |
|  Multiple Entities   |      Single Entity      |      Multiple Entities       |
| Single Point in time | Multiple points in time |   Multiple points in time    |
|  Predict new entity  |  Predict future value   | Predict new entity in future |

</center>

<!-- Obtained from Vecteezy -->
![DATA_TYPES](Assets/10.%20Time%20Series.md/DATA_TYPES.png){.center}

Up until this point, the focus has been on Cross Sectional data. Thus, this section will focus exclusively on **Time Series data**:

* Observations are made at **fixed intervals** in time (EG. Daily, Monthly)
* Observations are **ordered** over time
* Observations are visualized using a **timeplot**:

<!-- Obtained from Coaching Actuaries -->
![TIME_SERIES_PLOT](Assets/10.%20Time%20Series.md/TIME_SERIES_PLOT.png){.center}

!!! Info

    Predicting the **future value** of an variable is known as **Forecasting**. The terminology is used to make a clear distinction between cross-sectional and time series analysis.

## **Decomposition**

Time series data can be split into four main components:

* **Trends** ($T$) - Long term increase or decrease
* **Season** ($S$) - Short term changes occuring at **fixed intervals**
* **Cyclical** ($C$) - Long term changes occuring at **unknown intervals**
* **White Noise** ($\varepsilon$) - Random fluctuations

<!-- Obtained from Medium -->
![TIME_SERIES_COMPONENTS](Assets/10.%20Time%20Series.md/TIME_SERIES_COMPONENTS.png){.center}

!!! Note

    Cyclical effects are sometimes not decomposed seperately but considered as part of the trend as both are long-term.

There are two types of decomposition, depending on the nature of the data:

<center>

|               **Additive**                |          **Multiplicative**           |
| :---------------------------------------: | :-----------------------------------: |
|         $Y = T + S + \varepsilon$         |   $Y = T \cdot S \cdot \varepsilon$   |
| Fluctuation **NOT proportional to trend** | Fluctuation **proportional** to trend |

</center>

<!-- Obtained From Crystall Ball Services -->
![TIME_SERIES_DECOMPOSITION](Assets/10.%20Time%20Series.md/TIME_SERIES_DECOMPOSITION.png){.center}

!!! Tip

    Using a **variance stabilizing transformation** such as the Log function will convert a multiplicative model to become additive:

    $$
    \begin{aligned}
        Y_{t} &= T_{t} \cdot S_{t} \cdot \varepsilon_{t} \\
        \ln Y_{t} &= \ln \left(T_{t} \cdot S_{t} \cdot \varepsilon_{t} \right) \\
        \ln Y_{t} &= \ln T_{t} + \ln S_{t} + \ln \varepsilon_{t}
    \end{aligned}
    $$

## **Stationary**

Stationarity refers to how certain properties of the **data do not change over time**; stationary relative to moving time. There are two levels:

* **Weakly Stationary**: **Only** Mean and Variance are unchanging
* **Strongly Stationary**: Entire distribution (**all properties**) are unchanging

The implicit assumption for all predictive models is that the **underlying data-generating-process will not change**. Stationarity is key to forecasting as it ensures that the underlying DGP has not changed.

!!! Note

    For the purposes of this exam, we are only concerned with the properties of **weak stationarity**. Weak stationarity is a **subset** of strong stationarity; thus it does not matter which of the two are present.

!!! Tip

    The actual statistical definition of weak stationarity is:
    
    * Mean is independent of $t$
    * Covariance of two variables is only dependent on the **difference in time** between them:

    $$
        |a-b| = |c-d| \implies \text{Cov}(Y_{a}, Y_{b}) = \text{Cov}(Y_{c}, Y_{d}) 
    $$

    Using the above, it can be shown that the **variance is the same** between two time points as well:

    $$
    \begin{aligned}
        \text{Var}(Y_{t}) &= \text{Cov}(Y_{t}, Y_{t}) \\
        \text{Var}(Y_{s}) &= \text{Cov}(Y_{s}, Y_{s}) \\
        \\
        |t-t| = |s-s| = 0 &\implies \text{Cov}(Y_{t}, Y_{t}) = \text{Cov}(Y_{s}, Y_{s}) \\
        \therefore \text{Var}(Y_{t}) &= \text{Var}(Y_{s}) 
    \end{aligned}
    $$

Stationarity can be identified via the use of **Control Limit Plots**. The limits are **derived based on the mean and variance**. If stationarity holds, the observations should almost always **stay within the limits** with **no observable pattern**:

* **Upper Control Limit** (UCL): $\bar{y} + 3 \cdot \s_{y}$
* **Lower Control Limit** (LCL): $\bar{y} - 3 \cdot \s_{y}$

<!-- Obtained from Coaching Actuaries -->
![CONTROL_LIMIT_CHART](Assets/10.%20Time%20Series.md/CONTROL_LIMIT_CHART.png){.center}

!!! Info

    The limits are likely based on the idea that the 99.7% of the density of a normal distribution is within 3 standard deviations of the mean.

!!! Tip

    There are two other variations of the control chart, which **directly checks the mean and variance** over time, known as **x-bar and R charts** respectively. They plot the sample statistics of **samples taken over time**.

    <!-- Obtained from Coaching Actuaries -->
    ![XBAR_CHART](Assets/10.%20Time%20Series.md/XBAR_CHART.png){.center}

    To be precise, the actual variance is not plotted but rather the range of the sample. Hence it is called a *R*-chart:

    <!-- Obtained from Coaching Actuaries -->
    ![R_CHART](Assets/10.%20Time%20Series.md/R_CHART.png){.center}

    The **same formula applies** for the limits of the x-bar and R chart. Naturally, they are the mean and variances of the mean and range respectively; all three charts will have different numerical limits.

## **Benchmark Models**

### **White Noise Model**

A variable that is naturally stationary is often referred to as **White Noise**, which is a term used to refer to a process that is **completely random**.

The best forecast for such a variable is **simply its mean**; since it is assumed to be completely random: 

$$
\begin{aligned}
    \hat{y}_{t+m} &= \bar{y} \\
    \text{SE}({\hat{y}_{t+m}}) &= s_{y} \cdot \sqrt{1 + \frac{1}{n}} \\
    \text{PI} &= \bar{y} \pm t_{\alpha} \cdot s_{y} \cdot \sqrt{1 + \frac{1}{n}}
\end{aligned}
$$

!!! Tip

    Note that the forecasts are **independent of time**; the forecast is the **same for all future periods**.

<!-- Obtained from Coaching Actuaries -->
![WHITE_NOISE_MODEL](Assets/10.%20Time%20Series.md/WHITE_NOISE_MODEL.png){.center}

### **Random Walk Model**

A variable that **grows randomly** (grows by a white noise) each time is known as a **Random Walk**:

$$
\begin{aligned}
    y_{t+1}
    &= y_{t} + W_{1} \\
    &= \dots \\
    &= y_{1} + \sum W_{t}
\end{aligned}
$$

!!! Warning

    The random walk itself is clearly not stationary. However, the **DIFFERENCE** in the observations is stationary:

    <!-- Obtained from Coaching Actuaries -->
    ![DIFFERENCED_STATIONARY](Assets/10.%20Time%20Series.md/DIFFERENCED_STATIONARY.png)

Thus, the modelling uses the **differencing** to estimate the effect of white noise:

$$
\begin{aligned}
    \hat{y}_{t+m} &= y_{t} + s \cdot \bar{w} \\
    \text{SE}(\hat{y}_{t+m}) &= s_{w} \cdot \sqrt{m} \\
    \text{PI} &= y_{t} + s \cdot \bar{w} \pm t_{\alpha} \cdot s_{w} \cdot \sqrt{m} \\
    \\
    w_{t} &= y_{t} - y_{t-1}
\end{aligned}
$$

<!-- Obtained from Coaching Actuaries -->
![RANDOM_WALK_MODEL](Assets/10.%20Time%20Series.md/RANDOM_WALK_MODEL.png){.center}

!!! Warning

    Random walks are said to have **increasing variance** over time. It is difficult to see this over one random walk, but over many different random walks it becomes clear:

    <!-- Obtained from Github -->
    ![RANDOM_WALK_VARIANCE](Assets/10.%20Time%20Series.md/RANDOM_WALK_VARIANCE.png){.center}

### **Autoregression**

## **Other Models**

### Smoothing?

    Violation of Independence Assumption: In linear regression, one of the key assumptions is that the observations (data points) are independent of each other. In time series data, observations are often dependent on previous observations, violating this assumption. Time series data typically exhibit autocorrelation, where the value at one time point is correlated with values at previous time points.
    Temporal Order Matters: Time series data has a natural temporal ordering, and the order of observations is crucial. Linear regression does not account for this temporal structure, which is important in understanding and modeling time-dependent patterns.
    Seasonality and Trends: Time series data often contains seasonality (repeating patterns) and trends, which linear regression may not capture well without additional modifications. Seasonal and trend components introduce non-linearity into the data.
    Non-Constant Variance: Time series data can have changing variance over time, known as heteroscedasticity. Linear regression assumes constant variance, which may not hold in many time series datasets.
    Auto-Regressive Components: Time series data can exhibit autoregressive behavior, where future values depend on past values. Linear regression does not incorporate lagged values as predictors, which are essential for modeling autoregressive behavior.