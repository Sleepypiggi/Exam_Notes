# **Time Series**

## **Overview**

Generally speaking, there are **three main types** of data:

<center>

| **Cross Sectional**  |     **Time Series**     |          **Panel**           |
| :------------------: | :---------------------: | :--------------------------: |
|  Multiple Entities   |      Single Entity      |      Multiple Entities       |
| Single Point in time | Multiple points in time |   Multiple points in time    |
|  Predict new entity  |  Predict future value   | Predict new entity in future |

</center>

<!-- Obtained from Vecteezy -->
![DATA_TYPES](Assets/10.%20Time%20Series.md/DATA_TYPES.png){.center}

Up until this point, the focus has been on Cross Sectional data. Thus, this section will focus exclusively on **Time Series data**:

* Observations are made at **fixed intervals** in time (EG. Daily, Monthly)
* Observations are **ordered** over time
* Observations are visualized using a **timeplot**:

<!-- Obtained from Coaching Actuaries -->
![TIME_SERIES_PLOT](Assets/10.%20Time%20Series.md/TIME_SERIES_PLOT.png){.center}

!!! Info

    Predicting the **future value** of an variable is known as **Forecasting**. The terminology is used to make a clear distinction between cross-sectional and time series analysis.

## **Stationary Models**

### **Stationarity**

Stationarity refers to how certain properties of the **data do not change over time**; stationary relative to moving time. There are two levels:

* **Weakly Stationary**: **Only** Mean and Variance are unchanging
* **Strongly Stationary**: Entire distribution (**all properties**) are unchanging

The implicit assumption for all predictive models is that the **underlying data-generating-process will not change**. Stationarity is **key to forecasting** as it ensures that the underlying DGP has not changed. It represents the **underlying Signal** in the data.

!!! Note

    For the purposes of this exam, we are only concerned with the properties of **weak stationarity**. Weak stationarity is a **subset** of strong stationarity; thus it does not matter which of the two are present.

!!! Tip

    The actual statistical definition of weak stationarity is:
    
    * Mean is independent of $t$
    * Covariance of two variables is only dependent on the **difference in time** between them:

    $$
        |a-b| = |c-d| \implies \text{Cov}(Y_{a}, Y_{b}) = \text{Cov}(Y_{c}, Y_{d}) 
    $$

    Using the above, it can be shown that the **variance is the same** between two time points as well:

    $$
    \begin{aligned}
        \text{Var}(Y_{t}) &= \text{Cov}(Y_{t}, Y_{t}) \\
        \text{Var}(Y_{s}) &= \text{Cov}(Y_{s}, Y_{s}) \\
        \\
        |t-t| = |s-s| = 0 &\implies \text{Cov}(Y_{t}, Y_{t}) = \text{Cov}(Y_{s}, Y_{s}) \\
        \therefore \text{Var}(Y_{t}) &= \text{Var}(Y_{s}) 
    \end{aligned}
    $$

!!! Tip

    A stationary model naturally implies that there are neither Trends nor Seasonal effects in the data.

Stationarity can be identified via the use of **Control Limit Plots**. The limits are **derived based on the mean and variance**. If stationarity holds, the observations should almost always **stay within the limits** with **no observable pattern**:

* **Upper Control Limit** (UCL): $\bar{y} + 3 \cdot \s_{y}$
* **Lower Control Limit** (LCL): $\bar{y} - 3 \cdot \s_{y}$

<!-- Obtained from Coaching Actuaries -->
![CONTROL_LIMIT_CHART](Assets/10.%20Time%20Series.md/CONTROL_LIMIT_CHART.png){.center}

!!! Info

    The limits are likely based on the idea that the 99.7% of the density of a normal distribution is within 3 standard deviations of the mean.

!!! Tip

    There are two other variations of the control chart, which **directly checks the mean and variance** over time, known as **x-bar and R charts** respectively. They plot the sample statistics of **samples taken over time**.

    <!-- Obtained from Coaching Actuaries -->
    ![XBAR_CHART](Assets/10.%20Time%20Series.md/XBAR_CHART.png){.center}

    To be precise, the actual variance is not plotted but rather the range of the sample. Hence it is called a *R*-chart:

    <!-- Obtained from Coaching Actuaries -->
    ![R_CHART](Assets/10.%20Time%20Series.md/R_CHART.png){.center}

    The **same formula applies** for the limits of the x-bar and R chart. Naturally, they are the mean and variances of the mean and range respectively; all three charts will have different numerical limits.

### **White Noise**

A variable that is naturally stationary is often referred to as **White Noise**, which is a term used to refer to a process that is **completely random**. The best forecast for such a variable is **simply its mean**; since it is assumed to be completely random: 

$$
\begin{aligned}
    y_{t} = \varepsilon_{t} \\
    \\
    \hat{y}_{t+m} &= \bar{y} \\
    \text{SE}({\hat{y}_{t+m}}) &= s_{y} \cdot \sqrt{1 + \frac{1}{n}} \\
    \text{PI} &= \bar{y} \pm t_{\alpha} \cdot s_{y} \cdot \sqrt{1 + \frac{1}{n}}
\end{aligned}
$$

!!! Tip

    Note that the forecasts are **independent of time**; the forecast is the **same for all future periods**.

<!-- Obtained from Coaching Actuaries -->
![WHITE_NOISE_MODEL](Assets/10.%20Time%20Series.md/WHITE_NOISE_MODEL.png){.center}

### **Random Walk**

A variable that **grows randomly** (grows by a white noise) each time is known as a **Random Walk**:

$$
\begin{aligned}
    y_{t+1}
    &= y_{t} + \varepsilon_{1} \\
    &= \dots \\
    &= y_{1} + \sum \varepsilon_{t}
\end{aligned}
$$

!!! Note

    Thus, it is said to be a sum of partial sum of white noise processes. Partial because the entire thing is not white noise; there is an initial value.

!!! Warning

    The random walk itself is clearly not stationary. However, the **DIFFERENCE** in the observations is stationary:

    <!-- Obtained from Coaching Actuaries -->
    ![DIFFERENCED_STATIONARY](Assets/10.%20Time%20Series.md/DIFFERENCED_STATIONARY.png)

Thus, the modelling uses the **differencing** to estimate the effect of white noise:

$$
\begin{aligned}
    \hat{y}_{t+m} &= y_{t} + m \cdot \bar{\varepsilon} \\
    \text{SE}(\hat{y}_{t+m}) &= \sigma \cdot \sqrt{m} \\
    \text{PI} &= y_{t} + m \cdot \bar{\varepsilon} \pm t_{\alpha} \cdot \sigma \cdot \sqrt{m} \\
    \\
    w_{t} &= y_{t} - y_{t-1}
\end{aligned}
$$

<!-- Obtained from Coaching Actuaries -->
![RANDOM_WALK_MODEL](Assets/10.%20Time%20Series.md/RANDOM_WALK_MODEL.png){.center}

!!! Warning

    Random walks are said to have **increasing variance** over time. It is difficult to see this over one random walk, but over many different random walks it becomes clear:

    <!-- Obtained from Github -->
    ![RANDOM_WALK_VARIANCE](Assets/10.%20Time%20Series.md/RANDOM_WALK_VARIANCE.png){.center}

    Mathematically, it is the variance arising from the sum of errors:

    $$
        \text{Var} \left(\sum \varepsilon_{t} \right) = t \cdot \sigma^{2}
    $$

### **Autoregressive**

Autoregression refers to using past observations to predict future observations of the same series. The number of past observations used for the prediction is known as the **Order** ($k$) of the model:

$$
    Y_{t} = \beta_{0} + \beta_{1} \cdot Y_{t-1} + \dots + \beta_{k} \cdot Y_{t-k} + \varepsilon_{t}
$$

!!! Note

    For the purposes of this exam, Autoregressive model of order 1 are considered; AR(1).

An AR(1) model can be seen as a generalization of a White Noise and Random Walk:

* $\beta_{1} = 0$: Reduces to **White Noise** Model
* $\beta_{1} = 1$: Reduces to **Random Walk** Model

$$
\begin{aligned}
    \beta_{1} &= 0 \\
    Y_{t} &= \beta_{0} + 0 \cdot Y_{t-1} + \varepsilon_{t} \\
    Y_{t} &= \beta_{0} + \varepsilon_{t} \\
    \\
    \beta_{1} &= 1 \\
    Y_{t} &= \beta_{0} + 1 \cdot Y_{t-1} + \varepsilon_{t} \\
    Y_{t} &= \beta_{0} + Y_{t-1} + \varepsilon_{t} \\
    Y_{t} - Y_{t-1} &= \beta_{0} + \varepsilon_{t}
\end{aligned}
$$

!!! Tip

    Adding a constant to a white noise process **still results in a white noise process**, thus the above is valid.

AR(1) models are stationary, as they have a constant mean, variance and covariance:

$$
\begin{aligned}
    E(y_{t}) &= \frac{\beta_{0}}{1 - \beta_{1}} \\
    \text{Var}(y_{t}) &= \frac{\sigma^{2}}{1 - \beta^{2}_{1}} \\
    \text{Cov}(Y_{t}, Y_{t-k}) &= \beta^{k}_{1}
\end{aligned}
$$

!!! Warning

    The above is true because an AR(1) model can be **recursively expressed** as a sum of itself. Using the geometric **sum to infinity**, the mean and variance are derived:

    $$
    \begin{aligned}
        y_{t}
        &= \beta_{0} + \beta_{1} \cdot y_{t-1} + \varepsilon_{t} \\
        &= \beta_{0} + \beta_{1} \cdot (\beta_{0} + \beta_{1} \cdot y_{t-2} + \varepsilon_{t}) + \varepsilon_{t-1} \\
        &= \beta_{0} + \beta_{0} \cdot \beta_{1} + \beta^{2}_{1} \cdot y_{t-2}
        + \varepsilon_{t} + \beta_{1} \cdot \varepsilon_{t-1} \\
        &= \dots \\
        &= (\beta_{0} + \beta_{0} \cdot \beta_{1} + \dots)
        + (\varepsilon_{t} + \beta_{1} \cdot \varepsilon_{t} + \dots)
    \end{aligned}
    $$

    Thus, for the model to be stationary, the sum to infinity MUST exist. This is fulfilled if $-1 \lt \beta_{1} \lt 1$.

!!! Tip

    We can determine if autoregression is a good choice to model the data based on the following, which can be determined using different plots:

    1. If the series is stationary
    2. If adjacent (in time) values follow a linear trend
    3. Autocorrelations follow a **decreasing geometric series** as $k$ increases

    The third point is in reference to the relationship between autocorrelation and the regression coefficient. Since $|\beta_{1}| \lt 1$, the autocorrelation will be **decreasing for higher powers**.

Thus, the model predicts **one period ahead** based on the most recent observation. For future periods, it uses the **previous prediction** as a starting point:

$$
\begin{aligned}
    \hat{y}_{t+m}
    &=
    \begin{cases}
        \beta_{0} + \beta_{1} \cdot y_{t+m-1},& m = 1 \\
        \beta_{0} + \beta_{1} \cdot \hat{y}_{t+m-1},& m \gt 1 \\    
    \end{cases}
\end{aligned}
$$

<!-- Obtained from Coaching Actuaries -->
![FORECAST_INTERVAL](Assets/10.%20Time%20Series.md/FORECAST_INTERVAL.png){.center}

#### **Autocorrelation**

Autocorrelation refers to the correlation between an observation and a "lagged" version of itself. The k-lagged autocorrelation determines the correlation for **each observation** and a $k$ lagged version of itself:

$$
\begin{aligned}
    \varrho_{k} &= \text{Corr}(Y_{t}, Y_{t-k}) \\
    \hat{\varrho}_{k} &= \frac{\sum^{n}_{k+1} (y_{t-k} -\bar{y})(y_{t} -\bar{y})}{\sum^{n}_{1} (y_{t} - \bar{y})}
\end{aligned}
$$

!!! Tip

    It is helpful to use an illustration to know which pairs of observations are considered together for autocorrelation:

    <!-- Self Made -->
    ![AUTOCORRELATION_EXAMPLE](Assets/10.%20Time%20Series.md/AUTOCORRELATION_EXAMPLE.png){.center}

    Using the above example, the autocorrelation is calculated as follows:

    $$
    \begin{aligned}
        \text{Autocorrelation}
        &=\frac
            {(A-\bar{y})(C-\bar{y}) + (B-\bar{y})(D-\bar{y}) + (A-\bar{y})(C-\bar{y})}{\sum (y - \bar{y})^{2}}
    \end{aligned}
    $$

!!! Note

    If the 1-lag autocorrelation is positive, it is said to be **Meandering process**. This fact is stated in the text but does not have an intuitive explanation.

In order to determine if autocorrelation is truly present, a hypothesis test can be used:

* $H_{0}: \varrho_{k} = 0$
* $H_{1}: \varrho_{k} \ne 0$

$$
\begin{aligned}
    \text{SE}(\hat{\varrho}_{k}) &= \frac{1}{\sqrt{n}} \\
    Q_{\text{Statistic}} &= \frac{\hat{\varrho}_{k}}{\text{SE}(\hat{\varrho}_{k})} \\
    Q_{\text{Statistic}} &\sim N(0,1)
\end{aligned}
$$

!!! Warning

    For the purposes of this exam, it is assumed that the standard error of the autocorrelation estimate is **constant regardless of the lag**. In reality, there would intuitively, be some dependency.

#### **Model Fitting**

The model coefficients can be estimated using **Conditional Least Squares**.

!!! Tip

    Alternatively, the coefficients can be estimated using the following:
    
    $$
    \begin{aligned}
        \beta_{0} &= \bar{y} (1 - \varrho_{1}) \\
        \beta_{1} &= \varrho_{1}
    \end{aligned}
    $$

    The difference between the estimation and original formula becomes negligible for a large number of observations.

### **Dickey Fuller**

Stationary models can more generally be expressed as the following, known as the **Dickey Fuller Equation**. It can be shown that when certain conditions are met, the general equation reduces to one of the following:

* $\varphi = 1$: Random Walk
* $\varphi = 0$: Linear Trend in Time
* $\mu_{1} = 0, \varphi \lt 1$: Autoregressive

$$
\begin{aligned}
    Y_{t} &= \mu_{0} (1 - \varphi) + \mu_{1} \varphi + \varphi Y_{t-1} + \mu_{1} (1 - \varphi) t + \varepsilon_{t} \\
    \\
    \varphi &= 1 \\
    Y_{t} &= \mu_{1} + Y_{t-1} + \varepsilon_{t} \\
    \\
    \varphi &= 0 \\
    Y_{t} &= \mu_{0} + \mu_{1} t + \varepsilon_{t} \\
    \\
    \mu_{1} &= 0 \\
    Y_{t} &= (1 - \varphi) \mu_{0} + \varphi Y_{t-1} + \varepsilon_{t} \\
\end{aligned}
$$

!!! Tip

    Notice the following relationships:

    * $\varphi$ is equivalent to $\beta_{1}$ in an AR(1) model
    * $\mu_{1}$ is equivalent to $\beta_{1}$ in a Linear Trend model

!!! Note

    The linear trend in time is a simple model that regresses the dependent variable against time:

    <!-- Obtained from Coaching Actuaries -->
    ![LINEAR_TIME](Assets/10.%20Time%20Series.md/LINEAR_TIME.png){.center}

    This model tends to perform poorly as **small or large values of $t$ tend to have high leverage**, causing them to have much greater influence on the model.

    The other time series model recommended tend to put **more weight on recent observations** rather than old ones, which is more desirable when working with time series data.

The main purpose for expressing it in this way is to perform a **Dickey Fuller Test**, which tests for the presence of a **Unit Root**. A "root" refers to the solutions of an equation. In a time series context, it is typically referring to the regression coefficients $\beta_{1}$ (or $\varphi$ in this context). A Unit Root specifically means root is **equal to a unit value (equal to 1)** - which is when the series **follows a random walk**:

* $\varphi = 1$
* $\varphi \gt 1$

The test implicitly assumes that the white noise term is serially uncorrelated. If this is not the case, the test can be modified to account for it, known as an **Augmented Dickey Fuller Test**.

!!! Tip

    It is NOT necessary to know the test-statistic or its sampling distribution. If needed, the question will provide them.

!!! Info

    The exact mathematical details about why the regression coefficient is the root of an autoregressive model is beyond the scope of this exam.

## **Smoothing Models**

Smoothing is a technique that involves averaging data points over a period to account for the various components in the time series (Level, Trend, Seasonality), resulting in a stationary time series with the underlying signal.

!!! Tip

    The key intuition is that the Trend and Seasonal components are known - by predicting the underlying signal, those components can be added back to obtain the final prediction.

### **Decomposition**

Time series data can be split into four main components:

* **Level** (L) - Average **value** for a specific time period (Baseline value without the other components)
* **Trends** (T) - Average **increase or decrease** over a period of time
* **Season** (S) - Average **fluctuations** with a fixed periodicity
* **White Noise** ($\varepsilon$) - Random fluctuations

<!-- Obtained from Medium -->
![TIME_SERIES_COMPONENTS](Assets/10.%20Time%20Series.md/TIME_SERIES_COMPONENTS.png){.center}

!!! Note

    There are also Cyclical effects that are long-term changes that occur at unknown intervals. For instance, an economic recession is a well-known Cyclical effect.

There are two types of decomposition, depending on the nature of the data:

<center>

|               **Additive**                |          **Multiplicative**           |
| :---------------------------------------: | :-----------------------------------: |
|       $Y = L + T + S + \varepsilon$       | $Y = L \cdot T \cdot S \cdot \varepsilon$ |
| Fluctuation **NOT proportional to trend** | Fluctuation **proportional** to trend |

</center>

<!-- Obtained From Crystall Ball Services -->
![TIME_SERIES_DECOMPOSITION](Assets/10.%20Time%20Series.md/TIME_SERIES_DECOMPOSITION.png){.center}

!!! Tip

    Using a **variance stabilizing transformation** such as the Log function will convert a multiplicative model to become additive:

    $$
    \begin{aligned}
        Y_{t} &= L_{t} \cdot T_{t} \cdot S_{t} \cdot \varepsilon_{t} \\
        \ln Y_{t} &= \ln \left(L_{t} \cdot T_{t} \cdot S_{t} \cdot \varepsilon_{t} \right) \\
        \ln Y_{t} &= \ln L_{t} + \ln T_{t} + \ln S_{t} + \ln \varepsilon_{t}
    \end{aligned}
    $$

### **Arithmetic Smoothing**

The **simple moving average** at a given time is the **arithmetic average** of the **LAST $k$** observations, where $k$ is known as the **moving average length**. It can also be shown that it is recursive:

$$
\begin{aligned}
    s_{t}
    &= \frac{y_{t} + y_{t-1} + \dots + y_{t-k+1}}{k} \\
    &= s_{t-1} + \frac{y_{t} - y_{t-k}}{k} 
\end{aligned}
$$

!!! Tip
    
    The recursion is simply accounting for the **averaged difference between the change in observations** at the boundaries:

    <!-- Self Made -->
    ![SMA_RECUSION](Assets/10.%20Time%20Series.md/SMA_RECURSION.png){.center}

!!! Warning

    The effect of smoothing is **proportional** to $k$. However, there is **no smoothing** is $k=1$.

Assuming that the data has **no trend or seasonal effects**, the smoothed series accounts for the level and hence can be used for predictions:

$$
\begin{aligned}
    \hat{y}_{t+1} &= \beta_{0} \\
    \beta_{0} &= s_{t}
\end{aligned}
$$

!!! Tip

    Notice that the functional form above is similar to linear regression. Thus, it is possible to use **Weighted Least Squares** to estimate $\beta_{0}$:

    $$
        \min \sum w_{t} (y_{t} - \hat{y}_{t})^{2}
    $$

    Since only the last $k$ observations are used, weights are only assigned to observations within the indow:

    $$
    \begin{aligned}
        w_{t}
        \begin{cases}
            1, t \ge n - k + 1
            0, t \le n - k + 1
        \end{cases}
    \end{aligned}
    $$

    It can be shown that this reduces to the moving average at time $t$. It is also known as a **Locally Constant Mean** model, as it uses the local mean of the last $k$ observations as the prediction.

If data has an underlying trend, then a **Doubly Smoothed** time series can be used instead, with the following functional form:

$$
\begin{aligned}
   s^{(2)}_{t}
   &= \frac{s_{t} + s_{t-1} + \dots + s_{t-k+1}}{k} \\
   &= s_{t-1} + \frac{s_{t} - s_{t-k}}{k} \\
   \\
    \hat{y}_{t+1} &= \beta_{0} + \beta_{1} \cdot t \\
    \beta_{0} &= s_{t} \\
    \beta_{1} &= \frac{2 \cdot \left(s_{t} - s^{(2)}_{t} \right)}{k-1}
\end{aligned}
$$

!!! Info

    A similar proof can likely be shown using linear regression, but that it out of scope for the purposes of this exam.

### **Exponential Smoothing**

**Exponential Smoothing** follows a similar process, instead using a **geometric average** of ALL past observations. It can also be shown to be recursive:

$$
\begin{aligned}
    s_{t}
    &= (1-w) y_{t} + w \cdot s_{t-1} \\
    &= (1-w) y_{t} + w (1-w) \cdot y_{t-1} + w^{2} \cdot s_{t-2} \\
    &= (1-w) y_{t} + w (1-w) \cdot y_{t-1} + w^{2} (1-w) \cdot y_{t-2} + \dots
\end{aligned}
$$

!!! Note

    The effect of smoothing is proportional to $w$. Note that higher weights are placed recent observations while lower weights are placed on older observations.

!!! Tip

    The smoothing parameter can be chosen by considering the **one step prediction errors** for each $w$, then choosing the $w$ that results in smallest error:

    $$
        SS(w) = \sum (y_{t} - \hat{s}_{t-1})
    $$

    Note that $s_{t-1}$ is the prediction for $y_{t}$, NOT $s_{t}$. This is because $s_{t}$ already includes the actual observation at $t$.

    Naturally, this can be used to determine the **in-sample goodness of fit** as well. A similar process can be done for moving average smoothing as well to determine the optimal $k$.

Assuming that the data has **no trend or seasonal effects**, the smoothed series accounts for the level and hence can be used for predictions:

$$
\begin{aligned}
    \hat{y}_{t+1} &= \beta_{0} \\
    \beta_{0} &= s_{t}
\end{aligned}
$$

!!! Tip

    Similarly, the coefficient can be found via **weighted least squares** as well:

    $$
    \begin{aligned}
        \min \sum w_{t} (y_{t} - \hat{y}_{t})^{2} \\
        w_{t} = w^{n-t}    
    \end{aligned}
    $$

    The resulting estimate is different from the exponential smoothing average, but is **approximately equal for large n**:

    $$
    \begin{aligned}
        \beta_{0}
        &= \frac{s_{t} - (1-w)w^{t} \cdot y_{0}}{1-w^{n}}
        &\approx s_{t} 
    \end{aligned}
    $$

However, if data has an underlying trend, then a **Doubly Smoothed** time series can be used instead, with the following functional form:

$$
\begin{aligned}
    s^{(2)}_{t}
    &= (1-w) s_{t} + w \cdot s^{(2)}_{t-1} \\
    &= (1-w) s_{t} + w (1-w) \cdot s_{t-1} + w^{2} \cdot s^{(2)}_{t-2} \\
    &= (1-w) s_{t} + w (1-w) \cdot s_{t-1} + w^{2} (1-w) \cdot s_{t-2} + \dots \\
    \\
    \hat{y}_{t+1} &= \beta_{0} + \beta_{1} \cdot t \\
    \beta_{0} &= s_{t} \\
    \beta_{1} &= \frac{1-w}{w} \cdot \left(s_{t} - s^{(2)}_{t} \right) 
\end{aligned}
$$

!!! Info

    It is possible to even have a **triple smoothed model** to account for seasonal effects, but such model are out of scope for this exam.

### **Seasonal Models**

## **Other Models**

### **Volatility Models**

So far, all the models discussed cannot work with non-stationary data. However, if the data exhibits **non-constant variance**, there are two models that can be used:

* Autoregressive Changing Heteroscedasticity (**ARCH**) - Based on past error terms
* Generalized Autoregressive Changing Heteroscedasticity (**GARCH**) - Based on past error terms AND past conditional variances

$$
\begin{aligned}
    \text{ARCH}(p)
    &= \sigma^{2}_{t} \\
    &= \theta + \gamma_{1} \cdot \varepsilon^{2}_{t-1} + \dots + \gamma_{p} \cdot \varepsilon^{2}_{t-p} \\
    \\
    \text{GARCH}(p,q)
    &= \sigma^{2}_{t} \\
    &= \theta + \gamma_{1} \cdot \varepsilon^{2}_{t-1} + \dots + \gamma_{p} \cdot \varepsilon^{2}_{t-p}
    + \delta_{1} \cdot \sigma^{2}_{t-1} + \dots + \delta_{q} \cdot \sigma^{2}_{t-q} \\
\end{aligned}
$$

Take note of the following key facts for the model:

* The regression coefficients are estimated using MLE
* The regression coefficients must be **POSITIVE** and sum to 1
* The intercept is known as the **Long Run Volatility Parameter** and must also be **POSITIVE**
* The emphasis on positive above is because the coefficients MUST be there; they cannot be 0

!!! Warning

    The above two models assume that conditional variance can change, but the **unconditional variance is still constant**; still weakly stationary.

    Thus, the unconditional variance for both ARCH and GARCH can be expressed as:

    $$
        \text{Var}(\varepsilon_{t}) = \frac{\theta}{1 - \sum \text{Coefficients}} 
    $$

## **Model Diagnostics**

Similar to other statistical learning methods, the observations are split into a training ($N_{1}$)and test set ($N_{2}$). However, the key difference is that **ALL PRIOR observations** are used to make a prediction, even if the observation belongs to the test set:

<!-- Self Made -->
![TS_TEST_TRAIN_SPLIT](Assets/10.%20Time%20Series.md/TS_TEST_TRAIN_SPLIT.png){.center}

Time series models uses slightly different metrics compared:

$$
\begin{aligned}
    \text{Mean Square Error (MSE)}
    &= \frac{1}{N_{2}} \cdot \sum (y_{t} - \hat{y}_{t})^{2} \\
    \\
    \text{Mean Error (ME)}
    &= \frac{1}{N_{2}} \cdot (y_{t} - \hat{y}_{t}) \\
    \\
    \text{Mean Absolute Error (MAE)}
    &= \frac{1}{N_{2}} \cdot | y_{t} - \hat{y}_{t} | \\
    \\
    \text{Mean Percentage Error (MPE)}
    &= 100 \cdot \frac{1}{N_{2}} \cdot \frac{y_{t} - \hat{y}_{t}}{y_{t}} \\
    \\
    \text{Mean Absolute Percentage Error (MAPE)}
    &= 100 \cdot \frac{1}{N_{2}} \cdot \frac{| y_{t} - \hat{y}_{t} |}{y_{t}}
\end{aligned}
$$

!!! Note

    The above metrics can be used to identify trend patterns in the data. However, MSE and the absolute measures are slightly better as they do NOT allow for offsetting effects.