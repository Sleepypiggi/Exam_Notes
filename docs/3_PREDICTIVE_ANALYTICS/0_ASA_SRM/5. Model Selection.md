# **Model Selection**

## **Overview**

As previously covered, there are **goodness of fit** measures that can be used to determine the best model from a given spread. However, the number of possible models tend to **increase exponentially** with the number of possible predictors as **each combination of predictors** is considered a different model.

Thus, it is **impractical** to fit every possible model. There are well-accepted algorithms that can be used to find the determine the best model in an efficient manner.

<!-- Obtained from Kaggle -->
![MODEL_SELECTION](Assets/5.%20Model%20Selection.md/MODEL_SELECTION.png){.center}

!!! Tip

    The goal is to achieve a **Parsimonious Model** (derived from the word Parsimony), which is a model that achieves the desired fit by using the **minimum number** of variables.
    
    Generally speaking, these simpler models are less flexible which allows them to **perform better on new data**, improving their predictive performance:

    <!— Obtained from Coaching Actuaries —>
    ![SUBSET_PERFORMANCE](Assets/5.%20Model%20Selection.md/SUBSET_PERFORMANCE.png){.center}

## **Selection Algorithm**

### **Best Subset Selection**

The **Best Subset Selection** algorithm is the ideal model selection algorithm, assuming there are no resource constraints:

1. For every possible number of predictors ($p$), fit every combination of available predictors ($g$)
2. Among the models with the **same number of predictors**, select the model with the highest $R^{2}$ as the best model for that number of predictors ($M_{p}$)
3. Among the $M_{p}$, select the best overall model using a **Selection Criteria** of choice

!!! Tip

    The process is akin to using a double loop:

    ```
    FOR p = 0 TO p THEN
        FOR g = 1 TO g THEN
    ```
!!! Warning

    The number of predictors allowed in the model starts at 0 to reflect that the **null model** is also a possible model.

Best Subset Selection considers EVERY possible model, which is **impractical** for large $g$. However, it serves as a **theoretical benchmark** for the "best" algorithm:

$$
    \text{Models Considered} = 2^{g}
$$

!!! Warning

    Even though it considers all possible models, it does NOT guarantee it will find best possible model.

    It is able to find the “best” model from a training error perspective, but the *true* best model typically refers to a **test error** perspective.

    Since there is NO way to perfectly estimate the test error, thus there is **no guarantee** to find the true best model. This is true **regardless of whichever selection method** is used.

### **Stepwise Selection**

**Forward Stepwise** starts with NO predictors and **iteratively adds** one predictor at a time:

1. Fit all possible models for $p=0$
2. Among the fitted models, select the model with the highest $R^{2}$ as $M_{p}$
3. Fit a new model by **ADDING a predictor** to the previous model; consider all possibilities
4. Repeat step (2) - (3) for all $p$ to determine all $M_{p}$
5. Among the $M_{p}$, select the best overall model using a **Selection Criteria** of choice

<!-- Obtained from Quantifying Health -->
![FORWARD_STEPWISE](Assets/5.%20Model%20Selection.md/FORWARD_STEPWISE.png){.center}

!!! Warning

    The starting point for the forward selection algorithm is the INTERCEPT-ONLY model.

**Backward Stepwise** starts with the maximum and **iteratively drops** one predictor at a time:

1. Fit all possible models for $p=p$
2. Among the fitted models, select the model with the highest $R^{2}$ as $M_{p}$
3. Fit a new model by **DROPPING a predictor** from the previous model; consider all possibilities
4. Repeat step (2) - (3) for all $p$ to determine all $M_{p}$
5. Among the $M_{p}$, select the best overall model using a **Selection Criteria** of choice

<!-- Obtained from Quantifying Health -->
![BACKWARD_STEPWISE](Assets/5.%20Model%20Selection.md/BACKWARD_STEPWISE.png){.center}

Using either of the approaches, the **number of models fitted reduces**, making it an appealing choice compared to best subset selection:

$$
    \text{Number of Stepwise} = 1 + \frac{g(g+1)}{2}
$$

However, stepwise selection is a **greedy algorithm** as it picks the best available model at that moment, which may cause it to **miss the overall best model**, since it does not allow for retrospective changes.

!!! Tip

    One unique limitation to each that is NOT applicable to the other is:
    
    * **Forward**: Ignores joint effect of predictors, since only one added at a time
    * **Backward**: Cannot be used when there are **high dimensions** due to fitting issues as the algorithm starts with the full problematic model

### **Mixed Stepwise**

A hybrid approach that combines elements of the Forward and Backward method is known as the **Mixed Stepwise** method:

1. Start with the null model
2. Consider each possible variable to be added - **add** the variable with the **smallest p-value** from a two-tailed t-test; do NOT add the variable if the p-value is above a **specified threshold**
3. Consider each possible that can be dropped - **drop** the variable with the highest **p-value** from a two-tailed t-test; do NOT drop the variable if the p-value is below a **specified threshold**
4. Repeat until the algorithm no longer adds or drops a variable

The main benefit over regular stepwise approaches is that it covers a **larger number** of models. However, it comes with its own unique issues:

* It looks **only at p-values**, not any other metric
* It involves a **sequence of t-tests**, which affects the overall power of the test

!!! Tip

    Notice that algorithm is most similar to the forward selection. As a result, it also suffers from the similar problem of not considering the joint effect.

!!! Tip
    
    At each step, there might be multiple variables that can be added or dropped. It is important to only add or drop variables **ONE at a time** (most or least significant), as there might **second order impact** which changes its significance to the model.

There are problems **inherent** to the selection process regardless of the method used:

1. All methods **fail to account for domain knowledge** that the user has
2. All methods are **susceptible to data snooping** (repeating the analysis slightly differently to achieve a certain outcome)
3. All methods are **NOT guaranteed** to find the best model

### **Comparison**

The number of models considered for every possible $p$ and in totality across the three approaches are summarized below:

<center>

| **Number of Predictors** |   **Best**    |      **Forward**       |      **Backward**      |
| :----------------------: | :-----------: | :--------------------: | :--------------------: |
|            0             |       1       |           1            |           1            |
|    $1 \le k \le p-1$     | $g \choose k$ |        $p-k+1$         |          k+1           |
|            p             |       1       |           1            |           1            |
|          Total           |    $2^{g}$    | $1 + \frac{g(g+1)}{2}$ | $1 + \frac{g(g+1)}{2}$ |

</center>

!!! Tip

    For $p=0$ and $p=p$, all three algorithms will result in the **SAME best model** for that number of predictors $M_{p}$. This is because there is only **ONE possible model for each** - a model with no predictors and one with all predictors, respectively.

!!! Tip

    To remember the general expression for the middle row, think intuitively about the second iteration of the algorithm.

    For **Forward Stepwise**, the second iteration of the algorithm is $k=1$. The algorithm needs to select one predictor out of $p$ to add to the model:

    $$
    \begin{aligned}
        \text{Number of Models Forward}
        &= p \\
        &= p - \underbrace{1}_{k} + 1 \\
        &= p - k + 1
    \end{aligned}
    $$

    For Backward Stepwise, the second iteration of the algorithm is $k=p-1$. The algorithm needs to select one predictor out of $p$ to drop from the model:

    $$
    \begin{aligned}
        \text{Number of Models Forward}
        &= p \\
        &= \underbrace{p - 1}_{k} + 1 \\
        &= k + 1
    \end{aligned}
    $$   

!!! Note

    The number of models looked at for mixed selection is **not fixed** but is typically larger than Forward/Backward methods.

## **Selection Criteria**

Selection Criteria is the metric used to choose the **final model** of all the shortlisted models, each with **DIFFERENT number of predictors**.

During the shortlisting process, the models being compared had the **same number of predictors**, thus $R^{2}$ provided a **fair comparison**. However, since each model now has a **different number of predictors**, $R^{2}$ is **no longer appropriate** as it is biased by the number of predictors. Thus, the following metrics can be used instead:

1. Adjusted $R^{2}$
2. Mallow's $C_{p}$
3. Information Criteria (AIC, BIC)

The above metrics are essentially trying to **indirectly estimate the test error**, by applying an adjustment on the training error based on the number of predictors (flexibility). An alternative approach is to **directly estimate** the test error using **Validation Set** approaches.

### **Training Error Approach**

There are three new metrics to be covered, all based off the same intuition:

* Mallow's $C_p$
* Aikake’s Information Criterion (AIC)
* Bayesian Information Criterion (BIC)

$$
\begin{aligned}
    \text{AIC}
    &= \frac{1}{n} \cdot (\text{RSS} + 2 \cdot p \cdot \hat{\sigma}^{2}_{\text{Full}}) \\
    \\
    \text{BIC}
    &= \frac{1}{n} \cdot (\text{RSS} + \ln n \cdot p \cdot \hat{\sigma}^{2}_{\text{Full}}) \\
    \\
    C_{p}
    &= \frac{1}{n} \cdot (\text{RSS} + 2 \cdot p \cdot \hat{\sigma}^{2}_{\text{Full}}) \\
    &= \text{AIC}
\end{aligned}
$$

!!! Tip

    The MSE in the above equations represent the MSE for a **model with ALL predictors**, hence "full".

    If this MSE used is unbiased, then the resulting measures are **unbiased** estimates of the test error as well.

!!! Note

    Adjusted $R^{2}$ is also a valid metric, but the other three are preferred as they have **stronger theoretical justifications** as a measure of model quality.

    Regardless, all metrics are not feasible when there is **high dimensions**. The metrics all rely on the fitted residuals - but if the model has difficulty being fit in the first place, then the metrics cannot be calculated.

!!! Tip

    Note that Mallow's C has the exact same formula as AIC. There is an **alternative formulation**; the original can shown to be a function of this alternate formula:

    $$
    \begin{aligned}
        C'_{p} &= \frac{\text{RSS}}{\hat{\sigma}^{2}_{\text{Full}}} + 2p  - n \\
        C_{p} &= \frac{1}{n} \hat{\sigma}^{2}_{\text{Full}} \cdot \left(C'_{p} + n \right)
    \end{aligned}
    $$

    The two formulas are equivalent in the sense that the model that results in the smallest $C_{p}$ also results in the smallest $C'_{p}$, thus it does not matter which formula is used (unless the question requires exact computation of the value).

The metrics are meant to compare the **relative effectiveness** of models; as a model selection criteria. The **lower the metrics**, the **better the model** in comparison to others. On its own, it is still technically an estimate of the test error.

Notice that the first term is a measure of the **training error**, while the second term is the **adjustment** for the number of predictors. The lower the metric the better, thus the adjustment **penalizes models with more predictors**.

!!! Tip

    Notice that AIC and BIC are extremely similar, with the only **difference being on the penalty**:
    
    * **AIC**: Flat 2
    * **BIC**: $\ln n$
    
    Both will produce the **same selection** for small sample sizes, however for large sample sizes (8 and above), BIC **penalizes the larger models more**:

    $$
        \ln 7 \approx 2
    $$

!!! Note

    To be explicit, the above measures can also be said to be Goodness of fit measures.

### **Validation Approach**

As mentioned in the beginning, a model's effectiveness should NOT be evaluated using data that was used to train it. Thus, for model selection, the selection of the final model should be **based on data that was not used to train it**; using **Validation Data**.

The MSE determined using the Validation data is known as the **Validation Error**, which is an **DIRECT estimate of the Test Error** (since both are based on data that was not used to train the model).

The model with the **lowest Validation Error** is chosen as the final model. Once determined, **re-train the final model** using ALL non-test observations (Training + Validation data).

!!! Note

    The final model must be retrained as it should be **using as much as information as possible**.

    The model that was selected was trained only using the validation data. When retrained with the all data available (Training + Validation), a **slightly different model** will be produced.

<!-- Obtained from Galaxy Inferno -->
![VALIDATION_SET](Assets/5.%20Model%20Selection.md/VALIDATION_SET.png){.center}

There are two main disadvtanges to using Validation Set:

1. Results tend to be **fickle** as only one iteration is run with only a subset of the data
2. Since only a subset of the data is used, the **true signal might not have been captured**, resulting in **overestimations** of the test error

#### **Cross Validation**

A more *systematic* method is to use **Cross Validation** instead:

* Split the model into **$k$ groups** of roughly equal size; each group is known as a **Fold**
* For each fold, train the model on the data **outside the fold** and calculate the validation error against the fold
* Determine the **Cross Validation Error** as the average of the errors from each fold

<!-- Obtained from Statology -->
![CROSS_VALIDATION](Assets/5.%20Model%20Selection.md/CROSS_VALIDATION.png){.center}

!!! Note

    The systematic approach of CV helps to alleviate the problems of regular validation:

    1. The **average of multiple iterations** are used that covers all the observations; tends to be **less fickle**
    2. **Majority of the observations are used** in each iteration; true signal should be captured which results in **more accurate estimations** of the test error

#### **LOOCV**

An extreme version of Cross Validation is known as the **Leave One Out Cross Validation** (LOOCV), where the number of folds is equal to the number of observations; **each fold contains only one observation**.

If there are $k$ folds, then $k$ models will be fitted. Thus, for LOOCV, as many models will be fitted **as there are observations**, making it extremely **computationally intensive**. However, one upside is that the LOOCV error is **constant**; it only needs to be run once.

!!! Note

    Other variations have some judgment involved in how to partition the datasets, thus different iterations of the algorithm could lead to different results.

For linear regression specifically, there is a **closed form expression** for LOOCV. The key idea is that due to the **linear nature** of the model, the effect of each predictor on the observation is easily known (Leverage). Thus, the leverage can be used to **remove the effect of a particular observation** on the residuals, achieving the effect of LOOCV:

$$
    \text{LOOCV} = \frac{1}{n} \cdot \sum \frac{y - \hat{y}}{1 - h_{i}}
$$

!!! Tip

    The primary advantage of this is that **ONLY ONE model needs to be fitted** to obtain the LOOCV error for SLR; compared to having to fit $n$ models when using other approaches.

#### **Comparisons**

Recall that the purpose of the validation error is to **estimate the test MSE**. Thus, it can be evaluated based on the typical **estimator metrics**:

<center>

|  **Metric**  | **VS** | **CV** | **LOOCV** |
| :----------: | :----: | :----: | :-------: |
|   **Bias**   |  High  | Medium |    Low    |
| **Variance** |  Low   | Medium |   High    |

</center>

!!! Tip

    Note that the above comparisons are RELATIVE to one another.
    
    An easy but **theoretically wrong** way to remember is based on the number of observations used in the training:
    
    * LOOCV uses the **most observations**, thus **overfits** with low bias and high variance
    * Validation uses the **least observations**, thus **underfits** with high bias but low variance 

    Thus, Cross Validation with $k=5$ provides a **good balance**, which also takes into account computational efficiency.

!!! Warning

    It is often difficult to understand why LOOCV has higher variance, given that the great similarity in the datasets used in each split should lead to very **similar predictions** each time and hence low variance.

    The key intuition is that the mean of **highly correlated quantities** (LOOCV prediction) tends to have **higher variance** than the mean of less correlated quantities (all else equal). This is consistent with the argument for Random Forests over Bagging as well.

    Note that in practice this might not hold true; there are several online discussions debating the validity of this claim. However, for the purposes of this exam, the above explanation is sufficient.

### **Complete Process**

1. Choose a model selection method (Best, Stepwise)
2. Perform the model selection method in conjunction with a validation method to determine the **optimal number** of predictors
3. Perform the model selection method again with the **whole dataset** (Training + Validation)
4. Select the model with the number of predictors determined in (2) as the final model

### **Hyperparameter Tuning**

Hyperparameters are **inputs decided by the modeller** that affects *how* the model should learn.

It is NOT possible to determine the optimal hyper parameter prior to the experiment. Thus, rather than arbitrarily picking a value, cross validation can be used to **test a range of values** and choose the best one.

The intuition is that we want to choose the model that has the **best predictive performance**. The (cross) validation error is the **most direct estimate** of that, thus the hyperparameter should be chosen such that that error is minimized.

!!! Tip

    Rather than choosing the model with the absolute lowest cross validation error, an alternative rule of thumb is to choose the **simplest model** out of all the models within **one standard error** of the lowest:

    <!-- Obtained from CMU -->
    ![ONE_STANDARD_ERROR](Assets/5.%20Model%20Selection.md/ONE_STANDARD_ERROR.png)

    The intuition is that the difference in cross validation errors is **not material** within the one standard error. Since **simplicity** is usually the goal, the simplest model is chosen. Naturally, the model chosen this way will be **less flexible**, but that is intentional.