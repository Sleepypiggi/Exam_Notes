# **Principal Component Analysis**

## **Overview**

**Principal Component Analaysis** (PCA) is an unsupervised statistical learning method that transforms high dimensional data into a **lower dimensional space** by creating **new key variables** ("Principal Components") that are **linear combinations** of the other variables.

Thus, the two key uses of PCA are:

1. **Data Visualization tool** for high dimensional datasets (Dimension reduction)
2. *Feature Generation tool** for overfitted/highly correlated datasets OR to find the most important predictors in a dataset

!!! Warning

    There are alternative methods of covering PCA via **Linear Algebra** (Eigenvector, Eigenvalues). For the purposes of this exam, it is sufficient to know the below terminology.

    For simplicity, the examples in this section will use two principal components. However, it is possible to have more than two in the exam and in practice.

!!! Note

    For the purposes of this exam, it is assumed that PCA can only be performed with **Quantitative Variables**.

Naturally, after performing PCA, the underlying original variables that were used to form the PC should no longer be used to **avoid double counting**.

## **Principal Components**

Each principal component is intended to explain **most of the REMAINING variation** in the dataset after accounting for previous PCs. Naturally, the first principal component will **explain the most variation** and each subsequent one will explain lesser and lesser.

!!! Note

    Although not explicitly stated, the manner of calculating the variation is similar to MLR. Thus, the distances are squared, which places higher weight on observations that are further away. Thus, this makes PCA sensitive to outliers as well.

However, subsequent PCs should NOT have any overlap in the variation explained; thus should be **uncorrelated** with the previous PCs. Due to certain mathematical properties assumed, this is equivalent to the PCs being **Orthogonal** (perpendicular) to one another.

<!-- Obtained from Learnche -->
![PCA_TWO_COMPONENTS](Assets/8.%20Principal%20Components.md/PCA_TWO_COMPONENTS.png){.center}

!!! Tip

    Recall that being Orthogonal also means that the **Dot Products** of the principal component **LOADINGS** are 0:

    $$
        \sum \varphi_{i,j} \cdot \varphi_{i,k} = 0
    $$

    <!-- Obtained from Datahackers -->
    ![DOT_PRODUCT](Assets/8.%20Principal%20Components.md/DOT_PRODUCT.png){.center}

    Note that only the dot product of PAIRS of principal components are 0, not for all principal components.

Due to the linear algebra required, the data must first be **Centered** and **Scaled** ("standardization"). Each PC then **passes through the mean** of the data (which is now the origin post centering):

<!-- Obtained from Learnche -->
![PCA_CENTER_SCALE](Assets/8.%20Principal%20Components.md/PCA_CENTER_SCALE.png){.center}

PCA seeks to minimise the **absolute variance** among all the variables. Variables with **larger units/range will naturally have larger variance**, causing them to dominate the analysis. Thus, there is a need to scale the variables to a **common unitless form** for a fair comparison.

!!! Warning

    It is **NOT strictly necessary** to standardize variables - if the underlying variables have similar variances, then the analysis can proceed without complication.

!!! Warning

    The first PCs is essentially the **line of best fit** that passes through the data - "closest" to the data:
    
    However, it is DIFFERENT from that of linear regression for two reasons:
    
    1. PCA finds the best fit line with respect to **ALL variables** (not just the target)
    2. PCA finds the best line based on **perpendicular distance** (not the vertical distance)

## **Loading and Scores**

The **direction** of the PC in the multi-dimensional space is known as its **Loading**. Mathematically, it is the **contribution of each variable** on the PC. Naturally, the **higher the loading**, the **more that variable contributes** to the variance explained for that PC.

Consider the perpendicular projection of each observation onto the PC; the straight line distance between an observation and the PC. The **distance** from the origin to this point on the PC is known as the **Score of the observation**.

<!-- Obtained from LearnChe -->
![PCA_SCORE](Assets/8.%20Principal%20Components.md/PCA_SCORE.png){.center}
 
!!! Note

    Each observation has a score for EACH PC; not just the nearest one.

Mathematically, it can be represented as the following:

* **i**: Observation Index
* **j**: PC Index
* **k**: Feature Index

$$
    z_{i,j} = \sum_{k} \varphi_{j,k} \cdot x_{i,k}
$$

!!! Note

    If the data is scaled, then the **sum of SQUARED loadings** for each PC is 1:

    $$
        \sum \varphi^{2}_{1, k} = 1
    $$
    
    Since the loadings are squared, there are technically **two possible solutions** to each loading. The **dot product** of the loadings MUST be used to verify which of the signs are correct.

    Since the Dot Product must be 0, there MUST be **at least one negative loading** among each pair of PC loadings to ensure that it is 0.
    
!!! Note

    For the purposes of this exam, it should be noted that the loadings are **NOT unique**. Different softwares will produce the **same absolute value** of the loadings, but with **different signs** for each iteration (likely due to the above). The sign flip must be applied **consistently** across all loadings, NOT just a select few.

    The ultimate PC scores remain the same; only the interpretation changes.

Geometrically, the PCs also form a **plane** in the multi-dimensional space. The Score of each observation with respect to the respective PCs is the coordinate of the observations within the **coordinate space of the plane**. Thus, observations with high scores are "outliers" in the PC space.

<!-- Obtained from Learnche -->
![PCA_PLANE](Assets/8.%20Principal%20Components.md/PCA_PLANE.png){.center}

!!! Info

    For scenarios with more than two PCs, the resulting shape will be a Cube, Tesseract or other equivalent higher dimensional shape.

!!! Tip

    In this sense, PCA is said to find the "shape" that minimizes the sum of perpendicular distances from each observation; the "shape" that is closest to the observations.

!!! Tip

    It is necessary to understand how to interpret a PC - how does a high or low value for the PC translate into the underlying variables?

    * Only consider variables with **large loadings** as they are more significant
    * Use the **sign of the loading** to indicate whether the they should take on a high or low value

    EG. "PC1 takes on high values for animals who are old and unable to have offspring and low values...". Do not simply state equation; explain what groups the PC distinguishes.

Naturally, the resulting coordinates in the PC system is **different from the original**. This is better appreciated by changing the perspective of a plot to use the PCs as the vertical and horizontal axis respectively:

<!-- Obtained from Coaching Actuaries -->
![PCA_COORDINATE](Assets/8.%20Principal%20Components.md/PCA_COORDINATE.png){.center}

!!! Note

    The first PC is always the horizontal, followed by the vertical, so and so forth. They are more generally referred to as the PC1 direction, PC2 direction etc.

A **bi-plot** combines the Loadings and Scores into a **single plot** (hence "bi") and uses the principal components as the main coordinate space:

* **Primary Axis**: Score; characteristic of observations 
* **Secondary Axis**: Loading; characteristic of PCs

<!-- Obtained from Coaching Actuaries -->
![BI_PLOT](Assets/8.%20Principal%20Components.md/BI_PLOT.png){.center}

The relative position of the vectors indicate their relationship with one another. **Closer vectors** tend to be more correlated - In the example below, Rape, Assualt and Murder are more correlated to each other than they are to Urban Population.

<!-- Obtained from ISLR -->
![BIPLOT_CORRELATION](Assets/8.%20Principal%20Components.md/BIPLOT_CORRELATION.png){.center}

The key idea is that *within* the same PC:

* **Loading Magnitude** (Relative position on Biplot): **Strength** of correlation
* **Loading Sign** (Direction on Biplot): **Type** of correlation

!!! Warning

    Do not mistakenly read the primary axis for the loading vectors. The above plot colour codes them, but it is unlikely to be the case in the actual exam.

    <!-- Obtained from Coaching Actuaries -->
    ![BI_PLOT_SECONDARY](Assets/8.%20Principal%20Components.md/BI_PLOT_SECONDARY.png)

## **Variance**

Recall that the PCs are created such that the **total variation explained is maximized**. As alluded to previously, it refers to the variation in the **underling features**. This is in contrast to linear regression where the variation is with respect to the target variable.

!!! Note

    The "variation" that has been used so far refers to the literal Variance & Standard Deviation of the PCs.

The variation of features explained by each principle component is the variation of that particular principal component; **squared Score**:

$$
\begin{aligned}
    s^{2}_{z_{j}}
    &= \frac{1}{n-1} \cdot \sum (z_{i,j} - \bar{z}_{j})^{2} \\
    &= \frac{1}{n-1} \cdot \sum (z_{i,j})^{2} \\
    &= \frac{1}{n-1} \cdot \sum (\sum_{k} \varphi_{j,k} \cdot x_{i,k})^{2}
\end{aligned}
$$

!!! Note

    The key is noticing that since the data was centered, the **mean of the PC is always 0**. The only way for this to occur is if the **sum of scores** for each principal component is 0:

    $$
        \sum_{i} z_{i,j} = 0
    $$

!!! Tip

    It is possible to infer the relative variance from a Biplot, ONLY if the data was NOT scaled. If unscaled, then **larger loadings would lead to larger variances**, all else equal. Thus, the feature with the **largest scale** would be the feature with the largest PC1 loading (as PC1 explains the **largest variance**).

    This cannot be used for scaled data as the **variance for all features would be 1**.

A **Scree Plot** summarizes the above information by plotting the proportion of variance explained by each successive PC. As alluded to previously, the plot will always be **decreasing** as the variance explained should decrease per PC:

$$
\begin{aligned}
    \text{Prop Var Explained by PC}
    &= \frac{\text{Var Explained by PC}}{\text{Total Variance for all underlying variables}} \\
    &= \frac{\text{Var Explained by PC}}{n}
\end{aligned}
$$

<!-- Obtained from Coaching Actuaries -->
![SCREE_PLOT](Assets/8.%20Principal%20Components.md/SCREE_PLOT.png){.center}

!!! Note

    The proportion of explained variance is **similar** to $R^{2}$ in linear regression.

The key tension (as always) is between flexibility and interpretability. The guiding principle is to choose a point where the **tradeoff** between the two is no longer worth it - where the variance explained by subsequent PCs **drops off significantly**, known as the **"Elbow" of the scree plot**. However, if a specific amount of variance is desired, then no judgment is needed - simply choose the **minimum number of PCs** to achieve that level. 

!!! Warning

    If there is no clear elbow and each PC explains roughly the same amount variance (especially if there are only a small amount of PCs), then it is a sign that PC might **NOT be suitable** for the dataset. This typically occurs when the underlying variables are **largely uncorrelated**, such that they cannot be easily compressed together to form PCs.

!!! Tip

    Another consideration is the **meaning** of what each PC represents. Ideally, each chosen PC comprises of a **different combination of variables**, making them complementary to one another.

    If the meaning of the PC does not make sense in the business context, it might be a proxy for another an underlying effect.

## **Other**

### **Collinearity**

In essence, PCA "compresses" the variables together into fewer PCs. **Highly correlated** variables have common signals that can be compressed together, thus **greatly benefitting** from PCA. This typically occurs for variables that share a **unifying concept**.

!!! Note

    Conversely, when a dataset is **uncorrelated**, there is **no room to compress the variables** together, hence rendering PCA ineffective. In fact, using PCA would actually **reduce the interpretability**!
    
    There are two other situations where PCA is not recommended:
    
    * Independent variables are NOT linearly related with one another; cannot effectively combine
    * **Low dimensionality**; loss of information from feature transformation outweighs potential model improvements

An extreme case would be when the variables are **Perfectly Collinear** (variables can be expressed as an exact linear combination of another). In this case, PCA compresses the perfectly collinear features into **ONE MAIN principal component** which is always the **FIRST PC** that explains **MOST** of the variation.

For a two variable case, this manifests itself on the Biplot where all observations have a **score of 0** for PC2; the observations are **entirely explainable by PC1**: 

<!-- Obtained from Coaching Actuaries -->
![BIPLOT_COLLINEAR](Assets/8.%20Principal%20Components.md/BIPLOT_COLLINEAR.png){.center}

For three or more variables, this can be seen from the scree plot where there are PCs that explain NONE of the variation:

<!-- Obtained from Coaching Actuaries -->
![COLLINEAR_SPREE](Assets/8.%20Principal%20Components.md/COLLINEAR_SCREE.png){.center}

### **Regression**

It is possible to perform Linear Regression using the principal component space. In this case, it is known as a **Principal Component Regression**:

$$
    E(Y \mid X) = \theta_{0} + \theta_{1} \cdot Z_{1} + \theta_{2} \cdot Z_{2} + \dots
$$

!!! Warning

    The implicit assumption made is that the directions which explain the most variation among the predictors is **ALSO the the directions** that are associated with the target variable. This is a reasonable approximation, but is naturally **NOT guaranteed**.

Each PC is a combination of ALL underlying variables. Thus, if **ALL PCs are used** for the regression, it is **akin to performing MLR** on the data. The two will provide the same prediction albeit with different functional forms (since one is direct on the data vs on the PCs). However, since the PCs are simply linear combinations of the original features, it is possible to re-calculate the **original regression coefficients** by **decomposing** the PCs back into the features and **regrouping** them:

$$
    \beta_{j} = \theta_{1} \cdot \varphi_{j, 1} + \theta_{2} \cdot \varphi_{j, 2} + \dots
$$

!!! Warning

    Even though they are numerically the same, PCs are less interpretable than the underlying variables given that they each a different combination of the underlying independent variables.

The number of PCs used is a **flexibility measure**. The whole point of PCA was to **reduce the dimensionality** of the data, thus it is likely that only a **subset of the PCs** will be used for the regression. The number of PCs can be treated as a **hyperparameter** and thus can be tuned for via cross-validation. 

!!! Warning

    PCR is NOT a variable selection method because **all variables are still used** - just to varying extents. By reducing dimensionality, it helps to reduce flexibility and mitigate overfitting.

!!! Note

    Since each PC is **uncorrelated** with the others, adding or removing PCs does NOT affect any of the coefficients.

Generally speaking, any other statistical learning method can be applied to the reduced PC dataset, not just linear regression. The resulting model should produce **less noisy results** as the signal would be captured in the first few PCs.
