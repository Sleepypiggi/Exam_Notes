# **Introduction to Statistical Learning**

## **Estimation VS Prediction**

The primary focus of traditional statistics lies in **Estimation**. However, the focus of statistical learning is **Prediction**. Both are **approximation methods** and although they are used interhchangeably in layman terms, there is a **technical difference** between the two:

<center>

|                 **Estimation**                  |                  **Prediction**                   |
| :---------------------------------------------: | :-----------------------------------------------: |
| Approximating a **parameter** of a distribution | Approximating a **realization** of a distribution |
|                 Non-stochastic                  |                    Stochastic                     |

</center>

For instance, given that $X \sim \text{Dist}(\theta)$:

* $X$ is the distribution
* $\theta$ is a parameter of the distribution (to be estimated)
* $x_{i}$ is a realization of the distribution (to be predicted)

## **Overview of Key Elements**

There are two main components to statistical learning:

<center>

|  **Dependent Variable**  | **Independent Variable** |
| :----------------------: | :----------------------: |
| Variable to be predicted | Variable used to predict |
|          Only 1          |    May have multiple     |
|     Output Variable      |      Input Variable      |
|         $y_{i}$          |     $x_{i}, x_{i,j}$     |

</center>

!!! Note

    The following notation is common:
    
    * $i$: Observation Index
    * $j$: Variable Index
    * Most subscripts follow (Observation, Variable)

Within the above, they are further split by their **nature**:

<center>

| **Quantitative Variable** |   **Qualitative Variable**    |
| :-----------------------: | :---------------------------: |
|  Discrete or Continuous   |      Nominal or Ordinal       |
|    Numerical Variable     |     Categorical Variable      |
|      EG. 3.14, 1.849      | EG. North, South, East & West |
|  **Regression** Problem   |  **Classification** Problem   |

</center>

!!! Note

    The possible values of a Quatlitative variable are known as its **Levels**. For simplicity, they can also be expressed as integers - they should NOT be confused with discrete numeric variables (Counts):

    * True (1), False (0)
    * North (0), East (1), South (2), West (4)

    If the **order** of the categories are important, then they are known as **Ordinal variables**. If not, they are known as **Nominal Variables**.

There are two broad categories of **how the "learning" occurs**:

<center>

|           **Supervised**            |         **Unsupervised**         |
| :---------------------------------: | :------------------------------: |
| Has a specified Indpendent Variable |     No Independent Variable      |
|   To make predictions for $y$   | To identify relationships between variables |
|        EG. Linear Regression        | EG. Principal Component Analysis |

</center>

!!! Note

    The key focus of the majority of the exam will be on **Supervised learning**, unless otherwise stated. A relatively small portion on Unsupervised learning will be covered towards the end.

    There technically exists a special class of learning known as **Semi-Supervised**, which typically occurs due to a lack of complete data. It is out of scope for the purposes of this exam.

!!! Note

    Unsupervised learning tends to be more challenging than supervised because:
    
    1. There is **no clear goal**; no simple goal like prediction
    2. There are no methods to **assess model quality**; no validation error to be used

!!! Tip

    Unsupervised learning can also be used to **create new features** to be used in the data, via principal component analysis.

Within **Supervised learning** methods, it can be further broken down into two forms:

<center>

|                   **Parametric**                   |              **Non-parametric**              |
| :------------------------------------------------: | :------------------------------------------: |
| Assumes a **functional form** for the relationship |  **No assumption** made about relationship   |
|    **Equation with parameters** to be estimated    |  Algorithmic method with **no parameters**   |
|    More Robust (Less sensitive) to data changes    | Less Robust (More sensitive) to data changes |
|        Does not require as large a dataset         |   Requires **large dataset** to work well    |
|               EG. Linear Regression                |              EG. Decision Trees              |

</center>

!!! Tip

    Non-Parametric models will outperform parametric ones when the assumed functional form is different from what was specified.
    
    This tends to occur when the underlying data generating process is **complicated**, as most specified functional forms are not that complicated (EG. *Linear* regression).

!!! Note

    Parametric tends to be more robust likely because they have an underlying functional form to anchor them; non-parametric methods are so to speak “at the mercy of the data”.

<!-- Obtained from Coaching Actuaries -->
![METHOD_OVERVIEW](Assets/1.%20Statistical%20Learning.md/METHOD_OVERVIEW.png){.center}

Lastly, there are two main aspects to evaluate a model:

|                    **Flexibility**                    |                  **Interpretability**                  |
| :---------------------------------------------------: | :----------------------------------------------------: |
|      How closely the model **follows** the data       |       How easy it is to **understand** the model       |
|      Used for **Prediction**; determining values      |  Used for **Inference**; understanding relationships   |
| **Directly proportional** to the number of parameters | **Inversely proportional** to the number of parameters |

* **Prediction**: How much will a home be worth given its **overall characteristics**?
* **Inference**: How much *more* will a home be worth if it has a **specific characteristic**?

!!! Note

    If prediction is the only focus, then understanding *how* the model arrives at the prediction is less irrelevant; as long as the prediction is reliable.
    
    This is especially so for complex models, which are typically referred to as “Black Boxes” where the users do not have “visibility” on the inner workings of the model.

The key is understanding that both are at odds with one another. Highly flexible models that tend to have **complex mathematical components** that allow it to match the data, at the cost of being **unable to inuitively explain** the relationship.

!!! Tip

    Less flexible models are **smoother** because they do NOT try to match the idiosyncrasies of the data:

    * **LHS**: Less Flexible but easier to intepret (Linear relationship)
    * **RHS**: More flexible but harder to intepret (Complex relationship)

    <!-- Obtained from Coaching Actuaries -->
    ![FLEXIBILITY_INFERENCE](Assets/1.%20Statistical%20Learning.md/FLEXIBILITY_INFERENCE.png){.center}

<!-- Obtained from ISLR -->
![METHOD_OVERVIEW_2](Assets/1.%20Statistical%20Learning.md/METHOD_OVERVIEW_2.png){.center}

!!! Warning

    Despite linear regression being more interpretable than trees, the ISLR author has also noted that **Trees are "easier to explain" than linear regression**.

    Although both intepretability and ease of explanation usually refer to the same thing, for the purposes of this exam treat them as **seperate**.

    The author notes that the **interpretability is in the eye of the beholder** - some prefer the coefficients from linear regression while others prefer to the tree visual - one or the other can be easier to explain to different parties.

## **Regression**

### **Specification**

There exists a **true relationship** between the Dependent and Independent Variables:

$$
    Y = f(X_{i}) + \varepsilon 
$$

The above can be understood as the combination of:

* **Signal**: **Systematic** relationship between $Y$ and $X$'s
* **Noise**: **Unpredictable** random variations (also known as the **Error**)

In the above, both $Y$ and $X$ are random variables. However, for statistical learning, we are interested in knowing what the value of $Y$ is **GIVEN a set of realized $X$'s**. Thus, we are focused on the **Conditional Distribution** of $Y \mid X$. The prediction is the **expected value** of this conditional distribution:

$$
\begin{aligned}
    Y \mid X = x_{i} &\sim f(x_{i}) \\
    E(Y \mid X = x_{i}) &= E \left[f(x_{i}) \right] + E(\varepsilon \mid X = x_{i}) \\
    E(Y \mid X = x_{i}) &= f(x_{i})
\end{aligned}
$$

The signal function is known what is commonly referred to as the **Regression Model**.

!!! Warning

    There is a **need to differentiate** the Conditional and Unconditional expectations. Recall the law of total expectation:

    $$
        E(Y) = \sum E(Y \mid X = x_{i}) \cdot P(X = x_{i}) 
    $$

    There is no need for the unconditional expectation in a regression setting as we already have a realized value of $X$ that we are interested in using; there is no probability involved.

!!! Note

    There are two points to take note of:

    1. The systematic relationship results in constant, thus the expectation is itself
    2. The mean of the error is 0, reflecting that random events are typically symmetrical

It is important to understand that the regression model is the **MEAN, not the actual observations** of $Y$:

<!-- Obtained from Colorado Uni -->
![CONDITIONAL_EXPECTATION](Assets/1.%20Statistical%20Learning.md/CONDITIONAL_EXPECTATION.png){.center}

<!-- Obtained from Cloudera -->
![ERROR_TERM](Assets/1.%20Statistical%20Learning.md/ERROR_TERM.png){.center}

The true specification for the **Population Regression Model** ($f$) is unknown. Different statistical learning methods assume a **different form for $f$**, which is then estimated based on the sample to obtain a **Sample Regression Model** $\hat{f}$.  

!!! Note

    The process of estimating $\hat{f}$ is also known as "Training" the model. It also sometimes referred to as "Fitting" the model to the data.

The fitted $\hat{f}$ is the **estimator of the mean of the conditional distribution**. Consequently, the difference between the observation and the fitted model is known as the **Residual**, which is the **estimator of the error**:

$$
\begin{aligned}
    y_{i}
    &= \hat{f} + \hat{\varepsilon} \\
    &= \widehat{E(Y \mid X)} + \hat{\varepsilon} \\
    &= \hat{y} + \hat{\varepsilon}
\end{aligned}
$$

!!! Warning

    For simplicity, the estimate of the mean of the conditional distribution is denoted as $\hat{y}$. **DO NOT** confuse this as the estimate of the actual observation!

    The residuals are also sometimes noted as $e_{i}$ for convenience.

<!-- Self Made -->
![ERROR_VS_RESIDUALS](Assets/1.%20Statistical%20Learning.md/ERROR_VS_RESIDUALS.png){.center}

If $\hat{f}$ was correctly specified and fitted, the resulting residuals should exhibit the **same characteristics** as the underlying errors. This is important, as many statistical learning methods make **assumptions about the nature of the Errors**. Thus, the residuals can be used to **check the validity of the assumptions** or make other inferences.

!!! Info

    For simplicity, the above diagrams were illustrated using Simple Linear Regression. However, the concepts apply more generally to all statistical learning methods.

### **Evaluation**

The main goal of the model is to be **used for prediction**. Thus, the strength of the model should be evaluated based on its ability to make **accurate predictions**.

The key consideration lies in **Overfitting**. If the model matches the underlying sample too closely, it will **mistakenly capture the noise present as a signal**. This impedes its ability to make accurate predictions. Thus, the model must be tested on data that was not used to train it:

* **Training Data**: Observations that were used to train model
* **Test Data**: Obervations that were NOT used to train the model; UNSEEN data

!!! Note

    Operationally, this implies that the sample should be partitioned at the start of the process.
    
    The size of the split between training and test is tricky - larger training sets result in more robust models but less reliable test evaluations.
    
    It is generally accepted that 80-20 or 70-30 between training and test sets are a good balance.


The above concerns can be quantified using the **Mean Squared Error** (MSE):

$$
\begin{aligned}
    \text{MSE}_{\text{Training}}
    &= \frac{\sum (y_{i, \text{Training}} - \hat{y}_{i})^{2}}{n_{\text{Training}}} \\
    \\
    \text{MSE}_{\text{Test}}
    &= \frac{\sum (y_{i, \text{Test}} - \hat{y}_{i})^{2}}{n_{\text{Test}}}    
\end{aligned}
$$

<center>

|      **Training MSE**      |      **Test MSE**       |
| :------------------------: | :---------------------: |
|   Based on Training Set    |    Based on Test Set    |
| Measure of **Flexibility** | Measure of **Accuracy** |
|     Moderate is better     |     Lower is better     |

</center>

The two share the following relationships:

* **Training MSE is always lower** than test MSE since the model was fit based on the training data
* **Training MSE decreases with flexibility** since more flexible models are **better able to match** the data
* **Test MSE has a U-Shape** reflecting that models which are too inflexible **may not capture the signal** while those that are too flexible may **capture the wrong signal**
* Thus, the **large difference** in the two MSEs is a result of high flexibility (overfitting)

<!-- Obtained from Coaching Actuaries -->
![TRAINING_TEST_MSE](Assets/1.%20Statistical%20Learning.md/TRAINING_TEST_MSE.png){.center}

!!! Note

    This is another reason why test sets are important - because using only training data will result in overly optimistic results.

!!! Tip

    A quick way to view how accurate the predictions are would be to plot the original value ($y$) against the predicted value ($\hat{y}$), with an identity line:

    <!-- Obtained from coaching actuaries -->
    ![IDENTITY_PLOT](Assets/1.%20Statistical%20Learning.md/IDENTITY_PLOT.png){.center}

    Predictions that are accurate will be **along the line**; the further away a observation is from the line, the more far off the prediction. 

!!! Tip

    For two methods with the roughly the same test error, it is better to choose the one that is NOT overfitted; relatively smaller difference between training and test MSE.

Recall that the MSE can be decomposed into a combination of **Bias and Variance**. In a regression context, there is an **additional component** representing the error term as well:

* **Reducible**: Error stemming from the **choice** of $\hat{f}$
* **Irreducible**: Error stemming from **inherent variability** (cannot be predicted by $x$)

$$
\begin{aligned}
    \text{MSE}
    &= \underbrace{(\text{Bias}[f(x_{i})])^{2} + \mathrm{Var}[f(x_{i})]}_{\text{Reducible}}
    + \underbrace{\mathrm{Var}(\varepsilon)}_{\text{Irreducible}}    
\end{aligned}
$$

!!! Tip

    Thus, the primary goal of statistical learning is to estimate $\hat{f}$ that the reducible error is minimized.

!!! Warning

    Some texts may refer to the above as:
    
    * **Accuracy** (Bias) - Error from by **approximating** $f$ (real world, complicated) with a simpler model ($\hat{f}$)
    * **Precision** (Variance) - Error from **changes** in $\hat{f}$ across different samples  
   
   Bias decreases with complexity
   Variance increases with complexity
   Convert to table
   
    Although they are used interchangeably in day to day conversations, they have a **specific meaning** in a statistical context.

It is **not possible to minimize both**; there is an inverse relationship known as the **Bias-Variance Tradeoff** which results in the U-shaped curve:

* Increasing complexity **decreases bias** as it allows the model to **capture the signal**
* However, it also **increases variance** as the model will **mistakenly pick up noise** in the data as signal
* Initially, the reduction in bias is higher as the model **rapidly improves**
* However, once majority of the signal has been captured, **more noise will be increasingly captured**, causing the increase in variance to be higher
* Thus, the goal is to choose a model with **moderate amount of flexibility** to minimize the MSE

<!-- Obtained from Geek for Geeks -->
![BIAS_VARIANCE_TRADEOFF](Assets/1.%20Statistical%20Learning.md/BIAS_VARIANCE_TRADEOFF.png){.center}

!!! Note

    The MSE curve is floored at the line representing the irreducible variance.

!!! Tip

    The inversion of test MSE depends largely on the underlying data generating process:

    * **LHS**: Underlying process is of moderate complexity (Typical case described above) 
    * **Center**: Underlying process is simple, thus **low flexibility** methods are sufficient (**Benefits less** from Bias reduction)
    * **RHS**: Underlying process is complicated, thus **high flexibility** methods needed (**Benefits more** from Bias reduction)

    The inversion of the curve thus depends on the underlying functional form - whenever the marginal benefit from Bias reduction falls off.

    <!-- Obtained from ISLR -->
    ![BIAS_MSE_CASES](Assets/1.%20Statistical%20Learning.md/BIAS_MSE_CASES.png){.center}

    Thus, even simpler methods can have high predictive accuracy. There is **NO universal best** learning method across all datasets; it is about choosing the best method for the data.

<center>

|      **Under-fitting**      |      **Over-fitting**       |
| :-------------------------: | :-------------------------: |
|   High Bias, Low Variance   |   Low Bias, High Variance   |
| High training, Low test MSE | Low training, High test MSE |
|  Fail to capture the noise  |  Mistakenly captures noise  |
|     Not flexible enough     |        Too flexible         |

</center>

<!-- Obtained from ACTEX Manual -->
![UNDER_OVER_FITTING](Assets/1.%20Statistical%20Learning.md/UNDER_OVER_FITTING.png){.center}

Ultimately, the above metrics are just for a single model, they need to be **compared to a benchmark** to determine if the model provides any improvement. Generally speaking, these benchmark models are the **most simple models**:

* Regression: Intercept-only model
* Classifier: Purely random classifier

!!! Tip

    The intuition is that these should be the worst performing models. If a given model cannot even surpass it, then it should not be considered.

### **Confidence & Prediction Intervals**

Recall that **Confidence Intervals** are a range of values that the **true parameter** will fall within for a given level of confidence. In a regression setting, recall that the model is estimating the **mean of the conditional distribution**; a parameter. Thus, the confidence interval for a model measures the **range of values for $E(Y \mid X)$**.

!!! Note

    In the case of Parametric learning methods, the estimate of each parameter that goes into $\hat{f}$ will also have an associated confidence interval.

On the other hand, **Prediction Intervals** are the range of values that a **REALIZATION of a random variable** will fall within for a given level of confidence. In a regression setting, this refers to the **ACTUAL dependent variable itself**, not the mean.

Note that since the actual dependent variable is a combination of the mean and the error term, the resulting prediction interval will **always be wider than the confidence interval**, reflecting the additional variance from the error term:

<center>

| **Prediction Interval**  | **Confidence Interval** |
| :----------------------: | :---------------------: |
| Range of Random Variable |   Range of Parameters   |
|   Range for $Y \mid X$   | Range of $E(Y \mid X)$  |
|    Mean & Error Term     |        Mean only        |

</center>

Both Confidence and Prediction Intervals are expressed using the **same general formula**:

$$
    \text{Interval} = \text{Estimate} \pm \text{Percentile} \cdot \text{SE of Estimate}
$$

!!! Warning

    For a prediction interval, the prediction is technically not an estimate; the term above is used for simplicity.

<!-- Obtained from Datacamp -->
![PREDICTION_CONFIDENCE](Assets/1.%20Statistical%20Learning.md/PREDICTION_CONFIDENCE.png){.center}

!!! Note

    Intuitively, the confidence interval represents the uncertainty arising from the **reducible variance** while the prediction interval additionally includes the **irreducible variance** from inherent randomness.

    In an extreme example, the confidence interval will **shrink to 0** as the sample size increases to the population, but the prediction interval will always **at least be equal to the variance of the error**.
    
    $$
    \begin{aligned}
        \lim_{n \to \infty} (\text{CI}) &= 0 \\
        \lim_{n \to \infty} (\text{PI}) &= \text{Var}(\varepsilon)
    \end{aligned}
    $$

## **Classification Problems**

Almost all of the key concepts from a Regression context applies in a classification context as well. The key difference being instead of measuring the absolute error, the **error rate** is used instead:

$$
    \text{Error Rate} = \frac{\text{Number of Correct Classifications}}{n}
$$

The **Bayes Error Rate** is known as the **lowest possible error rate**, akin to the **irreducible error** that arises due to the noise in the data itself.

!!! Note

    The Bayes Error Rate is the error rate that is achieved when using a **Bayes Classifier**, which is a **conceptual model** that assigns observations to the category with the **highest probability**, given the predictor values. It is conceptual because in reality it is NOT possible to know the conditional probabilities for each class.

    Graphically, the above can be described using a **Decision Boundary**:

    * LHS of boundary: Classified as Orange
    * RHS of boundary: Classified as Blue

    <!-- Obtained from ISLR -->
    ![BAYES_DECISION_BOUNDARY](Assets/1.%20Statistical%20Learning.md/BAYES_DECISION_BOUNDARY.png){.center}

Assume that there are two categories (A, B) and that **population probabilities are known** (since all available information is used). Based on this, such a model would assign categories based on the probability:

* The category with the **highest probability** for that observation will be assigned the prediction
* However, there is **no guarantee** that the actual observation will be that category
* Thus, the probability that it is **NOT the highest probability category** is the error rate

The **Bayes Error Rate** is determined using the above for all combinations of the independent variables:

$$
\begin{aligned}
    \text{Bayes Error Rate}
    &= 1 - E \left(\max P \left(Y = j \mid X \right) \right) \\
    &= 1 - \frac{\sum \max P \left(Y = j \mid X \right)}{n}
\end{aligned}
$$

<!-- Obtained from Coaching Actuaries -->
![BAYES_PROBABILITY](Assets/1.%20Statistical%20Learning.md/BAYES_PROBABILITY.png){.center}

### **Confusion Matrix**

One problem with the classification error rate is that it does not consider the distribution of the true and false predictions. Thus, the results can be expressed in the form of a **Confusion Matrix** to gain a better understanding:

$$
\begin{aligned}
    \text{Sensitivity}
    &= \text{True Positive Rate} \\
    &= \frac{\text{Number of true positives}}{\text{Number of actually positives}} \\
    &= 1 - \text{False Negative Rate} \\
    \\
    \text{Specificity}
    &= \text{True Negative Rate} \\
    &= \frac{\text{Number of true negatives}}{\text{Number of actually negative}} \\
    &= 1 - \text{False Positive Rate} \\
\end{aligned}
$$

<!-- Obtained from Glassboxmedicine -->
![CONFUSION_MATRIX](Assets/1.%20Statistical%20Learning.md/CONFUSION_MATRIX.png){.center}

!!! Warning

    The order of the confusion matrix is dependent on the person constructing it - always check to ensure that the correct quantity is used.

### **AUC**

Classification problems are **sensitive to the choice of cutoff**, resulting in vastly different results. For every set of predictions, there exists a **set of cutoffs** that will result in different classifications.

!!! Warning

    Depending on the number of predictions and the predictions themselves, the classifications will only change if the cutoff is changed past a certain threshold.

Thus, plot the sensitivity and specificity for all possible cutoff values, known as the **Receiver Operating Characteristic** (ROC) Curve. The theoretical best possible point is the top left corner, which represents the **highest possible sensitivity AND specificity**. Thus, the optimal cutoff is the one that corresponds to a the **top-leftmost point on the curve**, which often represents the **best possible combination** of the two metrics:

<!-- Obtained from Research Gate -->
![ROC_CURVE](Assets/1.%20Statistical%20Learning.md/ROC_CURVE.png){.center}

!!! Warning

    The x-axis is showing the **COMPLEMENT of the specificity**. Different softwares might instead plot the specificity **starting from 1 and decreasing** - both are equivalent.

!!! Tip

    There is often a trade-off between the two metrics as prioritizing one will result in a deterioration of the other.
    
    For instance, if a model prioritizes Sensitivity, it wants to decrease false negatives, which is done by being **less strict** with a lower cutoff. Consequently, this will **increase the false positive rate**, which lowers specificity.
    
    <insert diagram>

    Contextual information is required to make a decision on which should be prioritized.

The area under the ROC curve is known as the **Area Under Curve** (AUC). The higher the AUC, the more cutoff points there are that provide higher sensitivity and specificity. Thus, AUC can be used as a **basis to compare** across different models.

!!! Warning

    Despite the above, it does NOT provide a method of choosing a specific cutoff value.

!!! Tip

    An AUC of 1 represents a perfect classifier.

### **Class Imbalance**

One common problem is that there is Class Imbalance in the underlying dataset such that one of the classes is **under-represented**. For instance, when studying about gender inequality, there might be significantly fewer observations that are female.

The under-representation makes it **difficult to make predictions** for the minority class, which will likely result in high specificity or sensitivity.

Thus, it is common to employ one of the following:

* **Oversampling**: Randomly generating additional observations of the minority class from the existing pool
* **Undersampling**: Randomly deleting observations of the majority class from the existing pool

<center>

|           Oversample            |         Undersample          |
| :-----------------------------: | :--------------------------: |
| Randomly duplicate observations | Randomly delete observations |
|      Potential Overfitting      |    Potential Underfitting    |

</center>

Both methods are typically employed till the proportion between the two classes are **roughly equal**, which should reduce the specificity or sensitivity issue.

!!! Warning

    Oversampling should ONLY be done on the **training set**.
    
    This is because if it was done on the complete data, when splitting between training and test, there is a chance that the same duplicated observation appears in BOTH the training and test set, which **violates the principle** of having an unseen test set in the first place. 

!!! Note

    Oversampling is much more common as it is usually the case that we are missing data, rather than having an over abundance.
    
    However, it may lead to computational or operational issues with the increased amount of data.
 