# **Linear Regression Asssumptions**

## **Overview**

There are **5 key assumptions** that have been implicitly made thus far for Linear Regression, each allowing us to make key conclusions about the model:

<!-- Obtained from Stack Exchange User DVL -->
![REGRESSION_ASSUMPTIONS](Assets/4.%20Linear%20Regression%20Assumptions.md/REGRESSION_ASSUMPTIONS.png){.center}

!!! Note

    The colours of the arrows indicate which assumptions are cumulatively needed:

    * Red + Blue = Purple
    * Green + Purple = Black

The purpose of this section is to examine:

1. **WHAT** assumptions are made
2. **WHY** they are needed
3. How to **validate** if they fulfilled
4. **Mitigation** if they breached

If the first two blocks of assumptions are fulfilled, then the resulting regression esimate is known as the **Best Linear Unbiased Estimator** (BLUE), which has the **lowest variance** (most efficient) among all other linear unbiased estimates. This is known as the **Guass Markov Theorem**.

!!! Warning

    The last block of assumptions on the distribution of the errors being normal and iid are NOT needed for the estimates to be BLUE. They are however needed for **statistical inference**.

!!! Warning

    Remember that being unbiased means that the **EXPECTED VALUE** of the estimates are equal to the true value. Thus, for a particular sample, the estimated coefficients **might be different**.

## **Assumption 1: Linearity**

This assumes that the **true relationship** between the dependent and independent variables are Linear, but not the dependent or independent variables themselves. This allows for **transformations** of the respective variables:

$$
\begin{aligned}
    Y &= \beta_{0} + \beta_{1} \cdot X_{1} + \beta_{2} \cdot X_{2} \\
    Y &= \beta_{0} + \beta_{1} \cdot X_{1} + \beta_{2} \cdot X^{2}_{1} \\
    \ln Y &= \beta_{0} + \beta_{1} \cdot X_{1} + \beta_{2} \cdot X_{2}
\end{aligned}
$$

If the relationship between the two is not linear, a linear regression model will not be able to accurately capture the relationship between the two, resulting in **Bias estimates**, which consequently impacts the validity of BLUE and the statistical inferences.

Linearity can be verified using either of the following:

* **Scatterplot of the dependent and independent**: Plot should show a linear relationship (intuitive)
* **Scatterplot of the residuals and the fitted model** (Residual Plot): Plot should be centered around 0

!!! Tip

    In reality, Errors are unobservable, thus the **best estimate** of them are the residuals.

    IF the fitted model has **fully captured the relationship** between the dependent and independent, then the resulting residuals should be **random** - with an average of 0.

    Thus, if we observe the residuals to be centered around 0, it implied the fitted linear model has **fully captured** the relationship, implying that the **true relationship is indeed linear**.

<!-- Obtained from Lumen Learning -->
![SCATTER_RESIDUAL_PLOT](Assets/4.%20Linear%20Regression%20Assumptions.md/SCATTER_RESIDUAL_PLOT.png){.center}

If it is found that the relationship is non-linear, **transformations** could be done on the dependent and/or independent variable to make the resulting relationship linear.

!!! Info

    Failing which, another statistical learning method should be used.

## **Assumption 2: Perfect Collinearity**

Collinearity (or Multi-collinearity) refers to when an independent variable can be expressed as a **linear combination of one or more other independent variables**.

Perfect collinearity refers to when it can be **perfectly expressed** as another variable:

* Multiple of another variable: $x_{1} = c \cdot x_{2}$
* Differs by a constant: $x_{1} = x_{2} \pm c$
* Dummy variable trap
* Affine transformations

If there are perfectly collinear variables, then there will be **insufficient information** to solve for the parameter estimates (EG. Two equations to solve for three unknowns). This results in **non-unique solutions**.

!!! Warning

    It is NOT a necessity for the predictors to be independent; zero collinearity. It is generally **assumed that collinearity is low**.

If variables are perfectly collinear, the only solution is to **drop those variables** and keep only one of the bunch to represent them.

### **Imperfect Collinearity**

Imperfect collinearity occurs when one variable is **highly (but NOT perfectly) correlated** with one or more other variables. Intuitively, this means that the variable does not add much predictive power as its effects would have **already been captured via the related predictors**, hence can be removed from the model with little loss.

Imperfect collinearity does NOT prevent OLS from finding a solution and **does NOT affect the overall predictive power** of the model. It ONLY affects the inferences of the model:

* Usual intepretation of "holding other variables constant" no longer holds true as the variables move together; **hard to seperate the effects of individual variables**
* Consequently, OLS has difficulty estimating the estimates as it cannot clearly distinguish them; resulting in **unstable estimates** which can change drastically over different samples
* Consequently, this **increases the variance of the estimates**, which increases p-values, potentially leading to **more false negatives** and hence important variables being omitted from the regression

!!! Warning

    Collinearity ONLY affects t-test results, overall predictive power (measured by F-tests), are unaffected.

!!! Tip

    There is a special type of Variable known as a **Suppressor Variable**, which by itself is not strongly correlated with the dependent, but when combined with another variable, leads to a significantly stronger model.

    Collinearity is typically acceptable in the presence of a suppressor variable.

If imperfect collinearity is present, regularization methods (Ridge/LASSO) can be used to deal with them:

* Coefficient estimates are **unstable** as there are **many different ways to distribute** the coefficients to achieve the same overall impact
* Regularization imposes a **size constraint that biases them towards 0**, forcing the estimates to hover around that range, reducing variance
* EG. Without regularization, there could be a large positive and large negative coefficient offsetting each other. Regularization eliminates such a case from occuring as they are not allowed to vary so much

### **Detecting Collinearity**

The simplest way to detect collinearity is through a **Scatterplot or Correlation Matrix**, which shows the correlations between **pairs of variables**. A **correlation of 0.8 and higher** is typically considered high enough where the collinearity becomes problematic.

<!-- Obtained from ACTEX Manual -->
![CORRELATION_MATRIX](Assets/4.%20Linear%20Regression%20Assumptions.md/CORRELATION_MATRIX.png){.center}

HOWEVER, the above method can only detect collinearity between pairs of variables at a time. To check for collinearity between **3 or more** variables, the **Variance Inflation Factor** (VIF) can be used instead.

Recall that in the presence of collinearity, the variance of estimates increases; the VIF is a **measure of the increase in variance**. 

$$
    \text{VIF}_{p} = \frac{1}{1 - R^{2}_{p}}
$$

!!! Tip

    Different texts have **different thresholds** for high collinearity. For this exam, assume the **threshold to be 10**.
    
    The reciprocal of the VIF is known as the **Tolerance**.

$R^{2}_{p}$ is the R-squared for a model where the p-th predictor is regressed against ALL OTHER predictors. If this R-squared is high, it implies that this predictor is **well explained** by other predictors (high correlation, high collinearity).

!!! Warning

    The VIF needs to be calculated for EACH variable to conclude whether collinearity is present in the model. Collinearity is present **EVEN IF JUST ONE** combination of variables cross the threshold:
    
    * Variable 1 against Variable 2,3
    * Variable 2 against Variable 1,3
    * Variable 3 against Variable 1,2
    
    The VIF for uncorrelated predictors is 1, reflecting that the variance remains **unaffected** by the other variables.

The VIF directly impacts the standard error of the regression coefficients:

$$
\begin{aligned}
    \text{SE}'(\beta_{p})
    &= \sqrt{\text{VIF}} \cdot \text{SE}(\beta_{p}) \\
    &= \sqrt{\text{VIF}} \cdot \sigma^{2}_{\text{RSS}} \cdot \frac{1}{\sum (x - \bar{x})^{2}}
\end{aligned}
$$

!!! Tip

    Thus, given the final standard errors, this can be used to backsolve for the VIF as well.

## **Assumption 3: Exogenity**

**Exogenity** refers to a variable that is **outside** the model and thus must be **independent of the other variables inside the model**. In the context of linear regression, the **errors should be independent of any of the predictors**:

$$
\begin{aligned}
    E(\varepsilon \mid X) &= 0 \\
    \\
    E(\varepsilon)
    &= E[E(\varepsilon \mid X)] \\
    &= E[0] \\
    &= 0 \\
    \\
    E(X \cdot \varepsilon)
    &= E[X \cdot E(\varepsilon \mid X)] \\
    &= E(X \cdot 0) \\
    &= 0 \\
    \\
    \therefore \text{Cov}(\varepsilon, X)
    &= E(\varepsilon, X) - E(X) \cdot E(\varepsilon) \\
    &= 0 - E(X) \cdot 0 \\
    &= 0
\end{aligned}
$$

!!! Warning

    Recall that Zero Covariance is a result of independence, NOT the other way around.

If Exogenity is violated, there is a relationship between the predictors and the error (**reflecting unmodelled variables**). This means that the estimated coefficients would reflect the **effect of BOTH the predictor and the unmodelled variable**, resulting in a **biased estimate**, which consequently impacts the validity of BLUE and the statistical inferences.

<!-- Obtained from Statology -->
![OMITTED_VARIABLE_BIAS](Assets/4.%20Linear%20Regression%20Assumptions.md/OMITTED_VARIABLE_BIAS.png){.center}

!!! Info

    The ommitted correlated variable is said to Confound (mix up) the regression results. Thus, it is sometimes referred to as the **Confounding Variable**. This process in general is known as the **Omitted Variable Bias**.

Similar to before, a **residual plot of the residuals against the predictors** can be used to check for Exogenity; there should be no visible trend in the plots:

<!-- Obtained from Penn State University -->
![SCATTER_RESIDUAL_PREDICTOR](Assets/4.%20Linear%20Regression%20Assumptions.md/SCATTER_RESIDUAL_PREDICTOR.png)

!!! Note

    The above plot are the residuals against a single predictor. Plotting against the fitted values would imply plotting against all predictors at once.

There are two possible solutions if Exogenity is violated:

1. Include the Confounding Variable
2. Introduce an Instrumental Variable

<!-- Obtained From Shakrim Blog -->
![INSTRUMENTAL_VARIABLE](Assets/4.%20Linear%20Regression%20Assumptions.md/INSTRUMENTAL_VARIABLE.png){.center}

!!! Tip

    Instrumental variables are used when it is **not feasible to directly study** the target predictors relationship with the dependent (due to Collinearity, in this case).

    They are variables that are **not direcly correlated with the dependent**, but correlated with the target predictor. Thus, any correlation between the instrumental variable and the dependent MUST be coming through the target predictor.

## **Assumption 4: Homoscedasticity**

**Homoscedasticity** refers to the error terms having **constant variance** while **Heteroscedasticity** refers to having non-constant variance.

$$
    \text{Var}(\varepsilon \mid X) = \sigma^{2}
$$

!!! Info

    They come from the terms "Homo" and "Hetero" which means "Same" and "Different".

If Homoscedastacity is not present, the **sampling distribution of the estimates are not easily derived** and hence cannot be proven that they are the most efficient estimators. Consequently, statistical inference results might not be reliable as well.

Similar to before, a residual plot can be used to identify patterns. If the **spread of the plots are changing** (typically in the shape of a funnel for increasing variance), then Heteroscedasticity is present:

<!-- Obtained from Corporate Finance Institute -->
![HOMOSCEDASTICITY](Assets/4.%20Linear%20Regression%20Assumptions.md/HOMOSCEDASTICITY.png){.center}

!!! Tip

    A model can be both Unbiased and Heteroscedastic, they have no influence on one another:

    <!-- Obtained from Condor -->
    ![HETEROSCEDASTIC_SCENARIOS](Assets/4.%20Linear%20Regression%20Assumptions.md/HETEROSCEDASTIC_SCENARIOS.png){.center}

The most direct method to deal with Heteroscedasticity is to perform a **transformation** on the variables that will stabilize its variance, such the **Logarithm or Squareroot**. These transformations typically **compress larger values while expanding smaller values**, helping to shrink the variance.

!!! Tip

    The transformations listed above **require positive-only** data, thus a **positive constant** can be added to all variables to ensure that they are positive - EG. $\ln(1+Y)$

!!! Warning

    The **direction of heteroscedasticity** matters. Concave transformations like log or squareroot work for datasets with **increasing variance**, which is usually the case. For decreasing variance or more complex patterns, other methods should be used.

The inherent problem with Heteroscedasticity is its effect on Standard Errors; the current formulation assumes a homoscedastic one. Thus, it is possible to use a standard error that can account for it - known as a **Heteroscedastic Consistent Standard Error** or **Robust Standard Error**.

!!! Note

    The mathematics for robust standard errors are beyond the scope of this course. It is sufficient to know that they can be used.

### **Breusch Pagan Test**

Heteroskedasticity implies that the size of the residuals **changes with the predictors**. Thus, this can be formally tested using the **Breusch Pagan Test** where the **residuals are regressed against the predictors**. If there is found to be a relationship between the two, then heteroskedasticity is likely present.

* $H_{0}: \beta^{*}_{p} = 0$
* $H_{1}: \beta^{*}_{p} \ne 0$

$$
\begin{aligned}
    \text{Test Statistic} &= \frac{\text{RSS}^{*}}{2} \\
    \text{Test Statistic} &\sim \chi^{2}_{p}
\end{aligned}
$$

### **Weighted Least Squares**

If the source of heteroscedasticity is known, it is possible to use a **Weighted Least Squares** approach to account for it. Weights are assigned to **each observation**, such that the weight is the amount needed to **scale the the variance to a constant amount**:

$$
\begin{aligned}
    \text{Assumed Constant Variance} &= \sigma^{2} \\
    \text{Current Variance} &= \text{Var}(\varepsilon_{i}) \\
    \text{Var}(\varepsilon_{i}) &= \frac{\sigma^{2}}{w_{i}}
\end{aligned}
$$

!!! Info

    In practice, the weights are completely arbitrary and up to the modeller. They must be positive and do not need to sum to 1.

A new funcitonal form is assumed, where the **resulting variance is constant**:

$$
\begin{aligned}
    y_{i} &= \beta_{0} + \beta_{1} \cdot X_{1} + \varepsilon \\
    \sqrt{w_{i}} \cdot y_{i} &= \sqrt{w_{i}} \cdot \left(\beta_{0} + \beta_{1} \cdot x_{1} + \varepsilon_{i} \right) \\
    \\
    \text{Var}(\sqrt{w_{i}} \cdot \varepsilon_{i})
    &= \left(\sqrt{w_{i}} \right)^{2} \cdot \text{Var}(\varepsilon_{i}) \\
    &= w_{i} \cdot \frac{\sigma^{2}}{w_{i}} \\
    &= \sigma^{2}
\end{aligned}
$$

!!! Tip

    The above doesnt "fix" the heteroscedasticity, it simply *accounts* for it by using an alternative method.

    In another way, heteroscedasticity is **NOT inherently problematic**; the problem is applying a method that assumes Homoscedastacity (linear regression) to a dataset with heteroscedasticity.

## **Assumption 5: No Serial Correlation**

This assumption states that the error terms should **NOT be correlated** with one another:

$$
\begin{aligned}
    \text{Cov}(\varepsilon_{i}, \varepsilon_{j})
    &= E(\varepsilon_{i}, \varepsilon_{j}) - E(\varepsilon_{i}) \cdot E(\varepsilon_{j}) \\
    &= E(\varepsilon_{i}, \varepsilon_{j}) - 0 \\
    \\
    E(\varepsilon_{i}) &= E(\varepsilon_{j}) = 0, \text{(Assumption 3)} \\
    \\
    \therefore \text{Cov}(\varepsilon_{i}, \varepsilon_{j}) &= 0 \rightarrow E(\varepsilon_{i}, \varepsilon_{j}) = 0
\end{aligned}
$$

!!! Info

    This is also sometimes referred to as the **No Autocorrelation** assumption.

Similarly, this can be validated using a residual plot against other residuals **over different predictions**. The resulting plot should not have any exhibit any clear trend or pattern:

<!-- Obtained from Stack Exchange -->
![AUTOCORRELATION](Assets/4.%20Linear%20Regression%20Assumptions.md/AUTOCORRELATION_PLOT.png)

If violated, it implies that knowing one error can **systematically provide information about another**; meaning that there are additional **unmodelled factors** to consider. The residuals are no longer accurate, which consequently impacts the validity of BLUE and the statistical inferences.

## **Assumption 6: Normality**

The last assumption is assuming that the error terms follow a **Normal Distribution**:

$$
    \varepsilon \sim N(0, \sigma^{2})
$$

!!! Tip

    This assumption also implies that the dependent is normally distributed, as a linear function involving a normal variable is also normally distributed.

It can verified by using a **QQ plot** of the **STANDARDIZED residuals**. The residuals are expected to follow a standard normal distribution:

* 95% should fall within two standard deviations (-2 to 2)
* 99.7% should fall within three standard deviations (-3 to 3)

<!-- Obtained from Coaching Actuaries -->
![QQ_PLOT](Assets/4.%20Linear%20Regression%20Assumptions.md/QQ_PLOT.png){.center}

Violating the normality assumption does not impact the validity of the estimates or predictive power; it only impacts the **validity of statistical inference**. For large samples, CLT states that the distribution of the estimators are **approximately normal**, thus inferences are still valid. However, this **does NOT apply to prediction intervals** as they directly contain the variance of the error term, which follows an unknown distribution if normality does not hold.

## **Other Considerations**

### **Influence**

An **Influential Observation** is an observation, that when excluding from the model, leads to a **significant impact** on the model's estimation and results. It is typically a result a consequence of a **combination** of the following effects:

1. Unusual Values in X (known as **High Leverage** observations)
2. Unusual Values in Y (known as **Outliers**)

!!! Warning

    In other texts, outliers refer to unusual values in both X and Y; in the more general sense of the word. For the purposes of this exam, assume that they **only refer to Y values**.

<!-- Obtained from Cambridge -->
![INFLUENCE_EXAMPLE](Assets/4.%20Linear%20Regression%20Assumptions.md/INFLUENCE_EXAMPLE.png){.center}

!!! Warning

    Observations with high leverage or that is an outlier merely have the *potential* to be Influential. They are not guaranteed to be influential; it depends on other component.

!!! Tip

    For a variable with multiple predictors, it might be hard to identify high leverage points graphically of multi-dimensional aspect. The value of the predictor might be typical in each respective dimension, but **unusual when taken collectively**. This is why leverage **must be quantified** to obtain a true understanding.

#### **Leverage**

The **Leverage** for an observation is the element for that observation on the **diagonal of the Hat matrix**:

$$
\begin{aligned}
    h_{i}
    &= \boldsymbol{H}_{ii} \\
    &= \boldsymbol{X}^{T}_{i} (\boldsymbol{X}^{T} \cdot \boldsymbol{X})^{-1} \boldsymbol{X}_{i} \\
    &= \frac{SE(\hat{y})}{\sigma^{2}}
\end{aligned}
$$

<!-- Obtained from Stack Exchange -->
![LEVERAGE_HAT_MATRIX](Assets/4.%20Linear%20Regression%20Assumptions.md/LEVERAGE_HAT_MATRIX.png){.center}

!!! Tip

    The diagonal of the hat matrix has two key interpretations:

    1. It quantifies the **impact that the observed** value of $y$ has on its predicted value $\hat{y}$ (Hence the name "Leverage")
    2. It quantifies the distance between the observed $x$ value from the mean of the $x$ value

    Note that despite the interpretation (1), the impact is **entirely dependent on $x$ values**, as seen in the formula. The more unusual the $x$ value, the higher leverage it has.

!!! Note

    Intepretation (1) translated into mathematical terms is that it is the **partial derivative** of the prediction with respect to the observed value.

    Thus for **SLR ONLY**, it can be shown that the leverage reduces to the following:

    $$
        h_{i} = \frac{1}{n} + \frac{(x_{i} - \bar{x})^{2}}{\sum (x - \bar{x})^{2}}
    $$

Leverage MUST be a **positive value** and the **sum of all leverages** is equivalent to the **number of predictors AND the intercept**. It is generally accepted that a high leverage observation must have be **at least 3 times** the average leverage:

$$
\begin{aligned}
    0 &\lt h_{ii} \lt 1 \\
    \\
    \bar{h}
    &= \frac{\sum h}{n} \\
    &= \frac{p+1}{n} \\
    \\
    \therefore h_{ii} &\gt 3 \cdot \bar{h}
\end{aligned}
$$

#### **Outliers**

The **size of the residual** can be used to determine the presence of Outliers. However, since the size of the residual is affected by its underlying units, we typically **standardize the residuals** before analyzing them. It is generally accepted that observations with a **standardized residual larger than 2** are considered outliers:

$$
\begin{aligned}
    \hat{\varepsilon}_{\text{Sta}}
    &= \frac{\hat{\varepsilon}^{2}}{\sqrt{\sigma^{2}_{\text{RSS}} (1 - h_{i})}} \\
    \\
    \therefore \hat{\varepsilon}_{\text{Sta}} &\gt 2
\end{aligned}
$$

!!! Tip

    It can be shown the distribution of the **residuals** are:
   
    $$
    \begin{aligned}
        \hat{\varepsilon}_{i} &\sim N(0, \sigma^{2} \cdot (1 - h_{i})) \\
        \\
        \text{Var}(\hat{\varepsilon})
        &= \text{Var}(\boldsymbol{y} - \hat{\boldsymbol{y}}) \\
        &= \text{Var}((\boldsymbol{I} - \boldsymbol{H}) \boldsymbol{y}) \\
        &= (\boldsymbol{I} - \boldsymbol{H})^{2} \cdot \text{Var}(\boldsymbol{y}) \\
        &= (\boldsymbol{I} - \boldsymbol{H}) \cdot \text{Var}(\boldsymbol{y}), \text{Idempotent} \\
        &= (1 - h_{i}) \cdot \sigma^{2}
    \end{aligned}
    $$

    Note that the above is for the residuals, NOT the actual errors themselves.

!!! Tip

    One consequence of having many outliers is that the calculated MSE is **inflated** because the outliers artifically increase the RSS. This created a **TENDENCY** for statistical tests to fail to reject the null.

    In reality, if there are outliers and the statistical test are able to reject the null, there is no reason to doubt its validity. This is because even under "adverse" situations, the test can still reject the null, thus under normal circumstances it would continue to reject the null.

    However, if the test currently fails to reject the null, it could be due to the outliers that weakens the test when it would have otherwise passed it.

However, one issue with the above analysis is that the **fitted model has already been influenced** by the observation, thus the residuals **may NOT flag out** as being large. This can be overcome by fitting a **new model WITHOUT the observation** and computing the residual based on it, known as a **Deleted Residual**. Standardizing them results in **Studentized Residuals**.

$$
\begin{aligned}
    d_{i}
    &= y_{i} - \hat{y}_{(i)} \\
    \\
    \hat{\varepsilon}_{\text{Stu}}
    &= \frac{d_{i}}{\text{SE}(d_{i})} \\
    &= \frac{\hat{\varepsilon}_{t}}{\sqrt{\sigma^{2}_{(i)} (1 - h_{i})}} \\
    &= \hat{\varepsilon}_{\text{Sta}} \cdot \left(\frac{n-k-2}{n-k-1 - \hat{\varepsilon}^{2}_{\text{Sta}}} \right)^{0.5}
\end{aligned}
$$

!!! Tip

    $\hat{y}_{(i)}$ is the prediction for a model with i-th observation removed; similar for the MSE.

!!! Note

    As its name suggests, the above residuals follow the **student's t-distribution**.

!!! Warning

    Do NOT be confused - the standardized deleted residual can be expressed as a function of the standardised regular residual.

#### **Cooks Distance**

The effect of Leverage and Residual Size can be combined into a single measure known as **Cook's Distance**, which is a measure of **how the fitted model changes** if the observation were removed:

$$
\begin{aligned}
    D_{i}
    &= \frac{d_{i}^{2}}{\sigma^{2} (p+1)} \\
    &= d_{i}^{2} \cdot \frac{1}{\sigma^{2} (p+1)} \\
    &= \hat{\varepsilon}^{2} \cdot \frac{h_{i}}{(1 - h_{i})^{2}} \cdot \frac{1}{\sigma^{2} (p+1)} \\
    &= \frac{\hat{\varepsilon}^{2}}{\sigma^{2} (p+1)} \cdot \frac{h_{i}}{(1 - h_{i})^{2}} \\
    &= \frac{\hat{\varepsilon}^{2}}{\sigma^{2} (1 - h_{i})} \cdot \frac{h_{i}}{(1 - h_{i})(p+1)} \\
    &= \frac{\hat{\varepsilon}^{2}}{(\sqrt{\sigma^{2} (1 - h_{i})})^{2}} \cdot \frac{h_{i}}{(1 - h_{i})(p+1)} \\
    &= \left(\frac{\hat{\varepsilon}}{\sqrt{\sigma^{2} (1 - h_{i})}} \right)^{2} \cdot \frac{h_{i}}{(1 - h_{i})(p+1)} \\
    &= (\hat{\varepsilon}_{\text{Sta}})^{2} \cdot \frac{h_{i}}{(1 - h_{i})(p+1)}
\end{aligned}
$$

!!! Tip

    It is computationally expensive to compute the Cooks Distance because a **separate model has to be fit for every observation**.
    
    However, since the leverage of an observation already tells us how much an observation impacts the predicted value, there is **no need to run a seperate model** each time; the **original results can be scaled accordingly** to determine its impact.

    Thus, the final result can be reduced to a combination of the **Standardized Residual** (Outlier) and **Leverage**.

    <!-- Obtained from rpubs -->
    ![LEVERAGE_INFLUENCE](Assets/4.%20Linear%20Regression%20Assumptions.md/LEVERAGE_INFLUENCE.png){.center}

!!! Note

    A typical value for Cooks Distance is $\frac{1}{n}$. It can be understood as each observation having equal importance.

There are a few methods to deal with influential observations:

1. **Delete** the observation (Only if it was a result of a data collection error)
2. Include the observation, but **make a note** about it
3. Create a **binary variable** to indicate the presence of the observation (Outliers ONLY)
4. Use a **robust estimation** method (Outliers ONLY)
5. **Substitute OR transform** the variable (Leverage ONLY)

!!! Note

    On point (5) - assume that the current variable the floor space in an apartment. Using floorspace, it is noted that there are several high leverage points. A similar variable that captures the effect of floor space could be the number of rooms. Using number of rooms (discrete) rather than floor space (continuous), **reduces the chances** that an observation has high leverage.

    If the substitute cannot be performed due to a lack of data or other reasons, it is also possible to perform a **transformation** on the data to stabilize it.

### **High Dimensionality**

In recent years, due to the advent of **big data**, the amount of information collected has increased drastically, resulting in many situations where there are **just as many or even more features** than there are observations ($n \lt p$). This is problematic for a **traditional learning methods** such as linear regression which does not account for such possibilities.

* **More Features than Observations** ($p \gt n$): No unique fits to the model
* **Features exactly equal Observations** ($p = n$): Perfect fit (Overfitting)
* **Slighly Less Features than Observations** ($p \approx n$): Limited degrees of freedom; largely fits the data (Overfitting)

!!! Note

    The deteriorative performance of models due to overfitting with higher dimensions is known as the **Curse of Dimensionality**.

    Any other traditional measures also cannot be used as a result (EG. R2, P-values). Similarly, AIC and BIC cannot be used as well due to **difficuly in estimating the MSE**.

!!! Note

    It is also a possibility that increasing the number of predictors results in choosing two or more highly correlated variables. Thus, high dimensionality has a tendency to result in **Multicollinearity**.

The natural solution would be to **limit the number of predictors** by choosing only truly relevant variables and/or to **increase the sample size**. This can be achieved via:

* Model Selection
* Dimensionality Reduction methods - PCA, PLS, LASSO

#### **Partial Least Squares**

Another method is to use **Partial Least Squares** (PLS). It seeks to create **new variables** that are **linear combinations** of existing variables (known as **Directions**) which are then used to perform the regression instead.

* **First** PLS direction (PL1) is a **direct combination** of the original variables with the goal of explaining the most variation in the dependent variable
* **Second** PLS direction onwards (PL2) is obtained by **regressing each predictor on the previous PLS direction** and using the residuals (left over information; **orthogonalized data**) to form the combination, similar to how the first direction was formed

!!! Note

    PLS can thus be thought of as a supervised version of PCA/PCR - with the **express goal of explaining the dependent**. The formation of the second direction onwards is similar to boosting process in decision trees.

    That said, in practice, it performs **no better** than regularization or PCR.

!!! Tip

    Similar to PCR, ALL features are used to create the directions; there is **NO variable selection** (dropping of variables) involved, only involvement to different extents.
