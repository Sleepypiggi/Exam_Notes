# **Clustering**

## **Overview**

Clustering is an **Unsupervised** statistical learning method that is used to find **Clusters** within the dataset. They are a **subgroup of homogenous observations** that have **similar characteristics** to one another; it is important that different clusters are distinct from one another.

!!! Note

    An observation can only be part of ONE cluster.

!!! Note

    Generally speaking there are two types of clustering - **Observation vs Feature** Clustering:

    * **Observation Clustering**: Grouping Customers
    * **Feature Clustering**: Grouping Customer attributes (EG. Age, Income)

The goal of clustering is to separate observations into **distinct groups**. Generally speaking, this is more easily achieved by choosing variables with **larger ranges**, as the observations are likely more spread out. For similar reasons, **more clusters** will generally lead to each cluster being more distinct as well.

!!! Note

    Some data might already have **pre-defined groupings**. These are typically have some **significance** that otherwise could not be captured by the available data, which could be **easier to interpret**.

    However, the converse is true as well - clustering might result in different groupings based on patterns in the data that are not captured by the pre-defined groupings, generally known as "Latent" groupings. The same can be said for PCA as well.


!!! Warning

    When there are many dimensions, the distance between points because more or less equal on average, resulting in clustering as a whole having little effect.

## **Euclidean Distance**

Similarity between observations is quantified via **Euclidean Distance**. It is essentially the **shortest possible distance** between two observations in a multi-dimensional space.

$$
    D = \sqrt{(x_{1} - x_{2})^{2} + (y_{1} - y_{2})^{2} + (z_{1} - z_{2})^{2}}
$$

!!! Warning

    The order of the subtraction (1-2 or 2-1) is **irrelevant** as the difference is squared.

    For higher dimensions, simply **add on another squared difference** for each additional dimension. Squareroot is used REGARDLESS of the number of dimensions.

!!! Tip

    By default, the calculator will display the exact form; (EG. $\sqrt{2}$ instead of $1.4 \dots$). This can make it hard to compare quantities, thus change the 'Mode' to 'Classic' to force all calculated values to be displayed in decimal form.
    
    Alternatively, since square root is monotonic, the squared EDs can be used for relative comparison.

Since the distance is squared, outliers tend to have a large effect on the results.

### **Cluster Variation**

The variation within a cluster is the sum of all **squared euclidean distances** for ALL PAIRS of observations within that cluster, **divided by the number of observations**:

$$
\begin{aligned}
    \text{Cluster Variation} &= \frac{\sum \text{Pairwise Distance}}{n_{\text{Cluster}}} \\
    \text{Total Cluster Variation} &= \sum \text{Cluster Variation}
\end{aligned}
$$

This is best visualized through a table:

* Each cell contains the squared euclidean distance between the two observations
* The sum of all cells is the total squared euclidean distances for the cluster

<!-- Self Made -->
![VARIATION_TABLE](Assets/9.%20Clustering.md/VARIATION_TABLE.png){.center}

!!! Warning

    The **diagonal is always 0** because the euclidean distance between a point and itself is 0.

    The table considers **ALL pairs of observations**, not just unique pairs. In other words, 1-2 and 2-1 are both considered, even though they result in the same euclidean distance. This is why the values are **symmetrical about the diagonal**.

    Cluster Variation requires ALL values in the table to be summed. However, since the table is symmetrical, it is equivalent to TWO TIMES the sum of the **triangle of values**:

    $$
        \text{Pairwise Distance} = 2 \cdot \text{Unique Pairwise Distance}
    $$

### **Centroid**

Another related concept is the **Centroid of the cluster**. As its name suggests, it is the **centre of the cluster** in multi-dimensional space, equivalent to the average of each individual dimension.

<!-- Self Made -->
![CLUSTER_CENTROID](Assets/9.%20Clustering.md/CLUSTER_CENTROID.png){.center}

Similar to Variance, the centroid can be used to determine the cluster variation as well:

$$
    \text{Cluster Variation} = 2 \cdot \text{Sum of Squared ED from Centroid}
$$

!!! Tip

    The two reflects the idea from before that the variation should consider all pairs of observations (symmetry). 
    
    Unlike the previous formula, there is NO need to divide by the number of observations; that has already been taken care of when calculating the centroid.

## **K-Means Clustering**

The first type of clustering is known as **K-means clustering**. It works by clustering the data into a **pre-specified number of clusters** ($k$), such that the **total within cluster variation is minimized**:

1. **RANDOMLY assign each observation** to one of the $k$ clusters
2. Calculate the **Centroid** of the clusters
3. Re-assign observations to the **nearest cluster**, based on their Euclidean Distance from the Centroids in step (2)
4. Repeat steps (2) and step (3) until there are **no more changes** in clusters

!!! Warning

    K-means clustering is a **greedy algorithm** as it finds the **local best fit** rather than the global best. Since the **initial cluster assignment is random**, the algorithm will produce **different clusters each time** it is run, thus may need to be **ran multiple times**.

!!! Note

    Until the final cluster assignments, each step is **guaranteed to reduce** the squared euclidean distances.
    
    Additionally, ALL observations are assigned, regardless if they are an outlier or not.

!!! Tip

    The **number of clusters remain the same** in each iteration.
    
    There is technically a small chance that the number of clusters decrease, because no observations are assigned to that cluster. This should be treated as a special case.

The choice of $k$ is **arbitrary**, thus it might not yield the best results. It is thus recommended to **start with a large $k$ (~20)** then **repeat the algorithm** several times for different $k$ based on the results to have a better understanding of the data.

### **Elbow Plots**

There are systematic ways to determine the optimal number of clusters for a given starting point. Similar to PCA, k-means clustering can use an **Elbow Plot** that tracks the **cumulative proportion of variance explained by all clusters**. Similarly, we select the clusters at the "Elbow" of the plot, where there is a sharp drop off between additional variance explained and the number of clusters.

<!-- Obtained from SOA Past Year Exams -->
![ELBOW_PLOT](Assets/9.%20Clustering.md/ELBOW_PLOT.png){.center}

A drop off in the gradient of the elbow plot indicates that adding the additional cluster may result in different clusters containing observations which have some similarities.

!!! Warning

    This is NOT the same as a scree plot - this version is cumulative and hence moves upwards while scree plots focus on the added variance and hence are downward sloping (given the nature of PCA).

## **Hierarchical Clustering**

The second type is known as **Hierarchical Clustering**. It works by **iteratively merging or splitting** observations based on their **similarity**, till all observations are either in one giant cluster or in individual clusters.

<!-- Obtained from Medium -->
![DENDROGRAM](Assets/9.%20Clustering.md/DENDROGRAM.png){.center}

Due to the iterative nature of the method, a *hierarchy* of clusters are formed, where clusters are **nested subsets of one another**. The clustering can be **"sliced"** to determine the final number of clusters. It is thus best visualized in a tree-like structure known as a **Dendrogram**:

<!-- Obtained from Medium -->
![SLICED_DENDROGRAM](Assets/9.%20Clustering.md/SLICED_DENDROGRAM.png){.center}

!!! Tip

    The Dendrogram can be **sliced at any possible level**, resulting in a different number of clusters:

    <!-- Obtained from Medium -->
    ![SLICED_DENDROGRAM_ALT](Assets/9.%20Clustering.md/SLICED_DENDROGRAM_ALT.png){.center}

    Note that not all dendrogram extend the branches **ALL the way down**, which may lead to incorrectly counting the number of clusters.

!!! Note

    The act of slicing the dendrogram is akin to choosing $k$ in $k$ means clustering. However, unlike K-means, Hierarchical Clustering **only needs to be run ONCE** (for a given measure and linkage) to obtain all possible clusters.
    
    Generally speaking, it is recommended to slice the dendrogram such that there only a **small number of clusters** (ease of interpretation) with a roughly **even number of observations** in each (balanced).
    
    Similar to the “elbow” principle, it is recommended NOT to slice at a point that has a dissimilarity significantly higher than the next threshold - because it means that slicing lower might produce better results.
    
    Ultimately, **Domain knowledge** is important to ensure that the number of clusters produced are reasonable.

Generally speaking, there are two approaches to hierarchical clustering:

<center>

|           **Bottom-Up**            |              **Top-Down**               |
| :--------------------------------: | :-------------------------------------: |
|   Each starts in its own cluster   |      All start in one big cluster       |
| Iteratively merge until all in one | Iteratively split until each in its own |
|           Agglomerative            |                Divisive                 |
|             SRM focus              |              Out of Scope               |

</center>

1. Assign each observation to its own cluster
2. Compute the distance between each pair of clusters
3. Merge the two clusters with the **LEAST dissimilarity** (most similar)
4. Repeat step (2) and (3) until there is one cluster left

If there two or more pairs of clusters have the same dissimilarity, then the pair of clusters which contain the **lowest indexed observation** will be merged.

!!! Tip

    At each step, if there $n$ clusters before merging, then $n \choose 2$ **unique pairs** of clusters are considered. The number naturally decreases per iteration of the algorithm:

    <!-- Self Made -->
    ![PAIRWISE_CLUSTERS](Assets/9.%20Clustering.md/PAIRWISE_CLUSTERS.png){.center}

    Another fact is that if there are a total of $n$ clusters, the algorithm will result in $n-1$ merges.

### **Similarity Measure**

The basis for merging a cluster is based on how similar the two clusters are, known as **Dissimilarity Measure**. For the purposes of this exam, assume that **Euclidean Distance** is used unless explicitly stated otherwise.

The key difference is that dissimilarity is measuring the Euclidean Distance **between two clusters** rather than two observations. There are four main ways to perform this, known as the choice of **Linkage**:

* **Complete**: Compute all pairwise distances, use the **highest**
* **Single**: Compute all pairwise distances, use the **lowest**
* **Average**: Compute all pairwise distances, use the **average**
* **Centroid**: Use the distance **between centroids**

!!! Tip

    When merging clusters with only ONE observation each (starting clusters) - the choice of linkage does NOT matter as there is only one Euclidean distance to use.
    
    The linkage only matters when deciding between which Euclidean Distance to use for merging (A-B, A-C).

The choice of linkage does affect how the **shape** of the resulting Dendrogram. In general, Complete and Average linkage is preferred:

* **Single**: Tends to merge single observation clusters first, resulting in **extended and trailing** diagrams
* **Centroid**: May result in **Inversions** where the clusters are merged at a lower level; **unintuitive**
* **Complete & Average**: Tend to produce more **balanced trees**; complete tends to fuse at **higher levels**

<!-- Obtained from Researchgate -->
![SINGLE_VS_COMPLETE](Assets/9.%20Clustering.md/SINGLE_VS_COMPLETE.png){.center}

!!! Note

    The two linkages can be explained via the following:

    * **Single Linkage**: Nearest neighbour
    * **Complete Linkage**: Furthest neighbour
    
    <!-- Obtained from Statistics How To -->
    ![LINKAGE_REASON](Assets/9.%20Clustering.md/LINKAGE_REASON.png){.center}

    Thus, single linkage favours merging a large cluster (close observations) with one observation (nearest observation) at a time, resulting in an elongated dendrogram; while complete linkage prefers merging two individual observations at a time.

### **Dendrogram Analysis**

The vertical axis on the Dendrogram is the **dissimilarity measure**. Thus, we are able to tell the **exact dissimilarity** when two clusters merge. However, they do not provide direct insight into the relationship between sub-clusters; that **must be inferred**.

Consider the following simple Dendrogram:

<!-- Self Made -->
![DENDROGRAM_ANALYSIS](Assets/9.%20Clustering.md/DENDROGRAM_ANALYSIS.png){.center}

Firstly, the **order of fusion** matters - Clusters that **merge earlier are more similar** than clusters that merge later - $(B,C)$ is more similar than $(A,B)$ or $(A,C)$!

!!! Tip

    Phrased another way, clusters that merge lower on the dendrogram are more similar to each other than those that merge on top.

Next, the **choice of linkage matters**, as it allows us to draw simple relationships about the distance:

* **Complete**: $\max [(A,B), (A,C)] = 10$; pairs cant be larger than 10
* **Single**: $\min [(A,B), (A,C)] = 10$; pairs cant be smaller than 10

Questions of this nature will generally be simple in nature.

!!! Warning

    The horizontal axis does NOT provide any insight into the relationship between observations.

    The reason is because at each of the $n-1$ points where fusions occur, the two fused branches **could be swapped without affecting the meaning** of the dendrogram, leading to about $2^{n-1}$ **possible reorderings**.

!!! Warning

    If asked to compare between dendrograms, note that most of the time they are NOT drawn on the same scale. Always refer to the corresponding axis to determine the correct dissimilarity.

Thus, the vertical distance between two clusters when they merge is the **extent of the dissimilarity**. Large distances indicates that the two clusters are not very similar at all - merging them would result in a loss of information.

!!! Tip

    This information could be used to decide where to slice a dendrogram - if there is a point where the distance between merging clusters becomes large, then it should be sliced before that to ensure that they remain as distinct clusters.

### **Correlation Distance**

Another way to perform hierarchical clustering would be to use **Peason Correlation Distance** instead. Loosely speaking, it measures similarity in the “shape” of the observation rather than its magnitude (Euclidean).

For instance, consider the following example:

* Observation C is always 10 times larger than Observation A
* The same is not true for A-B or B-C; the relationship varies across variables
* Thus, Observation C and A will be clustered together

<!-- Obtained from SOA Past Year Paper -->
![CORRELATION_CLUSTERING](Assets/9.%20Clustering.md/CORRELATION_CLUSTERING.png){.center}

However, one caveat is that it requires the dataset to have **at least three features** for it to be used.

## **Feature Creation**

It is possible to use the resulting clusters **as a feature** in a supervised learning method. However, it might lead to similar results an existing method as the clusters might be accounting for a **similar of set of features** as the existing model:

* **Cluster Groups** as a categorical variable
* **Cluster Means** as a numerical variable

!!! Warning

    This method should only be used when there are a **sizeable number** of observations in each cluster; otherwise the model could result in overfitting.

!!! Note

    This is in contrast to PCA where only a numeric variable is created.

## **K-Nearest Neighbours**

**K-Nearest Neighbours** (KNN) is non-parametric method that can be used in a regression or classification setting:

1. Identify the observation in vector space
2. Identify the $k$ **nearest observations** (based on Euclidean Distance)
3. Take the **average** of the observations in (2) as the prediction (regression) or the **most frequent** category among them (classification)

<!-- Obtained from Research Gate -->
![KNN](Assets/9.%20Clustering.md/KNN.png){.center}

!!! Warning

    Do NOT confuse K-means clustering and K-Nearest Neighbours! Both contain $k$ but have very **different functions**. This section is placed here only because no other section suits it.

!!! Tip

    Given a table of data, it might be helpful to draw the **scatterplot** (to some reasonable scale) to get a sense of the distance. If the data has more than three dimensions, manually calculating the euclidean distance is the only method.

The choice of $k$ is critical as it is a measure of **flexibility**:

* $k=1$: **Only that observation** used as its own prediction; **perfectly fits the data** (High Variance, Low Bias, **Overfit**)
* $k=n$: **All observations** used for each prediction; **constant prediction** (Low Variance, High bias, **Underfit**)  

<!-- Obtained from ISLR -->
![KNN_MSE](Assets/9.%20Clustering.md/KNN_MSE.png){.center}

!!! Warning

    The test error follows the usual U-shape pattern, given that the boundary values for $k$ both have high MSE. However, the training error is the other way around.

    Typically, the training error is proportional with the flexibility measure. However, $k=1$ has the lowest training error in this case - it is **inversely proportional**. Another way of thinking about it is that the flexibility measure is $\frac{1}{k}$ instead.
    
The larger the value of $k$, the more neighbours it considers - the **smoother the decision boundary** becomes as it is less sensitive to nearby observations, making a more **generalized prediction**:

<!-- Obtained from ISLR -->
![KNN_DECISION_BOUNDARY](Assets/9.%20Clustering.md/KNN_DECISION_BOUNDARY.png){.center}

!!! Tip

    Despite being a rather simplisitc method, KNN (as a classifier) performs quite well; similar to the Bayes Classifier, given that the right $K$ is chosen.
    
    It does NOT perform well when there are high a number of dimensions.
