# **Multiple Linear Regression**

## **Overview**

The natural extension of the SLR model is to include **more than one independent variable**, which thus results in a **Multiple Linear Regression** (MLR) model.

$$
\begin{aligned}
    f &= \beta_{0} + \beta_{1} \cdot X_{1} + \beta_{2} \cdot X_{2} + ... + \beta_{p} \cdot X_{p} \\
    \\
    \therefore E(Y \mid X_{1} = x_{i, 1}, ... X_{i, p} = x_{i,p})
    &= \beta_{0} + \beta_{1} \cdot x_{i, 1} + \beta_{2} \cdot x_{i, 2} + \dots
    \\ 
    \therefore y_{i}
    &= \beta_{0} + \beta_{1} \cdot x_{i, 1} + \beta_{2} \cdot x_{i, 2} + \dots + \varepsilon_{i}
\end{aligned}
$$

In fact, SLR is considered a **special case of MLR** where the number of predictors is exactly one ($p=1$). Thus. many of the concepts translate into a MLR setting; this section mainly focuses on the **differences or additional considerations** needed in a MLR setting.

!!! Note

    When linear regression or OLS are used to refer to something, they are usually referring more generally to MLR.

## **Interpretation**

MLR models study how the independent variables **operate together** to influence the dependent variable, thus the regression coefficients have a slightly different interpretation:

* **Intercept Parameter** ($\beta_{0}$): **Expected value** of $Y$ when $X_{1} = X_{2} = \dots = 0$
* **Slope Parameter** ($\beta_{p}$): **Change** in **Expected Value** of $Y$ given a **one unit increase** in $X_{p}$, assuming *all others are constant*

To better understand the relationship between individual predictors, the **Partial Correlation Coefficient** can be used. It is the correlation between the predictor and response, **holding all other predictors constant**.

$$
    \text{PCF} = \frac{t}{\sqrt{t^{2} + (n-p-1)}}
$$

An **Added Variable Plot** plots the **residuals** from the following two regression runs against each other:

* **A**: Independent variable against all OTHER variables
* **B**: Independent variable against ONLY the variable of interest

The correlation between the residuals in the above plot **IS the partial correlation coefficient**. This is because the residuals from (A) **already accounts for the effects of the other variables** - it is a measure of how much of the remaining difference is explained by the new variable; measure of its predictive power.

!!! Warning

    Variables should NOT be added or dropped solely based on predictive power. There are **many other considerations** to consider - EG. theoretical or interaction effects.

## **Model Fitting**

Similar to the SLR model, the regression parameters can be found by minimizing the sum of squared residuals:

$$
\begin{aligned}
    \text{RSS} 
    &= \sum (y_{i} - \hat{y}_{i})^{2} \\
    &= \sum [y_{i} - (\hat{\beta}_{0} + \hat{\beta}_{1} \cdot x_{i,1}
            + ... + \hat{\beta}_{p} \cdot x_{i, p})]^{2}   
\end{aligned}
$$

Given that there are $p$ independent variables, there are thus $p+1$ minimization problems to solve ($p$ slopes and 1 intercept). It is necessary to use linear algebra to solve the **system of equations**, resulting in the following **matrix solution**:

$$
    \hat{\boldsymbol{\beta}}
    = \begin{pmatrix} \hat{\beta_0} \\ \hat{\beta_1} \\ \vdots \\ \end{pmatrix}
    = \left(\boldsymbol{X}^{T} \boldsymbol{X} \right)^{-1} \boldsymbol{X}^{T} \boldsymbol{y}
$$

!!! Note

    The operation $X^{T}$ means to **Transpose** the matrix - **swapping the Rows and Columns** of the Matrix. It has **several useful properties** which is why it is beneficial to leave in this form:

    <!-- Obtained from Cuemath -->
    ![TRANSPOSE_MATRIX](Assets/3.%20Multiple%20Linear%20Regression.md/TRANSPOSE_MATRIX.png){.center}

The resulting fitted model is thus a **regression plane of best fit**, reflecting the multi-dimensional nature of the MLR:

$$
    \hat{y}
    = \hat{\beta}_{0} + \hat{\beta}_{1} \cdot x_{i ,1} + ... + \hat{\beta}_{p} \cdot x_{i,p}
$$

<!-- Obtained from ISLR -->
![REGRESSION_PLANE](Assets/3.%20Multiple%20Linear%20Regression.md/REGRESSION_PLANE.png){.center} 

It is common to express the entire fitted equation in **Matrix Notation**, especially when there are a large number of predictors:

$$
\begin{aligned}
    \hat{\boldsymbol{y}}
    &= \boldsymbol{X} \left(\boldsymbol{X}^{T} \boldsymbol{X} \right)^{-1} \boldsymbol{X}^{T} \boldsymbol{y} \\
    &= \boldsymbol{X} \hat{\boldsymbol{\beta}} \\
    &= \boldsymbol{H} \boldsymbol{y}
\end{aligned}
$$

* $\boldsymbol{X}$: **Design Matrix**; containing all predictor values
* $\boldsymbol{H}$: **Hat Matrix**; as it puts the a "hat" on $y$

<!-- Self Made -->
![FITTED_MATRIX](Assets/3.%20Multiple%20Linear%20Regression.md/FITTED_MATRIX.png){.center}

!!! Tip

    Notice that the first column of the design matrix is **all 1's for the intercept parameter**:

    $$
    \begin{aligned}
        \boldsymbol{X} &=
        \begin{pmatrix}
            1 & x_{11} & x_{12} & ... & x_{1p} \\
            1 & x_{21} & x_{22} & ... & x_{2p} \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            1 & x_{n1} & x_{n2} & ... & x_{np}
        \end{pmatrix}
    \end{aligned}
    $$

    If **only a specific row** of the design matrix is wanted (for a specific prediction), then we need to use the following notation:

    $$
    \begin{aligned}
        \boldsymbol{X}^{T}_{i}
        &=
        \begin{pmatrix}
            1 & X_{i,1} & X_{i,2} & \dots & X_{i,p}
        \end{pmatrix} \\
        \\
        \therefore \hat{y}_{i}
        &= \boldsymbol{X}^{T}_{i}
            \left(\boldsymbol{X}^{T} \boldsymbol{X} \right)^{-1} \boldsymbol{X}^{T} \boldsymbol{y}
    \end{aligned}
    $$

    The reason why the transpose is needed is because vectors are typically denoted as **Column vectors by default**; thus the transpose is needed to convert it into a row vector.

!!! Note

    Recall the key properties of matrices:

    * They are defined using a **"(Row x Column)"** notation
    * They are differentiated from scalar quantities with a **Bolded** symbol
    * **Addition and subtraction** of matrices simply add or subtract the corresponding elements
    * **Multiplication** of matrices involves an **inverted 7** (See image below)

    The **order of multiplication** is important as the resulting matrix will retain the Rows of the LHS and columns of the RHS (which is intuitive from the inverted 7):

    $$
        (m * n) \cdot (n * k) = (m * k)
    $$

    <!-- Obtained from Resourceaholic -->
    ![MATRIX_MULTIPLICATION](Assets/3.%20Multiple%20Linear%20Regression.md/MATRIX_MULTIPLICATION.png){.center}

    For the purposes of this exam, it is not necessary to know how to evaluate or manipulate matrices. It is however, necessary to recognize and intepret them.


## **ANOVA**

### **Goodness of Fit**

Similar to SLR, ANOVA can be used to decompose the variance into RegSS & RSS, and R-squared can be computed with the **same interpretation**:

$$
    R^{2} = \frac{\text{RegSS}}{\text{TSS}}
$$

!!! Warning

    Unlike SLR, the following expression does NOT hold true as there are multiple independent variables:

    $$
        R^{2} \ne r^{2}_{x_{p}, y}
    $$
    

Recall that for parametric learning methods, increasing the number of parameters (all else equal) tends to **increase the flexibility** of the model; ability to fit the data (RegSS). Thus, in a MLR setting, R-squared can be **artifically inflated** by adding more parameters, even when they are nonsensical!

!!! Note

    To elaborate, OLS is likely to assign **non-zero coefficients** to the irrelevant predictors because the training set may have exhibited some **chance-relationship** between the two. This relationship likely WONT exist in other datasets, thus estimating coefficients for these variables will definitely result in **overfitting**.

Thus, to account for this, the **Adjusted R-squared** is considered by **dividing each term by their degrees of freedom**:

$$
\begin{aligned}
    R^{2}_{\text{Adj}}
    &= 1 - \frac{\frac{\text{RSS}}{n-(p+1)}}{\frac{\text{TSS}}{n-1}} \\
    &= 1 - \frac{\text{RSS}}{\text{TSS}} \cdot \frac{n-1}{n-(p+1)} \\
    &= 1 - (1 - R^{2}) \cdot \frac{n-1}{n-(p+1)} \\
    \\
    R^{2}_{\text{Adj}}
    &= 1 - \frac{\frac{\text{RSS}}{n-(p+1)}}{\frac{\text{TSS}}{n-1}} \\
    &= 1 - \frac{\sigma^{2}_{\text{RSS}}}{s^{2}_{y}}
\end{aligned}
$$

!!! Tip

    It is NOT recommended to memorize the above formula as it can be derived from first principles.

!!! Warning

    The adjusted R-squared has **NO INTERPRETATION**. It is more of a **measure to compare** the goodness of fit between different MLR models.

!!! Tip

    Note that it is **always smaller than or equal** to the regular R-squared:

    $$
        R^{2}_{\text{Adj}} \le R^{2}
    $$

    Adjusted R-squared is R-squared **with a penalty applied** for the number of predictors. For a model with **no predictors** (null model), both are the same.

### **F-Test**

Similar to SLR, we can conduct an F-test using the same components as before, adjusted for the **new degrees of freedom**:

<center>

|   **Source**   | **Sum of Squares** |  **df**   |              **Variance/Mean Squared**               |
| :------------: | :----------------: | :-------: | :--------------------------------------------------: |
| **Regression** |       RegSS        |    $p$    | $\sigma^{2}_{\text{RegSS}} = \frac{\text{RegSS}}{p}$ |
| **Residuals**  |        RSS         | $n-(p+1)$ | $\sigma^{2}_\text{RSS} = \frac{\text{RSS}}{n-(p+1)}$ |
|   **Total**    |        TSS         |   $n-1$   |   $\sigma^{2}_\text{TSS} = \frac{\text{TSS}}{n-1}$   |

</center>

!!! Note

    The key is to remember the following regarding the degrees of freedom:

    * **RegSS**: Number of **predictors** in the model; excluding intercept
    * **RSS**: Number of observations less number of **coefficients**; including intercept

    The two are **NOT complementary** as one considers the intercept while the other does not. This because intercept (sample mean) has been accounted for in the TSS.

However, rather than testing if an individual independent variable is useful, it tests if the **ENTIRE model** (ALL the independent variables collectively) is useful in explaining the dependent variable:

* $H_{0}: \beta_{1} = \beta_{2} = ... = \beta_{p} = 0$
* $H_{1}$: At least one $\beta_{p} \ne 0$

$$
    \text{F}_{\text{Statistic}}
    = \frac{\sigma^{2}_{\text{RegSS}}}{\sigma^{2}_\text{RSS}}
$$

Thus, rejecting the null allows us to conclude that **at least one** of the independent variables are useful, but does not provide insight on *which* of them are useful.

!!! Tip

    Similar to the adjusted R-squared, the F-test measures whether the reduction in RSS is "worth" the loss of the degrees of freedom to estimate the new parameters.

    An easy way to remember this is that the F-tests uses the **mean squared**, which already accounts for the degrees of freedom.

!!! Tip

    The F-statistic can also be re-expressed as a function of $R^{2}$:

    $$
    \begin{aligned}
       \text{F}_{\text{Statistic}}
       &= \frac{\frac{\text{RegSS}}{p}}{\frac{\text{RSS}}{n-p-1}} \\
       &= \frac{\text{RegSS}}{\text{RSS}} \cdot \frac{n-p-1}{p} \\
       &= \frac{\frac{\text{RegSS}}{\text{TSS}}}{\frac{\text{RSS}}{\text{TSS}}} \cdot \frac{n-p-1}{p} \\
       &= \frac{R^{2}}{1 - R^{2}} \cdot \frac{n-p-1}{p} 
    \end{aligned}
    $$

    It is NOT recommended to memorize the above formula as it can be derived from first principles.

### **Partial F-test**

In order to determine which variables are useful, a **Partial F-test** can be used, which compares two models:

* **Full Model**: Model with all $p$ independent variables
* **Reduced Model**: Model with lesser than $q$ variables (where $q \lt p$); **subset** of full model

The partial F-test determines whether the additional variables are **jointly useful** in explaining the dependent. The intuition is that if the variables are useful, then there should be a **decrease in RSS** when moving from the reduced to full, known as the **Extra Sum of Squares** (ExtraSS):

$$
    \text{ExtraSS} = \text{RSS}_{\text{Reduced}} - \text{RSS}_{\text{Full}} 
$$

!!! Note

    The most intuitive way to quantify the degrees of freedom for the ExtraSS is to take the difference in degrees of freedom of the two models:

    $$
        \text{df}_{\text{ExtraSS}} = \text{df}_{\text{Full}} - \text{df}_{\text{Reduced}}
    $$

!!! Warning

    The RSS and the degrees of freedom are using different orders for the subtraction, which can be confusing:

    * **RSS**: **Inversely proportional** to the number of predictors; Reduced - Full
    * **DF**: **Proportional** to the number of predictors; Full - Reduced

Formally, the partial F-test can be expressed as the following:

* $H_{0}: \beta_{q+1} = ... = \beta_{p} = 0$
* $H_{1}$: At least one $\beta_{q+n} \ne 0$

$$
    \text{F}_{\text{Partial}}
    = \frac{\sigma^{2}_{\text{ExtraSS}}}{\sigma^{2}_{\text{RSS}_\text{Full}}}
$$

!!! Tip

    Notice that the regular F-test is a **special case** of the partial F-test where the reduced model is the null model.

## **Statistical Inference**

### **Sampling Distributions**

Similar to SLR, the regression parameters are normally distributed as well. However, since there are multiple regression parameters, they collectively follow a **multivariate normal distribution**:

$$
    \hat{\beta} \sim N_{p+1} \left(\beta, \sigma^{2} (\boldsymbol{X^{T} X})^{-1} \right)
$$

The matrix in the variance term is known as the **Variance Covariance Matrix**, which provides the covariance between every possible pair of regression parameters, INCLUDING the intercept:

$$
Var(\hat{\beta}) =
\begin{pmatrix}
    Var(\hat{\beta}_0) & Cov(\hat{\beta}_0, \hat{\beta}_1) & ... & Cov(\hat{\beta}_0, \hat{\beta}_p) \\
    Cov(\hat{\beta}_1, \hat{\beta}_0) & Var(\hat{\beta}_1) & ... & Cov(\hat{\beta}_1, \hat{\beta}_p) \\
    \vdots & \vdots & \ddots & \vdots \\
    Cov(\hat{\beta}_p, \hat{\beta}_0) & Cov(\hat{\beta}_1, \hat{\beta}_p) & ... & Var(\hat{\beta}_p)
\end{pmatrix}
$$

!!! Tip

    The Covariance of the same independent variable is its Variance. Thus, the **diagonals of the matrix** represent the variances of the estimators. The matrix is also **symmetrical about the diagonal**, since $\text{Cov}(X,Y) = \text{Cov}(Y,X)$.

!!! Warning

    The matrix **always starts with the intercept** parameter, thus do NOT mistakenly take the wrong value.

### **Hypothesis Testing**

Similar to SLR, a t-test can be used to test the usefulness of a single predictor. However, the interpretation is slightly different - it tests the usefulness of the variable while **holding the other variables constant**.

* $H_{0}: \beta_{p} = 0$
* $H_{1}: \beta_{p} \ne 0$

$$
    t = \frac{\hat{\beta_{p}} - \beta_{j}}{\text{SE}({\hat{\beta_{p}}})}
$$

!!! Note

    However, this could lead to several odd situations:

    * Individual variables are significant, but **NOT significant** when considered in totality (Pass t-test, fail F-test)
    * Individual variables are **NOT significant**, but significant when considered in totality (Fail t-test, pass F-test)

    The likely reason for such situations will be covered in a later section.

There are now two possible ways to test for statistical significance:

* **Single F-test** for the entire model
* **Multiple t-test** for each variable

Conducting multiple t-tests simultaneously could lead to the **Multiple Comparisons Problem**. Recall that hypothesis tests assume an $(1-\alpha)%$ chance of correctly rejecting the null. If each test is conducted on independent samples, then the **probability of correctly rejecting the null for ALL tests** drops to $(1-\alpha)^{p}$. The resulting type I error rate is much higher than the intended $\alpha$:

$$
    \text{Type I Error Rate across all tests} = 1 - (1-\alpha)^{p} \gt \alpha
$$

!!! Note

    There are methods to manage this problem through the use of several corrections such as the **Bonferroni Correction** which adjusts the $\alpha$ for each test such that on whole, the desired type I error rate is achieved. However, it has the consequence of **increasing the probability of type II errors**. The details are beyond the scope of the exam.

This is why the **F-test is generally preferred** for MLR, as it has the advantage of naturally controlling the error rate regardless of the number of predictors along with considering the joint effect of variables.

!!! Warning

    The relationship between t and F tests only holds true in a SINGLE variable context. For multiple variables, the two can result in DIFFERENT conclusions!

    * T-tests: Including a single variable is better than excluding it
    * F-tests: Including multiple variables is better than excluding them

    The difference lies in the fact that a variable alone might be strong, but **in conjunction with others** is not.

!!! Warning

    If a t-test is used to determine which variables should be dropped, only **ONE variable** (the least significant) should be dropped at a time. This is because the t-test only measures ONE coefficient at a time; ignoring any possible joint effects.

### **Confidence & Prediction Intervals**

Similar to SLR, the confidence and prediction intervals can be constructed using the usual expression:

$$
    \text{Interval} = \text{Estimate} \pm \text{Percentile} \cdot \text{SE of Estimate}
$$

It is mathematically intensive to show derive the variance of the prediction error. It is sufficient to know the result:

$$
    \text{Var}(y_{i} - \hat{y}_{i})
    = \sigma^{2} \left(1 + \boldsymbol{X}^{T}_{i} \cdot (\boldsymbol{X}^{T} \boldsymbol{X})^{-1} \cdot \boldsymbol{X}_{i} \right)
$$

The key intepretations remain the same.

## **Special Variations**

### **Polynomial Variables**

So far, we have implicitly assumed that all independent variables are first order only. However, it is possible to to use **Higher Order** ones, resulting in a **Polynomial Model**:

$$
    E(Y \mid X) = \beta_{0} + \beta_{1} \cdot x^{1}_{p} + \beta_{2} \cdot x^{2}_{p} + \beta_{3} \cdot x^{3}_{p} + \dots
$$

!!! Note

    This is still considered a **Linear Model** because the dependent variable is a **Linear Combination** of the independent variables; the independent variables themselves can be non-linear.

    The model can still include other independent variables, regardless of their powers; mix of polynomial and non-polynomials are possible.

!!! Note

    Typically, if there is a known relationship between $Y$ and the k-th order of the independent variable, **ALL orders from $1$ up to $k$** are added for the variable.

    The reason is because we are interested in the way that $Y$ relates to $X$ as a whole. Keeping all powers allows the relationship to better expressed and hence have a better fit.
    
    However, the choice of $k$ **may not be clear**, especially for higher powers where the relationship is not easily visualized.

Although the relationship is better quantified, the regression coefficients become **hard to intepret** because the terms all **move simultaneously**. 

!!! Warning

    Do NOT mistakenly think that the coefficient squared (for a second-order coefficient) is the impact on the dependent. The relationship must still be linear, which naturally invalidates this train of thought.
    
    Polynomials have a **non-constant slope**, thus they also CANNOT be intepreted as the change in the dependent given a one unit increase anymore.

<!-- Obtained from Medium -->
![POLYNOMIAL_REGRESSION](Assets/3.%20Multiple%20Linear%20Regression.md/POLYNOMIAL_REGRESSION.png){.center}

### **Dummy Variables**

So far, we have implicitly assumed that all independent variables are quantitative. However, it is possible to to use **Qualitative** ones as well. They are typically represented using a **Dummy Variables**, which are can only take on **1 (Yes) or 0 (No)**.

If there are $n$ possible categories, then ONLY $n-1$ dummy variables are needed to fully represent all possible categories. For instance,

$$
    E(Y \mid X)
    = \beta_{0} + \beta_{\text{North}} \cdot x_{\text{North}} + \beta_{\text{West}} \cdot x_{\text{West}} + \beta_{\text{South}} \cdot x_{\text{South}}
$$

!!! Warning

    Only $n-1$ are needed because the final level **can be deduced** from the other variables. If the final level is included, then the variables will become **Perfectly Collinear**, causing problems which will be discussed in the next section. This is known as the **Dummy Variable Trap**.

!!! Note

    The process of converting of a categorical variable (EG. Direction) into several dummy variables is known as Binarization.

The **sum of all dummy variables must equal to 1**. Consider the following two cases:

* **North Chosen**: $x_{\text{North}} = 1, x_{\text{West}} = 0, x_{\text{South}} = 0$
* **East Chosen**: $x_{\text{North}} = 0, x_{\text{West}} = 0, x_{\text{South}} = 0$

The last category that does not have its own variable is known as the **Baseline** of the model (known as the **Reference Category**, typically the most common factor), which is the default category when **all other variables are 0**. Thus, the intepretation of the coefficients is as follows:

* $\beta_{0}$: Value of $E(Y \mid X)$ at the Baseline
* $\beta_{1}$: Change in value of $Y$ when **moving** from the reference category to the current category

!!! Warning

    Given the above, the **choice of reference category matters** as it will result in a different model altogether.

Dummy variables are usually used in conjunction with quantitatve ones. This creates a **seperate but parallel regression line** for each of the levels:

$$
\begin{aligned}
    E(Y \mid X)
    &= \beta_{0} + \beta_{1} \cdot x_{1} + \beta_{2} \cdot x_{\text{Dummy}} \\
    &=
    \begin{cases}
        (\beta_{0} + \beta_{2}) + \beta_{1} \cdot x_{1}, & x_{\text{Dummy}} = 1 \\
        \beta_{0} + \beta_{1} \cdot x_{1}, & x_{\text{Dummy}} = 0
    \end{cases}
\end{aligned}
$$

<!-- Obtained from ACTEX Manual -->
![DUMMY_VARIABLE](Assets/3.%20Multiple%20Linear%20Regression.md/DUMMY_VARIABLE.png){.center}

### **Interaction Variables**

So far, we have assumed the individual predictors do not affect one another. However, it is possible that variables might interact with one another to produce **joint effects**.

!!! Note

    For instance, the production of a factory may depend on the number of **Machines and Workers**. However, the more machines there are, the **greater the effect of an additional worker**.

    Formally speaking, interactions are used when the relationship between the dependent and indepenent variable **depends on the value of another independent variable**. Thus, it captures the relationship **between 3 variables**.

This joint effect can be captured via an **Interaction Variable**, which is the product of the two predictors - allowing us to analyze a **three way relationship**:

$$
\begin{aligned}
    E(Y \mid X)
    &= \beta_{0} + \beta_{1} \cdot x_{1} + \beta_{2} \cdot x_{2} + \beta_{3} \cdot x_{1} \cdot x_{2} \\
    &= \beta_{0} + (\beta_{1} + \beta_{3} \cdot x_{2}) \cdot x_{1} + \beta_{2} \cdot x_{2} \\
    &= \beta_{0} + \beta_{1} \cdot x_{1} + (\beta_{2} + \beta_{3} \cdot x_{1}) \cdot x_{2} \\
\end{aligned}
$$

The previous intepretation about the coefficients NO longer holds true - a one unit increase in $x_{1}$ will increase $Y$ by $(\beta_{1} + \beta_{3} \cdot x_{2})$, which DEPENDS on the value of $x_{2}$.

!!! Warning

    This could lead to scenarios where the Interaction Variable is statistically significant but the **underlying variables individually are not**.
    
    Through the **Hierarchical Principle**, it is common practice that to **include the individual variables** as well regardless of their significance (ignore the results). This is because removing one of the underlying components **might significantly change the effect and interpretation** of the interaction.

    Note that this applies to **Polynomial Regression** as well, which is why all lower order terms are retained as well.

!!! Tip

    Each interaction variable DOES consume a degree of freedom.

If the interaction variable **contains a dummy variable**, then the interpretation changes from before. The existing coefficient of the dummy remains the same, but there is an additional **coefficient from the interaction** which represents the **difference in slope** from the two lines:

$$
\begin{aligned}
    E(Y \mid X)
    &= \beta_{0} + \beta_{1} \cdot x_{1} + \beta_{2} \cdot x_{\text{Dummy}} + \beta_{3} \cdot x_{1} \cdot x_{\text{Dummy}} \\
    &=
    \begin{cases}
        (\beta_{0} + \beta_{2}) + (\beta_{1} + \beta_{3}) \cdot x_{1}, & x_{\text{Dummy}} = 1 \\
        \beta_{0} + \beta_{1} \cdot x_{1}, & x_{\text{Dummy}} = 0
    \end{cases}
\end{aligned}
$$

<!-- Obtained from ACTEX Manual -->
![DUMMY_INTERACTION](Assets/3.%20Multiple%20Linear%20Regression.md/DUMMY_INTERACTION.png){.center}

!!! Tip

    As alluded to, the interpretation of the variables changes in the presence of an interaction. There are several scenarios to consider:
    
    * **Scenario 1**: Continuous Variable with Dummy Interaction
    * **Scenario 2**: Dummy Variable with Continuous Interaction
    * Applicability of Ceteris Paribus

    The key idea is that the variable on its own has an effect. The objective is to determine if the direction of the effect **changes in the presence of another variable**.  

    For scenario (1), only two equations are required, as it does NOT matter whether ceteris paribus is present for a dummy variable:
    
    * Equation assuming Dummy Variable is 0
    * Equation assuming Dummy Variable is 1
    * Determine whether the impact of the continuous variable is conclusive (same for both scenarios)
    
    For scenario (2), two *sets* of equations are required as the Ceteris Paribus assumption matters:
    
    * Equation assuming Continuous Variable is Min/Max
    * Equation assuming Continuous Variable is Min/Max
    * Determine whether the impact of the dummy variable is conclusive (Same for both scenarios)
    
    If Ceteris Paribus does NOT hold, then the comparison should use **DIFFERENT ends** for the continuous variable to reflect the **full range of variability**:

    * Equation assuming Continuous Variable is Min/Max
    * Equation assuming Continuous Variable is Max/Min
    * Determine whether the impact of the dummy variable is conclusive (Same for both scenarios)
  
    Interaction variables are **not easy to interpret**.

### **Indicator Variables**

In some cases, there might be an **change in the behaviour** of the predictor **across different values** it can take.

!!! Info

    For instance, high net worth individuals have different spending patterns than regular consumers - behaviour changes past a certain Income threshold.

This effect can be achieved through the use of an **Indicator Function**, which is a essentially a **Dummy Variable** that is dependent on the value of other variables:

$$
\begin{aligned}
    I_{x \gt c}
    &=
    \begin{cases}
        0, & x_{1} \lt c \\
        1, & x_{1} \ge c
    \end{cases} \\
\end{aligned}
$$

The resulting regression is known as a **Piecewise Model**. There are two possible variations:

* **Continuous**: **Kink** at the threshold; relationship is equal at the threshold
* **Non-continuous**: **Jump** at the threshold; relationship is NOT the same at the threshold

**Continuous piecewise** models directly consider the relationship after the threshold as a **seperate variable**:

* $\beta_{0}$: Intercept before the threshold
* $\beta_{1}$: Slope before the threshold
* $\beta_{2}$: Change in slope AFTER the threshold

$$
\begin{aligned}
    E(Y \mid X)
    &= \beta_{0} + \beta_{1} \cdot x_{1} + \beta_{2} \cdot (x_{1}-c) \cdot I_{x \gt c} \\
    &=
    \begin{cases}
        \beta_{0} + \beta_{1} \cdot x_{1}, & x_{1} \lt c \\
        (\beta_{0} - \beta_{2} \cdot c) + (\beta_{1} + \beta_{2}) \cdot x_{1}, & x_{1} \ge c
    \end{cases}
\end{aligned}
$$

<!-- Obtained from ACTEX Manual -->
![PIECEWISE_CONTINUOUS](Assets/3.%20Multiple%20Linear%20Regression.md/PIECEWISE_CONTINUOUS.png){.center}

**Non-continuous piecewise** models instead use an interaction variable instead:

* $\beta_{0}$: Intercept before the threshold
* $\beta_{1}$: Slope before the threshold
* $\beta_{2}$: Change in intercept **AT** the threshold; the "Jump"
* $\beta_{3}$: Change in slope **AFTER** the threshold

$$
\begin{aligned}
    E(Y \mid X)
    &= \beta_{0} + \beta_{1} \cdot x_{1} + \beta_{2} \cdot I_{x \gt c} + \beta_{3} \cdot (x_{1}-c) \cdot I_{x \gt c} \\
    &=
    \begin{cases}
        \beta_{0} + \beta_{1} \cdot x_{1}, & x_{1} \lt c \\
        (\beta_{0} + \beta_{2} - \beta_{3} \cdot c) + (\beta_{1} + \beta_{3}) \cdot x_{1}, & x_{1} \ge c
    \end{cases}
\end{aligned}
$$

<!-- Obtained from ACTEX Manual -->
![PIECEWISE_NON_CONTINUOUS](Assets/3.%20Multiple%20Linear%20Regression.md/PIECEWISE_NON_CONTINUOUS.png){.center}

!!! Note

    In practice, it is hard to determine the exact breakpoints where such changes occur.

## **Regularization Methods**

A key problem with MLR is that the addition of more variables makes it **more prone to overfitting** (Low Bias, High Variance). Thus, **regularization methods** have been devised to **reduce variance** at the cost of *slightly* increasing bias.

!!! Tip

    It is generally assumed that the **reduction in variance is greater than the increase in Bias**, resulting in an overall **stronger performance**. 

The mathematics behind the methods is complicated, it is sufficient to understand the intuition.

### **L2: Ridge**

The first method is known as the **Ridge Regression**. It works by adding a **penalty** to the OLS minimization problem:

$$
    \min \left(\text{RSS} + \lambda \cdot \sum \beta^{2}_{p} \right)
$$

!!! Note

    The sum of the **squared predictor coefficients** is known as the **$\ell_{2}$ norm**. It is **constrained** to be smaller than some specified **budget**:

    $$
        \sum \beta^{2}_{p} \le a
    $$

    When there are only two predictors, the $\ell_{2}$ norm forms a **Circle**, where all points within the Circle fulfill the penalty; the points along the edge **just fulfil** the penalty.

    The Ridge coefficients are the point where the set of coefficients (Contours) that minimize the penalized RSS **also fulfil** the $\ell_{2}$ constraint; where the **contours meet the Circle**:
    
    <!-- Obtained from Towards Data Science -->
    ![RIDGE_CIRCLE](Assets/3.%20Multiple%20Linear%20Regression.md/RIDGE_CIRCLE.png){.center}

    The key is that due to the shape of the circle, the contours will never intersect the circle along the axis. Thus, it will **NEVER shrink any coefficient to 0**.

!!! Tip

    One way to remember the “norm” used for each method is based on the power of the coefficient in the penalty:
    
    * **Ridge**: Power 2; L2 norm
    * **LASSO**: Power 1; L1 norm 

!!! Tip

    If given the choice of multiple possible variables for the Ridge (and Lasso), remember that the method will choose the model with the **lowest constrained RSS**. Thus, calculate the constrained RSS for each model and then determine which is the regularized model.

    Do NOT make an assumption that the regularization coefficients should be small (especially for LASSO!)

!!! Warning

    Regularization only affects the PREDICTOR coefficients; the intercept is untouched.

The penalty causes the predictor coefficients to "shrink" towards zero, functionally **reducing the amount of information learnt** from the data and hence combatting overfitting. The **shrinkage parameter** $\lambda$ controls the **strength of the shrinkage**:

* $\lambda = 0$: No Shrinkage; same as OLS (maximum flexibiliy)
* $\lambda = \infty$: Fully shrinked; all estimates are 0 (minimum flexibility)

!!! Tip

    Ridge Regression will **NEVER** fully shrink a predictor coefficient to 0 for any finite $\lambda$. The resulting coefficients will be **close but never exactly 0**.

!!! Note

    The budget and shrinkage parameter have opposing effects on the penalty:

    * **Budget**: Causes more shrinkage as it **decreases**
    * **Shrinkage**: Causes more shrinkage as it **increases**

!!! Warning

    Note that the penalty is applied on the **SUM** of squared predictors; the sum must shrink to 0. Thus, it is *possible* for most predictors to shrink while a **specific predictor increases** (offsetting effects).

    <!-- Obtained from Coaching Actuaries -->
    ![RIDGE_SHRINKAGE](Assets/3.%20Multiple%20Linear%20Regression.md/RIDGE_SHRINKAGE.png){.center}

!!! Tip

    It is important to have a clear distinction between:

    * **Penalty**: $\lambda \cdot \sum \beta^{2}_{p}$
    * **Constraint**: $\sum \beta^{2}_{p} \lt s$

    The penalty will always be applied to the RSS, the constraint is a condition that the penalty must fulfil.

### **L1: LASSO**

The second method is known as **LASSO regression**. Similarly, it works by adding a **L1 penalty** to the OLS minimization problem:

$$
    \min \left(\text{RSS} + \lambda \cdot \sum |\beta_{p}| \right)
$$

!!! Note

    The sum of the **absolute predictor coefficients** is known as the **$\ell_{1}$ norm**. It is **constrained** to be smaller than some specified **budget**:

    $$
        \sum |\beta_{p}| \le a
    $$

    When there are only two predictors, the $\ell_{1}$ norm forms a **Diamond**, where all points within the diamond fulfill the penalty; the points along the edge **just fulfil** the penalty.

    The LASSO coefficients are the point where the set of coefficients that minimize the penalized RSS also fulfil the $\ell_{1}$ constraint; where the **contours meet the diamond**:
    
    <!-- Obtained from DS100 -->
    ![LASSO_DIAMOND](Assets/3.%20Multiple%20Linear%20Regression.md/LASSO_DIAMOND.png){.center}

    The key is that since the corners of the diamond are **always on the axes**, it is possible for the contours to intersect the diamond at that point (where one of the other coefficients is 0), thus allowing for LASSO to shrink coefficients to 0.

!!! Note

    If the budget is **sufficiently high**, the diamond will grow larger and the intersection point will **no longer be at the corners**. In this case, LASSO will not drop any predictors (resulting in a more complex model):

    <!-- Obtained from DS100 -->
    ![LASSO_DIAMOND_2](Assets/3.%20Multiple%20Linear%20Regression.md/LASSO_DIAMOND_2.png){.center}

    If the budget is too high, the diamond will grow larger such that the **original OLS estimates already fulfil** the constraints, resulting in the regularization method having **no effect at all**:

    <!-- Obtained from DS100 -->
    ![LASSO_DIAMOND_3](Assets/3.%20Multiple%20Linear%20Regression.md/LASSO_DIAMOND_3.png){.center}

    The second point above applies to Ridge regression as well. Thus, the **size of the constraint** can also be used to determine the extent of shrinkage.

Similarly, $\lambda$ is the **Shrinkage Parameter** of the model and **functions identically** to the ridge model. The key difference is that the predictor coefficients **can be shrunk to 0 for large finite lambda**; the LASSO model **can drop variables**.

!!! Tip

    This it is known as the **Least Absolute Shrinktage Selection Operator** (LASSO). Its ability to drop predictors allows it to be used as a **model selection** tool.

    Since the dropping of variables is done via maximization, the process ignores qualitative aspects such as the hierarchical principle.

### **Comparison**

There is no clear advantage between the two; the key difference lies in the **ability to drop predictors**:

<center>

|           **Ridge**           |            **LASSO**            |
| :---------------------------: | :-----------------------------: |
|          Cannot Drop          |            Can Drop             |
|    Lower Interpretability     |     Higher Interpretability     |
|      Higher Flexibility       |        Lower Flexibility        |
| Able to capture small effects | Unable to capture small effects |

</center>

<!-- Obtained from Towards Data Science -->
![REGULARIZATION_VISUAL](Assets/3.%20Multiple%20Linear%20Regression.md/REGULARIZATION_VISUAL.png){.center}

!!! Note

    LASSO performs better when the underlying process is only a function of a few key variables, while Ridge performs better when there are multiple predictors. This is intuitive, given that the key difference is the dropping of variables.

Another explanation for the difference in behaviour is the following:

* **Ridge**: Shrinks all predictor coefficients by the **same proportion**
* **LASSO**: Shrinks all predictor coefficients by a **constant amount**; but if **smaller than the constant**, then the coefficient is set to 0 (Dropped)

<!-- Obtained from ISLR -->
![SOFT_THRESHOLD](Assets/3.%20Multiple%20Linear%20Regression.md/SOFT_THRESHOLD.png){.center}

!!! Info

    This behaviour in the LASSO is known as **Soft Thresholding**, as all variables below the threshold become dropped.

!!! Tip

    From a dimension reduction perspective, both Ridge and Lasso are **highly efficient** compared to best subset selection. It can be shown that fitting a model for all possible values of $\lambda$ is similar to OLS.

!!! Note

    To be clear on the definition of L1 and L2 norms:

    $$
    \begin{aligned}
        \ell_{1} &= |a| + |b| \\
        \ell_{2} &= \sqrt{a^{2} + b^{2}}
    \end{aligned}
    $$

    <!-- Self Made -->
    ![L1_L2_NORM](Assets/3.%20Multiple%20Linear%20Regression.md/L1_L2_NORM.png){.center}

    Apart from expressing the shrinkage based on $\lambda$, it is also common to express the shrinkage based on the **proportion of the norms**, which is slightly easier to interpret as it ranges from 0 to 1, which is **proportional to the amount of shrinkage** done:

    $$
        \text{Norm Proportion}
        = \frac{\text{Norm of shrunk coefficients}}{\text{Norm of original coefficients}}
    $$

    The key is that the directional effects are **opposite** of $\lambda$, which often results in mirrored graphs:

    <!-- Obtained from ISLR -->
    ![LAMBDA_VS_NORM](Assets/3.%20Multiple%20Linear%20Regression.md/LAMBDA_VS_NORM.png){.center}

An interesting point to note is that LASSO can sometimes be used to identify variable performance. Since there is a penalty on the absolute variable coefficients, the algorithm might set all **non-important coefficients to 0** to ensure that an especially important variable has a **large coefficient**.

### **Elastic Net**

Elastic Net is an approach the combines the Ridge and LASSO. It adds a constraint that is the **weighted average** of the L1 and L2 norms:

$$
    \min \left(\text{RSS} + \alpha \sum |\beta_{p}| + (1 - \alpha) \sum \beta^{2}_{p} \right)
$$

!!! Note

    It is called elastic net because the shape of the method can change to fit either the ridge or LASSO.

Note that the hyperparameter for the elastic net approach is NOT the usual $\lambda$ but rather $\alpha$ (**Mixing Parameter**) which controls the **weight** that is placed on the underlying approaches:

* $\alpha = 0$: Ridge
* $\alpha = 1$: LASSO

!!! Warning

    The elastic net is a weighted combination of the two approaches. Thus, only if the underlying LASSO can drop variables (sufficiently large $\lambda$), then only the resulting elastic net can drop variables.

<!-- Obtained from Corporate Finance Institute -->
![ELASTIC_NET](Assets/3.%20Multiple%20Linear%20Regression.md/ELASTIC_NET.png){.center}

### **Transformations**

Before using regularization, the predictors might need to be first **transformed**. This is because each predictor is of a **different scale** (different units), thus they need to be on the **same basis** to ensure proper results, since a penalty is placed on the **magnitude of the coefficient** (proportional to scale).

!!! Note

    This was not a problem in regular OLS as the residuals were used, which were all following the scale of the dependent.

!!! Warning

    The above concept is DIFFERENT from **Scale Equivariance** - which refers to the fact that the predicted values **remain unchanged** (hence "equi") despite a change in scale of the predictors. This occurs because the fitting process accounts for the change in scale and **rescales the coefficients** accordingly:

    * **Regular OLS**: Scale Equivariant
    * **Regularization**: NOT scale equivariant (Due to the penalty applied)

There are two main methods of transforming predictors:

* **Centering**: Subtracting the sample mean of each predictor
* **Scaling**: Dividing each predictor by its sample standard error

Regardless of which method is used, after expanding the relevant terms, the expression remains the same. The difference lies in the **allocation between the intercept and slope**:

<center>

|   **Centering**   |     **Scaling**     |
| :---------------: | :-----------------: |
| Intercept changed | Intercept unchanged |
|  Slope unchanged  |    Slope changed    |

</center>

$$
\begin{aligned}
    \hat{y}
    &= 50.7909 - 1.6955 \cdot x_{1} - 0.3227 \cdot x_{2} \\
    \\
    \hat{y}_{\text{Centered}}
    &= 34 - 1.6995 \cdot (x_{1} - 8) - 0.3227 \cdot (x_{2} - 10) \\
    &= 34 + 1.6995(8) + 0.3227(10) - 1.6955 \cdot x_{1} - 0.3227 \cdot x_{2} \\
    &= 50.7909 - 1.6955 \cdot x_{1} - 0.3227 \cdot x_{2} \\
    \\
    \hat{y}_{\text{Scaled}}
    &= 34 - 3.909 \cdot \frac{x_{1}}{2} - 1.2909 \cdot \frac{x_{2}}{4} \\
    &= 50.7909 - 1.6955 \cdot x_{1} - 0.3227 \cdot x_{2} \\
\end{aligned}
$$
