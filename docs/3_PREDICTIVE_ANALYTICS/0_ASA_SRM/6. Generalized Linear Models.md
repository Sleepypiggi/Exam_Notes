# **Generalized Linear Models**

## **Overview**

Linear Regression assumes that the dependent variable **can change constantly and indefinitely** in either direction, which is not appropriate for certain quantities that **changes non-linearly** or for values that can **only take values of a certain range** (EG. Positive only or between 0 and 1).

Another assumption made is that the dependent variable is normally distributed. Despite it being one of the most common distributions in reality, not all quantities follow it.

Thus, a **Generalized Linear Model** (GLM) overcomes the above two constraints by:

1. Allowing the dependent variable to have a distribution from the **Linear Exponential Family** (LEF)
2. Specifying a **link function** that relates the dependent to the linear equation, allowing it to change **non-linearly**

<!-- Obtained from Daily Dose of Data Science -->
![SLR_VS_GLM](Assets/6.%20Generalized%20Linear%20Models.md/SLR_VS_GLM.png){.center}

!!! Tip

    It may be useful to think of them this way:

    * **Distribution**: Random component (Affecting errors)
    * **Link**: Systematic component (Affecting the prediction)

## **Key Concepts**

### **Linear Exponential Family**

The LEF of distributions encompasses a wide range of distributions, each with their own **unique parameterization**. However, they have common characteristics that allow them them to be **re-parameterized** into a common format:

$$
\begin{aligned}
    \text{Y} &\sim \text{LEF}(\theta, \phi) \\
    \\
    f(Y \mid \theta, \phi) &= \exp \left(\frac{y \theta - b(\theta)}{\phi} + a(y, \phi) \right) \\
    \\
    E(Y) &= b'(\theta) \\
    \text{Var}(Y) &= \phi \cdot b''(\theta)
\end{aligned}
$$

!!! Info

    Apart from providing a common framework, re-paramterizing this way has certain mathematical properties which makes the resulting calculations smoother. It is not necessary to understand the intricate mathematical details.

!!! Note

    The two parameters are referred to as the **Canonical ($\theta$) and the Dispersion ($\phi$) Parameters** respectively. They have no clear interpretation, it is sufficient to know that they affect the **mean and variance** respectively.

    Consequently, the first and second derivatives are known as the **Mean and Variance Function** respectively. The key is remembering that the Variance is **not entirely determined by the variance function**, unlike the mean.

    It is typically assumed that $\phi$ is a **known constant**.

!!! Tip

    There are multiple ways to re-parameterize a PMF. Thus, it is important to know how to convert a PMF into LEF format:

    1. Introduce an **exponential base** to the function; $x = e^{\ln x}$
    2. Assume $\phi=1$ unless otherwise stated
    3. If there is a **complicated standalone component** involving $y$, that is likely $a(y, \phi)$
    4. **Factorize remaining $y$ terms** to form $y\theta$, the remaining terms should be $b(\theta)$

    Given additional information, the individual components might be able to be broken down again.

<!-- Obtained from Coaching Actuaries -->
![LEF](Assets/6.%20Generalized%20Linear%20Models.md/LEF.png){.center}

!!! Note

    Recall from earlier exams that the **Exponential Distribution** is a special case of the Gamma distribution. Thus, it is also part of the LEF and hence their canonical link functions are the same.

    There is also a special distribution known as the **Tweedie Distribution** which is also part of the LEF. It is a **Poisson Sum of Gamma Variables**, which has properties **somewhere between the two** underlying distributions. It is a **Discrete Continuous Mixture** which allows for a large mass exactly at 0, making it particularly useful for modelling claims (aggregate losses).

<!-- Self Made -->
![COMMON_GLM](Assets/6.%20Generalized%20Linear%20Models.md/COMMON_GLM.png){.center}

!!! Tip

    It is possible to transform the underlying data to fit the requirements - EG. Adding a small positive constant to ensure all values are positive or non-negative.

### **Link Functions**

Under linear regression, the model **directly affects** the dependent variable. Since the model changes linearly, the dependent variable **must change linearly** as well:

$$
    E(Y \mid X)
    = \beta_{0} + \beta_{1} \cdot x_{1} + \beta_{2} \cdot x_{2} + \dots
$$

Thus, this can be overcome by introducing an *intermediate* **Link Function** between the dependent variable and the model output. Thus, even if the **model changes linearly**, the link function can cause a **non-linear impact on the dependent variable**:

$$
\begin{aligned}
    E(Y \mid X)
    &= g^{-1}(\beta_{0} + \beta_{1} \cdot x_{1} + \beta_{2} \cdot x_{2} + \dots) \\
    &= g^{-1}(\boldsymbol{X}^{T} \boldsymbol{\beta}) 
\end{aligned}
$$

!!! Warning

    By definition, the link functions maps the dependent variable to the linear predictors. Hence, going the other way round, it is known as the **Inverse Link Function**.

The key consideration when choosing a link function is the **domain**:

* Linear Predictor generally has an **unrestricted domain** $(-\infty, \infty)$
* Dependent Variable generally has a **restricted domain** $(a, b)$
* Link function must be able to **map the two domains together**

!!! Note

    Another consideration is **interpretability**. Since link functions directly impact the dependent variable, it is desirable to use a **simpler function** (all else equal) to ease understanding.

    Ultimately, the link function should describe the relationship between the mean of the target variable and the linear predictors. For instance, if the relationship is linear, then the identity link can be used; no further transformation needed. But if the relationship IS non-linear, then another link can be used.

!!! Tip

    The link function is applied on the **MEAN of the target variable**, not the target variable itself. Thus, as long as the mean itself satisfies the domain of the link function, it can be used.

    This is why Log link can be used for the Poisson distribution, even though values of 0 are allowed - because the mean is unlikely to be 0.

!!! Note

    Due to the need to match domains, the link function (relationship between target and numeric predictors) must be **monotonic** - strictly increasing or decreasing.

    Thus, if the relationship is non-linear, it would be better to use factor variables instead as they do not have this monotonic constraint.

<!-- Self Made -->
![LINK_FUNCTION](Assets/6.%20Generalized%20Linear%20Models.md/LINK_FUNCTION.png){.center}

!!! Tip

    The existence of link functions means that the intepretation of regression coefficients is not the same as OLS. The effect on the predictor **must consider the link function** as well.

Within the subset of possible link functions, there exists a **Canonical Link Function** which is the **Inverse Mean Function**. This means that the linear predictor will be equivalent to the **canonical parameter** for the distribution:

$$
\begin{aligned}
    \boldsymbol{X}^{T} \boldsymbol{\beta}
    &= g \left[E(Y \mid X) \right] \\
    &= b'^{-1} \left[E(Y \mid X) \right] \\
    &= b'^{-1} \left[b'(\theta) \right] \\
    &= \theta
\end{aligned}
$$

!!! Tip

    Parameter estimation is typically done through an iterative process, which **may ultimately fail to converge** to a solution. The canonical link **simplifies the expression**, which makes it easier to fit parameters, especially when there would have been issues.

!!! Tip

    Thus, if given an unknown mean function, the link function can be solved by solving for $\theta$.

<!-- Self Made -->
![CANONICAL_LINK](Assets/6.%20Generalized%20Linear%20Models.md/CANONICAL_LINK.png){.center}

!!! Tip

    Any **multiplicative constant** within the canonical link function itself does not make a difference to the result. An expression **with or without** the constant can be deemed as the Canonical Link function.

    This INCLUDES the negative operator, as it can be viewed as a constant of -1.

!!! Warning

    Although most GLMs will use their canonical link, it is NOT a requirement that they will. Always read the question to check which link is being used!

It is important to choose a **combination** of distribution AND link that will result in the modelled target variable having a **suitable domain** (EG. Positive continuous values):

* **Distribution**: Directly consider characteristics of the target variable (Domain, Spread etc)
* **Link Function**: Consider whether resulting predictions are valid realizations of the distribution

!!! Warning

    It is important to know the difference between the following:

    |            **GLM Log Link**             |           **MLR Log Transform**            |
    | :-------------------------------------: | :----------------------------------------: |
    |           $\ln[E(Y \mid X)]$            |            $E[\ln (Y \mid X)]$             |
    |   Models relationship directly with Y   | Models relationship indirectly with $ln Y$ |
    | Indirectly controls variance (via Mean) |        Directly stabilizes the mean        |
    |  Easier to interpret (Original target)  |  Harder to intepret (Transformed target)   |
    |          Any LEF distribution           |           LogNormal distribution           |

### **Maximum Likelihood**

The parameters are estimated using **Maximum Likelihood Estimation** (MLE), where the likelihood function can be expressed as the following:

$$
\begin{aligned}
    \ell(\beta)
    &= \ln \prod \exp \left(\frac{y \theta - b(\theta)}{\phi} + a(y, \phi) \right) \\
    &= \sum \ln \exp \left(\frac{y \theta - b(\theta)}{\phi} + a(y, \phi) \right) \\
    &= \sum \left(\frac{y \theta - b(\theta)}{\phi} + a(y, \phi) \right) \\
    \\
    \frac{\partial^{2} \beta}{\partial \beta^{2}} &= 0
\end{aligned}
$$

Since $\beta$ and $\theta$ are linked, solving for $\beta$ provides $\theta$ as well. It is typically asummed that $\phi$ is 1 for most key distributions.

!!! Info

    There are **no closed form solutions** to the above system of equations, they are typically solved through numerical methods. It is not necessary to know the details for this exam.

!!! Note

    Recall that MLE achieves **Asymptotic Normality**. This will be the basis used for GLM-related statistical inference. The actual normal distribution will be used (rather than t-distribution) because in the limit, the variance of the normal distribution is **known**.

    However, the "adequateness" of the asymptotic results are dependent on the **choice of distribution**, but not the distribution of the underlying data:

    * Consider a **continuous** dataset with **equal mean and variance**
    * The key is to choose a distribution with **equal mean and variance as well**, regardless of what the underlying distribution is
    * For instance, if a poisson distribution was used, it is technically "wrong" because it assumes discrete data while the underlying data is continuous, but since it models the mean and variance correctly, it can be used

### **Deviance**

For linear rgeression, the parameters were estimated based on the residuals; thus, the goodness of fit was measured using the residuals via ANOVA. Similarly, since GLMs use MLE, their fit should be **measuring using likelihood**.

!!! Tip

    Naturally, this means that traditional ANOVA measures are NOT applicable in a GLM context.

Consider an extreme case of GLM where the **number of parameters in the model is exactly equal to the number observations**, causing it to **fit the data perfectly**. This is known as a **Saturated Model**. This saturated model obviously suffers from overfitting (as it perfectly captures the noise in each sample), thus should not be used for prediction.

However, it can be used as a **benchmark** for the total information available in the sample and consequently **how much is lost** through the fitting process. This is measured via the **Deviance** of the model, which is the **difference in the likelihood** of the two models:

$$
    D^{*} = 2 \cdot (\ell_{\text{Saturated}} - \ell_{\text{Fitted}})
$$

The above are the **MAXIMIZED log-likelihoods**, which uses the likelihoods with the **MLE estimate**:

* **Saturated**: Uses actual observations (Since a saturated model fits the data perfectly)
* **Fitted**: Uses fitted observations

The fitted model should have **sufficiently high likelihood** to ensure that it captures the signals, but be reasonably smaller than the saturated model to prevent overfitting. Thus, models with **smaller (but NOT too small) deviance** are preferred.

Note that the subtraction of the two likelihoods would cause the second term in the probability function to cancel, leaving only the difference in the numerators:

$$
    D^{*} = \frac{D}{\varphi}
$$

Thus, the most accurate terminology would be that the difference in the two likelihoods is known as the **Scaled Deviance**, while the differences in the numerator only is the **Deviance**. However, since $\varphi$ is **typically assumed to be 1** for most distributions used in SRM, the two are **identical**.

!!! Note

    Consider the Likelihood for the following two distributions - Binomial and Poisson. The key is understanding that whenever the predicted quantity ($\pi$ or $\lambda$) comes up in the equation:

    * **Saturated Likelihood**: Fill in the actual observation (since perfectly predict)
    * **Fitted Likelihood**: Fill in the predicted value
    * Terms with JUST the observed value will naturally cancel out during the deviance process

    $$
    \begin{aligned}
        L_{\text{Binomial}} &= \sum {n \choose y} \pi^{y} (1 - \pi)^{n-y} \\
        \ell_{\text{Binomial}} &= \sum y \ln \pi + (n-y) \ln (1 - \pi) \\
        \ell_{\text{Poisson, Saturated}} &= \sum y_{i} \ln y_{i} + (n - y_{i}) \ln (1 - y_{i}) \\
        \ell_{\text{Poisson, Fitted}} &= \sum y_{i} \ln \hat{y}_{i} + (n - \hat{y}_{i}) \ln (1 - \hat{y}_{i}) \\
        \\
        \therefore D^{*}_{\text{Binomial}}
        &= 2 \cdot (\ell_{\text{Binomial, Saturated}} - \ell_{\text{Binomial, Fitted}}) \\
        &= 2 \cdot \sum \left[ (y_{i} \ln y_{i} - y_{i} \ln \hat{y}_{i}) + ((n - y_{i}) \ln (1 - y_{i}) - (n - \hat{y}_{i}) \ln (1 - \hat{y}_{i})) \right] \\
        &= 2 \cdot \sum \left[ y_{i} \ln \frac{y_{i}}{\hat{y}_{i}} + (n - y_{i}) \ln \frac{1 - y_{i}}{1 - \hat{y}_{i}} \right] \\
        \\
        L_{\text{Poisson}} &= \sum \frac{\lambda^{y} e^{-\lambda}}{y!} \\
        \ell_{\text{Poisson}} &= \sum y \ln \lambda - \lambda - \ln y! \\
        \ell_{\text{Poisson, Saturated}} &= \sum y_{i} \ln y_{i} - y_{i} - \ln y_{i}! \\
        \ell_{\text{Poisson, Fitted}} &= \sum y_{i} \ln \hat{y}_{i} - \hat{y}_{i} - \ln y_{i}! \\
        \\
        \therefore D^{*}_{\text{Poisson}}
        &= 2 \cdot (\ell_{\text{Poisson, Saturated}} - \ell_{\text{Poisson, Fitted}}) \\
        &= 2 \cdot \sum \left[ (y_{i} \ln y_{i} - y_{i} \ln \hat{y}_{i}) - (y_{i} - \hat{y}_{i}) - (\ln y_{i}! - \ln y_{i}!) \right] \\
        &= 2 \cdot \sum \left[ y_{i} \ln \left(\frac{y_{i}}{\hat{y}_{i}} \right) - (y_{i} - \hat{y}_{i}) \right] \\
    \end{aligned}
    $$

    For a **normal distribution**, in other words regular linear regression, the Deviance is equivalent to the **RSS**.

The concept of deviance is similar to goodness of fit, thus can be used similarly to R-squared, known as **Pseduo R-squared**:

$$
    R^{2}_{\text{Pse}}
    = \frac{\ell_{\text{fitted}} - \ell_{\text{Null}}}{\ell_{\text{Sat}} - \ell_{\text{Null}}}
$$

!!! Note

    The key idea remains the same. It is the proportion of Deviance explained by the model over the total possible deviance that could be explained. The key difference is that the values are **relative to the likelihoods of the null model** - representing the change in likelihood from the model parameters.
    
### **Deviance Residual**

Recall that since the likelihood is already a sum of each observation, there exists a deviance for each observation as well. The squareroot of that amount is known as the **Deviance Residual**:

$$
\begin{aligned}
    D^{*}
    &= \sum D^{*}_{i} \\
    \\
    \text{Deviance Residual}
    &= \text{sign}(y_{i} - \hat{y}_{i}) \sqrt{D^{*}_{i}} \end{aligned}
$$

!!! Warning

    Only the SIGN of the deviance residuals follow the raw residual. Thus, a positive residual represents under-prediction and vice-versa. Otherwise, the two are completely **different concepts** - do NOT confuse them!

It can be shown that if the model holds true, then the Deviance Residuals are approximately normal with a **mean of zero and constant variance** of $\phi$. Thus, the model diagnostic methods for MLR **also apply to GLM**!

!!! Warning

    Although the final outcome is the same, the method of reaching here is completely different.

    Heteroskedasticity assumes variance in the errors such that they have constant variance. The constant variance for deviance residuals is NOT the same assumption, because the constant variance is for the deviance residuals of the model, not the errors.

!!! Tip

    Due to the different possible distributions used, note that for two models with different distributions (but identical otherwise) might have **different scales**.

    Plots do not allow for comparisons on the actual total deviance, but one approach would be to comment on the range of the deviance that the plot takes. A **smaller range** is definitely more preferred.

## **Statistical Inference**

### **Likelihood Ratio Test**

Similar to how F-tests made use of the ANOVA results to perform statistical inference, GLMs can make use of the Deviance results in a similar fashion, via a **Likelihood Ratio Test (LRT)**, which compares a pair of **NESTED GLMs**:

* $H_{0}$: $\beta_{q} = 0$
* $H_{1}$: $\beta_{q} \ne 0$

$$
\begin{aligned}
    \chi_{\text{Statistic}}
    &= 2 \cdot (\ell_{\text{Full}} - \ell_{\text{Restricted}}) \\
    &= 2 \cdot (\ell_{\text{Full}} - \ell_{\text{Saturated}} + \ell_{\text{Saturated}} - \ell_{\text{Restricted}}) \\
    &= 2 \cdot (\ell_{\text{Full}} - \ell_{\text{Saturated}}) + 2 \cdot (\ell_{\text{Saturated}} - \ell_{\text{Restricted}}) \\
    &= - 2 \cdot (\ell_{\text{Saturated}} - \ell_{\text{Full}}) + 2 \cdot (\ell_{\text{Saturated}} - \ell_{\text{Restricted}}) \\
    &= 2 \cdot (\ell_{\text{Saturated}} - \ell_{\text{Restricted}}) - 2 \cdot (\ell_{\text{Saturated}} - \ell_{\text{Full}}) \\
    &= D_{\text{Restricted}} - D_{\text{Full}} \\
    \\
    \chi_{\text{Statistic}} &\sim \chi^{2}_{p_{\text{Full}} - p_{\text{Restricted}}}
\end{aligned}
$$

If there is a **significant drop in likelihood** between the two models, then the reduced predictors are significant and thus should not be dropped from the model.

!!! Warning

    Even though Ratio is in the name, the test-statistic is NOT ratio of likelihoods.

!!! Tip

    For the purposes of this exam, it is typically assumed that the restricted model is the null model; similar overall set-up to an F-test.

!!! Note

    Under GLM, the total variance **CANNOT be decomposed** into Explained and Unexplained components; there is an addittional component:

    $$
    \begin{aligned}
        \text{TSS}
        &= \sum(y_{i} - \hat{y})^{2} + \sum(\hat{y} - \bar{y})^{2}
            + 2 \sum \left([y_{i} - \hat{y}][\hat{y}-\bar{y}] \right) \\
        &= \text{RSS} + \text{RegSS} + 2 \sum \left([y_{i} - \hat{y}][\hat{y}-\bar{y}] \right)
    \end{aligned}
    $$

    The above additional component would cancel out under OLS, thus was not issue. However, this **does NOT apply to GLMs**. Thus, the usual **ANOVA inferences do NOT apply in a GLM context**.

### **Pearson Residuals**

Pearson Residuals are the raw residuals **scaled by the standard deviation**:

$$
    e_{\text{Pearson}} = \frac{y - \hat{y}}{\sqrt{\hat{\text{Var}}(y)}}
$$

!!! Warning

    The standard deviation is from the variance of the underlying distribution, NOT of the coefficients or the sort:
    
    * Binomial: $\text{Var(Y)} = np(1-p)$
    * Poisson: $\text{Var(Y)} = \mu$

!!! Tip

    For the poisson distribution specifically, the reason why Pearson residuals are used is because the raw residuals are expected to be **Heteroscedastic**. Another interesting effect is that the **SUM OF SQUARED pearson residuals** is equivalent to the Chi-Square goodness of fit:

    $$
    \begin{aligned}
         \text{Pearson Goodness of Fit}
         &= \sum e^{2}_{\text{Pearson}} \\
         &= \sum \left(\frac{y - \hat{y}}{\sqrt{\hat{\text{Var}}(y)}} \right)^{2} \\
         &= \sum \left(\frac{(y - \hat{y})^{2}}{\hat{\text{Var}}(y)} \right) \\
         &= \sum \left(\frac{(y - \mu)^{2}}{\mu} \right) \\
         &\approx \sum \frac{(O - E)^{2}}{E} \\
         &\approx \text{Chi Square Goodness of fit} \\
         \\
         \text{Pearson Goodness of Fit} &\sim n - p -1
    \end{aligned}
    $$

!!! Note

    Generally, why "normal" residuals are not useful in GLMs is because each model follows a **different distribution**, thus there are is **no uniform behaviour for raw residuals**.

    However, as they are stills residuals, they can be used in a **similar way** - to find outliers or identify misspecification.

### **Dispersion**

Recall that for MLR, Homoscedasticity (constant variance) was an assumption made about the data. More generally for GLMs, there is also an **assumption made about the variance** of the target, which is based on the **choice of distribution**. If the actual distribution has a variance different from what was assumed:

* **Overdispersion**: Actual variability is larger than expected; **standard errors understated**; false positives
* **Underdispersion**: Actual variability is smaller than expected; **standard errors overstated**; false negative

Apart from changing the distribution of choice to better match the data (EG. Negative Binomial rather than Poisson due to flexibility), another is to use the **Quasi Likelihood Method**. This works by estimating a **new dispersion parameter** ($\delta$) such that it is a **Pearson Goodness of Fit Statistic scaled by its degrees of freedom**: 

$$
    \delta = \frac{1}{n-p-1} \cdot \text{Pearson Goodness of Fit} \\
$$

!!! Tip

    This also means that the pearson chi-squared statistic can also be used to **indicate when large over-dispersion is present**, since the adjustment required is **proportional to the amount needed to re-scale** the variance. Thus, another definition for dispersion is the **relative size of the Deviance compared to its degrees of freedom**:

    * **Higher than one**: Overdispersion
    * **Lower than one**: Underdisperion

    Poisson is mentioned because it is usually goes **hand in hand with dispersion**. Poisson assumes that the distribution has equal and mean and variance, which means any deviation will result in violation of dispersion.

The above method only works for distributions which have a **restrictive variance**; the existing dispersion parameter is fixed ($\phi = 1$). However, applying this method fixes the dispersion but also fundamentally "changes" the distribution to become fake ("Quasi"):

* Coefficients **remain unchanged** - predictions are unchanged
* Standard errors should increase - Affecting variable significance
* Aymptotic normality no longer holds; t-distribution should be used instead - Affecting variable significance
* No longer poisson distribution - Interpretation should change
* Log-likelihood doesnt make sense

### **Fitting Metrics**

Although traditional metrics such as $R^{2}$ does not apply, fitting metrics such as AIC and BIC are still applicable. However, they have a slightly different formulation:

$$
\begin{aligned}
    \text{AIC} &= \text{Deviance} + 2 \cdot \text{Degrees of Freedom} \\
    \text{BIC} &= \text{Deviance} + 2 \cdot \text{Degrees of Freedom} \\
\end{aligned}
$$

## **Categorical Models**

### **Binary Response**

A Binary response variable has only two outcomes (0,1), thus a **Bernoulli Distribution** is used to model it. There are **three main link functions** that can map the the range into (0,1):

* **Logit Link**: $\ln \frac{\mu}{1-\mu}$
* **Probit Link**: $\Phi^{-1}(\mu)$
* **Complementary Log-Log Link**: $\ln[- \ln(1-\mu)]$

<!-- Obtained from Coaching Actuaries -->
![BINARY_LINK](Assets/6.%20Generalized%20Linear%20Models.md/BINARY_LINK.png){.center}

!!! Warning

    Recall the difference the function and the link function:

    $$
    \begin{aligned}
        Z &= \ln \left( \frac{P}{1-P} \right) \\
        e^{Z} &= \frac{P}{1-P} \\
        e^{Z} - P \cdot e^{Z} &= P \\
        P + P \cdot e^{Z} &= e^{Z} \\
        P (1 + e^{Z}) &= e^{Z} \\
        P &= \frac{e^{Z}}{1 + e^{Z}} \\
        \therefore \text{Logit} &= \frac{e^{Z}}{1 + e^{Z}} \\ 
        \\
        Z &= \Phi^{-1}(P) \\
        P &= \phi(Z) \\
        \therefore \text{Probit} &= \phi(Z) \\
        \\
        Z &= \ln (- \ln(1-p)) \\
        e^{Z} &= - \ln (1-p) \\
        -e^{Z} &= \ln (1-p) \\
        e^{-e^{z}} &= 1 - p \\
        P &= 1 - e^{-e^{z}}
    \end{aligned}
    $$

    <!-- Self Made -->
    ![LOGIT_VS_LOGIT_LINK](Assets/6.%20Generalized%20Linear%20Models.md/LOGIT_VS_LOGIT_LINK.png){.center}

!!! Tip

    Notice the similarities between the three:

    * **Negative IV**: Complementary Log-Log & Logit are similar
    * **Positive IV**: Complementary Log-Log & Probit are similar

    Generally speaking, it is **NOT EASY** to distinguish between the three methods graphically.
    
!!! Tip

    Logit and Probit are the most popular methods. However, logit is preferred as it is;
    
    1. The canonical link for the Binomial distribution
    2. Can be expressed in a closed form
    3. Easier to intepret
    
!!! Warning

    Logistic Regression exists in a grey area:

    * It is technically a **regression method** as it estimates a **continuous value of probabilities**
    * However, it is also regarded as a **classification method** as it ultimately predicts one of two **categories** 

Logit is the canonical link function; the resulting GLM is known as a **Logistic Regression**. It models the **Log-Odds** of an event occuring:

$$
\begin{aligned}
    \ln \frac{1-\mu}{\mu} &= \beta_{0} + \beta_{1} \cdot X_{1} + \dots \\
    \frac{1-\mu}{\mu} &= \exp (\beta_{0} + \beta_{1} \cdot X_{1} + \dots) \\
    \text{Odds} &= \exp (\beta_{0} + \beta_{1} \cdot X_{1} + \dots) \\
    \\
    \therefore \ln \text{Odds} &= Y \\
    \frac{1-\mu}{\mu} &= e^{Y} \\
    \mu &= e^{Y} - \mu \cdot e^{Y} \\
    \mu &= \frac{e^{Y}}{1 + e^{Y}} \\
    \text{P(A)} &= \frac{e^{Y}}{1 + e^{Y}}
\end{aligned}
$$

!!! Note

    Odds is the **ratio of the probability** of an event occuring to it not occuring:

    $$
        \text{Odds} = \frac{\text{P(A)}}{1 - \text{P(A)}}
    $$

    It is an alternative way of representing probabilities, typically used in a gambling context.

It can be shown that the exponent of the regression coefficient is the **multiplicate increase in the odds** given a one unit increae in that variable (continuous) or presence of that variable (categorical):

$$
\begin{aligned}
    \frac{\text{Odds}_{1}}{\text{Odds}_{0}}
    &= \frac{\exp (\beta_{0} + \beta_{1} \cdot (X_{1}+1))}{\exp (\beta_{0} + \beta_{1} \cdot X_{1})} \\
    &= \exp (\beta_{1}) \\
    \\
    \text{Percentage Change} &= \exp (\beta_{1}) - 1
\end{aligned}
$$

!!! Note

    The primary motivation for the Logistic Regression is because a **regular linear model** is unable to effectively model dummy variables due to the following limitations:

    * **Domain Mismatch**: Linear models range from negative to positive infinity; Probabilities can only range from 0 to 1
    * **Heteroscedasticity**: Linear models require homoscedasticity; Bernouilli exhibits heteroscedasticity (variance function linked to mean)
    * **Invalid Residuals**: Linear models require continuous residuals; Residuals for probabilities dont make sense

The output of the logistic regression is the Odds of the event, which can be converted into probability. At this point, a **cutoff** must be chosen such that all probabilities **above the cutoff** will be classified as the event of interest occuring.

!!! Tip

    0.5 is often chosen as the cutoff value - but it can also be algorithmically determined via AUC.

### **Nominal Response**

For a variable with more than two levels, a **Generalized Logit Model** is used. Rather than odds, it models the log of the **relative probability** of the target category and reference category.

For a variable with $k$ categories, $k-1$ models will be fitted, for each of the **non-reference** categories:

$$
\begin{aligned}
    \ln \left(\frac{P(1)}{P(k)} \right) &= \beta_{1} + \beta_{1, 1} \cdot X \\
    \ln \left(\frac{P(2)}{P(k)} \right) &= \beta_{2} + \beta_{1, 2} \cdot X \\
    \ln \left(\frac{P(3)}{P(k)} \right) &= \beta_{3} + \beta_{1, 3} \cdot X
\end{aligned}
$$

The system of equations above will result in $k-1$ equations (since the last category does not have its own equation) for $k$ unknowns. The final equation is the fact that the unknowns are probabilities, thus all have to **sum to 1**:

$$
    P(1) + P(2) + P(3) + \dots = 1
$$

!!! Note

    If the model has $p$ predictors and $k$ categories, then a total of $p \cdot (k-1)$ coefficients are needed, reflecting the above fact.

### **Ordinal Response**

When there is an Order to the nominal variables, they are distinguished based on their **cumulative probabilities**. In essence, the prediction space is split into **distinct regions** representing each of the categories:

<!-- Obtained from Medium -->
![ORDINAL_TAU_CUTS](Assets/6.%20Generalized%20Linear%20Models.md/ORDINAL_TAU_CUTS.png){.center}

There are will always be **$k-1$ cut points** which are always **non-decreasing**. Thus, a larger prediction which will lead to a **"higher" category**, hence preserving the order of the prediction.

In terms of modelling, a **Cumulative Logit Model** is used, which models the **cumulative odds** of each of the categories except the last. Given the system of equations, the **probabilities for each category** can be determined. The prediction of the model is the category with the **highest probability**:

* $\alpha$: Cut Points
* $\beta_{j}$: Regression coefficient

$$
\begin{aligned}
    \ln \left(\frac{P(1)}{1 - P(1)} \right) &= \alpha_{1} + \beta_{1, 1} \cdot X \\
    \ln \left(\frac{P(1) + P(2)}{1 - (P(1) + P(2))} \right) &= \alpha_{2} + \beta_{1, 2} \cdot X \\
    \ln \left(\frac{P(1) + P(2) + P(3)}{1 - (P(1) + P(2) + P(3))} \right) &= \alpha_{3} + \beta_{1, 3} \cdot X
\end{aligned}
$$

!!! Tip

    Similar to before, the model only provides $k-1$ equations for $k$ probabilities to be solved for. Thus, the same approach can be taken to solve for the final probability:

    $$
        P(1) + P(2) + P(3) + \dots = 1
    $$

    Similarly, the model requires a large number of coefficients to be estimated, one set for each of the modelled $k-1$ categories.

A common variation of is to use the **Proportional Odds** Cumulative Model. It assumes that the **odds ratios are the same across thresholds**, thus allowing each equation to use the **SAME $\beta$**, with only the $\alpha$ changing for each, reducing the number of cofficients needed:

$$
\begin{aligned}
    \ln \left(\frac{P(1)}{1 - P(1)} \right) &= \alpha_{1} + \beta_{1} \cdot X \\
    \ln \left(\frac{P(1) + P(2)}{1 - (P(1) + P(2))} \right) &= \alpha_{2} + \beta_{1} \cdot X \\
    \ln \left(\frac{P(1) + P(2) + P(3)}{1 - (P(1) + P(2) + P(3))} \right) &= \alpha_{3} + \beta_{1} \cdot X
\end{aligned}
$$

!!! Warning

    It is simply that the $\beta$'s are constant across predictions, not that they are not needed.

    In the event that the base model already does not include any predictors (simplest null model) then the proportional odds model does not provide any advantage.

Thus, it can be easily seen that the **ratio of two cumulative odds** is simply the difference in cut points:

$$
\begin{aligned}
    \frac{\frac{P(1)}{1 - P(1)}}{\frac{P(1) + P(2)}{1 - (P(1) + P(2))}}
    &= \frac{\exp (\alpha_{1} + \beta_{1} \cdot X)}{\exp (\alpha_{2} + \beta_{1} \cdot X)} \\
    &= \exp (\alpha_{1} - \alpha_{2})
\end{aligned}
$$

!!! Tip

    This is useful for situations where the same model is used to make predictions for different observations. If the outcome for one observation is known, then the other can be easily solved.

## **Count Models**

### **Poisson Model**

Since counts are non-negative integers, the **Poisson Distribution** can be used. The canonical link is the **natural log**, where the resulting model is known as a **Poisson Count Model**. It models the **log of the mean**, where the exponent of the regression coefficients represent a **multiplicative increase** in the mean:

$$
    \ln \mu = \beta_{0} + \beta_{1} \cdot X_{1} + \dots
$$

!!! Note

    Count models are sometimes also known as **Frequency** Models.

    Poisson CANNOT be used to non-count integer variables, although they seemingly look the same.

The poisson coefficients have a very similar intepretation with the logit model - it represents the **multiplicative increase** in the count given a one unit increase or in the presence of another variable.

#### **Offsets**

Recall that the poisson distribution can be interpreted as the **average number of occurences for a given exposure** (w). The average thus be re-expressed as a combination of exposure and occurrence per exposure (rate, $\lambda$):

$$
    \mu = w \cdot \lambda
$$

If the underlying data has **different subgroups with different exposures**, it will be more appropriate to model the **rate per exposure** rather than the actual count:

$$
\begin{aligned}
    \ln \lambda &= \beta_{0} + \beta_{1} \cdot X_{1} + \dots \\
    \ln \frac{\mu}{w} &= \beta_{0} + \beta_{1} \cdot X_{1} + \dots \\
    \ln \mu &= \ln w + \beta_{0} + \beta_{1} \cdot X_{1} + \dots \\
\end{aligned}
$$

The exposure term is known as an **Offset**. It is a variable that is added to the model with a **fixed coefficient of 1**, representing a known relationship with the target variable. For instance, consider the following data:

<center>

| **Subject** | **Number of A's (Count)** | **Number of Students (Exposure)** |
| :---------: | :-----------------------: | :-------------------------------: |
|    Math     |             5             |                10                 |
|   Science   |             8             |                20                 |
|   History   |            10             |                30                 |

</center>

It is **not fair** to directly compare the number of A's for each subject and conclude which is doing best - we should consider the **underlying exposure** as well to consider the A-rate.

#### **Weights**

Another similar concept is **Weights**. It represents the Credibility of each observations, usually based on another variable within the dataset. Observations with more credibility have more information and less variability, thus the model will make effort to better fit to high weight observations. In other words,  weights adjust the influence of each observation.

Note that weights only affect the model estimation process - they are NOT included in the final model as a predictor.

### **Dispersion**

Although the poisson distribution is suited to model counts, it has a **restrictive** property where the **mean and variance MUST be equal**. This makes it inadequate where the underlying data exhibits **overdispersion**. As mentioned previously, one method is to adjust the re-scale the variance using the pearson residuals. This section addresses more drastic approaches to deal with it.

#### **Negative Binomial**

Another method is to use a different distribution which does not have this restrictive property - such as the  **Negative Binomial Distribution**:

1. It has **two parameters** to fit the data, allowing for more flexibility
2. Poisson is a **limiting case** of the negative binomial (**reasonable change**)
3. Negative Binomial is a **mixture of Poisson and Gamma** (thus is a reasonable distribution)

!!! Note

    If the Negative Binomial is more flexible, why isnt the negative binomial used directly in the first place? Because Poisson is much **simpler to model and interpret**, with only parameter.

#### **Zero Inflated**

A common reason for Over-dispersion is due to an **excess of zeroes** in reality compared to our expectation, known as **Zero Inflation**. In these cases, the original distribution must be **adjusted to account for probability of zeroes** occuring.

A **Zero Inflated Model** is appropriate when the underlying **event occurred, but no count was made**. For instance, a policyholder might choose not to make a claim despite incurring a loss due to considering premium rate increases.

Thus, it is necessary to account for and **distinguish between the types** of zeroes. This can be done via a **Logistic Regression** which is set to identify **False Zeroes**:

* False Zeroes (EG. Policyholders who incurred a loss but did not report)
* True Zeroes (EG. Policyholders who did not incur a loss)

Thus, mixing the Poisson and the Logistic Regression output, the PMF of the **mixture** can be constructed:

* **Logistic** - Identify observations with false zeroes
* **Poisson** - Underlying data generating process (assuming no false)

$$
\begin{aligned}
    \text{Mixture PMF}
    &= \begin{cases}
        \pi + (1 - \pi) \cdot P(Y = 0) ,&Y = 0 \\
        (1 - \pi) \cdot P(Y \gt 0) ,&Y \gt 0
    \end{cases}
\end{aligned}
$$

!!! Tip

    The key idea is that the original probabilities are adjusted to account for the possibility of being False Zeroes.

#### **Hurdle Model**

A **Hurdle Model** is appropriate when the zeroes occur because a certain **hurdle or threshold was not crossed**. For instance, when projecting the number of laps swam based on the number of people who visited the swimming pool, it should account for individuals who **never entered** the pool (Hurdle) (EG. Parents).

In this case, there is a clear reason for the zeroes, thus the probability for the zero count should be **modelled seperate** for the non-zero values:

* **Logistic** - Identify observations that do NOT cross the threshold ($\pi$)
* **Poisson** - Underlying data generating process (assuming all cross)

$$
\begin{aligned}
    \text{Mixture PMF}
    &= \begin{cases}
        \pi ,&Y = 0 \\
        (1 - \pi) \cdot \frac{P(Y \gt 0)}{1 - P(Y =0)} ,&Y \gt 0
    \end{cases}
\end{aligned}
$$

!!! Tip

    The resulting distribution should use a **zero-truncated** probability to reflect that the original distribution should not be used for zero counts.

!!! Tip

    Although not shown explicitly, Hurdle models are the only **one of two models** discussed in this section that allows for UNDER-dispersion as well; the other is the Latent Class Model.

### **Heterogeneity Model**

A **Heterogeneity Model** is where one or more of the model parameters are **modelled as a seperate distribution**, known as the **Heterogeneity Component**. The resulting distribution is mixture as well:

* Poisson & Log-Gamma
* Poisson & Lognormal

!!! Info

    The word Heterogeneity means **Diversity**.

    In essence, Heterogeneity models **account for the diversity** in the data that is **NOT observed** but still significantly impact the target variable. 

#### **Latent Class Models**

A **Latent Class Model** is where the modeller would like to split the data into classes:

* Healthy vs Ill People
* Low vs High Risk individuals

These classes are "unobserved" (**Latent**) in the data as they are **not explicit**. Thus, the classes are treated as a **Discrete Random Variable** and modelled via a **Mixture** as well.

!!! Note

    Latent Class Models can be said to be modelling the heterogeneity described in the previous section as well.

!!! Tip

    Although not shown explicitly, Latent Class models are the only **one of two models** discussed in this section that allows for UNDER-dispersion as well; the other is the **Hurdle Model**.
