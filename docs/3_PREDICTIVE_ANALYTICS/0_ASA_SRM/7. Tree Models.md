# **Tree Models**

## **Overview**

Tree Models are the first **supervised AND non-parametric** statistical learning method covered. As such, the model is mainly **algorithmic** in nature.

Broadly speaking, tree models **partition the sample** into several **distinct regions** based on the values of independent variables. For a prediction, the model determines which region the prediction falls in and sets the prediction equal to the **average of the observations within that region**.

!!! Tip

    This allows tree models to be more resilient to outliers as they can simply partition the outliers away, leaving the main bulk of the distribution unaffected.

Using the **boundaries** of the partitions, a **tree-like structure** can be formed to easily visualize the model. This is highly effective for datasets with many independent variables as the sample space itself is not easily visualized.

<!-- Obtained from Research Gate -->
![TREE_PARTITION](Assets/7.%20Tree%20Models.md/TREE_PARTITION.png){.center}

!!! Warning

    Only “vertical” and “horizontal” lines (lines **parallel to the axes** in higher dimensions) can be used to partition the observation space.

    When the lines intersect, each region is considered to be unique, even if the split does not change the prediction - regions are **NOT combined**. Graphically, this means that the predictor space must have always lines that touch both ends of the boundaries.
    
    <!-- Self Made -->
    ![REGION_SPLIT](Assets/7.%20Tree%20Models.md/REGION_SPLIT.png)
    
    The above points are mainly applicable for questions which ask to identify **valid predictor spaces**, typically when there are only two predictors. If however, the question is asking for predictor values, then the image on the RHS is valid.
    
!!! Warning   

    The predictor space is split using the **Independent Variable** as the boundaries, NOT the dependent variable.

!!! Warning

    Take note of the following "quirks":

    1. Not all features will be used to build the tree
    2. The same features can be used more than once in the tree
    3. Relatively **more important** features will be used for splits **first**

    On point (3), the first predictor used in the split is the **MOST important** because it has the largest reduction in RSS.
    
    In a similar vein, any **possible interactions** between variables are automatically accounted for in the tree; no need special adjustments are needed.

The different points along the tree are known as **Nodes**:

* **Internal Nodes** (Parent Nodes): Nodes where the tree **splits further**
* **Terminal Nodes** (Child Nodes): Nodes where the tree **no longer splits**

!!! Info

    They can also be interpreted using an actual tree analogy - Roots, Branches & Leaves. In this case, the structure is more of an inverted tree.

!!! Tip

    For the purposes of this exam, assume that **TRUE is on the left** while FALSE is on the right. In practice, either is fine as long as it is clearly defined.
    
!!! Note
    
    Internal Nodes are also sometimes referred to as **Split**, given that the tree splits at that point. They are also commonly denoted using $d$.
    
    For a tree with $n$ internal nodes, it will always have $n+1$ terminal nodes:
    
    $$
        T = d + 1
    $$

<!-- Self Made -->
![TREE_TERMINOLOGY](Assets/7.%20Tree%20Models.md/TREE_TERMINOLOGY.png){.center}

Mathematically, each region of the tree can also be expressed using a **series of conditions**. For large trees, this becomes tedious, which is why the diagram is generally preferred:

$$
\begin{aligned}
    R_{1} &= {y \mid x_{1} \lt a, x_{2} \lt b, x_{3} \lt c, \dots} \\
    R_{2} &= {y \mid x_{1} \lt a, x_{2} \lt b, x_{3} \gt c, \dots} \\
    &\dots
\end{aligned}
$$

!!! Note

    A tree with only a **single internal node** (only one split) is known as a **Stump**. It is a tree with only **one predictor**:

    <!-- Self Made -->
    ![TREE_STUMP](Assets/7.%20Tree%20Models.md/TREE_STUMP.png){.center}

## **Construction**

For the purposes of this exam, there is only one algorithm that is used to construct trees, **Recursive Binary Splitting**:

1. Start with all observations in one region (the entire sample space)
2. Consider **all possible splits** for ALL predictors (Regardless if previously used)
3. For each possibility, compute the **total RSS** from the resulting two regions
4. Select the split with the **lowest total RSS**
5. Repeat steps (2) - (4) for each resulting region

!!! Note

    The method gets its name from the following properties:

    * **Binary**: One region is always split into two; no more
    * **Recursive**: Splits rely on previously made splits

    The possible split boundaries are not arbitrarily chosen. They are always the **midpoint of between two observations**:

    * $(X_{1}, X_{2}) = {(1,6), (2, 7), (3, 8), \dots}$
    * Split $X_{1}$ at the following: ${1.5, 2.5, \dots}$
    * Split $X_{2}$ at the following: ${6.5, 7.5, \dots}$

!!! Warning

    If a categorical variable is being used for the split, the tree can split **ANY combination** of the categories:
    
    * Group 1: Category 1 & 2
    * Group 2: Category 3, 4 & 5

    However, if the categorical variable has been converted into dummy variables, then the split can only be for one category at a time:
    
    * Group 1: Category 1
    * Group 2: Category 2, 3, 4 & 5

    Thus, it is generally **NOT necessary to binarize** variables beforehand when working with decision trees, which makes decision trees preferred when working with categorical variables.

!!! Note

    Only useful variables will be used in the tree - it is possible that a variable is NOT used at all.

    If two or more USEFUL variables are highly collinear, it is possible that only one of those variables will be used, given the great similarity between the two.

In general, the algorithm can be said to choose the **split that minimizes the RSS AT THAT TIME**; it does NOT consider future splits. Similar to subset selection, it is a **greedy algorithm** as it chooses the local best choice at each step, not the global best.

!!! Tip

    Notice that since the **prediction is the average** of the observations in the node, the RSS is essentially the **mean square variance** of the observations in the node. If given the observations or the relevant summed components, the RSS can be easily calculated.
    
    The total RSS is simply the sum of the RSS for each node, **no weighting** required.
    
    $$
        \text{Total Tree RSS} = \sum \text{Node RSS}
    $$

!!! Warning

    Trees tend to prefer variables that have many possible splits (EG. Numeric variables or Categorical variables with many levels). This is because the higher possibilities increases the chance of finding a split that will decrease the RSS, even if the split is spurious.

The performance of the tree is often compared against the Null Rate - performance of a tree with **NO splits**. The idea is similar to that of an F-test.

!!! Note

    **Competitor Splits** are the next best alternative split(s) among the variables that were NOT chosen.
    
    **Surrogate Splits** are alternative splits that results in a similar division as the chosen split. It is best used when there are missing values in the chosen split, thus the surrogate split can be used to achieve the same effect.

!!! Warning

    The above methodology only works for regression trees; continuous data. For count data, the RSS should not be used.
    
    Instead, the split decision should be based on **Log-Likelihood** instead; Poisson Deviance. The resulting tree is thus known as a **Poisson Tree**.

## **Pruning**

Recursive binary splitting tends to result in **large complex trees** with many internal nodes, which is prone to **overfitting**. In this case, certain **internal nodes should be dropped**, akin to **Pruning the leaves** of a tree.

!!! Note

    Recall that the complexity of a linear regression model was proportional to the **number of predictors** it had. In a similar fashion, the **number of internal nodes** is proportional to its complexity.

The first type of pruning is known as **Pre-pruning**. The algorithm includes a **stopping condition** which **prevents the tree from becoming too complex** in the first place.

!!! Tip

    The conditions are typically chosen such that they avoid overfitting (smaller tree):
    
    * Minimum number of observations in a node (internal vs terminal node; can be different)
    * Maximum number of terminal nodes
    * Minimum reduction in RSS per split

!!! Warning

    This type of pruning could **already have been baked into the training algorithm**, which is why it is not what people think of when pruning in is mentioned.

<!-- Obtained from Research Gate -->
![PRE_PRUNING](Assets/7.%20Tree%20Models.md/PRE_PRUNING.png){.center}

The second type of pruning is known as **Post-pruning**. A **penalty** is added to the RSS:

* $\alpha$: Hyperparameter
* T: Number of terminal nodes (Leaves)

$$
    \min (\text{RSS} + \alpha \cdot T)
$$

!!! Note

    Similar to Ridge/Lasso, the RSS in the above is the POST-pruning RSS. This is important to distinguish as questions can provide the values in a format like this:
    
    1. Terminal Nodes - RSS for that node
    2. Internal Nodes - RSS for that node, IF the leaves were pruned at that point

    <!-- Obtained from Coaching Actuaries -->
    ![PRUNING_RSS](Assets/7.%20Tree%20Models.md/PRUNING_RSS.png){.center}

    As stated previously, the RSS of the tree at any point is the sum of all terminal RSS. However, if given the above, it is important to sum the correct ones (while also applying the correct amount of penalty).

For every possible $T$, there exists a **subtree that minimizes** the above function. Among the above shortlisted subtrees, the one with the **lowest penalized RSS** is chosen as the final tree.

!!! Note

    This is why this method is known as **Cost Complexity Pruning**, as it adds a cost per terminal node (cost per complexity) to the minimization problem.

    As part of this process, splits that do not significantly reduce RSS (weak splits) will be pruned. As a result, it is known as **Weakest Link** pruning.
    
The key idea is that $\alpha$ and $T$ are **inversely related**. The higher the penalty, the smaller the tree must be; hence different $\alpha$ will result in a **different final tree**.

!!! Warning

    The ideal scenario for pruning would be that *every possible subtree* is tested via Cross Validation. However, this is **computationally expensive**, which is why Cost Complexity pruning is used instead - which will always provide a **sequence of subtrees** with varying levels of $\alpha$:
    
    <!-- Obtained from Coaching Actuaries -->
    ![ALPHA_INDEX](Assets/7.%20Tree%20Models.md/ALPHA_INDEX.png){.center}

    As seen above, since $T$ is an integer while $\alpha$  is continuous, it leads to a **step like function** where a range of $\alpha$ results in the same subtree:

    <!-- Obtained from Coaching Actuaries -->
    ![alpha_RANGE](Assets/7.%20Tree%20Models.md/ALPHA_RANGE.png){.center}

!!! Tip

    When $\alpha=0$, the penalized RSS is the **same as the regular RSS** and hence will result in the original tree. Thus, the trees that minimizes the penalized RSS will **always be a subtree** of the original.

    Intuitively, this is because the tree was fitted using a **forward greedy algorithm** to begin with. Thus, going backwards should result in the same tree.

!!! Warning

    Note that converting a subtree into a leaf is NOT simply taking the average of the two original leaves, be it for the predicted value or its RSS.

Generally speaking, **Post-Pruning is preferred**. Stunting the growth of the tree early **might reduce a small amount of RSS early, but might also result in an even large RSS later on**. Thus, it is better to let the tree grow and then prune it back to the right size with a full view.

!!! Tip

    The main point of using a decision tree is that it can be illustrated with a **simple visual**. If the tree is too large, the visual becomes too complicated, undercutting the appeal of the tree.

    For this reason, pruning is important. However, if cross validation suggests that the best $\alpha$ is 0; ie that the full tree is best, then decision trees might not work well with the data.

!!! Tip

    Another scenario is the consideration of pruning a tree versus building a new tree with the same complexity parameter.
    
    The reason is similar to above - it is better to allow the tree to fully grow first as it will allow for valuable splits to appear, that might have otherwise not happened if a new tree was grown under stricter conditions.

After pruning, it is recommended to analyze the following aspects for reasonableness:

1. **Number of splits**: Is it manageable?
2. **Sequence of splits**: Does it make sense? Any interactions?
3. **Size of terminal nodes**: Any imbalances?

## **Classification Trees**

Decision Trees can also be used for **categorical predictors**, in which case it is known as a **Classification Tree**. The model assigns a category to the prediction based on the **most common** category within that region of the tree.

!!! Note

    Most of the key concepts apply, but there are some notable exceptions due to having a categorical dependent variable.

Since residuals are unintuitive for categorical data, classification trees focus on **Node Purity** instead. Purity is a measure of the **distribution/variance of the categories**, which can measured using one of the following metrics:

1. Classification Error Rate
2. Gini Index
3. Cross Entropy

!!! Tip

    Thus, it can be said that the algorithm divides the predictor space such that **node impurity is minimized**; **variability** among categories is minimized.

!!! Tip

    The lower the above metrics, the higher the purity of the node - the node is LARGELY made up of mainly one category. Mathematically, the **proportion of categories** within each region are **close to 0 or 1**.
    
    This can be remembered by the fact that the Gini Index is often a measure of **socio-economic inequality** - it is higher when there is are a large number of poor and rich (loosely speaking).

### **Purity Measures**

A key quantity that will be used is the **proportion** of each class in a particular region ($p_{c}$).

$$
    p_{c} = \frac{\text{Num Obs in Category & Region}}
            {\text{Total Num Obs in Region}}
$$

Thus, the number of misclassifications in that region ONLY is the **complement of the highest** proportion:

$$
    \text{Classification Error (Region)} = 1 - \max(p_{1}, p_{2}, \dots)
$$

!!! Tip

    Recall that the model will predict a category based on the **most frequent category** (highest $p_{c}$) in that region. Thus, any observations that are NOT actually part of that category are being **misclassified**.

Thus, the overall misclassification rate for the **entire tree** is the **weighted average** of the misclassifications for each region:

$$
    \text{Classification Error (Total)} = \sum \text{Classification Error (Region)} \cdot \frac{n_{\text{Region}}}{n_{\text{Total}}}
$$

The above concept can be applied to the Gini Index and Cross Entropy as well, thus their total formulas will not be shown:

$$
\begin{aligned}
    \text{Gini Index (Region)} &= \sum_{c} p_{c} (1 - p_{c}) \\
    \text{Cross Entropy (Region)} &= - \sum_{c} p_{c} \ln p_{c}
\end{aligned}
$$

!!! Warning

    All measures **must be positive**, but ONLY entropy can be larger than 1.

    Gini Index and Entropy are **similar**, but the key difference is that it uses the **SAME PROBABILITY** in the summation.

!!! Tip

    Gini Index and Class Entropy are the preferred purity measures when **growing** out a tree. This is because they are **sensitive to the distribution** of categories, while classification error is NOT. Consider the following two examples - the **classification error rate is the same but the purity is higher** in the second region:

    <!-- Self Made -->
    ![PURITY_DISTRIBUTION](Assets/7.%20Tree%20Models.md/PURITY_DISTRIBUTION.png){.center}

    Classification Error is a **one-dimensional and less sensitive** approach that tends to result in simpler trees, which is why is often used as the purity measure for **pruning**. This is coupled with the fact that the main goal of pruning is to improve predictive accuracy, which is directly what the classification error measures.

!!! Tip

    In light of this, it is possible for a split to produce **two nodes with the same predicted category**. This is because the **Gini or Entropy would have increased** with the split, even though classification error remains the same.

    When such a split occurs, it increases **confidence** in the data as the node is purer; we can be more sure that this split is correct:

    * Scenario 1: 55% Pure Node results in classification A
    * Scenario 2: 90% Pure Node results in classification A (More certain)

### **Two Category Model**

The most common scenario to appear on the exam would be a model with **ONLY TWO** possible categories.

In such cases, there is a clearly defined order between the purity measures:

$$
    \text{Entropy} \gt \text{Gini} \ge \text{Class Error}
$$

<!—- Obtained from Coaching Actuaries —>
![RELATIVE_PURITY](Assets/7.%20Tree%20Models.md/RELATIVE_PURITY.png){.center}

!!! Tip

    There is only one situation where Gini and Classification error are the same - when the two categories are **equally distributed**.

!!! Warning

    The above relationship only holds for the measures WITHIN a single node. It CANNOT be used to compare across nodes.

## **Ensemble Methods**

Ensemble Methods use a **collection** of statistical learning  techniques in conjunction with one another to produce better results than any of the individual methods.

Ensemble Methods are typically used with basic models known as **Weak Learners** to produce a stronger model.

!!! Tip

    The above implies that a tree on its own is **NOT as good** (in terms of predictive performance) as other statistical learning methods; likely due to overfitting.

!!! Info

    Although Ensemble methods are only discussed in the context of trees, they can be used for other methods as well.

For regression trees, the output from all the underlying trees are **averaged to form the final prediction**. For classification trees, there are two possible methods:

1. Determine the predicted class for each underlying tree then take the majority as the final prediction
2. Determine the average probability and use that as the prediction

!!! Tip

    Unless otherwise stated, assume that the first method is used.  

### **Bagging**

Bagging refers to **Bootstrap Aggregation**. Bootstrapping is a process whereby “new” samples are created based on the existing sample. Thus, Bagging refers to constructing **multiple trees using bootstrap samples and aggregating their results** to form the final prediction.

Bootstrapping uses **random sampling with replacement** to create artificial samples of the **same size** from the original sample:

* $n^{n}$ **possible samples** (Different permutations count)
* $2n-1 \choose n-1$ **distinct samples** (Different permutations NOT counted)

!!! Note

    Since every sample has an equal chance of being selected, the probability that an observation is **NOT selected for a draw** is:

    $$
        \text{Probability NOT selected for draw} = 1 - \frac{1}{n}
    $$

    Thus, the probability that an observation is **NOT selected at all across the entire sample** ($n$ draws) is:

    $$
        \text{Probability NOT selected} = \left(1 - \frac{1}{n} \right)^{n}
    $$

    As the sample size approaches infinity, the above converges to the following, showing that **about two-thirds of the observations are used**:

    $$
    \begin{aligned}
        \lim \left(1 - \frac{1}{n} \right)^{n} & \approx \frac{1}{3}
        \\
        \text{Probability selected}
        &= 1 - \frac{1}{3} \\
        &= \frac{2}{3}
    \end{aligned}
    $$

Something unique about Bagging is that since **only two-thirds** of the original observations are used for each tree, it is possible to use the **observations that were NOT drawn** as the **validation set**. The resulting (average) error across all instances is known as the **Out of Bag Error** (OOB).

<!— Obtained from ISLR —>
![OOB_ERROR](Assets/7.%20Tree%20Models.md/OOB_ERROR.png){.center}

!!! Note

    This would mean that there about $\frac{1}{3}$ OOB predictions for each observation.

!!! Tip

    The intuition is similar to cross-validation where the **OOB observations were not used to train that particular bagged tree**, thus serves as a good estimate for the test MSE.

    In fact, for a sufficiently large number of trees, the OOB provides an reasonably good **estimate of the LOOCV error**:

    1. LOOCV error requires that all but one observation is used to train the model
    2. For a large number of bootstrap samples, it increases the chances that ALL observations from the original dataset are used
    3. Thus, the OOB will calculated based on ALL observations EXCEPT the observation being tested; akin to LOOCV
   
    The OOB error also has the added advantage of being **computationally efficient** compared to an actual LOOCV, as it only needs to be **run once** (the main run) to get the result.  

!!! Warning

    OOB does NOT use every single tree in the Ensemble; it only uses those for which the specific observation was NOT used to train the tree.

The **key advantage** to bagging lies in **Variance**. An estimate coming from multiple samples is always **more stable** than estimate coming from just one sample.

!!! Tip

    Since bagging helps to reduce the variance, the underlying trees are often **NOT pruned** (pre & post) as the nature of bagging helps to counter the problem of high variance for unpruned trees.

    In general, having **more bootstrapped samples is beneficial**; having too many samples does **NOT cause overfitting**. However, it is **computationally expensive**. Thus, the number of samples should be chosen such that the increase in accuracy is higher than the cost of computing.

However, the **key disadvantage** is that the method is that is **not easily interpreted**. A single tree is easily visualized while a “forest” is not. If interpretation is required, it is not recommended to use an ensemble method.

!!! Info

    These type of methods are known as a **Black Box**, because it is not clear to the user *how* the model arrives at the output.

!!! Tip  
    
    One way to add more insight to the model is through the use of **Variable Importance Plots**:

    * **Sensitivity** analysis, **removing one variable at a time** from the model
    * AVERAGE **Change in MSE** is recorded for each variable
    * The variable whose absence results in the **greatest increase in MSE is the most important** variable

    <!— Obtained from Research Gate —>
    ![VARIABLE_IMPORTANCE](Assets/7.%20Tree%20Models.md/VARIABLE_IMPORTANCE.png){.center}

    The above is plotted based on the absolute increase or the percentage increase for that variable. It also possible to present the data as a **percentage of the most important variable** instead:

    <!— Obtained from Coaching Actuaries —>
    ![VARIABLE_IMPORTANCE_2](Assets/7.%20Tree%20Models.md/VARIABLE_IMPORTANCE_2.png){.center}
    
    However, despite the above, the plots do not ultimately describe how the variables impact the target.

### **Random Forest**

One of the primary issues with Bagging is **Correlation**. Since the bootstrapped **samples are all similar** to the underlying sample, it is likely that each bootstrapped tree will contain the **same set of predictors** (especially if there especially strong predictors).

This can be overcome by **limiting each bootstrapped tree** to be constructed only based on a **random subset of the predictors available** at EACH SPLIT, reducing the monopoly of certain predictors and hence reducing the similarities. Hence, the method is called “Random Forest”.

!!! Tip

    It can be understood that Bagging is a **special case of Random Forests** where all the predictors are allowed; bagging is a **non-random forest**.

    Note that although Bagging generally refers to the learning method, it is also a shorthand for bootstrap aggregation. Thus, some texts may say that “Bagging is used in Random Forest”.

!!! Note

    Phrased in a different way, the variable importance plot for Bagging tends to be **dominated** by the 1 or 2 most important variables. Random Forests naturally create a more balanced plot given that it is forced to use “less important” variables:

    <!— Obtained from Coaching Actuaries —>
    ![VARIABLE_IMPORTANCE_COMPARISON](Assets/7.%20Tree%20Models.md/VARIABLE_IMPORTANCE_COMPARISON.png){.center}

    Note that even though both plots show the same strongest variable as 100%, the absolute amount of change in MSE is **lower for the random forest**, as the variable is **used less in the random forest** model.

If there are $p$ available predictors and $m$ are INCLUDED, then the probability that a *particular predictor* is not chosen at a given split is:

$$
\begin{aligned}
    \text{P(Predictor not Chosen)}
    &= \frac{{p-1 \choose m} \cdot {1 \choose 0}}{{p \choose m}} \\
    &= \frac{p-m}{p}   
\end{aligned}
$$

!!! Note

    Naturally, a smaller $m$ will result in a greater de-correlation effect. However, a **sufficiently large $m$** still needs to be chosen to ensure the signal is correctly captured, thus the following values are common:

    * **Regression Trees**: $m = \frac{p}{3}$
    * **Classification Trees**: $m = \sqrt{p}$

### **Boosting**

Boosting refers to the process of **sequentially training** a series of **shallow trees** on the **error of the previous tree**. The strength of weak trees are “boosted” by **aggregating** them together to form a stronger model.

The key idea is that the errors of the trees contain the effects of the **predictors not included in the model**. Thus, training each successive model on the errors of the prior ones forces the model to **explained the unexplained**.

<!— Obtained from Research Gate —>
![BOOSTING](Assets/7.%20Tree%20Models.md/BOOSTING.png){.center}

<!— Obtained from Affine —>
![BOOSTING_VS_BAGGING](Assets/7.%20Tree%20Models.md/BOOSTING_VS_BAGGING.png){.center}

There are three hyperparameters for Boosting:

* **Number of Trees** ($B$) - How many trees should be used
* **Depth of Trees** ($d$) - How many splits each tree should have
* **Learning Rate** ($\alpha$) - How much the trees contributes to the overall prediction 

$$
\begin{aligned}
    \text{Boosted Tree Prediction} &= \alpha \cdot \text{Tree Prediction} \\
    \text{Overall Prediction} &= \sum \text{Boosted Tree Prediction}
\end{aligned}
$$

!!! Note

    The number of trees (B) is typically selected via Cross Validation.

    Shallow trees are preferred, thus $d$ typically selected to be 1. In this case, it is known as an **Additive Model**.

    The learning rate is also referred to as the **Shrinkage Parameter** as it reduces the influence of each tree on the overall prediction. Typically, a **slow learning rate** is preferred ($\alpha \lt 1$). Consequently, a **large number of trees** are required to achieve effective results.

Boosting as a method will naturally result in **low bias**. However, if an *extreme* number of trees are used, the model could become **overfitted**:

<!— Self Made —>
![BOOSTING_ERROR](Assets/7.%20Tree%20Models.md/BOOSTING_ERROR.png){.center}

!!! Tip

    Random Forests typically outperform Bagging due to de-correlation.
    
    Boosting typically outperforms the other two as it **aggressively reduces bias**, making up for any potential increase in Variance.

!!! Warning

    Some questions might cut off the graph at a certain point, omitting the potential increase in Test Error for Boosting. 
